<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 83 results for author: <span class="mathjax">Arora, A</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Arora%2C+A">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Arora, A">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Arora%2C+A&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Arora, A">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Arora%2C+A&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Arora%2C+A&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Arora%2C+A&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01649">arXiv:2205.01649</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01649">pdf</a>, <a href="https://arxiv.org/format/2205.01649">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Enriched Features for Fast Image Restoration and Enhancement
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01649v1-abstract-short" style="display: inline;">
        Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01649v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01649v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01649v1-abstract-full" style="display: none;">
        Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatial details are preserved but the contextual information cannot be precisely encoded. In the latter case, generated outputs are semantically reliable but spatially less accurate. This paper presents a new architecture with a holistic goal of maintaining spatially-precise high-resolution representations through the entire network, and receiving complementary contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing the following key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) non-local attention mechanism for capturing contextual information, and (d) attention based multi-scale feature aggregation. Our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on six real image benchmark datasets demonstrate that our method, named as MIRNet-v2 , achieves state-of-the-art results for a variety of image processing tasks, including defocus deblurring, image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNetv2
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01649v1-abstract-full').style.display = 'none'; document.getElementById('2205.01649v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This article supersedes arXiv:2003.06792. Accepted for publication in TPAMI</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.01469">arXiv:2204.01469</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.01469">pdf</a>, <a href="https://arxiv.org/format/2204.01469">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Estimating the Entropy of Linguistic Distributions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aryaman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meister%2C+C">Clara Meister</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cotterell%2C+R">Ryan Cotterell</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.01469v2-abstract-short" style="display: inline;">
        Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropy must typically be estimated from observed data because researchers do not have access to the underlying probability distribution that gives rise to these data. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive explor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01469v2-abstract-full').style.display = 'inline'; document.getElementById('2204.01469v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.01469v2-abstract-full" style="display: none;">
        Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropy must typically be estimated from observed data because researchers do not have access to the underlying probability distribution that gives rise to these data. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. Finally, we end our paper with concrete recommendations for entropy estimation depending on distribution type and data availability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01469v2-abstract-full').style.display = 'none'; document.getElementById('2204.01469v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">21 pages (5 pages main text). 4 figures. Accepted to ACL 2022</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          94A17 (Primary) 62B10 (Secondary)
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7; E.4
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13722">arXiv:2203.13722</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13722">pdf</a>, <a href="https://arxiv.org/format/2203.13722">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Probing Pre-Trained Language Models for Cross-Cultural Differences in Values
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Arnav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaffee%2C+L">Lucie-Aimée Kaffee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Augenstein%2C+I">Isabelle Augenstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13722v1-abstract-short" style="display: inline;">
        Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13722v1-abstract-full').style.display = 'inline'; document.getElementById('2203.13722v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13722v1-abstract-full" style="display: none;">
        Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13722v1-abstract-full').style.display = 'none'; document.getElementById('2203.13722v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.12524">arXiv:2203.12524</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.12524">pdf</a>, <a href="https://arxiv.org/format/2203.12524">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Computational historical linguistics and language diversity in South Asia
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aryaman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Farris%2C+A">Adam Farris</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Basu%2C+S">Samopriya Basu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kolichala%2C+S">Suresh Kolichala</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.12524v1-abstract-short" style="display: inline;">
        South Asia is home to a plethora of languages, many of which severely lack access to new language technologies. This linguistic diversity also results in a research environment conducive to the study of comparative, contact, and historical linguistics -- fields which necessitate the gathering of extensive data from many languages. We claim that data scatteredness (rather than scarcity) is the prim&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12524v1-abstract-full').style.display = 'inline'; document.getElementById('2203.12524v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.12524v1-abstract-full" style="display: none;">
        South Asia is home to a plethora of languages, many of which severely lack access to new language technologies. This linguistic diversity also results in a research environment conducive to the study of comparative, contact, and historical linguistics -- fields which necessitate the gathering of extensive data from many languages. We claim that data scatteredness (rather than scarcity) is the primary obstacle in the development of South Asian language technology, and suggest that the study of language history is uniquely aligned with surmounting this obstacle. We review recent developments in and at the intersection of South Asian NLP and historical-comparative linguistics, describing our and others&#39; current efforts in this area. We also offer new strategies towards breaking the data barrier.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12524v1-abstract-full').style.display = 'none'; document.getElementById('2203.12524v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages; accepted to ACL 2022 Theme Track</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.12521">arXiv:2203.12521</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.12521">pdf</a>, <a href="https://arxiv.org/format/2203.12521">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CoMeFa: Compute-in-Memory Blocks for FPGAs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anand%2C+T">Tanmay Anand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Borda%2C+A">Aatman Borda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sehgal%2C+R">Rishabh Sehgal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hanindhito%2C+B">Bagus Hanindhito</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kulkarni%2C+J">Jaydeep Kulkarni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=John%2C+L+K">Lizy K. John</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.12521v1-abstract-short" style="display: inline;">
        Block RAMs (BRAMs) are the storage houses of FPGAs, providing extensive on-chip memory bandwidth to the compute units implemented using Logic Blocks (LBs) and Digital Signal Processing (DSP) slices. We propose modifying BRAMs to convert them to CoMeFa (Compute-In-Memory Blocks for FPGAs) RAMs. These RAMs provide highly-parallel compute-in-memory by combining computation and storage capabilities in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12521v1-abstract-full').style.display = 'inline'; document.getElementById('2203.12521v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.12521v1-abstract-full" style="display: none;">
        Block RAMs (BRAMs) are the storage houses of FPGAs, providing extensive on-chip memory bandwidth to the compute units implemented using Logic Blocks (LBs) and Digital Signal Processing (DSP) slices. We propose modifying BRAMs to convert them to CoMeFa (Compute-In-Memory Blocks for FPGAs) RAMs. These RAMs provide highly-parallel compute-in-memory by combining computation and storage capabilities in one block. CoMeFa RAMs utilize the true dual port nature of FPGA BRAMs and contain multiple programmable single-bit bit-serial processing elements. CoMeFa RAMs can be used to compute in any precision, which is extremely important for evolving applications like Deep Learning. Adding CoMeFa RAMs to FPGAs significantly increases their compute density. We explore and propose two architectures of these RAMs: CoMeFa-D (optimized for delay) and CoMeFa-A (optimized for area). Compared to existing proposals, CoMeFa RAMs do not require changing the underlying SRAM technology like simultaneously activating multiple rows on the same port, and are practical to implement. CoMeFa RAMs are versatile blocks that find applications in numerous diverse parallel applications like Deep Learning, signal processing, databases, etc. By augmenting an Intel Arria-10-like FPGA with CoMeFa-D (CoMeFa-A) RAMs at the cost of 3.8% (1.2%) area, and with algorithmic improvements and efficient mapping, we observe a geomean speedup of 2.5x (1.8x), across several representative benchmarks. Replacing all or some BRAMs with CoMeFa RAMs in FPGAs can make them better accelerators of modern compute-intensive workloads.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12521v1-abstract-full').style.display = 'none'; document.getElementById('2203.12521v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 12 figures, 4 tables, FCCM conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10425">arXiv:2203.10425</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10425">pdf</a>, <a href="https://arxiv.org/format/2203.10425">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Study on Robustness to Perturbations for Representations of Environmental Sound
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Srivastava%2C+S">Sangeeta Srivastava</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Ho-Hsiang Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rulff%2C+J">Joao Rulff</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fuentes%2C+M">Magdalena Fuentes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cartwright%2C+M">Mark Cartwright</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Silva%2C+C">Claudio Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Anish Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bello%2C+J+P">Juan Pablo Bello</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10425v2-abstract-short" style="display: inline;">
        Audio applications involving environmental sound analysis increasingly use general-purpose audio representations, also known as embeddings, for transfer learning. Recently, Holistic Evaluation of Audio Representations (HEAR) evaluated twenty-nine embedding models on nineteen diverse tasks. However, the evaluation&#39;s effectiveness depends on the variation already captured within a given dataset. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10425v2-abstract-full').style.display = 'inline'; document.getElementById('2203.10425v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10425v2-abstract-full" style="display: none;">
        Audio applications involving environmental sound analysis increasingly use general-purpose audio representations, also known as embeddings, for transfer learning. Recently, Holistic Evaluation of Audio Representations (HEAR) evaluated twenty-nine embedding models on nineteen diverse tasks. However, the evaluation&#39;s effectiveness depends on the variation already captured within a given dataset. Therefore, for a given data domain, it is unclear how the representations would be affected by the variations caused by myriad microphones&#39; range and acoustic conditions -- commonly known as channel effects. We aim to extend HEAR to evaluate invariance to channel effects in this work. To accomplish this, we imitate channel effects by injecting perturbations to the audio signal and measure the shift in the new (perturbed) embeddings with three distance measures, making the evaluation domain-dependent but not task-dependent. Combined with the downstream performance, it helps us make a more informed prediction of how robust the embeddings are to the channel effects. We evaluate two embeddings -- YAMNet, and OpenL$^3$ on monophonic (UrbanSound8K) and polyphonic (SONYC UST) datasets. We show that one distance measure does not suffice in such task-independent evaluation. Although Fréchet Audio Distance (FAD) correlates with the trend of the performance drop in the downstream task most accurately, we show that we need to study this in conjunction with the other distances to get a clear understanding of the overall effect of the perturbation. In terms of the embedding performance, we find OpenL$^3$ to be more robust to YAMNet, which aligns with the HEAR evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10425v2-abstract-full').style.display = 'none'; document.getElementById('2203.10425v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10135">arXiv:2203.10135</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10135">pdf</a>, <a href="https://arxiv.org/format/2203.10135">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Compressed Embeddings for On-Device Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pansare%2C+N">Niketan Pansare</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Katukuri%2C+J">Jay Katukuri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cipollone%2C+F">Frank Cipollone</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shaik%2C+R">Riyaaz Shaik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tokgozoglu%2C+N">Noyan Tokgozoglu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Venkataraman%2C+C">Chandru Venkataraman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10135v1-abstract-short" style="display: inline;">
        In deep learning, embeddings are widely used to represent categorical entities such as words, apps, and movies. An embedding layer maps each entity to a unique vector, causing the layer&#39;s memory requirement to be proportional to the number of entities. In the recommendation domain, a given category can have hundreds of thousands of entities, and its embedding layer can take gigabytes of memory. Th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10135v1-abstract-full').style.display = 'inline'; document.getElementById('2203.10135v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10135v1-abstract-full" style="display: none;">
        In deep learning, embeddings are widely used to represent categorical entities such as words, apps, and movies. An embedding layer maps each entity to a unique vector, causing the layer&#39;s memory requirement to be proportional to the number of entities. In the recommendation domain, a given category can have hundreds of thousands of entities, and its embedding layer can take gigabytes of memory. The scale of these networks makes them difficult to deploy in resource constrained environments. In this paper, we propose a novel approach for reducing the size of an embedding table while still mapping each entity to its own unique embedding. Rather than maintaining the full embedding table, we construct each entity&#39;s embedding &#34;on the fly&#34; using two separate embedding tables. The first table employs hashing to force multiple entities to share an embedding. The second table contains one trainable weight per entity, allowing the model to distinguish between entities sharing the same embedding. Since these two tables are trained jointly, the network is able to learn a unique embedding per entity, helping it maintain a discriminative capability similar to a model with an uncompressed embedding table. We call this approach MEmCom (Multi-Embedding Compression). We compare with state-of-the-art model compression techniques for multiple problem classes including classification and ranking. On four popular recommender system datasets, MEmCom had a 4% relative loss in nDCG while compressing the input embedding sizes of our recommendation models by 16x, 4x, 12x, and 40x. MEmCom outperforms the state-of-the-art techniques, which achieved 16%, 6%, 10%, and 8% relative loss in nDCG at the respective compression ratios. Additionally, MEmCom is able to compress the RankNet ranking model by 32x on a dataset with millions of users&#39; interactions with games while incurring only a 1% relative loss in nDCG.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10135v1-abstract-full').style.display = 'none'; document.getElementById('2203.10135v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.08280">arXiv:2203.08280</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.08280">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data Transfer and Network Services management for Domain Science Workflows
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lehman%2C+T">Tom Lehman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+X">Xi Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guok%2C+C">Chin Guok</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wuerthwein%2C+F">Frank Wuerthwein</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sfiligoi%2C+I">Igor Sfiligoi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Graham%2C+J">John Graham</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aashay Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mishin%2C+D">Dima Mishin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Davila%2C+D">Diego Davila</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guiang%2C+J">Jonathan Guiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hutton%2C+T">Tom Hutton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Newman%2C+H">Harvey Newman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Balcas%2C+J">Justas Balcas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.08280v2-abstract-short" style="display: inline;">
        This paper describes a vision and work in progress to elevate network resources and data transfer management to the same level as compute and storage in the context of services access, scheduling, life cycle management, and orchestration. While domain science workflows often include active compute resource allocation and management, the data transfers and associated network resource coordination i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.08280v2-abstract-full').style.display = 'inline'; document.getElementById('2203.08280v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.08280v2-abstract-full" style="display: none;">
        This paper describes a vision and work in progress to elevate network resources and data transfer management to the same level as compute and storage in the context of services access, scheduling, life cycle management, and orchestration. While domain science workflows often include active compute resource allocation and management, the data transfers and associated network resource coordination is not handled in a similar manner. As a result data transfers can introduce a degree of uncertainty in workflow operations, and the associated lack of network information does not allow for either the workflow operations or the network use to be optimized. The net result is that domain science workflow processes are forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. Developing mechanisms to allow an application workflow to obtain information regarding the network services, capabilities, and options, to a degree similar to what is possible for compute resources is the primary motivation for this work. The initial focus is on the Open Science Grid (OSG)/Compact Muon Solenoid (CMS) Large Hadron Collider (LHC) workflows with Rucio/FTS/XRootD based data transfers and the interoperation with the ESnet SENSE (Software-Defined Network for End-to-end Networked Science at the Exascale) system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.08280v2-abstract-full').style.display = 'none'; document.getElementById('2203.08280v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">contribution to Snowmass 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.06220">arXiv:2203.06220</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.06220">pdf</a>, <a href="https://arxiv.org/format/2203.06220">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Infrastructure-free, Deep Learned Urban Noise Monitoring at $\sim$100mW
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yun%2C+J">Jihoon Yun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Srivastava%2C+S">Sangeeta Srivastava</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roy%2C+D">Dhrubojyoti Roy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stohs%2C+N">Nathan Stohs</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mydlarz%2C+C">Charlie Mydlarz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Salman%2C+M">Mahin Salman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Steers%2C+B">Bea Steers</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bello%2C+J+P">Juan Pablo Bello</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Anish Arora</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.06220v1-abstract-short" style="display: inline;">
        The Sounds of New York City (SONYC) wireless sensor network (WSN) has been fielded in Manhattan and Brooklyn over the past five years, as part of a larger human-in-the-loop cyber-physical control system for monitoring, analyzing, and mitigating urban noise pollution. We describe the evolution of the 2-tier SONYC WSN from an acoustic data collection fabric into a 3-tier in situ noise complaint moni&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.06220v1-abstract-full').style.display = 'inline'; document.getElementById('2203.06220v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.06220v1-abstract-full" style="display: none;">
        The Sounds of New York City (SONYC) wireless sensor network (WSN) has been fielded in Manhattan and Brooklyn over the past five years, as part of a larger human-in-the-loop cyber-physical control system for monitoring, analyzing, and mitigating urban noise pollution. We describe the evolution of the 2-tier SONYC WSN from an acoustic data collection fabric into a 3-tier in situ noise complaint monitoring WSN, and its current evaluation. The added tier consists of long-range (LoRa), multi-hop networks of a new low-power acoustic mote, MKII (&#34;Mach 2&#34;), that we have designed and fabricated. MKII motes are notable in three ways: First, they advance machine learning capability at mote-scale in this application domain by introducing a real-time Convolutional Neural Network (CNN) based embedding model that is competitive with alternatives while also requiring 10$\times$ lesser training data and $\sim$2 orders of magnitude fewer runtime resources. Second, they are conveniently deployed relatively far from higher-tier base station nodes without assuming power or network infrastructure support at operationally relevant sites (such as construction zones), yielding a relatively low-cost solution. And third, their networking is frequency agile, unlike conventional LoRa networks: it tolerates in a distributed, self-stabilizing way the variable external interference and link fading in the cluttered 902-928MHz ISM band urban environment by dynamically choosing good frequencies using an efficient new method that combines passive and active measurements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.06220v1-abstract-full').style.display = 'none'; document.getElementById('2203.06220v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in ICCPS 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.01479">arXiv:2203.01479</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.01479">pdf</a>, <a href="https://arxiv.org/format/2203.01479">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Weightless Neural Networks for Efficient Edge Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Susskind%2C+Z">Zachary Susskind</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Miranda%2C+I+D+D+S">Igor Dantas Dos Santos Miranda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Villon%2C+L+A+Q">Luis Armando Quintanilla Villon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Katopodis%2C+R+F">Rafael Fontella Katopodis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Araujo%2C+L+S">Leandro Santiago de Araujo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dutra%2C+D+L+C">Diego Leonel Cadette Dutra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lima%2C+P+M+V">Priscila Machado Vieira Lima</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Franca%2C+F+M+G">Felipe Maia Galvao Franca</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Breternitz%2C+M">Mauricio Breternitz Jr.</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=John%2C+L+K">Lizy K. John</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.01479v1-abstract-short" style="display: inline;">
        Weightless Neural Networks (WNNs) are a class of machine learning model which use table lookups to perform inference. This is in contrast with Deep Neural Networks (DNNs), which use multiply-accumulate operations. State-of-the-art WNN architectures have a fraction of the implementation cost of DNNs, but still lag behind them on accuracy for common image recognition tasks. Additionally, many existi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01479v1-abstract-full').style.display = 'inline'; document.getElementById('2203.01479v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.01479v1-abstract-full" style="display: none;">
        Weightless Neural Networks (WNNs) are a class of machine learning model which use table lookups to perform inference. This is in contrast with Deep Neural Networks (DNNs), which use multiply-accumulate operations. State-of-the-art WNN architectures have a fraction of the implementation cost of DNNs, but still lag behind them on accuracy for common image recognition tasks. Additionally, many existing WNN architectures suffer from high memory requirements. In this paper, we propose a novel WNN architecture, BTHOWeN, with key algorithmic and architectural improvements over prior work, namely counting Bloom filters, hardware-friendly hashing, and Gaussian-based nonlinear thermometer encodings to improve model accuracy and reduce area and energy consumption. BTHOWeN targets the large and growing edge computing sector by providing superior latency and energy efficiency to comparable quantized DNNs. Compared to state-of-the-art WNNs across nine classification datasets, BTHOWeN on average reduces error by more than than 40% and model size by more than 50%. We then demonstrate the viability of the BTHOWeN architecture by presenting an FPGA-based accelerator, and compare its latency and resource usage against similarly accurate quantized DNN accelerators, including Multi-Layer Perceptron (MLP) and convolutional models. The proposed BTHOWeN models consume almost 80% less energy than the MLP models, with nearly 85% reduction in latency. In our quest for efficient ML on the edge, WNNs are clearly deserving of additional attention.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01479v1-abstract-full').style.display = 'none'; document.getElementById('2203.01479v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.05994">arXiv:2202.05994</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.05994">pdf</a>, <a href="https://arxiv.org/format/2202.05994">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Physics-Guided Problem Decomposition for Scaling Deep Learning of High-dimensional Eigen-Solvers: The Case of Schrödinger&#39;s Equation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Srivastava%2C+S">Sangeeta Srivastava</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Olin%2C+S">Samuel Olin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Podolskiy%2C+V">Viktor Podolskiy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Karpatne%2C+A">Anuj Karpatne</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+W">Wei-Cheng Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Anish Arora</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.05994v2-abstract-short" style="display: inline;">
        Given their ability to effectively learn non-linear mappings and perform fast inference, deep neural networks (NNs) have been proposed as a viable alternative to traditional simulation-driven approaches for solving high-dimensional eigenvalue equations (HDEs), which are the foundation for many scientific applications. Unfortunately, for the learned models in these scientific applications to achiev&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.05994v2-abstract-full').style.display = 'inline'; document.getElementById('2202.05994v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.05994v2-abstract-full" style="display: none;">
        Given their ability to effectively learn non-linear mappings and perform fast inference, deep neural networks (NNs) have been proposed as a viable alternative to traditional simulation-driven approaches for solving high-dimensional eigenvalue equations (HDEs), which are the foundation for many scientific applications. Unfortunately, for the learned models in these scientific applications to achieve generalization, a large, diverse, and preferably annotated dataset is typically needed and is computationally expensive to obtain. Furthermore, the learned models tend to be memory- and compute-intensive primarily due to the size of the output layer. While generalization, especially extrapolation, with scarce data has been attempted by imposing physical constraints in the form of physics loss, the problem of model scalability has remained.
  In this paper, we alleviate the compute bottleneck in the output layer by using physics knowledge to decompose the complex regression task of predicting the high-dimensional eigenvectors into multiple simpler sub-tasks, each of which are learned by a simple &#34;expert&#34; network. We call the resulting architecture of specialized experts Physics-Guided Mixture-of-Experts (PG-MoE). We demonstrate the efficacy of such physics-guided problem decomposition for the case of the Schrödinger&#39;s Equation in Quantum Mechanics. Our proposed PG-MoE model predicts the ground-state solution, i.e., the eigenvector that corresponds to the smallest possible eigenvalue. The model is 150x smaller than the network trained to learn the complex task while being competitive in generalization. To improve the generalization of the PG-MoE, we also employ a physics-guided loss function based on variational energy, which by quantum mechanics principles is minimized iff the output is the ground-state solution.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.05994v2-abstract-full').style.display = 'none'; document.getElementById('2202.05994v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, Submitted to SIGKDD in Feb 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.02815">arXiv:2202.02815</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.02815">pdf</a>, <a href="https://arxiv.org/ps/2202.02815">ps</a>, <a href="https://arxiv.org/format/2202.02815">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Sparse Graphs via Majorization-Minimization for Smooth Node Signals
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fatima%2C+G">Ghania Fatima</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aakash Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Babu%2C+P">Prabhu Babu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stoica%2C+P">Petre Stoica</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.02815v1-abstract-short" style="display: inline;">
        In this letter, we propose an algorithm for learning a sparse weighted graph by estimating its adjacency matrix under the assumption that the observed signals vary smoothly over the nodes of the graph. The proposed algorithm is based on the principle of majorization-minimization (MM), wherein we first obtain a tight surrogate function for the graph learning objective and then solve the resultant s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.02815v1-abstract-full').style.display = 'inline'; document.getElementById('2202.02815v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.02815v1-abstract-full" style="display: none;">
        In this letter, we propose an algorithm for learning a sparse weighted graph by estimating its adjacency matrix under the assumption that the observed signals vary smoothly over the nodes of the graph. The proposed algorithm is based on the principle of majorization-minimization (MM), wherein we first obtain a tight surrogate function for the graph learning objective and then solve the resultant surrogate problem which has a simple closed form solution. The proposed algorithm does not require tuning of any hyperparameter and it has the desirable feature of eliminating the inactive variables in the course of the iterations - which can help speeding up the algorithm. The numerical simulations conducted using both synthetic and real world (brain-network) data show that the proposed algorithm converges faster, in terms of the average number of iterations, than several existing methods in the literature.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.02815v1-abstract-full').style.display = 'none'; document.getElementById('2202.02815v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.09202">arXiv:2201.09202</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.09202">pdf</a>, <a href="https://arxiv.org/format/2201.09202">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/BigData.2018.8622257">10.1109/BigData.2018.8622257 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        One-Shot Learning on Attributed Sequences
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuang%2C+Z">Zhongfang Zhuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kong%2C+X">Xiangnan Kong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rundensteiner%2C+E">Elke Rundensteiner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zouaoui%2C+J">Jihane Zouaoui</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.09202v1-abstract-short" style="display: inline;">
        One-shot learning has become an important research topic in the last decade with many real-world applications. The goal of one-shot learning is to classify unlabeled instances when there is only one labeled example per class. Conventional problem setting of one-shot learning mainly focuses on the data that is already in feature space (such as images). However, the data instances in real-world appl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.09202v1-abstract-full').style.display = 'inline'; document.getElementById('2201.09202v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.09202v1-abstract-full" style="display: none;">
        One-shot learning has become an important research topic in the last decade with many real-world applications. The goal of one-shot learning is to classify unlabeled instances when there is only one labeled example per class. Conventional problem setting of one-shot learning mainly focuses on the data that is already in feature space (such as images). However, the data instances in real-world applications are often more complex and feature vectors may not be available. In this paper, we study the problem of one-shot learning on attributed sequences, where each instance is composed of a set of attributes (e.g., user profile) and a sequence of categorical items (e.g., clickstream). This problem is important for a variety of real-world applications ranging from fraud prevention to network intrusion detection. This problem is more challenging than conventional one-shot learning since there are dependencies between attributes and sequences. We design a deep learning framework OLAS to tackle this problem. The proposed OLAS utilizes a twin network to generalize the features from pairwise attributed sequence examples. Empirical results on real-world datasets demonstrate the proposed OLAS can outperform the state-of-the-art methods under a rich variety of parameter settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.09202v1-abstract-full').style.display = 'none'; document.getElementById('2201.09202v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.01904">arXiv:2201.01904</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.01904">pdf</a>, <a href="https://arxiv.org/format/2201.01904">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Complexity">cs.CC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Oracle separations of hybrid quantum-classical circuits
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A+S">Atul Singh Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gheorghiu%2C+A">Alexandru Gheorghiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Singh%2C+U">Uttam Singh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.01904v1-abstract-short" style="display: inline;">
        An important theoretical problem in the study of quantum computation, that is also practically relevant in the context of near-term quantum devices, is to understand the computational power of hybrid models, that combine poly-time classical computation with short-depth quantum computation. Here, we consider two such models: CQ_d which captures the scenario of a polynomial-time classical algorithm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.01904v1-abstract-full').style.display = 'inline'; document.getElementById('2201.01904v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.01904v1-abstract-full" style="display: none;">
        An important theoretical problem in the study of quantum computation, that is also practically relevant in the context of near-term quantum devices, is to understand the computational power of hybrid models, that combine poly-time classical computation with short-depth quantum computation. Here, we consider two such models: CQ_d which captures the scenario of a polynomial-time classical algorithm that queries a d-depth quantum computer many times; and QC_d which is more analogous to measurement-based quantum computation and captures the scenario of a d-depth quantum computer with the ability to change the sequence of gates being applied depending on measurement outcomes processed by a classical computation. Chia, Chung &amp; Lai (STOC 2020) and Coudron &amp; Menda (STOC 2020) showed that these models (with d=log^O(1) (n)) are strictly weaker than BQP (the class of problems solvable by poly-time quantum computation), relative to an oracle, disproving a conjecture of Jozsa in the relativised world. We show that, despite the similarities between CQ_d and QC_d, the two models are incomparable, i.e. CQ_d $\nsubseteq$ QC_d and QC_d $\nsubseteq$ CQ_d relative to an oracle. In other words, there exist problems that one model can solve but not the other and vice versa. We do this by considering new oracle problems that capture the distinctions between the two models and by introducing the notion of an intrinsically stochastic oracle, an oracle whose responses are inherently randomised, which is used for our second result. While we leave showing the second separation relative to a standard oracle as an open problem, we believe the notion of stochastic oracles could be of independent interest for studying complexity classes which have resisted separation in the standard oracle model. Our constructions also yield simpler oracle separations between the hybrid models and BQP, compared to earlier works.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.01904v1-abstract-full').style.display = 'none'; document.getElementById('2201.01904v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">47 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.00812">arXiv:2201.00812</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.00812">pdf</a>, <a href="https://arxiv.org/format/2201.00812">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Digital Libraries">cs.DL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3488560.3498496">10.1145/3488560.3498496 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Wikipedia Reader Navigation: When Synthetic Data Is Enough
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Akhil Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gerlach%2C+M">Martin Gerlach</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Piccardi%2C+T">Tiziano Piccardi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garc%C3%ADa-Dur%C3%A1n%2C+A">Alberto García-Durán</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=West%2C+R">Robert West</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.00812v2-abstract-short" style="display: inline;">
        Every day millions of people read Wikipedia. When navigating the vast space of available topics using hyperlinks, readers describe trajectories on the article network. Understanding these navigation patterns is crucial to better serve readers&#39; needs and address structural biases and knowledge gaps. However, systematic studies of navigation on Wikipedia are hindered by a lack of publicly available&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.00812v2-abstract-full').style.display = 'inline'; document.getElementById('2201.00812v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.00812v2-abstract-full" style="display: none;">
        Every day millions of people read Wikipedia. When navigating the vast space of available topics using hyperlinks, readers describe trajectories on the article network. Understanding these navigation patterns is crucial to better serve readers&#39; needs and address structural biases and knowledge gaps. However, systematic studies of navigation on Wikipedia are hindered by a lack of publicly available data due to the commitment to protect readers&#39; privacy by not storing or sharing potentially sensitive data. In this paper, we ask: How well can Wikipedia readers&#39; navigation be approximated by using publicly available resources, most notably the Wikipedia clickstream data? We systematically quantify the differences between real navigation sequences and synthetic sequences generated from the clickstream data, in 6 analyses across 8 Wikipedia language versions. Overall, we find that the differences between real and synthetic sequences are statistically significant, but with small effect sizes, often well below 10%. This constitutes quantitative evidence for the utility of the Wikipedia clickstream data as a public resource: clickstream data can closely capture reader navigation on Wikipedia and provides a sufficient approximation for most practical downstream applications relying on reader data. More broadly, this study provides an example for how clickstream-like data can generally enable research on user navigation on online platforms while protecting users&#39; privacy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.00812v2-abstract-full').style.display = 'none'; document.getElementById('2201.00812v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 January, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">WSDM 2022, 11 pages, 16 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.11848">arXiv:2112.11848</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.11848">pdf</a>, <a href="https://arxiv.org/format/2112.11848">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Large-Scale Characterization of How Readers Browse Wikipedia
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Piccardi%2C+T">Tiziano Piccardi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gerlach%2C+M">Martin Gerlach</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Akhil Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=West%2C+R">Robert West</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.11848v2-abstract-short" style="display: inline;">
        Despite the importance and pervasiveness of Wikipedia as one of the largest platforms for open knowledge, surprisingly little is known about how people navigate its content when seeking information. To bridge this gap, we present the first systematic large-scale analysis of how readers browse Wikipedia. Using billions of page requests from Wikipedia&#39;s server logs, we measure how readers reach arti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.11848v2-abstract-full').style.display = 'inline'; document.getElementById('2112.11848v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.11848v2-abstract-full" style="display: none;">
        Despite the importance and pervasiveness of Wikipedia as one of the largest platforms for open knowledge, surprisingly little is known about how people navigate its content when seeking information. To bridge this gap, we present the first systematic large-scale analysis of how readers browse Wikipedia. Using billions of page requests from Wikipedia&#39;s server logs, we measure how readers reach articles, how they transition between articles, and how these patterns combine into more complex navigation paths. We find that navigation behavior is characterized by highly diverse structures. Although most navigation paths are shallow, comprising a single pageload, there is much variety, and the depth and shape of paths vary systematically with topic, device type, and time of day. We show that Wikipedia navigation paths commonly mesh with external pages as part of a larger online ecosystem, and we describe how naturally occurring navigation paths are distinct from targeted navigation in lab-based settings. Our results further suggest that navigation is abandoned when readers reach low-quality pages. Our work provides a conceptual and methodological framework for analyzing reader navigation patterns, which enables a more systematic understanding of readers&#39; information needs and allows for improving their experience on Wikipedia as well as the Web more generally.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.11848v2-abstract-full').style.display = 'none'; document.getElementById('2112.11848v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Single column - second revision</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.11716">arXiv:2112.11716</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.11716">pdf</a>, <a href="https://arxiv.org/format/2112.11716">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comparing radiologists&#39; gaze and saliency maps generated by interpretability methods for chest x-rays
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lanfredi%2C+R+B">Ricardo Bigolin Lanfredi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Ambuj Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Drew%2C+T">Trafton Drew</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schroeder%2C+J+D">Joyce D. Schroeder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tasdizen%2C+T">Tolga Tasdizen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.11716v1-abstract-short" style="display: inline;">
        The interpretability of medical image analysis models is considered a key research field. We use a dataset of eye-tracking data from five radiologists to compare the outputs of interpretability methods against the heatmaps representing where radiologists looked. We conduct a class-independent analysis of the saliency maps generated by two methods selected from the literature: Grad-CAM and attentio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.11716v1-abstract-full').style.display = 'inline'; document.getElementById('2112.11716v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.11716v1-abstract-full" style="display: none;">
        The interpretability of medical image analysis models is considered a key research field. We use a dataset of eye-tracking data from five radiologists to compare the outputs of interpretability methods against the heatmaps representing where radiologists looked. We conduct a class-independent analysis of the saliency maps generated by two methods selected from the literature: Grad-CAM and attention maps from an attention-gated model. For the comparison, we use shuffled metrics, which avoid biases from fixation locations. We achieve scores comparable to an interobserver baseline in one shuffled metric, highlighting the potential of saliency maps from Grad-CAM to mimic a radiologist&#39;s attention over an image. We also divide the dataset into subsets to evaluate in which cases similarities are higher.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.11716v1-abstract-full').style.display = 'none'; document.getElementById('2112.11716v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.12783">arXiv:2111.12783</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.12783">pdf</a>, <a href="https://arxiv.org/format/2111.12783">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        For the Purpose of Curry: A UD Treebank for Ashokan Prakrit
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Farris%2C+A">Adam Farris</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aryaman Arora</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.12783v2-abstract-short" style="display: inline;">
        We present the first linguistically annotated treebank of Ashokan Prakrit, an early Middle Indo-Aryan dialect continuum attested through Emperor Ashoka Maurya&#39;s 3rd century BCE rock and pillar edicts. For annotation, we used the multilingual Universal Dependencies (UD) formalism, following recent UD work on Sanskrit and other Indo-Aryan languages. We touch on some interesting linguistic features t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12783v2-abstract-full').style.display = 'inline'; document.getElementById('2111.12783v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.12783v2-abstract-full" style="display: none;">
        We present the first linguistically annotated treebank of Ashokan Prakrit, an early Middle Indo-Aryan dialect continuum attested through Emperor Ashoka Maurya&#39;s 3rd century BCE rock and pillar edicts. For annotation, we used the multilingual Universal Dependencies (UD) formalism, following recent UD work on Sanskrit and other Indo-Aryan languages. We touch on some interesting linguistic features that posed issues in annotation: regnal names and other nominal compounds, &#34;proto-ergative&#34; participial constructions, and possible grammaticalizations evidenced by sandhi (phonological assimilation across morpheme boundaries). Eventually, we plan for a complete annotation of all attested Ashokan texts, towards the larger goals of improving UD coverage of different diachronic stages of Indo-Aryan and studying language change in Indo-Aryan using computational methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12783v2-abstract-full').style.display = 'none'; document.getElementById('2111.12783v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To be presented at Universal Dependencies Workshop 2021 (UDW 2021)</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T50
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.09881">arXiv:2111.09881</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.09881">pdf</a>, <a href="https://arxiv.org/format/2111.09881">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Restormer: Efficient Transformer for High-Resolution Image Restoration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.09881v2-abstract-short" style="display: inline;">
        Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.09881v2-abstract-full').style.display = 'inline'; document.getElementById('2111.09881v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.09881v2-abstract-full" style="display: none;">
        Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.09881v2-abstract-full').style.display = 'none'; document.getElementById('2111.09881v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CVPR 2022. #CVPR2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.12243">arXiv:2110.12243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.12243">pdf</a>, <a href="https://arxiv.org/format/2110.12243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PASTRIE: A Corpus of Prepositions Annotated with Supersense Tags in Reddit International English
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kranzlein%2C+M">Michael Kranzlein</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Manning%2C+E">Emma Manning</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+S">Siyao Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wein%2C+S">Shira Wein</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aryaman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Salen%2C+B">Bradford Salen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schneider%2C+N">Nathan Schneider</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.12243v1-abstract-short" style="display: inline;">
        We present the Prepositions Annotated with Supersense Tags in Reddit International English (&#34;PASTRIE&#34;) corpus, a new dataset containing manually annotated preposition supersenses of English data from presumed speakers of four L1s: English, French, German, and Spanish. The annotations are comprehensive, covering all preposition types and tokens in the sample. Along with the corpus, we provide analy&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.12243v1-abstract-full').style.display = 'inline'; document.getElementById('2110.12243v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.12243v1-abstract-full" style="display: none;">
        We present the Prepositions Annotated with Supersense Tags in Reddit International English (&#34;PASTRIE&#34;) corpus, a new dataset containing manually annotated preposition supersenses of English data from presumed speakers of four L1s: English, French, German, and Spanish. The annotations are comprehensive, covering all preposition types and tokens in the sample. Along with the corpus, we provide analysis of distributional patterns across the included L1s and a discussion of the influence of L1s on L2 preposition choice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.12243v1-abstract-full').style.display = 'none'; document.getElementById('2110.12243v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Expanded from the version published at the Linguistic Annotation Workshop 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.05376">arXiv:2110.05376</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.05376">pdf</a>, <a href="https://arxiv.org/format/2110.05376">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Suyoun Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Le%2C+D">Duc Le</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+W">Weiyi Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Singh%2C+T">Tarun Singh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Abhinav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhai%2C+X">Xiaoyu Zhai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fuegen%2C+C">Christian Fuegen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kalinli%2C+O">Ozlem Kalinli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seltzer%2C+M+L">Michael L. Seltzer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.05376v1-abstract-short" style="display: inline;">
        Measuring automatic speech recognition (ASR) system quality is critical for creating user-satisfying voice-driven applications. Word Error Rate (WER) has been traditionally used to evaluate ASR system quality; however, it sometimes correlates poorly with user perception of transcription quality. This is because WER weighs every word equally and does not consider semantic correctness which has a hi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.05376v1-abstract-full').style.display = 'inline'; document.getElementById('2110.05376v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.05376v1-abstract-full" style="display: none;">
        Measuring automatic speech recognition (ASR) system quality is critical for creating user-satisfying voice-driven applications. Word Error Rate (WER) has been traditionally used to evaluate ASR system quality; however, it sometimes correlates poorly with user perception of transcription quality. This is because WER weighs every word equally and does not consider semantic correctness which has a higher impact on user perception. In this work, we propose evaluating ASR output hypotheses quality with SemDist that can measure semantic correctness by using the distance between the semantic vectors of the reference and hypothesis extracted from a pre-trained language model. Our experimental results of 71K and 36K user annotated ASR output quality show that SemDist achieves higher correlation with user perception than WER. We also show that SemDist has higher correlation with downstream NLU tasks than WER.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.05376v1-abstract-full').style.display = 'none'; document.getElementById('2110.05376v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted 2022 ICASSP</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.06050">arXiv:2109.06050</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.06050">pdf</a>, <a href="https://arxiv.org/format/2109.06050">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Few-Shot Cross-Lingual Stance Detection with Sentiment-Based Pre-Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hardalov%2C+M">Momchil Hardalov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Arnav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nakov%2C+P">Preslav Nakov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Augenstein%2C+I">Isabelle Augenstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.06050v2-abstract-short" style="display: inline;">
        The goal of stance detection is to determine the viewpoint expressed in a piece of text towards a target. These viewpoints or contexts are often expressed in many different languages depending on the user and the platform, which can be a local news outlet, a social media platform, a news forum, etc. Most research in stance detection, however, has been limited to working with a single language and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.06050v2-abstract-full').style.display = 'inline'; document.getElementById('2109.06050v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.06050v2-abstract-full" style="display: none;">
        The goal of stance detection is to determine the viewpoint expressed in a piece of text towards a target. These viewpoints or contexts are often expressed in many different languages depending on the user and the platform, which can be a local news outlet, a social media platform, a news forum, etc. Most research in stance detection, however, has been limited to working with a single language and on a few limited targets, with little work on cross-lingual stance detection. Moreover, non-English sources of labelled data are often scarce and present additional challenges. Recently, large multilingual language models have substantially improved the performance on many non-English tasks, especially such with limited numbers of examples. This highlights the importance of model pre-training and its ability to learn from few examples. In this paper, we present the most comprehensive study of cross-lingual stance detection to date: we experiment with 15 diverse datasets in 12 languages from 6 language families, and with 6 low-resource evaluation settings each. For our experiments, we build on pattern-exploiting training, proposing the addition of a novel label encoder to simplify the verbalisation procedure. We further propose sentiment-based generation of stance data for pre-training, which shows sizeable improvement of more than 6% F1 absolute in low-shot settings compared to several strong baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.06050v2-abstract-full').style.display = 'none'; document.getElementById('2109.06050v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 September, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to AAAI 2022 (Preprint version)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.09178">arXiv:2107.09178</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.09178">pdf</a>, <a href="https://arxiv.org/format/2107.09178">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compute RAMs: Adaptable Compute and Storage Blocks for DL-Optimized FPGAs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hanindhito%2C+B">Bagus Hanindhito</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=John%2C+L+K">Lizy K. John</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.09178v2-abstract-short" style="display: inline;">
        The configurable building blocks of current FPGAs -- Logic blocks (LBs), Digital Signal Processing (DSP) slices, and Block RAMs (BRAMs) -- make them efficient hardware accelerators for the rapid-changing world of Deep Learning (DL). Communication between these blocks happens through an interconnect fabric consisting of switching elements spread throughout the FPGA. In this paper, a new block, Comp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.09178v2-abstract-full').style.display = 'inline'; document.getElementById('2107.09178v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.09178v2-abstract-full" style="display: none;">
        The configurable building blocks of current FPGAs -- Logic blocks (LBs), Digital Signal Processing (DSP) slices, and Block RAMs (BRAMs) -- make them efficient hardware accelerators for the rapid-changing world of Deep Learning (DL). Communication between these blocks happens through an interconnect fabric consisting of switching elements spread throughout the FPGA. In this paper, a new block, Compute RAM, is proposed. Compute RAMs provide highly-parallel processing-in-memory (PIM) by combining computation and storage capabilities in one block. Compute RAMs can be integrated in the FPGA fabric just like the existing FPGA blocks and provide two modes of operation (storage or compute) that can be dynamically chosen. They reduce power consumption by reducing data movement, provide adaptable precision support, and increase the effective on-chip memory bandwidth. Compute RAMs also help increase the compute density of FPGAs. In our evaluation of addition, multiplication and dot-product operations across multiple data precisions (int4, int8 and bfloat16), we observe an average savings of 80% in energy consumption, and an improvement in execution time ranging from 20% to 80%. Adding Compute RAMs can benefit non-DL applications as well, and make FPGAs more efficient, flexible, and performant accelerators.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.09178v2-abstract-full').style.display = 'none'; document.getElementById('2107.09178v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, IEEE Signal Processing Society&#39;s ASILOMAR Conference on Signals, Systems and Computers</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.07087">arXiv:2106.07087</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.07087">pdf</a>, <a href="https://arxiv.org/format/2106.07087">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Koios: A Deep Learning Benchmark Suite for FPGA Architecture and CAD Research
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boutros%2C+A">Andrew Boutros</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rauch%2C+D">Daniel Rauch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rajen%2C+A">Aishwarya Rajen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Borda%2C+A">Aatman Borda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Damghani%2C+S+A">Seyed Alireza Damghani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mehta%2C+S">Samidh Mehta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kate%2C+S">Sangram Kate</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Patel%2C+P">Pragnesh Patel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kent%2C+K+B">Kenneth B. Kent</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Betz%2C+V">Vaughn Betz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=John%2C+L+K">Lizy K. John</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.07087v1-abstract-short" style="display: inline;">
        With the prevalence of deep learning (DL) in many applications, researchers are investigating different ways of optimizing FPGA architecture and CAD to achieve better quality-of-results (QoR) on DL-based workloads. In this optimization process, benchmark circuits are an essential component; the QoR achieved on a set of benchmarks is the main driver for architecture and CAD design choices. However,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07087v1-abstract-full').style.display = 'inline'; document.getElementById('2106.07087v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.07087v1-abstract-full" style="display: none;">
        With the prevalence of deep learning (DL) in many applications, researchers are investigating different ways of optimizing FPGA architecture and CAD to achieve better quality-of-results (QoR) on DL-based workloads. In this optimization process, benchmark circuits are an essential component; the QoR achieved on a set of benchmarks is the main driver for architecture and CAD design choices. However, current academic benchmark suites are inadequate, as they do not capture any designs from the DL domain. This work presents a new suite of DL acceleration benchmark circuits for FPGA architecture and CAD research, called Koios. This suite of 19 circuits covers a wide variety of accelerated neural networks, design sizes, implementation styles, abstraction levels, and numerical precisions. These designs are larger, more data parallel, more heterogeneous, more deeply pipelined, and utilize more FPGA architectural features compared to existing open-source benchmarks. This enables researchers to pin-point architectural inefficiencies for this class of workloads and optimize CAD tools on more realistic benchmarks that stress the CAD algorithms in different ways. In this paper, we describe the designs in our benchmark suite, present results of running them through the Verilog-to-Routing (VTR) flow using a recent FPGA architecture model, and identify key insights from the resulting metrics. On average, our benchmarks have 3.7x more netlist primitives, 1.8x and 4.7x higher DSP and BRAM densities, and 1.7x higher frequency with 1.9x more near-critical paths compared to the widely-used VTR suite. Finally, we present two example case studies showing how architectural exploration for DL-optimized FPGAs can be performed using our new benchmark suite.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07087v1-abstract-full').style.display = 'none'; document.getElementById('2106.07087v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.14082">arXiv:2105.14082</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.14082">pdf</a>, <a href="https://arxiv.org/format/2105.14082">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bhā$\unicode{x1E63}$ācitra: Visualising the dialect geography of South Asia
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aryaman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Farris%2C+A">Adam Farris</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%2C+G">Gopalakrishnan R</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Basu%2C+S">Samopriya Basu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.14082v2-abstract-short" style="display: inline;">
        We present Bhā$\unicode{x1E63}$ācitra, a dialect mapping system for South Asia built on a database of linguistic studies of languages of the region annotated for topic and location data. We analyse language coverage and look towards applications to typology by visualising example datasets. The application is not only meant to be useful for feature mapping, but also serves as a new kind of interact&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.14082v2-abstract-full').style.display = 'inline'; document.getElementById('2105.14082v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.14082v2-abstract-full" style="display: none;">
        We present Bhā$\unicode{x1E63}$ācitra, a dialect mapping system for South Asia built on a database of linguistic studies of languages of the region annotated for topic and location data. We analyse language coverage and look towards applications to typology by visualising example datasets. The application is not only meant to be useful for feature mapping, but also serves as a new kind of interactive bibliography for linguists of South Asian languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.14082v2-abstract-full').style.display = 'none'; document.getElementById('2105.14082v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 4 figures. To appear at LChange&#39;21 workshop located at ACL 2021</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.09938">arXiv:2105.09938</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.09938">pdf</a>, <a href="https://arxiv.org/format/2105.09938">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measuring Coding Challenge Competence With APPS
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hendrycks%2C+D">Dan Hendrycks</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Basart%2C+S">Steven Basart</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kadavath%2C+S">Saurav Kadavath</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mazeika%2C+M">Mantas Mazeika</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Akul Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+E">Ethan Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burns%2C+C">Collin Burns</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Puranik%2C+S">Samir Puranik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+H">Horace He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+D">Dawn Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Steinhardt%2C+J">Jacob Steinhardt</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.09938v3-abstract-short" style="display: inline;">
        While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.09938v3-abstract-full').style.display = 'inline'; document.getElementById('2105.09938v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.09938v3-abstract-full" style="display: none;">
        While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.09938v3-abstract-full').style.display = 'none'; document.getElementById('2105.09938v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021. Code and the APPS dataset is available at https://github.com/hendrycks/apps</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.08737">arXiv:2104.08737</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.08737">pdf</a>, <a href="https://arxiv.org/format/2104.08737">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low-Rank Subspaces for Unsupervised Entity Linking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Akhil Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garc%C3%ADa-Dur%C3%A1n%2C+A">Alberto García-Durán</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=West%2C+R">Robert West</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.08737v2-abstract-short" style="display: inline;">
        Entity linking is an important problem with many applications. Most previous solutions were designed for settings where annotated training data is available, which is, however, not the case in numerous domains. We propose a light-weight and scalable entity linking method, Eigenthemes, that relies solely on the availability of entity names and a referent knowledge base. Eigenthemes exploits the fac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.08737v2-abstract-full').style.display = 'inline'; document.getElementById('2104.08737v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.08737v2-abstract-full" style="display: none;">
        Entity linking is an important problem with many applications. Most previous solutions were designed for settings where annotated training data is available, which is, however, not the case in numerous domains. We propose a light-weight and scalable entity linking method, Eigenthemes, that relies solely on the availability of entity names and a referent knowledge base. Eigenthemes exploits the fact that the entities that are truly mentioned in a document (the &#34;gold entities&#34;) tend to form a semantically dense subset of the set of all candidate entities in the document. Geometrically speaking, when representing entities as vectors via some given embedding, the gold entities tend to lie in a low-rank subspace of the full embedding space. Eigenthemes identifies this subspace using the singular value decomposition and scores candidate entities according to their proximity to the subspace. On the empirical front, we introduce multiple strong baselines that compare favorably to (and sometimes even outperform) the existing state of the art. Extensive experiments on benchmark datasets from a variety of real-world domains showcase the effectiveness of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.08737v2-abstract-full').style.display = 'none'; document.getElementById('2104.08737v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 October, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2021, 18 pages, 22 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.07467">arXiv:2104.07467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.07467">pdf</a>, <a href="https://arxiv.org/format/2104.07467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-Domain Label-Adaptive Stance Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hardalov%2C+M">Momchil Hardalov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Arnav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nakov%2C+P">Preslav Nakov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Augenstein%2C+I">Isabelle Augenstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.07467v2-abstract-short" style="display: inline;">
        Stance detection concerns the classification of a writer&#39;s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.07467v2-abstract-full').style.display = 'inline'; document.getElementById('2104.07467v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.07467v2-abstract-full" style="display: none;">
        Stance detection concerns the classification of a writer&#39;s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the model performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.07467v2-abstract-full').style.display = 'none'; document.getElementById('2104.07467v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to EMNLP 2021 (Main Conference)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.07275">arXiv:2104.07275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.07275">pdf</a>, <a href="https://arxiv.org/format/2104.07275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shrivastava%2C+A">Akshat Shrivastava</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chuang%2C+P">Pierce Chuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Babu%2C+A">Arun Babu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Desai%2C+S">Shrey Desai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Abhinav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zotov%2C+A">Alexander Zotov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aly%2C+A">Ahmed Aly</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.07275v3-abstract-short" style="display: inline;">
        An effective recipe for building seq2seq, non-autoregressive, task-oriented parsers to map utterances to semantic frames proceeds in three steps: encoding an utterance $x$, predicting a frame&#39;s length |y|, and decoding a |y|-sized frame with utterance and ontology tokens. Though empirically strong, these models are typically bottlenecked by length prediction, as even small inaccuracies change the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.07275v3-abstract-full').style.display = 'inline'; document.getElementById('2104.07275v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.07275v3-abstract-full" style="display: none;">
        An effective recipe for building seq2seq, non-autoregressive, task-oriented parsers to map utterances to semantic frames proceeds in three steps: encoding an utterance $x$, predicting a frame&#39;s length |y|, and decoding a |y|-sized frame with utterance and ontology tokens. Though empirically strong, these models are typically bottlenecked by length prediction, as even small inaccuracies change the syntactic and semantic characteristics of resulting frames. In our work, we propose span pointer networks, non-autoregressive parsers which shift the decoding task from text generation to span prediction; that is, when imputing utterance spans into frame slots, our model produces endpoints (e.g., [i, j]) as opposed to text (e.g., &#34;6pm&#34;). This natural quantization of the output space reduces the variability of gold frames, therefore improving length prediction and, ultimately, exact match. Furthermore, length prediction is now responsible for frame syntax and the decoder is responsible for frame semantics, resulting in a coarse-to-fine model. We evaluate our approach on several task-oriented semantic parsing datasets. Notably, we bridge the quality gap between non-autogressive and autoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020). Furthermore, due to our more consistent gold frames, we show strong improvements in model generalization in both cross-domain and cross-lingual transfer in low-resource settings. Finally, due to our diminished output vocabulary, we observe 70% reduction in latency and 83% reduction in memory at beam size 5 compared to prior non-autoregressive parsers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.07275v3-abstract-full').style.display = 'none'; document.getElementById('2104.07275v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02138">arXiv:2104.02138</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02138">pdf</a>, <a href="https://arxiv.org/format/2104.02138">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Suyoun Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Abhinav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Le%2C+D">Duc Le</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yeh%2C+C">Ching-Feng Yeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fuegen%2C+C">Christian Fuegen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kalinli%2C+O">Ozlem Kalinli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seltzer%2C+M+L">Michael L. Seltzer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02138v1-abstract-short" style="display: inline;">
        Word Error Rate (WER) has been the predominant metric used to evaluate the performance of automatic speech recognition (ASR) systems. However, WER is sometimes not a good indicator for downstream Natural Language Understanding (NLU) tasks, such as intent recognition, slot filling, and semantic parsing in task-oriented dialog systems. This is because WER takes into consideration only literal correc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02138v1-abstract-full').style.display = 'inline'; document.getElementById('2104.02138v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02138v1-abstract-full" style="display: none;">
        Word Error Rate (WER) has been the predominant metric used to evaluate the performance of automatic speech recognition (ASR) systems. However, WER is sometimes not a good indicator for downstream Natural Language Understanding (NLU) tasks, such as intent recognition, slot filling, and semantic parsing in task-oriented dialog systems. This is because WER takes into consideration only literal correctness instead of semantic correctness, the latter of which is typically more important for these downstream tasks. In this study, we propose a novel Semantic Distance (SemDist) measure as an alternative evaluation metric for ASR systems to address this issue. We define SemDist as the distance between a reference and hypothesis pair in a sentence-level embedding space. To represent the reference and hypothesis as a sentence embedding, we exploit RoBERTa, a state-of-the-art pre-trained deep contextualized language model based on the transformer architecture. We demonstrate the effectiveness of our proposed metric on various downstream tasks, including intent recognition, semantic parsing, and named entity recognition.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02138v1-abstract-full').style.display = 'none'; document.getElementById('2104.02138v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to Interspeech 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.12116">arXiv:2103.12116</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.12116">pdf</a>, <a href="https://arxiv.org/format/2103.12116">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1051/epjconf/202125102001">10.1051/epjconf/202125102001 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Systematic benchmarking of HTTPS third party copy on 100Gbps links using XRootD
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fajardo%2C+E">Edgar Fajardo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aashay Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Davila%2C+D">Diego Davila</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+R">Richard Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=W%C3%BCrthwein%2C+F">Frank Würthwein</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bockelman%2C+B">Brian Bockelman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.12116v1-abstract-short" style="display: inline;">
        The High Luminosity Large Hadron Collider provides a data challenge. The amount of data recorded from the experiments and transported to hundreds of sites will see a thirty fold increase in annual data volume. A systematic approach to contrast the performance of different Third Party Copy(TPC) transfer protocols arises. Two contenders, XRootD-HTTPS and the GridFTP are evaluated in their performanc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12116v1-abstract-full').style.display = 'inline'; document.getElementById('2103.12116v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.12116v1-abstract-full" style="display: none;">
        The High Luminosity Large Hadron Collider provides a data challenge. The amount of data recorded from the experiments and transported to hundreds of sites will see a thirty fold increase in annual data volume. A systematic approach to contrast the performance of different Third Party Copy(TPC) transfer protocols arises. Two contenders, XRootD-HTTPS and the GridFTP are evaluated in their performance for transferring files from one server to an-other over 100Gbps interfaces. The benchmarking is done by scheduling pods on the Pacific Research Platform Kubernetes cluster to ensure reproducible and repeatable results. This opens a future pathway for network testing of any TPC transfer protocol.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12116v1-abstract-full').style.display = 'none'; document.getElementById('2103.12116v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.03874">arXiv:2103.03874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.03874">pdf</a>, <a href="https://arxiv.org/format/2103.03874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measuring Mathematical Problem Solving With the MATH Dataset
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hendrycks%2C+D">Dan Hendrycks</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burns%2C+C">Collin Burns</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kadavath%2C+S">Saurav Kadavath</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Akul Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Basart%2C+S">Steven Basart</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+E">Eric Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+D">Dawn Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Steinhardt%2C+J">Jacob Steinhardt</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.03874v2-abstract-short" style="display: inline;">
        Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.03874v2-abstract-full').style.display = 'inline'; document.getElementById('2103.03874v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.03874v2-abstract-full" style="display: none;">
        Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.03874v2-abstract-full').style.display = 'none'; document.getElementById('2103.03874v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021. Code and the MATH dataset is available at https://github.com/hendrycks/math/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.01399">arXiv:2103.01399</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.01399">pdf</a>, <a href="https://arxiv.org/format/2103.01399">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hindi-Urdu Adposition and Case Supersenses v1.0
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aryaman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Venkateswaran%2C+N">Nitin Venkateswaran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schneider%2C+N">Nathan Schneider</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.01399v1-abstract-short" style="display: inline;">
        These are the guidelines for the application of SNACS (Semantic Network of Adposition and Case Supersenses; Schneider et al. 2018) to Modern Standard Hindi of Delhi. SNACS is an inventory of 50 supersenses (semantic labels) for labelling the use of adpositions and case markers with respect to both lexical-semantic function and relation to the underlying context. The English guidelines (Schneider e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01399v1-abstract-full').style.display = 'inline'; document.getElementById('2103.01399v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.01399v1-abstract-full" style="display: none;">
        These are the guidelines for the application of SNACS (Semantic Network of Adposition and Case Supersenses; Schneider et al. 2018) to Modern Standard Hindi of Delhi. SNACS is an inventory of 50 supersenses (semantic labels) for labelling the use of adpositions and case markers with respect to both lexical-semantic function and relation to the underlying context. The English guidelines (Schneider et al., 2020) were used as a model for this document.
  Besides the case system, Hindi has an extremely rich adpositional system built on the oblique genitive, with productive incorporation of loanwords even in present-day Hinglish.
  This document is aligned with version 2.5 of the English guidelines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01399v1-abstract-full').style.display = 'none'; document.getElementById('2103.01399v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.00242">arXiv:2103.00242</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.00242">pdf</a>, <a href="https://arxiv.org/format/2103.00242">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Survey on Stance Detection for Mis- and Disinformation Identification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hardalov%2C+M">Momchil Hardalov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Arnav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nakov%2C+P">Preslav Nakov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Augenstein%2C+I">Isabelle Augenstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.00242v2-abstract-short" style="display: inline;">
        Understanding the attitudes expressed in texts, also known as stance detection, plays an important role in systems aiming to detect false information online, be it misinformation (unintentionally false) or disinformation (intentionally false, spread deliberately with malicious intent). Stance detection has been framed in different ways in the literature, including (a) as a component of fact-checki&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00242v2-abstract-full').style.display = 'inline'; document.getElementById('2103.00242v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.00242v2-abstract-full" style="display: none;">
        Understanding the attitudes expressed in texts, also known as stance detection, plays an important role in systems aiming to detect false information online, be it misinformation (unintentionally false) or disinformation (intentionally false, spread deliberately with malicious intent). Stance detection has been framed in different ways in the literature, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right; and here we look at both. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there has been no survey examining the relationship between stance detection and mis- and disinformation detection. Here we aim to bridge this gap. In particular, we review and analyse existing work in this area, with mis- and disinformation in focus, and then we discuss lessons learnt and future challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00242v2-abstract-full').style.display = 'none'; document.getElementById('2103.00242v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.02808">arXiv:2102.02808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.02808">pdf</a>, <a href="https://arxiv.org/format/2102.02808">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Stage Progressive Image Restoration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.02808v2-abstract-short" style="display: inline;">
        Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a novel synergistic design that can optimally balance these competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.02808v2-abstract-full').style.display = 'inline'; document.getElementById('2102.02808v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.02808v2-abstract-full" style="display: none;">
        Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a novel synergistic design that can optimally balance these competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, our model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, we introduce a novel per-pixel adaptive design that leverages in-situ supervised attention to reweight the local features. A key ingredient in such a multi-stage architecture is the information exchange between different stages. To this end, we propose a two-faceted approach where the information is not only exchanged sequentially from early to late stages, but lateral connections between feature processing blocks also exist to avoid any loss of information. The resulting tightly interlinked multi-stage architecture, named as MPRNet, delivers strong performance gains on ten datasets across a range of tasks including image deraining, deblurring, and denoising. The source code and pre-trained models are available at https://github.com/swz30/MPRNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.02808v2-abstract-full').style.display = 'none'; document.getElementById('2102.02808v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CVPR 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.11211">arXiv:2101.11211</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.11211">pdf</a>, <a href="https://arxiv.org/format/2101.11211">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Harvest: A Reliable and Energy Efficient Bulk Data Collection Service for Large Scale Wireless Sensor Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Naik%2C+V">Vinayak Naik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Anish Arora</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.11211v1-abstract-short" style="display: inline;">
        We present a bulk data collection service, Harvest, for energy constrained wireless sensor nodes. To increase spatial reuse and thereby decrease latency, Harvest performs concurrent, pipelined exfiltration from multiple nodes to a base station. To this end, it uses a distance-k coloring of the nodes, notably with a constant number of colors, which yields a TDMA schedule whereby nodes can communica&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.11211v1-abstract-full').style.display = 'inline'; document.getElementById('2101.11211v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.11211v1-abstract-full" style="display: none;">
        We present a bulk data collection service, Harvest, for energy constrained wireless sensor nodes. To increase spatial reuse and thereby decrease latency, Harvest performs concurrent, pipelined exfiltration from multiple nodes to a base station. To this end, it uses a distance-k coloring of the nodes, notably with a constant number of colors, which yields a TDMA schedule whereby nodes can communicate concurrently with low packet losses due to collision. This coloring is based on a randomized CSMA approach which does not exploit location knowledge. Given a bounded degree of the network, each node waits only O$(1)$ time to obtain a unique color among its distance-k neighbors, in contrast to the traditional deterministic distributed distance-k vertex coloring wherein each node waits O$(Δ^{2})$ time to obtain a color.
  Harvest offers the option of limiting memory use to only a small constant number of bytes or of improving latency with increased memory use; it can be used with or without additional mechanisms for reliability of message forwarding. We experimentally evaluate the performance of Harvest using 51 motes in the Kansei testbed. We also provide theoretical as well as TOSSIM-based comparison of Harvest with Straw, an extant data collection service implemented for TinyOS platforms that use one-node at a time exfiltration. For networks with more than 3-hops, Harvest reduces the latency by at least 33% as compared to that of Straw.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.11211v1-abstract-full').style.display = 'none'; document.getElementById('2101.11211v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          C.2.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.10524">arXiv:2101.10524</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.10524">pdf</a>, <a href="https://arxiv.org/format/2101.10524">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        El Volumen Louder Por Favor: Code-switching in Task-oriented Semantic Parsing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Einolghozati%2C+A">Arash Einolghozati</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Abhinav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lecanda%2C+L+S">Lorena Sainz-Maza Lecanda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+A">Anuj Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gupta%2C+S">Sonal Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.10524v3-abstract-short" style="display: inline;">
        Being able to parse code-switched (CS) utterances, such as Spanish+English or Hindi+English, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10524v3-abstract-full').style.display = 'inline'; document.getElementById('2101.10524v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.10524v3-abstract-full" style="display: none;">
        Being able to parse code-switched (CS) utterances, such as Spanish+English or Hindi+English, is essential to democratize task-oriented semantic parsing systems for certain locales. In this work, we focus on Spanglish (Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances alongside their semantic parses. We examine the CS generalizability of various Cross-lingual (XL) models and exhibit the advantage of pre-trained XL language models when data for only one language is present. As such, we focus on improving the pre-trained models for the case when only English corpus alongside either zero or a few CS training instances are available. We propose two data augmentation methods for the zero-shot and the few-shot settings: fine-tune using translate-and-align and augment using a generation model followed by match-and-filter. Combining the few-shot setting with the above improvements decreases the initial 30-point accuracy gap between the zero-shot and the full-data settings by two thirds.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.10524v3-abstract-full').style.display = 'none'; document.getElementById('2101.10524v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        EACL 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.00850">arXiv:2101.00850</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.00850">pdf</a>, <a href="https://arxiv.org/format/2101.00850">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low Light Image Enhancement via Global and Local Context Modeling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Haris%2C+M">Muhammad Haris</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.00850v1-abstract-short" style="display: inline;">
        Images captured under low-light conditions manifest poor visibility, lack contrast and color vividness. Compared to conventional approaches, deep convolutional neural networks (CNNs) perform well in enhancing images. However, being solely reliant on confined fixed primitives to model dependencies, existing data-driven deep models do not exploit the contexts at various spatial scales to address low&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00850v1-abstract-full').style.display = 'inline'; document.getElementById('2101.00850v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.00850v1-abstract-full" style="display: none;">
        Images captured under low-light conditions manifest poor visibility, lack contrast and color vividness. Compared to conventional approaches, deep convolutional neural networks (CNNs) perform well in enhancing images. However, being solely reliant on confined fixed primitives to model dependencies, existing data-driven deep models do not exploit the contexts at various spatial scales to address low-light image enhancement. These contexts can be crucial towards inferring several image enhancement tasks, e.g., local and global contrast, brightness and color corrections; which requires cues from both local and global spatial extent. To this end, we introduce a context-aware deep network for low-light image enhancement. First, it features a global context module that models spatial correlations to find complementary cues over full spatial domain. Second, it introduces a dense residual block that captures local context with a relatively large receptive field. We evaluate the proposed approach using three challenging datasets: MIT-Adobe FiveK, LoL, and SID. On all these datasets, our method performs favorably against the state-of-the-arts in terms of standard image fidelity metrics. In particular, compared to the best performing method on the MIT-Adobe FiveK dataset, our algorithm improves PSNR from 23.04 dB to 24.45 dB.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.00850v1-abstract-full').style.display = 'none'; document.getElementById('2101.00850v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.08011">arXiv:2012.08011</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.08011">pdf</a>, <a href="https://arxiv.org/format/2012.08011">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICMLA51294.2020.00067">10.1109/ICMLA51294.2020.00067 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DeepGamble: Towards unlocking real-time player intelligence using multi-layer instance segmentation and attribute detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Syed%2C+D">Danish Syed</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gandhi%2C+N">Naman Gandhi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Arushi Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kadam%2C+N">Nilesh Kadam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.08011v1-abstract-short" style="display: inline;">
        Annually the gaming industry spends approximately $15 billion in marketing reinvestment. However, this amount is spent without any consideration for the skill and luck of the player. For a casino, an unskilled player could fetch ~4 times more revenue than a skilled player. This paper describes a video recognition system that is based on an extension of the Mask R-CNN model. Our system digitizes th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.08011v1-abstract-full').style.display = 'inline'; document.getElementById('2012.08011v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.08011v1-abstract-full" style="display: none;">
        Annually the gaming industry spends approximately $15 billion in marketing reinvestment. However, this amount is spent without any consideration for the skill and luck of the player. For a casino, an unskilled player could fetch ~4 times more revenue than a skilled player. This paper describes a video recognition system that is based on an extension of the Mask R-CNN model. Our system digitizes the game of blackjack by detecting cards and player bets in real-time and processes decisions they took in order to create accurate player personas. Our proposed supervised learning approach consists of a specialized three-stage pipeline that takes images from two viewpoints of the casino table and does instance segmentation to generate masks on proposed regions of interest. These predicted masks along with derivative features are used to classify image attributes that are passed onto the next stage to assimilate the gameplay understanding. Our end-to-end model yields an accuracy of ~95% for the main bet detection and ~97% for card detection in a controlled environment trained using transfer learning approach with 900 training examples. Our approach is generalizable and scalable and shows promising results in varied gaming scenarios and test data. Such granular level gathered data, helped in understanding player&#39;s deviation from optimum strategy and thereby separate the skill of the player from the luck of the game. Our system also assesses the likelihood of card counting by correlating the player&#39;s betting pattern to the deck&#39;s scaled count. Such a system lets casinos flag fraudulent activity and calculate expected personalized profitability for each player and tailor their marketing reinvestment decisions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.08011v1-abstract-full').style.display = 'none'; document.getElementById('2012.08011v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.04062">arXiv:2011.04062</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.04062">pdf</a>, <a href="https://arxiv.org/format/2011.04062">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MLAS: Metric Learning on Attributed Sequences
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuang%2C+Z">Zhongfang Zhuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kong%2C+X">Xiangnan Kong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rundensteiner%2C+E">Elke Rundensteiner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zouaoui%2C+J">Jihane Zouaoui</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.04062v1-abstract-short" style="display: inline;">
        Distance metric learning has attracted much attention in recent years, where the goal is to learn a distance metric based on user feedback. Conventional approaches to metric learning mainly focus on learning the Mahalanobis distance metric on data attributes. Recent research on metric learning has been extended to sequential data, where we only have structural information in the sequences, but no&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.04062v1-abstract-full').style.display = 'inline'; document.getElementById('2011.04062v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.04062v1-abstract-full" style="display: none;">
        Distance metric learning has attracted much attention in recent years, where the goal is to learn a distance metric based on user feedback. Conventional approaches to metric learning mainly focus on learning the Mahalanobis distance metric on data attributes. Recent research on metric learning has been extended to sequential data, where we only have structural information in the sequences, but no attribute is available. However, real-world applications often involve attributed sequence data (e.g., clickstreams), where each instance consists of not only a set of attributes (e.g., user session context) but also a sequence of categorical items (e.g., user actions). In this paper, we study the problem of metric learning on attributed sequences. Unlike previous work on metric learning, we now need to go beyond the Mahalanobis distance metric in the attribute feature space while also incorporating the structural information in sequences. We propose a deep learning framework, called MLAS (Metric Learning on Attributed Sequences), to learn a distance metric that effectively measures dissimilarities between attributed sequences. Empirical results on real-world datasets demonstrate that the proposed MLAS framework significantly improves the performance of metric learning compared to state-of-the-art methods on attributed sequences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.04062v1-abstract-full').style.display = 'none'; document.getElementById('2011.04062v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE Big Data 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.12072">arXiv:2009.12072</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.12072">pdf</a>, <a href="https://arxiv.org/format/2009.12072">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AIM 2020 Challenge on Real Image Super-Resolution: Methods and Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+P">Pengxu Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+H">Hannan Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+L">Liang Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zuo%2C+W">Wangmeng Zuo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+Z">Zhihong Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Baopu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xi%2C+T">Teng Xi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+Y">Yanwen Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+G">Gang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jingtuo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Junyu Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+E">Errui Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+T">Tangxin Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+L">Liang Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+Y">Yan Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yi Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jialiang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+Y">Yu Jia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+K">Kaihua Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chenhuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yue Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Cen Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+Y">Yunbo Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+X">Xueyi Zou</a>
      , et al. (51 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.12072v1-abstract-short" style="display: inline;">
        This paper introduces the real image Super-Resolution (SR) challenge that was part of the Advances in Image Manipulation (AIM) workshop, held in conjunction with ECCV 2020. This challenge involves three tracks to super-resolve an input image for $\times$2, $\times$3 and $\times$4 scaling factors, respectively. The goal is to attract more attention to realistic image degradation for the SR task, wh&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.12072v1-abstract-full').style.display = 'inline'; document.getElementById('2009.12072v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.12072v1-abstract-full" style="display: none;">
        This paper introduces the real image Super-Resolution (SR) challenge that was part of the Advances in Image Manipulation (AIM) workshop, held in conjunction with ECCV 2020. This challenge involves three tracks to super-resolve an input image for $\times$2, $\times$3 and $\times$4 scaling factors, respectively. The goal is to attract more attention to realistic image degradation for the SR task, which is much more complicated and challenging, and contributes to real-world image super-resolution applications. 452 participants were registered for three tracks in total, and 24 teams submitted their results. They gauge the state-of-the-art approaches for real image SR in terms of PSNR and SSIM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.12072v1-abstract-full').style.display = 'none'; document.getElementById('2009.12072v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        European Conference on Computer Vision Workshops, 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.06401">arXiv:2009.06401</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.06401">pdf</a>, <a href="https://arxiv.org/format/2009.06401">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Hop Fact Checking of Political Claims
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ostrowski%2C+W">Wojciech Ostrowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Arnav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Atanasova%2C+P">Pepa Atanasova</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Augenstein%2C+I">Isabelle Augenstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.06401v3-abstract-short" style="display: inline;">
        Recent work has proposed multi-hop models and datasets for studying complex natural language reasoning. One notable task requiring multi-hop reasoning is fact checking, where a set of connected evidence pieces leads to the final verdict of a claim. However, existing datasets either do not provide annotations for gold evidence pages, or the only dataset which does (FEVER) mostly consists of claims&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.06401v3-abstract-full').style.display = 'inline'; document.getElementById('2009.06401v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.06401v3-abstract-full" style="display: none;">
        Recent work has proposed multi-hop models and datasets for studying complex natural language reasoning. One notable task requiring multi-hop reasoning is fact checking, where a set of connected evidence pieces leads to the final verdict of a claim. However, existing datasets either do not provide annotations for gold evidence pages, or the only dataset which does (FEVER) mostly consists of claims which can be fact-checked with simple reasoning and is constructed artificially. Here, we study more complex claim verification of naturally occurring claims with multiple hops over interconnected evidence chunks. We: 1) construct a small annotated dataset, PolitiHop, of evidence sentences for claim verification; 2) compare it to existing multi-hop datasets; and 3) study how to transfer knowledge from more extensive in- and out-of-domain resources to PolitiHop. We find that the task is complex and achieve the best performance with an architecture that specifically models reasoning over evidence pieces in combination with in-domain transfer learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.06401v3-abstract-full').style.display = 'none'; document.getElementById('2009.06401v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 September, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, to be published at Proceedings of IJCAI-2021</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07; 68T50
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.09335">arXiv:2008.09335</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.09335">pdf</a>, <a href="https://arxiv.org/format/2008.09335">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Haoran Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Abhinav Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+S">Shuohui Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gupta%2C+A">Anchit Gupta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gupta%2C+S">Sonal Gupta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mehdad%2C+Y">Yashar Mehdad</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.09335v2-abstract-short" style="display: inline;">
        Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional querie&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09335v2-abstract-full').style.display = 'inline'; document.getElementById('2008.09335v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.09335v2-abstract-full" style="display: none;">
        Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09335v2-abstract-full').style.display = 'none'; document.getElementById('2008.09335v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 2 figures, Accepted at EACL 2021</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        EACL 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.02385">arXiv:2008.02385</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.02385">pdf</a>, <a href="https://arxiv.org/format/2008.02385">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient MDI Adaptation for n-gram Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+R">Ruizhe Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+K">Ke Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Ashish Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Povey%2C+D">Dan Povey</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khudanpur%2C+S">Sanjeev Khudanpur</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.02385v1-abstract-short" style="display: inline;">
        This paper presents an efficient algorithm for n-gram language model adaptation under the minimum discrimination information (MDI) principle, where an out-of-domain language model is adapted to satisfy the constraints of marginal probabilities of the in-domain data. The challenge for MDI language model adaptation is its computational complexity. By taking advantage of the backoff structure of n-gr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.02385v1-abstract-full').style.display = 'inline'; document.getElementById('2008.02385v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.02385v1-abstract-full" style="display: none;">
        This paper presents an efficient algorithm for n-gram language model adaptation under the minimum discrimination information (MDI) principle, where an out-of-domain language model is adapted to satisfy the constraints of marginal probabilities of the in-domain data. The challenge for MDI language model adaptation is its computational complexity. By taking advantage of the backoff structure of n-gram model and the idea of hierarchical training method, originally proposed for maximum entropy (ME) language models, we show that MDI adaptation can be computed in linear-time complexity to the inputs in each iteration. The complexity remains the same as ME models, although MDI is more general than ME. This makes MDI adaptation practical for large corpus and vocabulary. Experimental results confirm the scalability of our algorithm on very large datasets, while MDI adaptation gets slightly worse perplexity but better word error rate results compared to simple linear interpolation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.02385v1-abstract-full').style.display = 'none'; document.getElementById('2008.02385v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in INTERSPEECH 2020. Appendix A of this full version will be filled soon</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.07898">arXiv:2006.07898</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.07898">pdf</a>, <a href="https://arxiv.org/format/2006.07898">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The JHU Multi-Microphone Multi-Speaker ASR System for the CHiME-6 Challenge
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Ashish Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Raj%2C+D">Desh Raj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Subramanian%2C+A+S">Aswin Shanmugam Subramanian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+K">Ke Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ben-Yair%2C+B">Bar Ben-Yair</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Maciejewski%2C+M">Matthew Maciejewski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=%C5%BBelasko%2C+P">Piotr Żelasko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garc%C3%ADa%2C+P">Paola García</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khudanpur%2C+S">Sanjeev Khudanpur</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.07898v1-abstract-short" style="display: inline;">
        This paper summarizes the JHU team&#39;s efforts in tracks 1 and 2 of the CHiME-6 challenge for distant multi-microphone conversational speech diarization and recognition in everyday home environments. We explore multi-array processing techniques at each stage of the pipeline, such as multi-array guided source separation (GSS) for enhancement and acoustic model training data, posterior fusion for spee&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.07898v1-abstract-full').style.display = 'inline'; document.getElementById('2006.07898v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.07898v1-abstract-full" style="display: none;">
        This paper summarizes the JHU team&#39;s efforts in tracks 1 and 2 of the CHiME-6 challenge for distant multi-microphone conversational speech diarization and recognition in everyday home environments. We explore multi-array processing techniques at each stage of the pipeline, such as multi-array guided source separation (GSS) for enhancement and acoustic model training data, posterior fusion for speech activity detection, PLDA score fusion for diarization, and lattice combination for automatic speech recognition (ASR). We also report results with different acoustic model architectures, and integrate other techniques such as online multi-channel weighted prediction error (WPE) dereverberation and variational Bayes-hidden Markov model (VB-HMM) based overlap assignment to deal with reverberation and overlapping speakers, respectively. As a result of these efforts, our ASR systems achieve a word error rate of 40.5% and 67.5% on tracks 1 and 2, respectively, on the evaluation set. This is an improvement of 10.8% and 10.4% absolute, over the challenge baselines for the respective tracks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.07898v1-abstract-full').style.display = 'none'; document.getElementById('2006.07898v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at the CHiME-6 workshop (colocated with ICASSP 2020)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.10353">arXiv:2004.10353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.10353">pdf</a>, <a href="https://arxiv.org/format/2004.10353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aryaman Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gessler%2C+L">Luke Gessler</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schneider%2C+N">Nathan Schneider</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.10353v2-abstract-short" style="display: inline;">
        Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.10353v2-abstract-full').style.display = 'inline'; document.getElementById('2004.10353v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.10353v2-abstract-full" style="display: none;">
        Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries. Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.10353v2-abstract-full').style.display = 'none'; document.getElementById('2004.10353v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages, 1 figure. To be published in the 2020 Annual Conference of the Association for Computational Linguistics (https://acl2020.org/)</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.09249">arXiv:2004.09249</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.09249">pdf</a>, <a href="https://arxiv.org/format/2004.09249">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CHiME-6 Challenge:Tackling Multispeaker Speech Recognition for Unsegmented Recordings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mandel%2C+M">Michael Mandel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barker%2C+J">Jon Barker</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vincent%2C+E">Emmanuel Vincent</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Ashish Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+X">Xuankai Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khudanpur%2C+S">Sanjeev Khudanpur</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Manohar%2C+V">Vimal Manohar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Povey%2C+D">Daniel Povey</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Raj%2C+D">Desh Raj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Snyder%2C+D">David Snyder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Subramanian%2C+A+S">Aswin Shanmugam Subramanian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Trmal%2C+J">Jan Trmal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yair%2C+B+B">Bar Ben Yair</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boeddeker%2C+C">Christoph Boeddeker</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ni%2C+Z">Zhaoheng Ni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fujita%2C+Y">Yusuke Fujita</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Horiguchi%2C+S">Shota Horiguchi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kanda%2C+N">Naoyuki Kanda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yoshioka%2C+T">Takuya Yoshioka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ryant%2C+N">Neville Ryant</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.09249v2-abstract-short" style="display: inline;">
        Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we organize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6). The new challenge revisits the previous CHiME-5 challenge and further considers the problem of distant multi-microphone conversational speech diarization and recognition in everyday home environments. Speech material is the same as the previous C&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.09249v2-abstract-full').style.display = 'inline'; document.getElementById('2004.09249v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.09249v2-abstract-full" style="display: none;">
        Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we organize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6). The new challenge revisits the previous CHiME-5 challenge and further considers the problem of distant multi-microphone conversational speech diarization and recognition in everyday home environments. Speech material is the same as the previous CHiME-5 recordings except for accurate array synchronization. The material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech. This paper provides a baseline description of the CHiME-6 challenge for both segmented multispeaker speech recognition (Track 1) and unsegmented multispeaker speech recognition (Track 2). Of note, Track 2 is the first challenge activity in the community to tackle an unsegmented multispeaker speech recognition scenario with a complete set of reproducible open source baselines providing speech enhancement, speaker diarization, and speech recognition modules.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.09249v2-abstract-full').style.display = 'none'; document.getElementById('2004.09249v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.07761">arXiv:2003.07761</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.07761">pdf</a>, <a href="https://arxiv.org/format/2003.07761">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CycleISP: Real Image Restoration via Improved Data Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.07761v1-abstract-short" style="display: inline;">
        The availability of large-scale datasets has helped unleash the true potential of deep convolutional neural networks (CNNs). However, for the single-image denoising problem, capturing a real dataset is an unacceptably expensive and cumbersome procedure. Consequently, image denoising algorithms are mostly developed and evaluated on synthetic data that is usually generated with a widespread assumpti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.07761v1-abstract-full').style.display = 'inline'; document.getElementById('2003.07761v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.07761v1-abstract-full" style="display: none;">
        The availability of large-scale datasets has helped unleash the true potential of deep convolutional neural networks (CNNs). However, for the single-image denoising problem, capturing a real dataset is an unacceptably expensive and cumbersome procedure. Consequently, image denoising algorithms are mostly developed and evaluated on synthetic data that is usually generated with a widespread assumption of additive white Gaussian noise (AWGN). While the CNNs achieve impressive results on these synthetic datasets, they do not perform well when applied on real camera images, as reported in recent benchmark datasets. This is mainly because the AWGN is not adequate for modeling the real camera noise which is signal-dependent and heavily transformed by the camera imaging pipeline. In this paper, we present a framework that models camera imaging pipeline in forward and reverse directions. It allows us to produce any number of realistic image pairs for denoising both in RAW and sRGB spaces. By training a new image denoising network on realistic synthetic data, we achieve the state-of-the-art performance on real camera benchmark datasets. The parameters in our model are ~5 times lesser than the previous best method for RAW denoising. Furthermore, we demonstrate that the proposed framework generalizes beyond image denoising problem e.g., for color matching in stereoscopic cinema. The source code and pre-trained models are available at https://github.com/swz30/CycleISP.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.07761v1-abstract-full').style.display = 'none'; document.getElementById('2003.07761v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2020 (Oral)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.06792">arXiv:2003.06792</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.06792">pdf</a>, <a href="https://arxiv.org/format/2003.06792">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Enriched Features for Real Image Restoration and Enhancement
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.06792v2-abstract-short" style="display: inline;">
        With the goal of recovering high-quality image content from its degraded version, image restoration enjoys numerous applications, such as in surveillance, computational photography, medical imaging, and remote sensing. Recently, convolutional neural networks (CNNs) have achieved dramatic improvements over conventional approaches for image restoration task. Existing CNN-based methods typically oper&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.06792v2-abstract-full').style.display = 'inline'; document.getElementById('2003.06792v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.06792v2-abstract-full" style="display: none;">
        With the goal of recovering high-quality image content from its degraded version, image restoration enjoys numerous applications, such as in surveillance, computational photography, medical imaging, and remote sensing. Recently, convolutional neural networks (CNNs) have achieved dramatic improvements over conventional approaches for image restoration task. Existing CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatially precise but contextually less robust results are achieved, while in the latter case, semantically reliable but spatially less accurate outputs are generated. In this paper, we present a novel architecture with the collective goals of maintaining spatially-precise high-resolution representations through the entire network and receiving strong contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing several key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) spatial and channel attention mechanisms for capturing contextual information, and (d) attention based multi-scale feature aggregation. In a nutshell, our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on five real image benchmark datasets demonstrate that our method, named as MIRNet, achieves state-of-the-art results for a variety of image processing tasks, including image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.06792v2-abstract-full').style.display = 'none'; document.getElementById('2003.06792v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at ECCV 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.11961">arXiv:2001.11961</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.11961">pdf</a>, <a href="https://arxiv.org/format/2001.11961">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Middle-mile Network Optimization in Rural Wireless Meshes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yung-Fu Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+A">Anish Arora</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.11961v2-abstract-short" style="display: inline;">
        The status quo of limited broadband connectivity in rural areas motivates the need for fielding alternatives such as long-distance wireless mesh networks. A key aspect of fielding wireless meshes cost-effectively is planning how to connect the last-mile networks to the core network service providers (i.e., the network between the edge access terminals and the landline / optical fiber terminals) wi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.11961v2-abstract-full').style.display = 'inline'; document.getElementById('2001.11961v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.11961v2-abstract-full" style="display: none;">
        The status quo of limited broadband connectivity in rural areas motivates the need for fielding alternatives such as long-distance wireless mesh networks. A key aspect of fielding wireless meshes cost-effectively is planning how to connect the last-mile networks to the core network service providers (i.e., the network between the edge access terminals and the landline / optical fiber terminals) with minimal infrastructure cost and throughput constraints. This so-called middle-mile network optimization, which includes topology construction, tower height assignment, antenna and orientation selection, as well as transmit power assignment, is known to be a computationally hard problem.
  In this paper, we provide the first polynomial time approximation solution for a generalized version of the middle-mile network optimization problem, wherein point-to-point (i.e., WiFi p2p) links are deployed to bridge last-mile networks. Our solution has a cost performance ratio of $O(\ln{|A|}+\frac{|B|}{|A|}+\frac{|A|+|B|}γ)$, where A and B respectively denote the number of terminals and non-terminals and $γ$ is the ratio of $\frac{link\ capacity}{terminal\ demand}$. Furthermore, our solution extends to hybrid networks, i.e., point-to-multipoint (i.e., WiFi p2mp) or omnidirectional (i.e., TV White Space) can serve as hyperlinks in addition to point-to-point links, to further reduce the cost of wireless links. We provide a complementary heuristic for our middle-mile network optimization solution that adds hyperlinks if and only if they reduce the cost.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.11961v2-abstract-full').style.display = 'none'; document.getElementById('2001.11961v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 January, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Arora%2C+A&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Arora%2C+A&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Arora%2C+A&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>