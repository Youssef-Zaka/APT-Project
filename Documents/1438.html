<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 717 results for author: <span class="mathjax">Sun, J</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Sun%2C+J">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Sun, J">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Sun%2C+J&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Sun, J">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12590">arXiv:2204.12590</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12590">pdf</a>, <a href="https://arxiv.org/format/2204.12590">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Security War in File Systems: An Empirical Study from A Vulnerability-Centric Perspective
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jinghan Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+S">Shaobo Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+J">Jun Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+J">Jian Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12590v1-abstract-short" style="display: inline;">
        This paper presents a systematic study on the security of modern file systems, following a vulnerability-centric perspective. Specifically, we collected 377 file system vulnerabilities committed to the CVE database in the past 20 years. We characterize them from four dimensions that include why the vulnerabilities appear, how the vulnerabilities can be exploited, what consequences can arise, and h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12590v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12590v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12590v1-abstract-full" style="display: none;">
        This paper presents a systematic study on the security of modern file systems, following a vulnerability-centric perspective. Specifically, we collected 377 file system vulnerabilities committed to the CVE database in the past 20 years. We characterize them from four dimensions that include why the vulnerabilities appear, how the vulnerabilities can be exploited, what consequences can arise, and how the vulnerabilities are fixed. This way, we build a deep understanding of the attack surfaces faced by file systems, the threats imposed by the attack surfaces, and the good and bad practices in mitigating the attacks in file systems. We envision that our study will bring insights towards the future development of file systems, the enhancement of file system security, and the relevant vulnerability mitigating solutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12590v1-abstract-full').style.display = 'none'; document.getElementById('2204.12590v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12463">arXiv:2204.12463</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12463">pdf</a>, <a href="https://arxiv.org/format/2204.12463">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Focal Sparse Convolutional Networks for 3D Object Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yukang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yanwei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+J">Jiaya Jia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12463v1-abstract-short" style="display: inline;">
        Non-uniformed 3D sparse data, e.g., point clouds or voxels in different spatial positions, make contribution to the task of 3D object detection in different ways. Existing basic components in sparse convolutional networks (Sparse CNNs) process all sparse data, regardless of regular or submanifold sparse convolution. In this paper, we introduce two new modules to enhance the capability of Sparse CN&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12463v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12463v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12463v1-abstract-full" style="display: none;">
        Non-uniformed 3D sparse data, e.g., point clouds or voxels in different spatial positions, make contribution to the task of 3D object detection in different ways. Existing basic components in sparse convolutional networks (Sparse CNNs) process all sparse data, regardless of regular or submanifold sparse convolution. In this paper, we introduce two new modules to enhance the capability of Sparse CNNs, both are based on making feature sparsity learnable with position-wise importance prediction. They are focal sparse convolution (Focals Conv) and its multi-modal variant of focal sparse convolution with fusion, or Focals Conv-F for short. The new modules can readily substitute their plain counterparts in existing Sparse CNNs and be jointly trained in an end-to-end fashion. For the first time, we show that spatially learnable sparsity in sparse convolution is essential for sophisticated 3D object detection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks validate the effectiveness of our approach. Without bells and whistles, our results outperform all existing single-model entries on the nuScenes test benchmark at the paper submission time. Code and models are at https://github.com/dvlab-research/FocalsConv.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12463v1-abstract-full').style.display = 'none'; document.getElementById('2204.12463v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2022 Oral. Code is at http://github.com/dvlab-research/FocalsConv</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.11762">arXiv:2204.11762</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.11762">pdf</a>, <a href="https://arxiv.org/format/2204.11762">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MFA-DVR: Direct Volume Rendering of MFA Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jianxin Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lenz%2C+D">David Lenz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+H">Hongfeng Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peterka%2C+T">Tom Peterka</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.11762v1-abstract-short" style="display: inline;">
        3D volume rendering is widely used to reveal insightful intrinsic patterns of volumetric datasets across many domains. However, the complex structures and varying scales of datasets make generating a high-quality volume rendering results efficiently a challenging task. Multivariate functional approximation (MFA) is a new data model that addresses some of the key challenges of volume visualization.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11762v1-abstract-full').style.display = 'inline'; document.getElementById('2204.11762v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.11762v1-abstract-full" style="display: none;">
        3D volume rendering is widely used to reveal insightful intrinsic patterns of volumetric datasets across many domains. However, the complex structures and varying scales of datasets make generating a high-quality volume rendering results efficiently a challenging task. Multivariate functional approximation (MFA) is a new data model that addresses some of the key challenges of volume visualization. MFA provides high-order evaluation of values and derivatives anywhere in the spatial domain, mitigating the artifacts caused by the zero- or first-order interpolation commonly implemented in existing volume visualization algorithms. MFA&#39;s compact representation improves the space complexity for large-scale volumetric data visualization, while its uniform representation of both structured and unstructured data allows the same ray casting algorithm to be used for a variety of input data types. In this paper, we present MFA-DVR, the first direct volume rendering pipeline utilizing the MFA model, for both structured and unstructured volumetric datasets. We demonstrate improved rendering quality using MFA-DVR on both synthetic and real datasets through a comparative study with raw and compressed data. We show that MFA-DVR not only generates more faithful volume rendering results with less memory footprint, but also performs faster than traditional algorithms when rendering unstructured datasets. MFA-DVR is implemented in the existing volume rendering pipeline of the Visualization Toolkit (VTK) in order to be accessible by the scientific visualization community.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11762v1-abstract-full').style.display = 'none'; document.getElementById('2204.11762v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This is the paper submitted to VIS 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.10605">arXiv:2204.10605</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.10605">pdf</a>, <a href="https://arxiv.org/format/2204.10605">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributed stochastic projection-free solver for constrained optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+X">Xia Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+X">Xianlin Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+L">Lihua Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jie Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.10605v1-abstract-short" style="display: inline;">
        This paper proposes a distributed stochastic projection-free algorithm for large-scale constrained finite-sum optimization whose constraint set is complicated such that the projection onto the constraint set can be expensive. The global cost function is allocated to multiple agents, each of which computes its local stochastic gradients and communicates with its neighbors to solve the global proble&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10605v1-abstract-full').style.display = 'inline'; document.getElementById('2204.10605v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.10605v1-abstract-full" style="display: none;">
        This paper proposes a distributed stochastic projection-free algorithm for large-scale constrained finite-sum optimization whose constraint set is complicated such that the projection onto the constraint set can be expensive. The global cost function is allocated to multiple agents, each of which computes its local stochastic gradients and communicates with its neighbors to solve the global problem. Stochastic gradient methods enable low computational cost, while they are hard and slow to converge due to the variance caused by random sampling. To construct a convergent distributed stochastic projection-free algorithm, this paper incorporates a variance reduction technique and gradient tracking technique in the Frank-Wolfe update. We develop a sampling rule for the variance reduction technique to reduce the variance introduced by stochastic gradients. Complete and rigorous proofs show that the proposed distributed projection-free algorithm converges with a sublinear convergence rate and enjoys superior complexity guarantees for both convex and non-convex objective functions. By comparative simulations, we demonstrate the convergence and computational efficiency of the proposed algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10605v1-abstract-full').style.display = 'none'; document.getElementById('2204.10605v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09831">arXiv:2204.09831</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09831">pdf</a>, <a href="https://arxiv.org/format/2204.09831">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Chemical Physics">physics.chem-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accurate Molecular-Orbital-Based Machine Learning Energies via Unsupervised Clustering of Chemical Space
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+L">Lixue Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiace Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Miller%2C+T+F">Thomas F. Miller III</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09831v1-abstract-short" style="display: inline;">
        We introduce an unsupervised clustering algorithm to improve training efficiency and accuracy in predicting energies using molecular-orbital-based machine learning (MOB-ML). This work determines clusters via the Gaussian mixture model (GMM) in an entirely automatic manner and simplifies an earlier supervised clustering approach [J. Chem. Theory Comput., 15, 6668 (2019)] by eliminating both the nec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09831v1-abstract-full').style.display = 'inline'; document.getElementById('2204.09831v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09831v1-abstract-full" style="display: none;">
        We introduce an unsupervised clustering algorithm to improve training efficiency and accuracy in predicting energies using molecular-orbital-based machine learning (MOB-ML). This work determines clusters via the Gaussian mixture model (GMM) in an entirely automatic manner and simplifies an earlier supervised clustering approach [J. Chem. Theory Comput., 15, 6668 (2019)] by eliminating both the necessity for user-specified parameters and the training of an additional classifier. Unsupervised clustering results from GMM have the advantage of accurately reproducing chemically intuitive groupings of frontier molecular orbitals and having improved performance with an increasing number of training examples. The resulting clusters from supervised or unsupervised clustering is further combined with scalable Gaussian process regression (GPR) or linear regression (LR) to learn molecular energies accurately by generating a local regression model in each cluster. Among all four combinations of regressors and clustering methods, GMM combined with scalable exact Gaussian process regression (GMM/GPR) is the most efficient training protocol for MOB-ML. The numerical tests of molecular energy learning on thermalized datasets of drug-like molecules demonstrate the improved accuracy, transferability, and learning efficiency of GMM/GPR over not only other training protocols for MOB-ML, i.e., supervised regression-clustering combined with GPR(RC/GPR) and GPR without clustering. GMM/GPR also provide the best molecular energy predictions compared with the ones from literature on the same benchmark datasets. With a lower scaling, GMM/GPR has a 10.4-fold speedup in wall-clock training time compared with scalable exact GPR with a training size of 6500 QM7b-T molecules.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09831v1-abstract-full').style.display = 'none'; document.getElementById('2204.09831v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">28 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09274">arXiv:2204.09274</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09274">pdf</a>, <a href="https://arxiv.org/format/2204.09274">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Causality-based Neural Network Repair
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+B">Bing Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jun Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pham%2C+H+L">Hong Long Pham</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+J">Jie Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09274v1-abstract-short" style="display: inline;">
        Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09274v1-abstract-full').style.display = 'inline'; document.getElementById('2204.09274v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09274v1-abstract-full" style="display: none;">
        Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor. The goal is to construct a neural network that satisfies the property by (minimally) adjusting the given neural network&#39;s parameters (i.e., weights). Specifically, we propose CARE (\textbf{CA}usality-based \textbf{RE}pair), a causality-based neural network repair technique that 1) performs causality-based fault localization to identify the `guilty&#39; neurons and 2) optimizes the parameters of the identified neurons to reduce the misbehavior. We have empirically evaluated CARE on various tasks such as backdoor removal, neural network repair for fairness and safety properties. Our experiment results show that CARE is able to repair all neural networks efficiently and effectively. For fairness repair tasks, CARE successfully improves fairness by $61.91\%$ on average. For backdoor removal tasks, CARE reduces the attack success rate from over $98\%$ to less than $1\%$. For safety property repair tasks, CARE reduces the property violation rate to less than $1\%$. Results also show that thanks to the causality-based fault localization, CARE&#39;s repair focuses on the misbehavior and preserves the accuracy of the neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09274v1-abstract-full').style.display = 'none'; document.getElementById('2204.09274v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08686">arXiv:2204.08686</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08686">pdf</a>, <a href="https://arxiv.org/ps/2204.08686">ps</a>, <a href="https://arxiv.org/format/2204.08686">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio-Visual Wake Word Spotting System For MISP Challenge 2021
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+Y">Yanguang Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jianwei Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+Y">Yang Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+S">Shuaijiang Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mei%2C+C">Chaoyang Mei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+T">Tingwei Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+S">Shuran Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+C">Chuandong Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+W">Wei Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xiangang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+S">Shuran Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+C">Chuandong Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+W">Wei Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xiangang Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08686v2-abstract-short" style="display: inline;">
        This paper presents the details of our system designed for the Task 1 of Multimodal Information Based Speech Processing (MISP) Challenge 2021. The purpose of Task 1 is to leverage both audio and video information to improve the environmental robustness of far-field wake word spotting. In the proposed system, firstly, we take advantage of speech enhancement algorithms such as beamforming and weight&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08686v2-abstract-full').style.display = 'inline'; document.getElementById('2204.08686v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08686v2-abstract-full" style="display: none;">
        This paper presents the details of our system designed for the Task 1 of Multimodal Information Based Speech Processing (MISP) Challenge 2021. The purpose of Task 1 is to leverage both audio and video information to improve the environmental robustness of far-field wake word spotting. In the proposed system, firstly, we take advantage of speech enhancement algorithms such as beamforming and weighted prediction error (WPE) to address the multi-microphone conversational audio. Secondly, several data augmentation techniques are applied to simulate a more realistic far-field scenario. For the video information, the provided region of interest (ROI) is used to obtain visual representation. Then the multi-layer CNN is proposed to learn audio and visual representations, and these representations are fed into our two-branch attention-based network which can be employed for fusion, such as transformer and conformed. The focal loss is used to fine-tune the model and improve the performance significantly. Finally, multiple trained models are integrated by casting vote to achieve our final 0.091 score.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08686v2-abstract-full').style.display = 'none'; document.getElementById('2204.08686v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICASSP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08332">arXiv:2204.08332</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08332">pdf</a>, <a href="https://arxiv.org/format/2204.08332">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Z">Ziwei Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Youwei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+S">Shen Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+L">Lei Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Q">Qi Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+Z">Zhihong Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+H">Haoqiang Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shuaicheng Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08332v2-abstract-short" style="display: inline;">
        This work addresses the Burst Super-Resolution (BurstSR) task using a new architecture, which requires restoring a high-quality image from a sequence of noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can significantly improve the capability of extracting inter-frame information and reconstructio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08332v2-abstract-full').style.display = 'inline'; document.getElementById('2204.08332v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08332v2-abstract-full" style="display: none;">
        This work addresses the Burst Super-Resolution (BurstSR) task using a new architecture, which requires restoring a high-quality image from a sequence of noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can significantly improve the capability of extracting inter-frame information and reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin Transformer Blocks and Groups as our main backbone. More specifically, we combine optical flows and deformable convolutions, hence our BSRT can handle misalignment and aggregate the potential texture information in multi-frames more efficiently. In addition, our Transformer-based structure can capture long-range dependency to further improve the performance. The evaluation on both synthetic and real-world tracks demonstrates that our approach achieves a new state-of-the-art in BurstSR task. Further, our BSRT wins the championship in the NTIRE2022 Burst Super-Resolution Challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08332v2-abstract-full').style.display = 'none'; document.getElementById('2204.08332v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPRW, Winner method in NTIRE 2022 Burst Super-Resolution Challenge Real-World Track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07913">arXiv:2204.07913</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07913">pdf</a>, <a href="https://arxiv.org/format/2204.07913">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What Goes beyond Multi-modal Fusion in One-stage Referring Expression Comprehension: An Empirical Study
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+G">Gen Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yiyi Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiamu Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+S">Shubin Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+X">Xiaoshuai Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+Q">Qixiang Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yongjian Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+R">Rongrong Ji</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07913v1-abstract-short" style="display: inline;">
        Most of the existing work in one-stage referring expression comprehension (REC) mainly focuses on multi-modal fusion and reasoning, while the influence of other factors in this task lacks in-depth exploration. To fill this gap, we conduct an empirical study in this paper. Concretely, we first build a very simple REC network called SimREC, and ablate 42 candidate designs/settings, which covers the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07913v1-abstract-full').style.display = 'inline'; document.getElementById('2204.07913v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07913v1-abstract-full" style="display: none;">
        Most of the existing work in one-stage referring expression comprehension (REC) mainly focuses on multi-modal fusion and reasoning, while the influence of other factors in this task lacks in-depth exploration. To fill this gap, we conduct an empirical study in this paper. Concretely, we first build a very simple REC network called SimREC, and ablate 42 candidate designs/settings, which covers the entire process of one-stage REC from network design to model training. Afterwards, we conduct over 100 experimental trials on three benchmark datasets of REC. The extensive experimental results not only show the key factors that affect REC performance in addition to multi-modal fusion, e.g., multi-scale features and data augmentation, but also yield some findings that run counter to conventional understanding. For example, as a vision and language (V&amp;L) task, REC does is less impacted by language prior. In addition, with a proper combination of these findings, we can improve the performance of SimREC by a large margin, e.g., +27.12% on RefCOCO+, which outperforms all existing REC methods. But the most encouraging finding is that with much less training overhead and parameters, SimREC can still achieve better performance than a set of large-scale pre-trained models, e.g., UNITER and VILLA, portraying the special role of REC in existing V&amp;L research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07913v1-abstract-full').style.display = 'none'; document.getElementById('2204.07913v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06837">arXiv:2204.06837</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06837">pdf</a>, <a href="https://arxiv.org/format/2204.06837">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Modeling Indirect Illumination for Inverse Rendering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yuanqing Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiaming Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+X">Xingyi He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+H">Huan Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+R">Rongfei Jia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+X">Xiaowei Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06837v1-abstract-short" style="display: inline;">
        Recent advances in implicit neural representations and differentiable rendering make it possible to simultaneously recover the geometry and materials of an object from multi-view RGB images captured under unknown static illumination. Despite the promising results achieved, indirect illumination is rarely modeled in previous methods, as it requires expensive recursive path tracing which makes the i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06837v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06837v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06837v1-abstract-full" style="display: none;">
        Recent advances in implicit neural representations and differentiable rendering make it possible to simultaneously recover the geometry and materials of an object from multi-view RGB images captured under unknown static illumination. Despite the promising results achieved, indirect illumination is rarely modeled in previous methods, as it requires expensive recursive path tracing which makes the inverse rendering computationally intractable. In this paper, we propose a novel approach to efficiently recovering spatially-varying indirect illumination. The key insight is that indirect illumination can be conveniently derived from the neural radiance field learned from input images instead of being estimated jointly with direct illumination and materials. By properly modeling the indirect illumination and visibility of direct illumination, interreflection- and shadow-free albedo can be recovered. The experiments on both synthetic and real data demonstrate the superior performance of our approach compared to previous work and its capability to synthesize realistic renderings under novel viewpoints and illumination. Our code and data are available at https://zju3dv.github.io/invrender/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06837v1-abstract-full').style.display = 'none'; document.getElementById('2204.06837v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06501">arXiv:2204.06501</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06501">pdf</a>, <a href="https://arxiv.org/format/2204.06501">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Clinical trial site matching with improved diversity using fair policy learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Srinivasa%2C+R+S">Rakshith S Srinivasa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+C">Cheng Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Theodorou%2C+B">Brandon Theodorou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Spaeder%2C+J">Jeffrey Spaeder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+C">Cao Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Glass%2C+L">Lucas Glass</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jimeng Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06501v1-abstract-short" style="display: inline;">
        The ongoing pandemic has highlighted the importance of reliable and efficient clinical trials in healthcare. Trial sites, where the trials are conducted, are chosen mainly based on feasibility in terms of medical expertise and access to a large group of patients. More recently, the issue of diversity and inclusion in clinical trials is gaining importance. Different patient groups may experience th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06501v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06501v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06501v1-abstract-full" style="display: none;">
        The ongoing pandemic has highlighted the importance of reliable and efficient clinical trials in healthcare. Trial sites, where the trials are conducted, are chosen mainly based on feasibility in terms of medical expertise and access to a large group of patients. More recently, the issue of diversity and inclusion in clinical trials is gaining importance. Different patient groups may experience the effects of a medical drug/ treatment differently and hence need to be included in the clinical trials. These groups could be based on ethnicity, co-morbidities, age, or economic factors. Thus, designing a method for trial site selection that accounts for both feasibility and diversity is a crucial and urgent goal. In this paper, we formulate this problem as a ranking problem with fairness constraints. Using principles of fairness in machine learning, we learn a model that maps a clinical trial description to a ranked list of potential trial sites. Unlike existing fairness frameworks, the group membership of each trial site is non-binary: each trial site may have access to patients from multiple groups. We propose fairness criteria based on demographic parity to address such a multi-group membership scenario. We test our method on 480 real-world clinical trials and show that our model results in a list of potential trial sites that provides access to a diverse set of patients while also ensuing a high number of enrolled patients.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06501v1-abstract-full').style.display = 'none'; document.getElementById('2204.06501v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          J.3; I.2.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06171">arXiv:2204.06171</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06171">pdf</a>, <a href="https://arxiv.org/format/2204.06171">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Traffic Advisors: Distributed, Multi-view Traffic Prediction for Smart Cities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiankai Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kousik%2C+S">Shreyas Kousik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fridovich-Keil%2C+D">David Fridovich-Keil</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schwager%2C+M">Mac Schwager</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06171v1-abstract-short" style="display: inline;">
        Connected and Autonomous Vehicles (CAVs) are becoming more widely deployed, but it is unclear how to best deploy smart infrastructure to maximize their capabilities. One key challenge is to ensure CAVs can reliably perceive other agents, especially occluded ones. A further challenge is the desire for smart infrastructure to be autonomous and readily scalable to wide-area deployments, similar to mo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06171v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06171v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06171v1-abstract-full" style="display: none;">
        Connected and Autonomous Vehicles (CAVs) are becoming more widely deployed, but it is unclear how to best deploy smart infrastructure to maximize their capabilities. One key challenge is to ensure CAVs can reliably perceive other agents, especially occluded ones. A further challenge is the desire for smart infrastructure to be autonomous and readily scalable to wide-area deployments, similar to modern traffic lights. The present work proposes the Self-Supervised Traffic Advisor (SSTA), an infrastructure edge device concept that leverages self-supervised video prediction in concert with a communication and co-training framework to enable autonomously predicting traffic throughout a smart city. An SSTA is a statically-mounted camera that overlooks an intersection or area of complex traffic flow that predicts traffic flow as future video frames and learns to communicate with neighboring SSTAs to enable predicting traffic before it appears in the Field of View (FOV). The proposed framework aims at three goals: (1) inter-device communication to enable high-quality predictions, (2) scalability to an arbitrary number of devices, and (3) lifelong online learning to ensure adaptability to changing circumstances. Finally, an SSTA can broadcast its future predicted video frames directly as information for CAVs to run their own post-processing for the purpose of control.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06171v1-abstract-full').style.display = 'none'; document.getElementById('2204.06171v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06113">arXiv:2204.06113</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06113">pdf</a>, <a href="https://arxiv.org/format/2204.06113">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Liuer Mihou: A Practical Framework for Generating and Evaluating Grey-box Adversarial Attacks against NIDS
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+K">Ke He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+D+D">Dan Dongseong Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jing Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yoo%2C+J+D">Jeong Do Yoo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+Y+H">Young Hun Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H+K">Huy Kang Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06113v1-abstract-short" style="display: inline;">
        Due to its high expressiveness and speed, Deep Learning (DL) has become an increasingly popular choice as the detection algorithm for Network-based Intrusion Detection Systems (NIDSes). Unfortunately, DL algorithms are vulnerable to adversarial examples that inject imperceptible modifications to the input and cause the DL algorithm to misclassify the input. Existing adversarial attacks in the NIDS&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06113v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06113v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06113v1-abstract-full" style="display: none;">
        Due to its high expressiveness and speed, Deep Learning (DL) has become an increasingly popular choice as the detection algorithm for Network-based Intrusion Detection Systems (NIDSes). Unfortunately, DL algorithms are vulnerable to adversarial examples that inject imperceptible modifications to the input and cause the DL algorithm to misclassify the input. Existing adversarial attacks in the NIDS domain often manipulate the traffic features directly, which hold no practical significance because traffic features cannot be replayed in a real network. It remains a research challenge to generate practical and evasive adversarial attacks.
  This paper presents the Liuer Mihou attack that generates practical and replayable adversarial network packets that can bypass anomaly-based NIDS deployed in the Internet of Things (IoT) networks. The core idea behind Liuer Mihou is to exploit adversarial transferability and generate adversarial packets on a surrogate NIDS constrained by predefined mutation operations to ensure practicality. We objectively analyse the evasiveness of Liuer Mihou against four ML-based algorithms (LOF, OCSVM, RRCF, and SOM) and the state-of-the-art NIDS, Kitsune. From the results of our experiment, we gain valuable insights into necessary conditions on the adversarial transferability of anomaly detection algorithms. Going beyond a theoretical setting, we replay the adversarial attack in a real IoT testbed to examine the practicality of Liuer Mihou. Furthermore, we demonstrate that existing feature-level adversarial defence cannot defend against Liuer Mihou and constructively criticise the limitations of feature-level adversarial defences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06113v1-abstract-full').style.display = 'none'; document.getElementById('2204.06113v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 8 figures, planning on submitting to ACM CCS 2022</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.05052">arXiv:2204.05052</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.05052">pdf</a>, <a href="https://arxiv.org/format/2204.05052">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Novel Channel Identification Architecture for mmWave Systems Based on Eigen Features
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yibin Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jinlong Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gui%2C+G">Guan Gui</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gacanin%2C+H">Haris Gacanin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Adachi%2C+F">Fumiyuki Adachi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.05052v1-abstract-short" style="display: inline;">
        Millimeter wave (mmWave) communication technique has been developed rapidly because of many advantages of high speed, large bandwidth, and ultra-low delay. However, mmWave communications systems suffer from fast fading and frequent blocking. Hence, the ideal communication environment for mmWave is line of sight (LOS) channel. To improve the efficiency and capacity of mmWave system, and to better b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05052v1-abstract-full').style.display = 'inline'; document.getElementById('2204.05052v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.05052v1-abstract-full" style="display: none;">
        Millimeter wave (mmWave) communication technique has been developed rapidly because of many advantages of high speed, large bandwidth, and ultra-low delay. However, mmWave communications systems suffer from fast fading and frequent blocking. Hence, the ideal communication environment for mmWave is line of sight (LOS) channel. To improve the efficiency and capacity of mmWave system, and to better build the Internet of Everything (IoE) service network, this paper focuses on the channel identification technique in line-of- sight (LOS) and non-LOS (NLOS) environments. Considering the limited computing ability of user equipments (UEs), this paper proposes a novel channel identification architecture based on eigen features, i.e. eigenmatrix and eigenvector (EMEV) of channel state information (CSI). Furthermore, this paper explores clustered delay line (CDL) channel identification with mmWave, which is defined by the 3rd generation partnership project (3GPP). Ther experimental results show that the EMEV based scheme can achieve identification accuracy of 99.88% assuming perfect CSI. In the robustness test, the maximum noise can be tolerated is SNR= 16 dB, with the threshold acc \geq 95%. What is more, the novel architecture based on EMEV feature will reduce the comprehensive overhead by about 90%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05052v1-abstract-full').style.display = 'none'; document.getElementById('2204.05052v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04918">arXiv:2204.04918</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04918">pdf</a>, <a href="https://arxiv.org/format/2204.04918">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When NAS Meets Trees: An Efficient Algorithm for Neural Architecture Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+G">Guocheng Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xuanyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+G">Guohao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+C">Chen Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yukang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ghanem%2C+B">Bernard Ghanem</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04918v1-abstract-short" style="display: inline;">
        The key challenge in neural architecture search (NAS) is designing how to explore wisely in the huge search space. We propose a new NAS method called TNAS (NAS with trees), which improves search efficiency by exploring only a small number of architectures while also achieving a higher search accuracy. TNAS introduces an architecture tree and a binary operation tree, to factorize the search space a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04918v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04918v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04918v1-abstract-full" style="display: none;">
        The key challenge in neural architecture search (NAS) is designing how to explore wisely in the huge search space. We propose a new NAS method called TNAS (NAS with trees), which improves search efficiency by exploring only a small number of architectures while also achieving a higher search accuracy. TNAS introduces an architecture tree and a binary operation tree, to factorize the search space and substantially reduce the exploration size. TNAS performs a modified bi-level Breadth-First Search in the proposed trees to discover a high-performance architecture. Impressively, TNAS finds the global optimal architecture on CIFAR-10 with test accuracy of 94.37\% in four GPU hours in NAS-Bench-201. The average test accuracy is 94.35\%, which outperforms the state-of-the-art. Code is available at: \url{https://github.com/guochengqian/TNAS}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04918v1-abstract-full').style.display = 'none'; document.getElementById('2204.04918v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages, accepted at CVPR Workshop 2022 (ECV2022)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04676">arXiv:2204.04676</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04676">pdf</a>, <a href="https://arxiv.org/format/2204.04676">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Simple Baselines for Image Restoration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+L">Liangyu Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+X">Xiaojie Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04676v1-abstract-short" style="display: inline;">
        Although there have been significant advances in the field of image restoration recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04676v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04676v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04676v1-abstract-full" style="display: none;">
        Although there have been significant advances in the field of image restoration recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that the nonlinear activation functions, e.g. Sigmoid, ReLU, GELU, Softmax, etc. are not necessary: they could be replaced by multiplication or removed. Thus, we derive a Nonlinear Activation Free Network, namely NAFNet, from the baseline. SOTA results are achieved on various challenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring), exceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs; 40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28 dB with less than half of its computational costs. The code and the pretrained models will be released at https://github.com/megvii-research/NAFNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04676v1-abstract-full').style.display = 'none'; document.getElementById('2204.04676v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Image Restoration</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04272">arXiv:2204.04272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04272">pdf</a>, <a href="https://arxiv.org/format/2204.04272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Matrix Syncer -- A Multi-chain Data Aggregator For Supporting Blockchain-based Metaverses
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+X">Xinyao Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+Y">Yi Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jinghan Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+B">Bohao Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rehak%2C+K+D">Kyle D. Rehak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shuyi Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04272v1-abstract-short" style="display: inline;">
        Due to the rising complexity of the metaverse&#39;s business logic and the low-latency nature of the metaverse, developers typically encounter the challenge of effectively reading, writing, and retrieving historical on-chain data in order to facilitate their functional implementations at scale. While it is true that accessing blockchain states is simple, more advanced real-world operations such as sea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04272v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04272v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04272v1-abstract-full" style="display: none;">
        Due to the rising complexity of the metaverse&#39;s business logic and the low-latency nature of the metaverse, developers typically encounter the challenge of effectively reading, writing, and retrieving historical on-chain data in order to facilitate their functional implementations at scale. While it is true that accessing blockchain states is simple, more advanced real-world operations such as search, aggregation, and conditional filtering are not available when interacting directly with blockchain networks, particularly when dealing with requirements for on-chain event reflection. We offer Matrix Syncer, the ultimate middleware that bridges the data access gap between blockchains and end-user applications. Matrix Syncer is designed to facilitate the consolidation of on-chain information into a distributed data warehouse while also enabling customized on-chain state transformation for a scalable storage, access, and retrieval. It offers a unified layer for both on- and off-chain state, as well as a fast and flexible atomic query. Matrix Syncer is easily incorporated into any infrastructure to aggregate data from various blockchains concurrently, such as Ethereum and Flow. The system has been deployed to support several metaverse projects with a total value of more than $15 million USD.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04272v1-abstract-full').style.display = 'none'; document.getElementById('2204.04272v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.02842">arXiv:2204.02842</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.02842">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and Development
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Luxem%2C+K">Kevin Luxem</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J+J">Jennifer J. Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bradley%2C+S+P">Sean P. Bradley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Krishnan%2C+K">Keerthi Krishnan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pereira%2C+T+D">Talmo D. Pereira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yttri%2C+E+A">Eric A. Yttri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zimmermann%2C+J">Jan Zimmermann</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Laubach%2C+M">Mark Laubach</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.02842v1-abstract-short" style="display: inline;">
        Recently developed methods for video analysis, especially models for pose estimation and behavior classification, are transforming behavioral quantification to be more precise, scalable, and reproducible in fields such as neuroscience and ethology. These tools overcome long-standing limitations of manual scoring of video frames and traditional &#34;center of mass&#34; tracking algorithms to enable video a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.02842v1-abstract-full').style.display = 'inline'; document.getElementById('2204.02842v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.02842v1-abstract-full" style="display: none;">
        Recently developed methods for video analysis, especially models for pose estimation and behavior classification, are transforming behavioral quantification to be more precise, scalable, and reproducible in fields such as neuroscience and ethology. These tools overcome long-standing limitations of manual scoring of video frames and traditional &#34;center of mass&#34; tracking algorithms to enable video analysis at scale. The expansion of open-source tools for video acquisition and analysis has led to new experimental approaches to understand behavior. Here, we review currently available open source tools for video analysis, how to set them up in a lab that is new to video recording methods, and some issues that should be addressed by developers and advanced users, including the need to openly share datasets and code, how to compare algorithms and their parameters, and the need for documentation and community-wide standards. We hope to encourage more widespread use and continued development of the tools. They have tremendous potential for accelerating scientific progress for understanding the brain and behavior.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.02842v1-abstract-full').style.display = 'none'; document.getElementById('2204.02842v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 2 figures, 2 tables; this is a commentary on video methods for analyzing behavior in animals that emerged from a working group organized by the OpenBehavior project (openbehavior.com)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.01707">arXiv:2204.01707</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.01707">pdf</a>, <a href="https://arxiv.org/format/2204.01707">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Heterogeneous Autoencoder Empowered by Quadratic Neurons
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+J">Jing-Xiao Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hou%2C+B">Bo-Jian Hou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+H">Hang-Cheng Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Hao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+J">Jianwei Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jinwei Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shiping Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+F">Feng-Lei Fan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.01707v1-abstract-short" style="display: inline;">
        Inspired by the complexity and diversity of biological neurons, a quadratic neuron is proposed to replace the inner product in the current neuron with a simplified quadratic function. Employing such a novel type of neurons offers a new perspective on developing deep learning. When analyzing quadratic neurons, we find that there exists a function such that a heterogeneous network can approximate it&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01707v1-abstract-full').style.display = 'inline'; document.getElementById('2204.01707v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.01707v1-abstract-full" style="display: none;">
        Inspired by the complexity and diversity of biological neurons, a quadratic neuron is proposed to replace the inner product in the current neuron with a simplified quadratic function. Employing such a novel type of neurons offers a new perspective on developing deep learning. When analyzing quadratic neurons, we find that there exists a function such that a heterogeneous network can approximate it well with a polynomial number of neurons but a purely conventional or quadratic network needs an exponential number of neurons to achieve the same level of error. Encouraged by this inspiring theoretical result on heterogeneous networks, we directly integrate conventional and quadratic neurons in an autoencoder to make a new type of heterogeneous autoencoders. Anomaly detection experiments confirm that heterogeneous autoencoders perform competitively compared to other state-of-the-art models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01707v1-abstract-full').style.display = 'none'; document.getElementById('2204.01707v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.16650">arXiv:2203.16650</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.16650">pdf</a>, <a href="https://arxiv.org/ps/2203.16650">ps</a>, <a href="https://arxiv.org/format/2203.16650">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Beamforming for Localization-Aided Millimeter Wave Communication Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Junchang Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+S">Shuai Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+S">Shiyin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+R">Ruixin Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Min%2C+M">Minghui Min</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seco-Granados%2C+G">Gonzalo Seco-Granados</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.16650v1-abstract-short" style="display: inline;">
        In this letter, we investigate a robust beamforming problem for localization-aided millimeter wave (mmWave) communication systems. To handle this problem, we propose a novel restriction and relaxation (R&amp;R) method. The proposed R&amp;R method aims at minimizing the total transmit power while the positioning error follows a Gaussian distribution. Specifically, in the restriction phase of R&amp;R, the proba&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.16650v1-abstract-full').style.display = 'inline'; document.getElementById('2203.16650v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.16650v1-abstract-full" style="display: none;">
        In this letter, we investigate a robust beamforming problem for localization-aided millimeter wave (mmWave) communication systems. To handle this problem, we propose a novel restriction and relaxation (R&amp;R) method. The proposed R&amp;R method aims at minimizing the total transmit power while the positioning error follows a Gaussian distribution. Specifically, in the restriction phase of R&amp;R, the probabilistic constraint is transformed into the deterministic form by using the Bernsteintype inequality. In the relaxation phase of R&amp;R, the non-convex optimization problem is reformulated into a convex semidefinite program (SDP) by using semidefinite relaxation (SDR) and firstorder Taylor expansion methods. To the best of our knowledge, we first consider the impact of the distribution of the positioning error on the channel state information (CSI), which further influences the data rate. Numerical results present the trade-off of the beamforming between the communication and positioning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.16650v1-abstract-full').style.display = 'none'; document.getElementById('2203.16650v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15613">arXiv:2203.15613</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15613">pdf</a>, <a href="https://arxiv.org/format/2203.15613">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Latency for CTC-Based Streaming Automatic Speech Recognition With Emformer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jingyu Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+G">Guiping Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+D">Dinghao Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Baoxiang Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15613v1-abstract-short" style="display: inline;">
        An inferior performance of the streaming automatic speech recognition models versus non-streaming model is frequently seen due to the absence of future context. In order to improve the performance of the streaming model and reduce the computational complexity, a frame-level model using efficient augment memory transformer block and dynamic latency training method is employed for streaming automati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15613v1-abstract-full').style.display = 'inline'; document.getElementById('2203.15613v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15613v1-abstract-full" style="display: none;">
        An inferior performance of the streaming automatic speech recognition models versus non-streaming model is frequently seen due to the absence of future context. In order to improve the performance of the streaming model and reduce the computational complexity, a frame-level model using efficient augment memory transformer block and dynamic latency training method is employed for streaming automatic speech recognition in this paper. The long-range history context is stored into the augment memory bank as a complement to the limited history context used in the encoder. Key and value are cached by a cache mechanism and reused for next chunk to reduce computation. Afterwards, a dynamic latency training method is proposed to obtain better performance and support low and high latency inference simultaneously. Our experiments are conducted on benchmark 960h LibriSpeech data set. With an average latency of 640ms, our model achieves a relative WER reduction of 6.0% on test-clean and 3.0% on test-other versus the truncate chunk-wise Transformer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15613v1-abstract-full').style.display = 'none'; document.getElementById('2203.15613v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, submitted to interspeech 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15609">arXiv:2203.15609</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15609">pdf</a>, <a href="https://arxiv.org/format/2203.15609">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Locality Matters: A Locality-Biased Linear Attention for Automatic Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jingyu Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+G">Guiping Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+D">Dinghao Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Baoxiang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Y">Yiran Zhong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15609v1-abstract-short" style="display: inline;">
        Conformer has shown a great success in automatic speech recognition (ASR) on many public benchmarks. One of its crucial drawbacks is the quadratic time-space complexity with respect to the input sequence length, which prohibits the model to scale-up as well as process longer input audio sequences. To solve this issue, numerous linear attention methods have been proposed. However, these methods oft&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15609v1-abstract-full').style.display = 'inline'; document.getElementById('2203.15609v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15609v1-abstract-full" style="display: none;">
        Conformer has shown a great success in automatic speech recognition (ASR) on many public benchmarks. One of its crucial drawbacks is the quadratic time-space complexity with respect to the input sequence length, which prohibits the model to scale-up as well as process longer input audio sequences. To solve this issue, numerous linear attention methods have been proposed. However, these methods often have limited performance on ASR as they treat tokens equally in modeling, neglecting the fact that the neighbouring tokens are often more connected than the distanced tokens. In this paper, we take this fact into account and propose a new locality-biased linear attention for Conformer. It not only achieves higher accuracy than the vanilla Conformer, but also enjoys linear space-time computational complexity. To be specific, we replace the softmax attention with a locality-biased linear attention (LBLA) mechanism in Conformer blocks. The LBLA contains a kernel function to ensure the linear complexities and a cosine reweighing matrix to impose more weights on neighbouring tokens. Extensive experiments on the LibriSpeech corpus show that by introducing this locality bias to the Conformer, our method achieves a lower word error rate with more than 22% inference speed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15609v1-abstract-full').style.display = 'none'; document.getElementById('2203.15609v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, submitted to interspeech 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15537">arXiv:2203.15537</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15537">pdf</a>, <a href="https://arxiv.org/ps/2203.15537">ps</a>, <a href="https://arxiv.org/format/2203.15537">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Metric Learning for Audio-Text Cross-Modal Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mei%2C+X">Xinhao Mei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xubo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jianyuan Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Plumbley%2C+M+D">Mark D. Plumbley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wenwu Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15537v2-abstract-short" style="display: inline;">
        Audio-text retrieval aims at retrieving a target audio clip or caption from a pool of candidates given a query in another modality. Solving such cross-modal retrieval task is challenging because it not only requires learning robust feature representations for both modalities, but also requires capturing the fine-grained alignment between these two modalities. Existing cross-modal retrieval models&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15537v2-abstract-full').style.display = 'inline'; document.getElementById('2203.15537v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15537v2-abstract-full" style="display: none;">
        Audio-text retrieval aims at retrieving a target audio clip or caption from a pool of candidates given a query in another modality. Solving such cross-modal retrieval task is challenging because it not only requires learning robust feature representations for both modalities, but also requires capturing the fine-grained alignment between these two modalities. Existing cross-modal retrieval models are mostly optimized by metric learning objectives as both of them attempt to map data to an embedding space, where similar data are close together and dissimilar data are far apart. Unlike other cross-modal retrieval tasks such as image-text and video-text retrievals, audio-text retrieval is still an unexplored task. In this work, we aim to study the impact of different metric learning objectives on the audio-text retrieval task. We present an extensive evaluation of popular metric learning objectives on the AudioCaps and Clotho datasets. We demonstrate that NT-Xent loss adapted from self-supervised learning shows stable performance across different datasets and training settings, and outperforms the popular triplet-based losses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15537v2-abstract-full').style.display = 'none'; document.getElementById('2203.15537v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, submitted to InterSpeech2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15334">arXiv:2203.15334</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15334">pdf</a>, <a href="https://arxiv.org/format/2203.15334">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AnyFace: Free-style Text-to-Face Synthesis and Manipulation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jianxin Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Q">Qiyao Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Qi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+M">Muyi Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+M">Min Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+Z">Zhenan Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15334v1-abstract-short" style="display: inline;">
        Existing text-to-image synthesis methods generally are only applicable to words in the training dataset. However, human faces are so variable to be described with limited words. So this paper proposes the first free-style text-to-face method namely AnyFace enabling much wider open world applications such as metaverse, social media, cosmetics, forensics, etc. AnyFace has a novel two-stream framewor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15334v1-abstract-full').style.display = 'inline'; document.getElementById('2203.15334v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15334v1-abstract-full" style="display: none;">
        Existing text-to-image synthesis methods generally are only applicable to words in the training dataset. However, human faces are so variable to be described with limited words. So this paper proposes the first free-style text-to-face method namely AnyFace enabling much wider open world applications such as metaverse, social media, cosmetics, forensics, etc. AnyFace has a novel two-stream framework for face image synthesis and manipulation given arbitrary descriptions of the human face. Specifically, one stream performs text-to-face generation and the other conducts face image reconstruction. Facial text and image features are extracted using the CLIP (Contrastive Language-Image Pre-training) encoders. And a collaborative Cross Modal Distillation (CMD) module is designed to align the linguistic and visual features across these two streams. Furthermore, a Diverse Triplet Loss (DT loss) is developed to model fine-grained features and improve facial diversity. Extensive experiments on Multi-modal CelebA-HQ and CelebAText-HQ demonstrate significant advantages of AnyFace over state-of-the-art methods. AnyFace can achieve high-quality, high-resolution, and high-diversity face synthesis and manipulation results without any constraints on the number and content of input captions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15334v1-abstract-full').style.display = 'none'; document.getElementById('2203.15334v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15325">arXiv:2203.15325</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15325">pdf</a>, <a href="https://arxiv.org/format/2203.15325">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Single Image Dehazing Based on Consistent and Contrast-Assisted Reconstruction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+D">De Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+D">Dingwen Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+N">Nannan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+X">Xinbo Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiande Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15325v1-abstract-short" style="display: inline;">
        Single image dehazing as a fundamental low-level vision task, is essential for the development of robust intelligent surveillance system. In this paper, we make an early effort to consider dehazing robustness under variational haze density, which is a realistic while under-studied problem in the research filed of singe image dehazing. To properly address this problem, we propose a novel density-va&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15325v1-abstract-full').style.display = 'inline'; document.getElementById('2203.15325v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15325v1-abstract-full" style="display: none;">
        Single image dehazing as a fundamental low-level vision task, is essential for the development of robust intelligent surveillance system. In this paper, we make an early effort to consider dehazing robustness under variational haze density, which is a realistic while under-studied problem in the research filed of singe image dehazing. To properly address this problem, we propose a novel density-variational learning framework to improve the robustness of the image dehzing model assisted by a variety of negative hazy images, to better deal with various complex hazy scenarios. Specifically, the dehazing network is optimized under the consistency-regularized framework with the proposed Contrast-Assisted Reconstruction Loss (CARL). The CARL can fully exploit the negative information to facilitate the traditional positive-orient dehazing objective function, by squeezing the dehazed image to its clean target from different directions. Meanwhile, the consistency regularization keeps consistent outputs given multi-level hazy images, thus improving the model robustness. Extensive experimental results on two synthetic and three real-world datasets demonstrate that our method significantly surpasses the state-of-the-art approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15325v1-abstract-full').style.display = 'none'; document.getElementById('2203.15325v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14628">arXiv:2203.14628</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14628">pdf</a>, <a href="https://arxiv.org/format/2203.14628">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FS6D: Few-Shot 6D Pose Estimation of Novel Objects
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Y">Yisheng He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+H">Haoqiang Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Q">Qifeng Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14628v1-abstract-short" style="display: inline;">
        6D object pose estimation networks are limited in their capability to scale to large numbers of object instances due to the close-set assumption and their reliance on high-fidelity object CAD models. In this work, we study a new open set problem; the few-shot 6D object poses estimation: estimating the 6D pose of an unknown object by a few support views without extra training. To tackle the problem&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14628v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14628v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14628v1-abstract-full" style="display: none;">
        6D object pose estimation networks are limited in their capability to scale to large numbers of object instances due to the close-set assumption and their reliance on high-fidelity object CAD models. In this work, we study a new open set problem; the few-shot 6D object poses estimation: estimating the 6D pose of an unknown object by a few support views without extra training. To tackle the problem, we point out the importance of fully exploring the appearance and geometric relationship between the given support views and query scene patches and propose a dense prototypes matching framework by extracting and matching dense RGBD prototypes with transformers. Moreover, we show that the priors from diverse appearances and shapes are crucial to the generalization capability under the problem setting and thus propose a large-scale RGBD photorealistic dataset (ShapeNet6D) for network pre-training. A simple and effective online texture blending approach is also introduced to eliminate the domain gap from the synthesis dataset, which enriches appearance diversity at a low cost. Finally, we discuss possible solutions to this problem and establish benchmarks on popular datasets to facilitate future research. The project page is at \url{https://fs6d.github.io/}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14628v1-abstract-full').style.display = 'none'; document.getElementById('2203.14628v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14607">arXiv:2203.14607</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14607">pdf</a>, <a href="https://arxiv.org/ps/2203.14607">ps</a>, <a href="https://arxiv.org/format/2203.14607">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Boosting Black-Box Adversarial Attacks with Meta Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+J">Junjie Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Gang Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14607v1-abstract-short" style="display: inline;">
        Deep neural networks (DNNs) have achieved remarkable success in diverse fields. However, it has been demonstrated that DNNs are very vulnerable to adversarial examples even in black-box settings. A large number of black-box attack methods have been proposed to in the literature. However, those methods usually suffer from low success rates and large query counts, which cannot fully satisfy practica&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14607v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14607v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14607v1-abstract-full" style="display: none;">
        Deep neural networks (DNNs) have achieved remarkable success in diverse fields. However, it has been demonstrated that DNNs are very vulnerable to adversarial examples even in black-box settings. A large number of black-box attack methods have been proposed to in the literature. However, those methods usually suffer from low success rates and large query counts, which cannot fully satisfy practical purposes. In this paper, we propose a hybrid attack method which trains meta adversarial perturbations (MAPs) on surrogate models and performs black-box attacks by estimating gradients of the models. Our method uses the meta adversarial perturbation as an initialization and subsequently trains any black-box attack method for several epochs. Furthermore, the MAPs enjoy favorable transferability and universality, in the sense that they can be employed to boost performance of other black-box adversarial attack methods. Extensive experiments demonstrate that our method can not only improve the attack success rates, but also reduces the number of queries compared to other methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14607v1-abstract-full').style.display = 'none'; document.getElementById('2203.14607v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14500">arXiv:2203.14500</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14500">pdf</a>, <a href="https://arxiv.org/format/2203.14500">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Biomolecules">q-bio.BM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MolGenSurvey: A Systematic Survey in Machine Learning Models for Molecule Design
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+Y">Yuanqi Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+T">Tianfan Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jimeng Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shengchao Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14500v1-abstract-short" style="display: inline;">
        Molecule design is a fundamental problem in molecular science and has critical applications in a variety of areas, such as drug discovery, material science, etc. However, due to the large searching space, it is impossible for human experts to enumerate and test all molecules in wet-lab experiments. Recently, with the rapid development of machine learning methods, especially generative methods, mol&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14500v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14500v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14500v1-abstract-full" style="display: none;">
        Molecule design is a fundamental problem in molecular science and has critical applications in a variety of areas, such as drug discovery, material science, etc. However, due to the large searching space, it is impossible for human experts to enumerate and test all molecules in wet-lab experiments. Recently, with the rapid development of machine learning methods, especially generative methods, molecule design has achieved great progress by leveraging machine learning models to generate candidate molecules. In this paper, we systematically review the most relevant work in machine learning models for molecule design. We start with a brief review of the mainstream molecule featurization and representation methods (including 1D string, 2D graph, and 3D geometry) and general generative methods (deep generative and combinatorial optimization methods). Then we summarize all the existing molecule design problems into several venues according to the problem setup, including input, output types and goals. Finally, we conclude with the open challenges and point out future opportunities of machine learning models for molecule design in real-world applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14500v1-abstract-full').style.display = 'none'; document.getElementById('2203.14500v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13241">arXiv:2203.13241</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13241">pdf</a>, <a href="https://arxiv.org/format/2203.13241">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TCSVT.2022.3143151">10.1109/TCSVT.2022.3143151 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VRNet: Learning the Rectified Virtual Corresponding Points for 3D Point Cloud Registration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhiyuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiadai Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+Y">Yuchao Dai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+B">Bin Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+M">Mingyi He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13241v1-abstract-short" style="display: inline;">
        3D point cloud registration is fragile to outliers, which are labeled as the points without corresponding points. To handle this problem, a widely adopted strategy is to estimate the relative pose based only on some accurate correspondences, which is achieved by building correspondences on the identified inliers or by selecting reliable ones. However, these approaches are usually complicated and t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13241v1-abstract-full').style.display = 'inline'; document.getElementById('2203.13241v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13241v1-abstract-full" style="display: none;">
        3D point cloud registration is fragile to outliers, which are labeled as the points without corresponding points. To handle this problem, a widely adopted strategy is to estimate the relative pose based only on some accurate correspondences, which is achieved by building correspondences on the identified inliers or by selecting reliable ones. However, these approaches are usually complicated and time-consuming. By contrast, the virtual point-based methods learn the virtual corresponding points (VCPs) for all source points uniformly without distinguishing the outliers and the inliers. Although this strategy is time-efficient, the learned VCPs usually exhibit serious collapse degeneration due to insufficient supervision and the inherent distribution limitation. In this paper, we propose to exploit the best of both worlds and present a novel robust 3D point cloud registration framework. We follow the idea of the virtual point-based methods but learn a new type of virtual points called rectified virtual corresponding points (RCPs), which are defined as the point set with the same shape as the source and with the same pose as the target. Hence, a pair of consistent point clouds, i.e. source and RCPs, is formed by rectifying VCPs to RCPs (VRNet), through which reliable correspondences between source and RCPs can be accurately obtained. Since the relative pose between source and RCPs is the same as the relative pose between source and target, the input point clouds can be registered naturally. Specifically, we first construct the initial VCPs by using an estimated soft matching matrix to perform a weighted average on the target points. Then, we design a correction-walk module to learn an offset to rectify VCPs to RCPs, which effectively breaks the distribution limitation of VCPs. Finally, we develop a hybrid loss function to enforce the shape and geometry structure consistency ...
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13241v1-abstract-full').style.display = 'none'; document.getElementById('2203.13241v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE Transactions on Circuits and Systems for Video Technology</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13239">arXiv:2203.13239</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13239">pdf</a>, <a href="https://arxiv.org/format/2203.13239">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/LGRS.2021.3132926">10.1109/LGRS.2021.3132926 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Representation Separation Perspective to Correspondences-free Unsupervised 3D Point Cloud Registration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhiyuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiadai Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+Y">Yuchao Dai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+D">Dingfu Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+X">Xibin Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+M">Mingyi He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13239v1-abstract-short" style="display: inline;">
        3D point cloud registration in remote sensing field has been greatly advanced by deep learning based methods, where the rigid transformation is either directly regressed from the two point clouds (correspondences-free approaches) or computed from the learned correspondences (correspondences-based approaches). Existing correspondences-free methods generally learn the holistic representation of the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13239v1-abstract-full').style.display = 'inline'; document.getElementById('2203.13239v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13239v1-abstract-full" style="display: none;">
        3D point cloud registration in remote sensing field has been greatly advanced by deep learning based methods, where the rigid transformation is either directly regressed from the two point clouds (correspondences-free approaches) or computed from the learned correspondences (correspondences-based approaches). Existing correspondences-free methods generally learn the holistic representation of the entire point cloud, which is fragile for partial and noisy point clouds. In this paper, we propose a correspondences-free unsupervised point cloud registration (UPCR) method from the representation separation perspective. First, we model the input point cloud as a combination of pose-invariant representation and pose-related representation. Second, the pose-related representation is used to learn the relative pose wrt a &#34;latent canonical shape&#34; for the source and target point clouds respectively. Third, the rigid transformation is obtained from the above two learned relative poses. Our method not only filters out the disturbance in pose-invariant representation but also is robust to partial-to-partial point clouds or noise. Experiments on benchmark datasets demonstrate that our unsupervised method achieves comparable if not better performance than state-of-the-art supervised registration methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13239v1-abstract-full').style.display = 'none'; document.getElementById('2203.13239v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE Geoscience and Remote Sensing Letters</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.12821">arXiv:2203.12821</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.12821">pdf</a>, <a href="https://arxiv.org/format/2203.12821">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GraphCoCo: Graph Complementary Contrastive Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiawei Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+J">Junchi Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chentao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+Y">Yue Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+R">Ruoxin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+X">Xiang Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+X">Xinyu Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jie Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.12821v1-abstract-short" style="display: inline;">
        Graph Contrastive Learning (GCL) has shown promising performance in graph representation learning (GRL) without the supervision of manual annotations. GCL can generate graph-level embeddings by maximizing the Mutual Information (MI) between different augmented views of the same graph (positive pairs). However, we identify an obstacle that the optimization of InfoNCE loss only concentrates on a few&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12821v1-abstract-full').style.display = 'inline'; document.getElementById('2203.12821v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.12821v1-abstract-full" style="display: none;">
        Graph Contrastive Learning (GCL) has shown promising performance in graph representation learning (GRL) without the supervision of manual annotations. GCL can generate graph-level embeddings by maximizing the Mutual Information (MI) between different augmented views of the same graph (positive pairs). However, we identify an obstacle that the optimization of InfoNCE loss only concentrates on a few embeddings dimensions, limiting the distinguishability of embeddings in downstream graph classification tasks. This paper proposes an effective graph complementary contrastive learning approach named GraphCoCo to tackle the above issue. Specifically, we set the embedding of the first augmented view as the anchor embedding to localize &#34;highlighted&#34; dimensions (i.e., the dimensions contribute most in similarity measurement). Then remove these dimensions in the embeddings of the second augmented view to discover neglected complementary representations. Therefore, the combination of anchor and complementary embeddings significantly improves the performance in downstream tasks. Comprehensive experiments on various benchmark datasets are conducted to demonstrate the effectiveness of GraphCoCo, and the results show that our model outperforms the state-of-the-art methods. Source code will be made publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12821v1-abstract-full').style.display = 'none'; document.getElementById('2203.12821v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.12338">arXiv:2203.12338</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.12338">pdf</a>, <a href="https://arxiv.org/format/2203.12338">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Real-time Object Detection for Streaming Perception
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jinrong Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Songtao Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zeming Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xiaoping Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.12338v2-abstract-short" style="display: inline;">
        Autonomous driving requires the model to perceive the environment and (re)act within a low latency for safety. While past works ignore the inevitable changes in the environment after processing, streaming perception is proposed to jointly evaluate the latency and accuracy into a single metric for video online perception. In this paper, instead of searching trade-offs between accuracy and speed lik&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12338v2-abstract-full').style.display = 'inline'; document.getElementById('2203.12338v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.12338v2-abstract-full" style="display: none;">
        Autonomous driving requires the model to perceive the environment and (re)act within a low latency for safety. While past works ignore the inevitable changes in the environment after processing, streaming perception is proposed to jointly evaluate the latency and accuracy into a single metric for video online perception. In this paper, instead of searching trade-offs between accuracy and speed like previous works, we point out that endowing real-time models with the ability to predict the future is the key to dealing with this problem. We build a simple and effective framework for streaming perception. It equips a novel DualFlow Perception module (DFP), which includes dynamic and static flows to capture the moving trend and basic detection feature for streaming prediction. Further, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to generate adaptive weights for objects with different moving speeds. Our simple method achieves competitive performance on Argoverse-HD dataset and improves the AP by 4.9% compared to the strong baseline, validating its effectiveness. Our code will be made available at https://github.com/yancie-yjr/StreamYOLO.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12338v2-abstract-full').style.display = 'none'; document.getElementById('2203.12338v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2022 Accepted Paper (Oral)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11754">arXiv:2203.11754</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11754">pdf</a>, <a href="https://arxiv.org/format/2203.11754">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring and Evaluating Image Restoration Potential in Dynamic Scenes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Cheng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+S">Shaolin Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yu Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+Q">Qingsen Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jinqiu Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yanning Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11754v2-abstract-short" style="display: inline;">
        In dynamic scenes, images often suffer from dynamic blur due to superposition of motions or low signal-noise ratio resulted from quick shutter speed when avoiding motions. Recovering sharp and clean results from the captured images heavily depends on the ability of restoration methods and the quality of the input. Although existing research on image restoration focuses on developing models for obt&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11754v2-abstract-full').style.display = 'inline'; document.getElementById('2203.11754v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11754v2-abstract-full" style="display: none;">
        In dynamic scenes, images often suffer from dynamic blur due to superposition of motions or low signal-noise ratio resulted from quick shutter speed when avoiding motions. Recovering sharp and clean results from the captured images heavily depends on the ability of restoration methods and the quality of the input. Although existing research on image restoration focuses on developing models for obtaining better restored results, fewer have studied to evaluate how and which input image leads to superior restored quality. In this paper, to better study an image&#39;s potential value that can be explored for restoration, we propose a novel concept, referring to image restoration potential (IRP). Specifically, We first establish a dynamic scene imaging dataset containing composite distortions and applied image restoration processes to validate the rationality of the existence to IRP. Based on this dataset, we investigate several properties of IRP and propose a novel deep model to accurately predict IRP values. By gradually distilling and selective fusing the degradation features, the proposed model shows its superiority in IRP prediction. Thanks to the proposed model, we are then able to validate how various image restoration related applications are benefited from IRP prediction. We show the potential usages of IRP as a filtering principle to select valuable frames, an auxiliary guidance to improve restoration models, and even an indicator to optimize camera settings for capturing better images under dynamic scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11754v2-abstract-full').style.display = 'none'; document.getElementById('2203.11754v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11670">arXiv:2203.11670</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11670">pdf</a>, <a href="https://arxiv.org/format/2203.11670">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yingxiu Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+Z">Zhiliang Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+H">Huaxiu Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Y">Yinhe Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+D">Dongkyu Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yiping Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+N+L">Nevin L. Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11670v1-abstract-short" style="display: inline;">
        Building models of natural language processing (NLP) is challenging in low-resource scenarios where only limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tend&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11670v1-abstract-full').style.display = 'inline'; document.getElementById('2203.11670v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11670v1-abstract-full" style="display: none;">
        Building models of natural language processing (NLP) is challenging in low-resource scenarios where only limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization overfitting issue, where the model tends to memorize the meta-training tasks while ignoring support sets when adapting to new tasks. To address this issue, we propose a memory imitation meta-learning (MemIML) method that enhances the model&#39;s reliance on support sets for task adaptation. Specifically, we introduce a task-specific memory module to store support set information and construct an imitation module to force query sets to imitate the behaviors of some representative support-set samples stored in the memory. A theoretical analysis is provided to prove the effectiveness of our method, and empirical results also demonstrate that our method outperforms competitive baselines on both text classification and generation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11670v1-abstract-full').style.display = 'none'; document.getElementById('2203.11670v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11506">arXiv:2203.11506</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11506">pdf</a>, <a href="https://arxiv.org/format/2203.11506">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rebalanced Siamese Contrastive Mining for Long-Tailed Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Z">Zhisheng Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cui%2C+J">Jiequan Cui</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lo%2C+E">Eric Lo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zeming Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+J">Jiaya Jia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11506v1-abstract-short" style="display: inline;">
        Deep neural networks perform poorly on heavily class-imbalanced datasets. Given the promising performance of contrastive learning, we propose $\mathbf{Re}$balanced $\mathbf{S}$iamese $\mathbf{Co}$ntrastive $\mathbf{m}$ining ( $\mathbf{ResCom}$) to tackle imbalanced recognition. Based on the mathematical analysis and simulation results, we claim that supervised contrastive learning suffers a dual c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11506v1-abstract-full').style.display = 'inline'; document.getElementById('2203.11506v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11506v1-abstract-full" style="display: none;">
        Deep neural networks perform poorly on heavily class-imbalanced datasets. Given the promising performance of contrastive learning, we propose $\mathbf{Re}$balanced $\mathbf{S}$iamese $\mathbf{Co}$ntrastive $\mathbf{m}$ining ( $\mathbf{ResCom}$) to tackle imbalanced recognition. Based on the mathematical analysis and simulation results, we claim that supervised contrastive learning suffers a dual class-imbalance problem at both the original batch and Siamese batch levels, which is more serious than long-tailed classification learning. In this paper, at the original batch level, we introduce a class-balanced supervised contrastive loss to assign adaptive weights for different classes. At the Siamese batch level, we present a class-balanced queue, which maintains the same number of keys for all classes. Furthermore, we note that the contrastive loss gradient with respect to the contrastive logits can be decoupled into the positives and negatives, and easy positives and easy negatives will make the contrastive gradient vanish. We propose supervised hard positive and negative pairs mining to pick up informative pairs for contrastive computation and improve representation learning. Finally, to approximately maximize the mutual information between the two views, we propose Siamese Balanced Softmax and joint it with the contrastive loss for one-stage training. ResCom outperforms the previous methods by large margins on multiple long-tailed recognition benchmarks. Our code will be made publicly available at: https://github.com/dvlab-research/ResCom.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11506v1-abstract-full').style.display = 'none'; document.getElementById('2203.11506v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Tech report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10759">arXiv:2203.10759</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10759">pdf</a>, <a href="https://arxiv.org/format/2203.10759">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Sai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Y">Yuwei Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yuchuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiaman Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yongbin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+C">Caixia Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaojie Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10759v1-abstract-short" style="display: inline;">
        A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog datas&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10759v1-abstract-full').style.display = 'inline'; document.getElementById('2203.10759v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10759v1-abstract-full" style="display: none;">
        A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset includes a total of 40K dialogs and 500K utterances from four different domains: Chinese names, phone numbers, ID numbers and license plate numbers. The data is well annotated with sub-slot values, slot values, dialog states and actions. We find some new linguistic phenomena and interactive manners in SSTOD which raise critical challenges of building dialog agents for the task. We test three state-of-the-art dialog models on SSTOD and find they cannot handle the task well on any of the four domains. We also investigate an improved model by involving slot knowledge in a plug-in manner. More work should be done to meet the new challenges raised from SSTOD which widely exists in real-life applications. The dataset and code are publicly available via https://github.com/shunjiu/SSTOD.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10759v1-abstract-full').style.display = 'none'; document.getElementById('2203.10759v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACL 2022 Findings</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T50
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10739">arXiv:2203.10739</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10739">pdf</a>, <a href="https://arxiv.org/format/2203.10739">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+Z">Zhiyuan Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+T">Tiancai Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+J">Jianbing Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10739v2-abstract-short" style="display: inline;">
        Sparsely annotated semantic segmentation (SASS) aims to train a segmentation network with coarse-grained (i.e., point-, scribble-, and block-wise) supervisions, where only a small proportion of pixels are labeled in each image. In this paper, we propose a novel tree energy loss for SASS by providing semantic guidance for unlabeled pixels. The tree energy loss represents images as minimum spanning&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10739v2-abstract-full').style.display = 'inline'; document.getElementById('2203.10739v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10739v2-abstract-full" style="display: none;">
        Sparsely annotated semantic segmentation (SASS) aims to train a segmentation network with coarse-grained (i.e., point-, scribble-, and block-wise) supervisions, where only a small proportion of pixels are labeled in each image. In this paper, we propose a novel tree energy loss for SASS by providing semantic guidance for unlabeled pixels. The tree energy loss represents images as minimum spanning trees to model both low-level and high-level pair-wise affinities. By sequentially applying these affinities to the network prediction, soft pseudo labels for unlabeled pixels are generated in a coarse-to-fine manner, achieving dynamic online self-training. The tree energy loss is effective and easy to be incorporated into existing frameworks by combining it with a traditional segmentation loss. Compared with previous SASS methods, our method requires no multistage training strategies, alternating optimization procedures, additional supervised data, or time-consuming post-processing while outperforming them in all SASS settings. Code is available at https://github.com/megvii-research/TreeEnergyLoss.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10739v2-abstract-full').style.display = 'none'; document.getElementById('2203.10739v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10719">arXiv:2203.10719</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10719">pdf</a>, <a href="https://arxiv.org/format/2203.10719">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LocATe: End-to-end Localization of Actions in 3D with Transformers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiankai Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+B">Bolei Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Black%2C+M+J">Michael J. Black</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chandrasekaran%2C+A">Arjun Chandrasekaran</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10719v1-abstract-short" style="display: inline;">
        Understanding a person&#39;s behavior from their 3D motion is a fundamental problem in computer vision with many applications. An important component of this problem is 3D Temporal Action Localization (3D-TAL), which involves recognizing what actions a person is performing, and when. State-of-the-art 3D-TAL methods employ a two-stage approach in which the action span detection task and the action reco&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10719v1-abstract-full').style.display = 'inline'; document.getElementById('2203.10719v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10719v1-abstract-full" style="display: none;">
        Understanding a person&#39;s behavior from their 3D motion is a fundamental problem in computer vision with many applications. An important component of this problem is 3D Temporal Action Localization (3D-TAL), which involves recognizing what actions a person is performing, and when. State-of-the-art 3D-TAL methods employ a two-stage approach in which the action span detection task and the action recognition task are implemented as a cascade. This approach, however, limits the possibility of error-correction. In contrast, we propose LocATe, an end-to-end approach that jointly localizes and recognizes actions in a 3D sequence. Further, unlike existing autoregressive models that focus on modeling the local context in a sequence, LocATe&#39;s transformer model is capable of capturing long-term correlations between actions in a sequence. Unlike transformer-based object-detection and classification models which consider image or patch features as input, the input in 3D-TAL is a long sequence of highly correlated frames. To handle the high-dimensional input, we implement an effective input representation, and overcome the diffuse attention across long time horizons by introducing sparse attention in the model. LocATe outperforms previous approaches on the existing PKU-MMD 3D-TAL benchmark (mAP=93.2%). Finally, we argue that benchmark datasets are most useful where there is clear room for performance improvement. To that end, we introduce a new, challenging, and more realistic benchmark dataset, BABEL-TAL-20 (BT20), where the performance of state-of-the-art methods is significantly worse. The dataset and code for the method will be available for research purposes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10719v1-abstract-full').style.display = 'none'; document.getElementById('2203.10719v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10473">arXiv:2203.10473</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10473">pdf</a>, <a href="https://arxiv.org/format/2203.10473">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ECAPA-TDNN for Multi-speaker Text-to-speech Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+J">Jinlong Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Y">Yayue Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+Y">Yichen Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Ya Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jianqing Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+J">Jiaen Liang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10473v2-abstract-short" style="display: inline;">
        In recent years, neural network based methods for multi-speaker text-to-speech synthesis (TTS) have made significant progress. However, the current speaker encoder models used in these methods still cannot capture enough speaker information. In this paper, we focus on accurate speaker encoder modeling and propose an end-to-end method that can generate high-quality speech and better similarity for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10473v2-abstract-full').style.display = 'inline'; document.getElementById('2203.10473v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10473v2-abstract-full" style="display: none;">
        In recent years, neural network based methods for multi-speaker text-to-speech synthesis (TTS) have made significant progress. However, the current speaker encoder models used in these methods still cannot capture enough speaker information. In this paper, we focus on accurate speaker encoder modeling and propose an end-to-end method that can generate high-quality speech and better similarity for both seen and unseen speakers. The proposed architecture consists of three separately trained components: a speaker encoder based on the state-of-the-art ECAPA-TDNN model which is derived from speaker verification task, a FastSpeech2 based synthesizer, and a HiFi-GAN vocoder. The comparison among different speaker encoder models shows our proposed method can achieve better naturalness and similarity. To efficiently evaluate our synthesized speech, we are the first to adopt deep learning based automatic MOS evaluation methods to assess our results, and these methods show great potential in automatic speech quality assessment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10473v2-abstract-full').style.display = 'none'; document.getElementById('2203.10473v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, submitted to interspeech2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.09041">arXiv:2203.09041</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.09041">pdf</a>, <a href="https://arxiv.org/format/2203.09041">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DATA: Domain-Aware and Task-Aware Self-supervised Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+Q">Qing Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+J">Junran Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+L">Lingxie Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiajun Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yin%2C+H">Haoran Yin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+Q">Qi Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhaoxiang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.09041v2-abstract-short" style="display: inline;">
        The paradigm of training models on massive data without label through self-supervised learning (SSL) and finetuning on many downstream tasks has become a trend recently. However, due to the high training costs and the unconsciousness of downstream usages, most self-supervised learning methods lack the capability to correspond to the diversities of downstream scenarios, as there are various data do&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09041v2-abstract-full').style.display = 'inline'; document.getElementById('2203.09041v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.09041v2-abstract-full" style="display: none;">
        The paradigm of training models on massive data without label through self-supervised learning (SSL) and finetuning on many downstream tasks has become a trend recently. However, due to the high training costs and the unconsciousness of downstream usages, most self-supervised learning methods lack the capability to correspond to the diversities of downstream scenarios, as there are various data domains, different vision tasks and latency constraints on models. Neural architecture search (NAS) is one universally acknowledged fashion to conquer the issues above, but applying NAS on SSL seems impossible as there is no label or metric provided for judging model selection. In this paper, we present DATA, a simple yet effective NAS approach specialized for SSL that provides Domain-Aware and Task-Aware pre-training. Specifically, we (i) train a supernet which could be deemed as a set of millions of networks covering a wide range of model scales without any label, (ii) propose a flexible searching mechanism compatible with SSL that enables finding networks of different computation costs, for various downstream vision tasks and data domains without explicit metric provided. Instantiated With MoCo v2, our method achieves promising results across a wide range of computation costs on downstream tasks, including image classification, object detection and semantic segmentation. DATA is orthogonal to most existing SSL methods and endows them the ability of customization on downstream needs. Extensive experiments on other SSL methods demonstrate the generalizability of the proposed method. Code is released at https://github.com/GAIA-vision/GAIA-ssl
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09041v2-abstract-full').style.display = 'none'; document.getElementById('2203.09041v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2022,8 pages,3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.07669">arXiv:2203.07669</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.07669">pdf</a>, <a href="https://arxiv.org/format/2203.07669">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Progressive End-to-End Object Detection in Crowded Scenes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+A">Anlin Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yuang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qi%2C+X">Xiaojuan Qi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.07669v3-abstract-short" style="display: inline;">
        In this paper, we propose a new query-based detection framework for crowd detection. Previous query-based detectors suffer from two drawbacks: first, multiple predictions will be inferred for a single object, typically in crowded scenes; second, the performance saturates as the depth of the decoding stage increases. Benefiting from the nature of the one-to-one label assignment rule, we propose a p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.07669v3-abstract-full').style.display = 'inline'; document.getElementById('2203.07669v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.07669v3-abstract-full" style="display: none;">
        In this paper, we propose a new query-based detection framework for crowd detection. Previous query-based detectors suffer from two drawbacks: first, multiple predictions will be inferred for a single object, typically in crowded scenes; second, the performance saturates as the depth of the decoding stage increases. Benefiting from the nature of the one-to-one label assignment rule, we propose a progressive predicting method to address the above issues. Specifically, we first select accepted queries prone to generate true positive predictions, then refine the rest noisy queries according to the previously accepted predictions. Experiments show that our method can significantly boost the performance of query-based detectors in crowded scenes. Equipped with our approach, Sparse RCNN achieves 92.0\% $\text{AP}$, 41.4\% $\text{MR}^{-2}$ and 83.2\% $\text{JI}$ on the challenging CrowdHuman \cite{shao2018crowdhuman} dataset, outperforming the box-based method MIP \cite{chu2020detection} that specifies in handling crowded scenarios. Moreover, the proposed method, robust to crowdedness, can still obtain consistent improvements on moderately and slightly crowded datasets like CityPersons \cite{zhang2017citypersons} and COCO \cite{lin2014microsoft}. Code will be made publicly available at https://github.com/megvii-model/Iter-E2EDET.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.07669v3-abstract-full').style.display = 'none'; document.getElementById('2203.07669v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.06958">arXiv:2203.06958</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.06958">pdf</a>, <a href="https://arxiv.org/format/2203.06958">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hui%2C+B">Binyuan Hui</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Geng%2C+R">Ruiying Geng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+L">Lihan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qin%2C+B">Bowen Qin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Bowen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yongbin Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.06958v1-abstract-short" style="display: inline;">
        The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing. The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well. In this paper, we propose S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectivel&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.06958v1-abstract-full').style.display = 'inline'; document.getElementById('2203.06958v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.06958v1-abstract-full" style="display: none;">
        The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing. The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well. In this paper, we propose S$^2$SQL, injecting Syntax to question-Schema graph encoder for Text-to-SQL parsers, which effectively leverages the syntactic dependency information of questions in text-to-SQL to improve the performance. We also employ the decoupling constraint to induce diverse relational edge embedding, which further improves the network&#39;s performance. Experiments on the Spider and robustness setting Spider-Syn demonstrate that the proposed approach outperforms all existing methods when pre-training models are used, resulting in a performance ranks first on the Spider leaderboard.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.06958v1-abstract-full').style.display = 'none'; document.getElementById('2203.06958v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ACL 2022 Findings</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.06717">arXiv:2203.06717</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.06717">pdf</a>, <a href="https://arxiv.org/format/2203.06717">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+X">Xiaohan Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yizhuang Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jungong Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+G">Guiguang Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.06717v4-abstract-short" style="display: inline;">
        We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient hig&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.06717v4-abstract-full').style.display = 'inline'; document.getElementById('2203.06717v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.06717v4-abstract-full" style="display: none;">
        We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code &amp; models at https://github.com/megvii-research/RepLKNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.06717v4-abstract-full').style.display = 'none'; document.getElementById('2203.06717v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.05625">arXiv:2203.05625</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.05625">pdf</a>, <a href="https://arxiv.org/format/2203.05625">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PETR: Position Embedding Transformation for Multi-View 3D Object Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yingfei Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+T">Tiancai Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.05625v1-abstract-short" style="display: inline;">
        In this paper, we develop position embedding transformation (PETR) for multi-view 3D object detection. PETR encodes the position information of 3D coordinates into image features, producing the 3D position-aware features. Object query can perceive the 3D position-aware features and perform end-to-end object detection. PETR achieves state-of-the-art performance (50.4% NDS and 44.1% mAP) on standard&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.05625v1-abstract-full').style.display = 'inline'; document.getElementById('2203.05625v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.05625v1-abstract-full" style="display: none;">
        In this paper, we develop position embedding transformation (PETR) for multi-view 3D object detection. PETR encodes the position information of 3D coordinates into image features, producing the 3D position-aware features. Object query can perceive the 3D position-aware features and perform end-to-end object detection. PETR achieves state-of-the-art performance (50.4% NDS and 44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark. It can serve as a simple yet strong baseline for future research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.05625v1-abstract-full').style.display = 'none'; document.getElementById('2203.05625v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Tech Report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.04920">arXiv:2203.04920</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.04920">pdf</a>, <a href="https://arxiv.org/format/2203.04920">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Why Interpretable Causal Inference is Important for High-Stakes Decision Making for Critically Ill Patients and How To Do It
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Parikh%2C+H">Harsh Parikh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hoffman%2C+K">Kentaro Hoffman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+H">Haoqi Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ge%2C+W">Wendong Ge</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jing%2C+J">Jin Jing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Amerineni%2C+R">Rajesh Amerineni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+L">Lin Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jimeng Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zafar%2C+S">Sahar Zafar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Struck%2C+A">Aaron Struck</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Volfovsky%2C+A">Alexander Volfovsky</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudin%2C+C">Cynthia Rudin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Westover%2C+M+B">M. Brandon Westover</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.04920v1-abstract-short" style="display: inline;">
        Many fundamental problems affecting the care of critically ill patients lead to similar analytical challenges: physicians cannot easily estimate the effects of at-risk medical conditions or treatments because the causal effects of medical conditions and drugs are entangled. They also cannot easily perform studies: there are not enough high-quality data for high-dimensional observational causal inf&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04920v1-abstract-full').style.display = 'inline'; document.getElementById('2203.04920v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.04920v1-abstract-full" style="display: none;">
        Many fundamental problems affecting the care of critically ill patients lead to similar analytical challenges: physicians cannot easily estimate the effects of at-risk medical conditions or treatments because the causal effects of medical conditions and drugs are entangled. They also cannot easily perform studies: there are not enough high-quality data for high-dimensional observational causal inference, and RCTs often cannot ethically be conducted. However, mechanistic knowledge is available, including how drugs are absorbed into the body, and the combination of this knowledge with the limited data could potentially suffice -- if we knew how to combine them. In this work, we present a framework for interpretable estimation of causal effects for critically ill patients under exactly these complex conditions: interactions between drugs and observations over time, patient data sets that are not large, and mechanistic knowledge that can substitute for lack of data. We apply this framework to an extremely important problem affecting critically ill patients, namely the effect of seizures and other potentially harmful electrical events in the brain (called epileptiform activity -- EA) on outcomes. Given the high stakes involved and the high noise in the data, interpretability is critical for troubleshooting such complex problems. Interpretability of our matched groups allowed neurologists to perform chart reviews to verify the quality of our causal analysis. For instance, our work indicates that a patient who experiences a high level of seizure-like activity (75% high EA burden) and is untreated for a six-hour window, has, on average, a 16.7% increased chance of adverse outcomes such as severe brain damage, lifetime disability, or death. We find that patients with mild but long-lasting EA (average EA burden &gt;= 50%) have their risk of an adverse outcome increased by 11.2%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04920v1-abstract-full').style.display = 'none'; document.getElementById('2203.04920v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.03966">arXiv:2203.03966</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.03966">pdf</a>, <a href="https://arxiv.org/format/2203.03966">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GaitStrip: Gait Recognition via Effective Strip-based Feature Representations and Multi-Level Framework
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+M">Ming Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+B">Beibei Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+X">Xianda Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L">Lincheng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Z">Zheng Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jiande Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shunli Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+X">Xin Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.03966v1-abstract-short" style="display: inline;">
        Many gait recognition methods first partition the human gait into N-parts and then combine them to establish part-based feature representations. Their gait recognition performance is often affected by partitioning strategies, which are empirically chosen in different datasets. However, we observe that strips as the basic component of parts are agnostic against different partitioning strategies. Mo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03966v1-abstract-full').style.display = 'inline'; document.getElementById('2203.03966v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.03966v1-abstract-full" style="display: none;">
        Many gait recognition methods first partition the human gait into N-parts and then combine them to establish part-based feature representations. Their gait recognition performance is often affected by partitioning strategies, which are empirically chosen in different datasets. However, we observe that strips as the basic component of parts are agnostic against different partitioning strategies. Motivated by this observation, we present a strip-based multi-level gait recognition network, named GaitStrip, to extract comprehensive gait information at different levels. To be specific, our high-level branch explores the context of gait sequences and our low-level one focuses on detailed posture changes. We introduce a novel StriP-Based feature extractor (SPB) to learn the strip-based feature representations by directly taking each strip of the human body as the basic unit. Moreover, we propose a novel multi-branch structure, called Enhanced Convolution Module (ECM), to extract different representations of gaits. ECM consists of the Spatial-Temporal feature extractor (ST), the Frame-Level feature extractor (FL) and SPB, and has two obvious advantages: First, each branch focuses on a specific representation, which can be used to improve the robustness of the network. Specifically, ST aims to extract spatial-temporal features of gait sequences, while FL is used to generate the feature representation of each frame. Second, the parameters of the ECM can be reduced in test by introducing a structural re-parameterization technique. Extensive experimental results demonstrate that our GaitStrip achieves state-of-the-art performance in both normal walking and complex conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03966v1-abstract-full').style.display = 'none'; document.getElementById('2203.03966v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.03436">arXiv:2203.03436</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.03436">pdf</a>, <a href="https://arxiv.org/format/2203.03436">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Neural Decision Forest for Acoustic Scene Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jianyuan Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xubo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mei%2C+X">Xinhao Mei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+J">Jinzheng Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Plumbley%2C+M+D">Mark D. Plumbley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=K%C4%B1l%C4%B1%C3%A7%2C+V">Volkan Kl</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wenwu Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.03436v1-abstract-short" style="display: inline;">
        Acoustic scene classification (ASC) aims to classify an audio clip based on the characteristic of the recording environment. In this regard, deep learning based approaches have emerged as a useful tool for ASC problems. Conventional approaches to improving the classification accuracy include integrating auxiliary methods such as attention mechanism, pre-trained models and ensemble multiple sub-net&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03436v1-abstract-full').style.display = 'inline'; document.getElementById('2203.03436v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.03436v1-abstract-full" style="display: none;">
        Acoustic scene classification (ASC) aims to classify an audio clip based on the characteristic of the recording environment. In this regard, deep learning based approaches have emerged as a useful tool for ASC problems. Conventional approaches to improving the classification accuracy include integrating auxiliary methods such as attention mechanism, pre-trained models and ensemble multiple sub-networks. However, due to the complexity of audio clips captured from different environments, it is difficult to distinguish their categories without using any auxiliary methods for existing deep learning models using only a single classifier. In this paper, we propose a novel approach for ASC using deep neural decision forest (DNDF). DNDF combines a fixed number of convolutional layers and a decision forest as the final classifier. The decision forest consists of a fixed number of decision tree classifiers, which have been shown to offer better classification performance than a single classifier in some datasets. In particular, the decision forest differs substantially from traditional random forests as it is stochastic, differentiable, and capable of using the back-propagation to update and learn feature representations in neural network. Experimental results on the DCASE2019 and ESC-50 datasets demonstrate that our proposed DNDF method improves the ASC performance in terms of classification accuracy and shows competitive performance as compared with state-of-the-art baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03436v1-abstract-full').style.display = 'none'; document.getElementById('2203.03436v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to the 30th European Signal Processing Conference (EUSIPCO), 5 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.03128">arXiv:2203.03128</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.03128">pdf</a>, <a href="https://arxiv.org/format/2203.03128">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Searching for Robust Neural Architectures via Comprehensive and Reliable Evaluation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jialiang Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+T">Tingsong Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+W">Weien Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiaoya Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+W">Wen Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xiaoqian Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.03128v1-abstract-short" style="display: inline;">
        Neural architecture search (NAS) could help search for robust network architectures, where defining robustness evaluation metrics is the important procedure. However, current robustness evaluations in NAS are not sufficiently comprehensive and reliable. In particular, the common practice only considers adversarial noise and quantified metrics such as the Jacobian matrix, whereas, some studies indi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03128v1-abstract-full').style.display = 'inline'; document.getElementById('2203.03128v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.03128v1-abstract-full" style="display: none;">
        Neural architecture search (NAS) could help search for robust network architectures, where defining robustness evaluation metrics is the important procedure. However, current robustness evaluations in NAS are not sufficiently comprehensive and reliable. In particular, the common practice only considers adversarial noise and quantified metrics such as the Jacobian matrix, whereas, some studies indicated that the models are also vulnerable to other types of noises such as natural noise. In addition, existing methods taking adversarial noise as the evaluation just use the robust accuracy of the FGSM or PGD, but these adversarial attacks could not provide the adequately reliable evaluation, leading to the vulnerability of the models under stronger attacks. To alleviate the above problems, we propose a novel framework, called Auto Adversarial Attack and Defense (AAAD), where we employ neural architecture search methods, and four types of robustness evaluations are considered, including adversarial noise, natural noise, system noise and quantified metrics, thereby assisting in finding more robust architectures. Also, among the adversarial noise, we use the composite adversarial attack obtained by random search as the new metric to evaluate the robustness of the model architectures. The empirical results on the CIFAR10 dataset show that the searched efficient attack could help find more robust architectures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03128v1-abstract-full').style.display = 'none'; document.getElementById('2203.03128v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.02884">arXiv:2203.02884</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.02884">pdf</a>, <a href="https://arxiv.org/format/2203.02884">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Self-Supervised Category-Level Object Pose and Size Estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Y">Yisheng He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+H">Haoqiang Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+H">Haibin Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Q">Qifeng Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.02884v2-abstract-short" style="display: inline;">
        In this work, we tackle the challenging problem of category-level object pose and size estimation from a single depth image. Although previous fully-supervised works have demonstrated promising performance, collecting ground-truth pose labels is generally time-consuming and labor-intensive. Instead, we propose a label-free method that learns to enforce the geometric consistency between category te&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.02884v2-abstract-full').style.display = 'inline'; document.getElementById('2203.02884v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.02884v2-abstract-full" style="display: none;">
        In this work, we tackle the challenging problem of category-level object pose and size estimation from a single depth image. Although previous fully-supervised works have demonstrated promising performance, collecting ground-truth pose labels is generally time-consuming and labor-intensive. Instead, we propose a label-free method that learns to enforce the geometric consistency between category template mesh and observed object point cloud under a self-supervision manner. Specifically, our method consists of three key components: differentiable shape deformation, registration, and rendering. In particular, shape deformation and registration are applied to the template mesh to eliminate the differences in shape, pose and scale. A differentiable renderer is then deployed to enforce geometric consistency between point clouds lifted from the rendered depth and the observed scene for self-supervision. We evaluate our approach on real-world datasets and find that our approach outperforms the simple traditional baseline by large margins while being competitive with some fully-supervised approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.02884v2-abstract-full').style.display = 'none'; document.getElementById('2203.02884v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.02838">arXiv:2203.02838</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.02838">pdf</a>, <a href="https://arxiv.org/format/2203.02838">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Leveraging Pre-trained BERT for Audio Captioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xubo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mei%2C+X">Xinhao Mei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Q">Qiushi Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jianyuan Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+J">Jinzheng Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Haohe Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Plumbley%2C+M+D">Mark D. Plumbley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=K%C4%B1l%C4%B1%C3%A7%2C+V">Volkan Kl</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wenwu Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.02838v2-abstract-short" style="display: inline;">
        Audio captioning aims at using natural language to describe the content of an audio clip. Existing audio captioning systems are generally based on an encoder-decoder architecture, in which acoustic information is extracted by an audio encoder and then a language decoder is used to generate the captions. Training an audio captioning system often encounters the problem of data scarcity. Transferring&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.02838v2-abstract-full').style.display = 'inline'; document.getElementById('2203.02838v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.02838v2-abstract-full" style="display: none;">
        Audio captioning aims at using natural language to describe the content of an audio clip. Existing audio captioning systems are generally based on an encoder-decoder architecture, in which acoustic information is extracted by an audio encoder and then a language decoder is used to generate the captions. Training an audio captioning system often encounters the problem of data scarcity. Transferring knowledge from pre-trained audio models such as Pre-trained Audio Neural Networks (PANNs) have recently emerged as a useful method to mitigate this issue. However, there is less attention on exploiting pre-trained language models for the decoder, compared with the encoder. BERT is a pre-trained language model that has been extensively used in Natural Language Processing (NLP) tasks. Nevertheless, the potential of BERT as the language decoder for audio captioning has not been investigated. In this study, we demonstrate the efficacy of the pre-trained BERT model for audio captioning. Specifically, we apply PANNs as the encoder and initialize the decoder from the public pre-trained BERT models. We conduct an empirical study on the use of these BERT models for the decoder in the audio captioning model. Our models achieve competitive results with the existing audio captioning methods on the AudioCaps dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.02838v2-abstract-full').style.display = 'none'; document.getElementById('2203.02838v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to the 30th European Signal Processing Conference (EUSIPCO), 5 pages, 2 figures</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Sun%2C+J&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>