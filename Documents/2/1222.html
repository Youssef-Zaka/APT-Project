<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 295 results for author: <span class="mathjax">Cho, K</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Cho%2C+K">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Cho, K">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Cho%2C+K&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Cho, K">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=200"
            class="pagination-link "
            aria-label="Page 5"
            aria-current="page">5
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=250"
            class="pagination-link "
            aria-label="Page 6"
            aria-current="page">6
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.14146">arXiv:2204.14146</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.14146">pdf</a>, <a href="https://arxiv.org/format/2204.14146">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Training Language Models with Natural Language Feedback
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Scheurer%2C+J">Jérémy Scheurer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Campos%2C+J+A">Jon Ander Campos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chan%2C+J+S">Jun Shern Chan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+A">Angelica Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Perez%2C+E">Ethan Perez</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.14146v2-abstract-short" style="display: inline;">
        Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.14146v2-abstract-full').style.display = 'inline'; document.getElementById('2204.14146v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.14146v2-abstract-full" style="display: none;">
        Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.14146v2-abstract-full').style.display = 'none'; document.getElementById('2204.14146v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The First Workshop on Learning with Natural Language Supervision at ACL 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13509">arXiv:2204.13509</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13509">pdf</a>, <a href="https://arxiv.org/format/2204.13509">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shin%2C+S">Seongjin Shin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+S">Sang-Woo Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ahn%2C+H">Hwijeen Ahn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Sungdong Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H">HyoungSeok Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+B">Boseop Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+G">Gichang Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Park%2C+W">Woomyoung Park</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ha%2C+J">Jung-Woo Ha</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sung%2C+N">Nako Sung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13509v1-abstract-short" style="display: inline;">
        Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-co&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13509v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13509v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13509v1-abstract-full" style="display: none;">
        Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13509v1-abstract-full').style.display = 'none'; document.getElementById('2204.13509v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NAACL2022 as a long paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13426">arXiv:2204.13426</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13426">pdf</a>, <a href="https://arxiv.org/format/2204.13426">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AE-NeRF: Auto-Encoding Neural Radiance Fields for 3D-Aware Object Manipulation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+M">Mira Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ko%2C+J">Jaehoon Ko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyusun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+J">Junmyeong Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+D">Daewon Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Seungryong Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13426v1-abstract-short" style="display: inline;">
        We propose a novel framework for 3D-aware object manipulation, called Auto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated in an auto-encoder architecture, extracts disentangled 3D attributes such as 3D shape, appearance, and camera pose from an image, and a high-quality image is rendered from the attributes through disentangled generative Neural Radiance Fields (NeRF). T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13426v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13426v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13426v1-abstract-full" style="display: none;">
        We propose a novel framework for 3D-aware object manipulation, called Auto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated in an auto-encoder architecture, extracts disentangled 3D attributes such as 3D shape, appearance, and camera pose from an image, and a high-quality image is rendered from the attributes through disentangled generative Neural Radiance Fields (NeRF). To improve the disentanglement ability, we present two losses, global-local attribute consistency loss defined between input and output, and swapped-attribute classification loss. Since training such auto-encoding networks from scratch without ground-truth shape and appearance information is non-trivial, we present a stage-wise training scheme, which dramatically helps to boost the performance. We conduct experiments to demonstrate the effectiveness of the proposed model over the latest methods and provide extensive ablation studies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13426v1-abstract-full').style.display = 'none'; document.getElementById('2204.13426v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07184">arXiv:2204.07184</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07184">pdf</a>, <a href="https://arxiv.org/format/2204.07184">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Separating the World and Ego Models for Self-Driving
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sobal%2C+V">Vlad Sobal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Canziani%2C+A">Alfredo Canziani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Carion%2C+N">Nicolas Carion</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=LeCun%2C+Y">Yann LeCun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07184v1-abstract-short" style="display: inline;">
        Training self-driving systems to be robust to the long-tail of driving scenarios is a critical problem. Model-based approaches leverage simulation to emulate a wide range of scenarios without putting users at risk in the real world. One promising path to faithful simulation is to train a forward model of the world to predict the future states of both the environment and the ego-vehicle given past&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07184v1-abstract-full').style.display = 'inline'; document.getElementById('2204.07184v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07184v1-abstract-full" style="display: none;">
        Training self-driving systems to be robust to the long-tail of driving scenarios is a critical problem. Model-based approaches leverage simulation to emulate a wide range of scenarios without putting users at risk in the real world. One promising path to faithful simulation is to train a forward model of the world to predict the future states of both the environment and the ego-vehicle given past states and a sequence of actions. In this paper, we argue that it is beneficial to model the state of the ego-vehicle, which often has simple, predictable and deterministic behavior, separately from the rest of the environment, which is much more complex and highly multimodal. We propose to model the ego-vehicle using a simple and differentiable kinematic model, while training a stochastic convolutional forward model on raster representations of the state to predict the behavior of the rest of the environment. We explore several configurations of such decoupled models, and evaluate their performance both with Model Predictive Control (MPC) and direct policy learning. We test our methods on the task of highway driving and demonstrate lower crash rates and better stability. The code is available at https://github.com/vladisai/pytorch-PPUU/tree/ICLR2022.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07184v1-abstract-full').style.display = 'none'; document.getElementById('2204.07184v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages main content, 14 with references and appendix. 5 figures in total. Submitted and accepted to ICLR 2022 workshop on Generalizable Policy Learning in the Physical World (https://ai-workshops.github.io/generalizable-policy-learning-in-the-physical-world/)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.05746">arXiv:2204.05746</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.05746">pdf</a>, <a href="https://arxiv.org/format/2204.05746">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BABD: A Bitcoin Address Behavior Dataset for Address Behavior Pattern Analysis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiang%2C+Y">Yuexin Xiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lei%2C+Y">Yuchen Lei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bao%2C+D">Ding Bao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+W">Wei Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+T">Tiantian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Q">Qingqing Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+W">Wenmao Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+T">Tianqing Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choo%2C+K+R">Kim-Kwang Raymond Choo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.05746v2-abstract-short" style="display: inline;">
        Cryptocurrencies are no longer just the preferred option for cybercriminal activities on darknets, due to the increasing adoption in mainstream applications. This is partly due to the transparency associated with the underpinning ledgers, where any individual can access the record of a transaction record on the public ledger. In this paper, we build a dataset comprising Bitcoin transactions betwee&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05746v2-abstract-full').style.display = 'inline'; document.getElementById('2204.05746v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.05746v2-abstract-full" style="display: none;">
        Cryptocurrencies are no longer just the preferred option for cybercriminal activities on darknets, due to the increasing adoption in mainstream applications. This is partly due to the transparency associated with the underpinning ledgers, where any individual can access the record of a transaction record on the public ledger. In this paper, we build a dataset comprising Bitcoin transactions between 12 July 2019 and 26 May 2021. This dataset (hereafter referred to as BABD-13) contains 13 types of Bitcoin addresses, 5 categories of indicators with 148 features, and 544,462 labeled data. We then use our proposed dataset on common machine learning models, namely: k-nearest neighbors algorithm, decision tree, random forest, multilayer perceptron, and XGBoost. The results show that the accuracy rates of these machine learning models on our proposed dataset are between 93.24% and 96.71%. We also analyze the proposed features and their relationships from the experiments, and propose a k-hop subgraph generation algorithm to extract a k-hop subgraph from the entire Bitcoin transaction graph constructed by the directed heterogeneous multigraph starting from a specific Bitcoin address node (e.g., a known transaction associated with a criminal investigation).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05746v2-abstract-full').style.display = 'none'; document.getElementById('2204.05746v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 4 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68-11
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          H.2.8
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.00145">arXiv:2204.00145</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.00145">pdf</a>, <a href="https://arxiv.org/format/2204.00145">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3491102.3517457">10.1145/3491102.3517457 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+Y">Young-Ho Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chou%2C+D">Diana Chou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+B">Bongshin Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Danilovich%2C+M">Margaret Danilovich</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lazar%2C+A">Amanda Lazar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Conroy%2C+D+E">David E. Conroy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kacorri%2C+H">Hernisa Kacorri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choe%2C+E+K">Eun Kyoung Choe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.00145v1-abstract-short" style="display: inline;">
        Current activity tracking technologies are largely trained on younger adults&#39; data, which can lead to solutions that are not well-suited for older adults. To build activity trackers for older adults, it is crucial to collect training data with them. To this end, we examine the feasibility and challenges with older adults in collecting activity labels by leveraging speech. Specifically, we built My&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00145v1-abstract-full').style.display = 'inline'; document.getElementById('2204.00145v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.00145v1-abstract-full" style="display: none;">
        Current activity tracking technologies are largely trained on younger adults&#39; data, which can lead to solutions that are not well-suited for older adults. To build activity trackers for older adults, it is crucial to collect training data with them. To this end, we examine the feasibility and challenges with older adults in collecting activity labels by leveraging speech. Specifically, we built MyMove, a speech-based smartwatch app to facilitate the in-situ labeling with a low capture burden. We conducted a 7-day deployment study, where 13 older adults collected their activity labels and smartwatch sensor data, while wearing a thigh-worn activity monitor. Participants were highly engaged, capturing 1,224 verbal reports in total. We extracted 1,885 activities with corresponding effort level and timespan, and examined the usefulness of these reports as activity labels. We discuss the implications of our approach and the collected dataset in supporting older adults through personalized activity tracking technologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00145v1-abstract-full').style.display = 'none'; document.getElementById('2204.00145v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear at ACM CHI 2022. 21 pages, 3 figures, 7 tables. For the NSF funded project, visit https://mymove-collective.github.io</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          H.5.2; H.5.1; I.2.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14416">arXiv:2203.14416</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14416">pdf</a>, <a href="https://arxiv.org/format/2203.14416">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bunched LPCNet2: Efficient Neural Vocoders Covering Devices from Cloud to Edge
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Park%2C+S">Sangjun Park</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choo%2C+K">Kihyun Choo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Joohyung Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Porov%2C+A+V">Anton V. Porov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Osipov%2C+K">Konstantin Osipov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sung%2C+J+S">June Sig Sung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14416v1-abstract-short" style="display: inline;">
        Text-to-Speech (TTS) services that run on edge devices have many advantages compared to cloud TTS, e.g., latency and privacy issues. However, neural vocoders with a low complexity and small model footprint inevitably generate annoying sounds. This study proposes a Bunched LPCNet2, an improved LPCNet architecture that provides highly efficient performance in high-quality for cloud servers and in a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14416v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14416v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14416v1-abstract-full" style="display: none;">
        Text-to-Speech (TTS) services that run on edge devices have many advantages compared to cloud TTS, e.g., latency and privacy issues. However, neural vocoders with a low complexity and small model footprint inevitably generate annoying sounds. This study proposes a Bunched LPCNet2, an improved LPCNet architecture that provides highly efficient performance in high-quality for cloud servers and in a low-complexity for low-resource edge devices. Single logistic distribution achieves computational efficiency, and insightful tricks reduce the model footprint while maintaining speech quality. A DualRate architecture, which generates a lower sampling rate from a prosody model, is also proposed to reduce maintenance costs. The experiments demonstrate that Bunched LPCNet2 generates satisfactory speech quality with a model footprint of 1.1MB while operating faster than real-time on a RPi 3B. Our audio samples are available at https://srtts.github.io/bunchedLPCNet2.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14416v1-abstract-full').style.display = 'none'; document.getElementById('2203.14416v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to INTERSPEECH 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.07621">arXiv:2203.07621</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.07621">pdf</a>, <a href="https://arxiv.org/format/2203.07621">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Practical Detectability for Persistent Lock-Free Data Structures
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyeongmin Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jeon%2C+S">Seungmin Jeon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+J">Jeehoon Kang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.07621v1-abstract-short" style="display: inline;">
        Persistent memory (PM) is an emerging class of storage technology that combines the benefits of DRAM and SSD. This characteristic inspires research on persistent objects in PM with fine-grained concurrency control. Among such objects, persistent lock-free data structures (DSs) are particularly interesting thanks to their efficiency and scalability. One of the most widely used correctness criteria&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.07621v1-abstract-full').style.display = 'inline'; document.getElementById('2203.07621v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.07621v1-abstract-full" style="display: none;">
        Persistent memory (PM) is an emerging class of storage technology that combines the benefits of DRAM and SSD. This characteristic inspires research on persistent objects in PM with fine-grained concurrency control. Among such objects, persistent lock-free data structures (DSs) are particularly interesting thanks to their efficiency and scalability. One of the most widely used correctness criteria for persistent lock-free DSs is durable linearizability (Izraelevitz et. al., DISC 2016). However, durable linearizability is insufficient to use persistent DSs for fault-tolerant systems requiring exactly-once semantics for storage systems, because we may not be able to detect whether an operation is performed when a crash occurs.
  We present a practical programming framework for persistent lock-free DSs with detectability. In contrast to the prior work on such DSs, our framework supports (1) primitive detectable operations such as space-efficient compare-and-swap, insertion, and deletion; (2) systematic transformation of lock-free DSs in DRAM into those in PM requiring modest efforts; (3) comparable performance with non-detectable DSs by DRAM scratchpad optimization; and (4) recovery from both full system and thread crashes. The key idea is memento objects serving as a lightweight, precise, and per-thread checkpoints in PM. As a case study, we implement lock-free and combining queues and hash tables with detectability that outperform (and perform comparably) the state-of-the-art DSs with (and without, respectively) detectability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.07621v1-abstract-full').style.display = 'none'; document.getElementById('2203.07621v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          D.3.3; E.1; D.1.3
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.05306">arXiv:2202.05306</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.05306">pdf</a>, <a href="https://arxiv.org/format/2202.05306">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+N">Nan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jastrz%C4%99bski%2C+S">Stanisław Jastrzębski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Geras%2C+K+J">Krzysztof J. Geras</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.05306v1-abstract-short" style="display: inline;">
        We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks, these models tend to rely on just one modality while under-fitting the other modalities. Such behavior is counter-intuitive and hurts the models&#39; generalization, as we observe empirically. To estimate the model&#39;s dependence on each modality, we compute the gain on the accuracy when the model has access to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.05306v1-abstract-full').style.display = 'inline'; document.getElementById('2202.05306v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.05306v1-abstract-full" style="display: none;">
        We hypothesize that due to the greedy nature of learning in multi-modal deep neural networks, these models tend to rely on just one modality while under-fitting the other modalities. Such behavior is counter-intuitive and hurts the models&#39; generalization, as we observe empirically. To estimate the model&#39;s dependence on each modality, we compute the gain on the accuracy when the model has access to it in addition to another modality. We refer to this gain as the conditional utilization rate. In the experiments, we consistently observe an imbalance in conditional utilization rates between modalities, across multiple tasks and architectures. Since conditional utilization rate cannot be computed efficiently during training, we introduce a proxy for it based on the pace at which the model learns from each modality, which we refer to as the conditional learning speed. We propose an algorithm to balance the conditional learning speeds between modalities during training and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm improves the model&#39;s generalization on three datasets: Colored MNIST, Princeton ModelNet40, and NVIDIA Dynamic Hand Gesture.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.05306v1-abstract-full').style.display = 'none'; document.getElementById('2202.05306v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.04136">arXiv:2202.04136</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.04136">pdf</a>, <a href="https://arxiv.org/format/2202.04136">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generative multitask learning mitigates target-causing confounding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Makino%2C+T">Taro Makino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Geras%2C+K">Krzysztof Geras</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.04136v1-abstract-short" style="display: inline;">
        We propose a simple and scalable approach to causal representation learning for multitask learning. Our approach requires minimal modification to existing ML systems, and improves robustness to prior probability shift. The improvement comes from mitigating unobserved confounders that cause the targets, but not the input. We refer to them as target-causing confounders. These confounders induce spur&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.04136v1-abstract-full').style.display = 'inline'; document.getElementById('2202.04136v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.04136v1-abstract-full" style="display: none;">
        We propose a simple and scalable approach to causal representation learning for multitask learning. Our approach requires minimal modification to existing ML systems, and improves robustness to prior probability shift. The improvement comes from mitigating unobserved confounders that cause the targets, but not the input. We refer to them as target-causing confounders. These confounders induce spurious dependencies between the input and targets. This poses a problem for the conventional approach to multitask learning, due to its assumption that the targets are conditionally independent given the input. Our proposed approach takes into account the dependency between the targets in order to alleviate target-causing confounding. All that is required in addition to usual practice is to estimate the joint distribution of the targets to switch from discriminative to generative classification, and to predict all targets jointly. Our results on the Attributes of People and Taskonomy datasets reflect the conceptual improvement in robustness to prior probability shift.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.04136v1-abstract-full').style.display = 'none'; document.getElementById('2202.04136v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.03651">arXiv:2202.03651</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.03651">pdf</a>, <a href="https://arxiv.org/format/2202.03651">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Causal Scene BERT: Improving object detection by searching for challenging groups of data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Resnick%2C+C">Cinjon Resnick</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Litany%2C+O">Or Litany</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kar%2C+A">Amlan Kar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kreis%2C+K">Karsten Kreis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lucas%2C+J">James Lucas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fidler%2C+S">Sanja Fidler</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.03651v2-abstract-short" style="display: inline;">
        Modern computer vision applications rely on learning-based perception modules parameterized with neural networks for tasks like object detection. These modules frequently have low expected error overall but high error on atypical groups of data due to biases inherent in the training process. In building autonomous vehicles (AV), this problem is an especially important challenge because their perce&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.03651v2-abstract-full').style.display = 'inline'; document.getElementById('2202.03651v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.03651v2-abstract-full" style="display: none;">
        Modern computer vision applications rely on learning-based perception modules parameterized with neural networks for tasks like object detection. These modules frequently have low expected error overall but high error on atypical groups of data due to biases inherent in the training process. In building autonomous vehicles (AV), this problem is an especially important challenge because their perception modules are crucial to the overall system performance. After identifying failures in AV, a human team will comb through the associated data to group perception failures that share common causes. More data from these groups is then collected and annotated before retraining the model to fix the issue. In other words, error groups are found and addressed in hindsight. Our main contribution is a pseudo-automatic method to discover such groups in foresight by performing causal interventions on simulated scenes. To keep our interventions on the data manifold, we utilize masked language models. We verify that the prioritized groups found via intervention are challenging for the object detector and show that retraining with data collected from these groups helps inordinately compared to adding more IID data. We also plan to release software to run interventions in simulated scenes, which we hope will benefit the causality community.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.03651v2-abstract-full').style.display = 'none'; document.getElementById('2202.03651v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In submission at JMLR; 0xe5110eA3B5014cd9a585Dc76c74Ee509F504Be14</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.06096">arXiv:2201.06096</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.06096">pdf</a>, <a href="https://arxiv.org/format/2201.06096">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        New Phenomena in Large-Scale Internet Traffic
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kepner%2C+J">Jeremy Kepner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kenjiro Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Claffy%2C+K">KC Claffy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gadepally%2C+V">Vijay Gadepally</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=McGuire%2C+S">Sarah McGuire</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Milechin%2C+L">Lauren Milechin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arcand%2C+W">William Arcand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bestor%2C+D">David Bestor</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bergeron%2C+W">William Bergeron</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Byun%2C+C">Chansup Byun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hubbell%2C+M">Matthew Hubbell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Houle%2C+M">Michael Houle</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jones%2C+M">Michael Jones</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Prout%2C+A">Andrew Prout</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reuther%2C+A">Albert Reuther</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rosa%2C+A">Antonio Rosa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Samsi%2C+S">Siddharth Samsi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yee%2C+C">Charles Yee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Michaleas%2C+P">Peter Michaleas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.06096v1-abstract-short" style="display: inline;">
        The Internet is transforming our society, necessitating a quantitative understanding of Internet traffic. Our team collects and curates the largest publicly available Internet traffic data sets. An analysis of 50 billion packets using 10,000 processors in the MIT SuperCloud reveals a new phenomenon: the importance of otherwise unseen leaf nodes and isolated links in Internet traffic. Our analysis&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.06096v1-abstract-full').style.display = 'inline'; document.getElementById('2201.06096v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.06096v1-abstract-full" style="display: none;">
        The Internet is transforming our society, necessitating a quantitative understanding of Internet traffic. Our team collects and curates the largest publicly available Internet traffic data sets. An analysis of 50 billion packets using 10,000 processors in the MIT SuperCloud reveals a new phenomenon: the importance of otherwise unseen leaf nodes and isolated links in Internet traffic. Our analysis further shows that a two-parameter modified Zipf-Mandelbrot distribution accurately describes a wide variety of source/destination statistics on moving sample windows ranging from 100{,}000 to 100{,}000{,}000 packets over collections that span years and continents. The measured model parameters distinguish different network streams, and the model leaf parameter strongly correlates with the fraction of the traffic in different underlying network topologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.06096v1-abstract-full').style.display = 'none'; document.getElementById('2201.06096v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">53 pages, 27 figures, 8 tables, 121 references. Portions of this work originally appeared as arXiv:1904.04396v1 which has been split for publication in the book &#34;Massive Graph Analytics&#34; (edited by David Bader)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.06068">arXiv:2201.06068</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.06068">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Zero Botnets: An Observe-Pursue-Counter Approach
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kepner%2C+J">Jeremy Kepner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bernays%2C+J">Jonathan Bernays</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Buckley%2C+S">Stephen Buckley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kenjiro Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Conrad%2C+C">Cary Conrad</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Daigle%2C+L">Leslie Daigle</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Erhardt%2C+K">Keeley Erhardt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gadepally%2C+V">Vijay Gadepally</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Greene%2C+B">Barry Greene</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jones%2C+M">Michael Jones</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Knake%2C+R">Robert Knake</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Maggs%2C+B">Bruce Maggs</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Michaleas%2C+P">Peter Michaleas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meiners%2C+C">Chad Meiners</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Morris%2C+A">Andrew Morris</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pentland%2C+A">Alex Pentland</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pisharody%2C+S">Sandeep Pisharody</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Powazek%2C+S">Sarah Powazek</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Prout%2C+A">Andrew Prout</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reiner%2C+P">Philip Reiner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Suzuki%2C+K">Koichi Suzuki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Takahashi%2C+K">Kenji Takahashi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tauber%2C+T">Tony Tauber</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Walker%2C+L">Leah Walker</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stetson%2C+D">Douglas Stetson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.06068v1-abstract-short" style="display: inline;">
        Adversarial Internet robots (botnets) represent a growing threat to the safe use and stability of the Internet. Botnets can play a role in launching adversary reconnaissance (scanning and phishing), influence operations (upvoting), and financing operations (ransomware, market manipulation, denial of service, spamming, and ad click fraud) while obfuscating tailored tactical operations. Reducing the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.06068v1-abstract-full').style.display = 'inline'; document.getElementById('2201.06068v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.06068v1-abstract-full" style="display: none;">
        Adversarial Internet robots (botnets) represent a growing threat to the safe use and stability of the Internet. Botnets can play a role in launching adversary reconnaissance (scanning and phishing), influence operations (upvoting), and financing operations (ransomware, market manipulation, denial of service, spamming, and ad click fraud) while obfuscating tailored tactical operations. Reducing the presence of botnets on the Internet, with the aspirational target of zero, is a powerful vision for galvanizing policy action. Setting a global goal, encouraging international cooperation, creating incentives for improving networks, and supporting entities for botnet takedowns are among several policies that could advance this goal. These policies raise significant questions regarding proper authorities/access that cannot be answered in the abstract. Systems analysis has been widely used in other domains to achieve sufficient detail to enable these questions to be dealt with in concrete terms. Defeating botnets using an observe-pursue-counter architecture is analyzed, the technical feasibility is affirmed, and the authorities/access questions are significantly narrowed. Recommended next steps include: supporting the international botnet takedown community, expanding network observatories, enhancing the underlying network science at scale, conducting detailed systems analysis, and developing appropriate policy frameworks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.06068v1-abstract-full').style.display = 'none'; document.getElementById('2201.06068v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">26 pages, 13 figures, 2 tables, 72 references, submitted to PlosOne</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.13969">arXiv:2112.13969</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.13969">pdf</a>, <a href="https://arxiv.org/format/2112.13969">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LINDA: Unsupervised Learning to Interpolate in Natural Language Processing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+Y">Yekyung Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jeong%2C+S">Seohyeong Jeong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.13969v1-abstract-short" style="display: inline;">
        Despite the success of mixup in data augmentation, its applicability to natural language processing (NLP) tasks has been limited due to the discrete and variable-length nature of natural languages. Recent studies have thus relied on domain-specific heuristics and manually crafted resources, such as dictionaries, in order to apply mixup in NLP. In this paper, we instead propose an unsupervised lear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.13969v1-abstract-full').style.display = 'inline'; document.getElementById('2112.13969v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.13969v1-abstract-full" style="display: none;">
        Despite the success of mixup in data augmentation, its applicability to natural language processing (NLP) tasks has been limited due to the discrete and variable-length nature of natural languages. Recent studies have thus relied on domain-specific heuristics and manually crafted resources, such as dictionaries, in order to apply mixup in NLP. In this paper, we instead propose an unsupervised learning approach to text interpolation for the purpose of data augmentation, to which we refer as &#34;Learning to INterpolate for Data Augmentation&#34; (LINDA), that does not require any heuristics nor manually crafted resources but learns to interpolate between any pair of natural language sentences over a natural language manifold. After empirically demonstrating the LINDA&#39;s interpolation capability, we show that LINDA indeed allows us to seamlessly apply mixup in NLP and leads to better generalization in text classification both in-domain and out-of-domain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.13969v1-abstract-full').style.display = 'none'; document.getElementById('2112.13969v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.08914">arXiv:2112.08914</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.08914">pdf</a>, <a href="https://arxiv.org/format/2112.08914">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kulikov%2C+I">Ilia Kulikov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eremeev%2C+M">Maksim Eremeev</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.08914v2-abstract-short" style="display: inline;">
        Neural autoregressive sequence models smear the probability among many possible sequences including degenerate ones, such as empty or repetitive sequences. In this work, we tackle one specific case where the model assigns a high probability to unreasonably short sequences. We define the oversmoothing rate to quantify this issue. After confirming the high degree of oversmoothing in neural machine t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.08914v2-abstract-full').style.display = 'inline'; document.getElementById('2112.08914v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.08914v2-abstract-full" style="display: none;">
        Neural autoregressive sequence models smear the probability among many possible sequences including degenerate ones, such as empty or repetitive sequences. In this work, we tackle one specific case where the model assigns a high probability to unreasonably short sequences. We define the oversmoothing rate to quantify this issue. After confirming the high degree of oversmoothing in neural machine translation, we propose to explicitly minimize the oversmoothing rate during training. We conduct a set of experiments to study the effect of the proposed regularization on both model distribution and decoding performance. We use a neural machine translation task as the testbed and consider three different datasets of varying size. Our experiments reveal three major findings. First, we can control the oversmoothing rate of the model by tuning the strength of the regularization. Second, by enhancing the oversmoothing loss contribution, the probability and the rank of &lt;eos&gt; token decrease heavily at positions where it is not supposed to be. Third, the proposed regularization impacts the outcome of beam search especially when a large beam is used. The degradation of translation quality (measured in BLEU) with a large beam significantly lessens with lower oversmoothing rate, but the degradation compared to smaller beam sizes remains to exist. From these observations, we conclude that the high degree of oversmoothing is the main reason behind the degenerate case of overly probable short sequences in a neural autoregressive model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.08914v2-abstract-full').style.display = 'none'; document.getElementById('2112.08914v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Ilia Kulikov and Maksim Eremeev contributed equally</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.08670">arXiv:2112.08670</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.08670">pdf</a>, <a href="https://arxiv.org/format/2112.08670">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Amortized Noisy Channel Neural Machine Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pang%2C+R+Y">Richard Yuanzhe Pang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+H">He He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.08670v1-abstract-short" style="display: inline;">
        Noisy channel models have been especially effective in neural machine translation (NMT). However, recent approaches like &#34;beam search and rerank&#34; (BSR) incur significant computation overhead during inference, making real-world application infeasible. We aim to build an amortized noisy channel NMT model such that greedily decoding from it would generate translations that maximize the same reward as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.08670v1-abstract-full').style.display = 'inline'; document.getElementById('2112.08670v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.08670v1-abstract-full" style="display: none;">
        Noisy channel models have been especially effective in neural machine translation (NMT). However, recent approaches like &#34;beam search and rerank&#34; (BSR) incur significant computation overhead during inference, making real-world application infeasible. We aim to build an amortized noisy channel NMT model such that greedily decoding from it would generate translations that maximize the same reward as translations generated using BSR. We attempt three approaches: knowledge distillation, 1-step-deviation imitation learning, and Q learning. The first approach obtains the noisy channel signal from a pseudo-corpus, and the latter two approaches aim to optimize toward a noisy-channel MT reward directly. All three approaches speed up inference by 1-2 orders of magnitude. For all three approaches, the generated translations fail to achieve rewards comparable to BSR, but the translation quality approximated by BLEU is similar to the quality of BSR-produced translations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.08670v1-abstract-full').style.display = 'none'; document.getElementById('2112.08670v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.08656">arXiv:2111.08656</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.08656">pdf</a>, <a href="https://arxiv.org/format/2111.08656">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Causal Effect Variational Autoencoder with Uniform Treatment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Im%2C+D+J">Daniel Jiwoong Im</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Razavian%2C+N">Narges Razavian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.08656v1-abstract-short" style="display: inline;">
        Causal effect variational autoencoder (CEVAE) are trained to predict the outcome given observational treatment data, while uniform treatment variational autoencoders (UTVAE) are trained with uniform treatment distribution using importance sampling. In this paper, we show that using uniform treatment over observational treatment distribution leads to better causal inference by mitigating the distri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08656v1-abstract-full').style.display = 'inline'; document.getElementById('2111.08656v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.08656v1-abstract-full" style="display: none;">
        Causal effect variational autoencoder (CEVAE) are trained to predict the outcome given observational treatment data, while uniform treatment variational autoencoders (UTVAE) are trained with uniform treatment distribution using importance sampling. In this paper, we show that using uniform treatment over observational treatment distribution leads to better causal inference by mitigating the distribution shift that occurs from training to test time. We also explore the combination of uniform and observational treatment distributions with inference and generative network training objectives to find a better training procedure for inferring treatment effect. Experimentally, we find that the proposed UTVAE yields better absolute average treatment effect error and precision in estimation of heterogeneous effect error than the CEVAE on synthetic and IHDP datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08656v1-abstract-full').style.display = 'none'; document.getElementById('2111.08656v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.07393">arXiv:2111.07393</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.07393">pdf</a>, <a href="https://arxiv.org/format/2111.07393">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DEEP: DEnoising Entity Pre-training for Neural Machine Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+J">Junjie Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hayashi%2C+H">Hiroaki Hayashi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Neubig%2C+G">Graham Neubig</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.07393v1-abstract-short" style="display: inline;">
        It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus. Earlier named entity translation methods mainly focus on phonetic transliteration, which ignores the sentence context for translation and is limited in domain and language coverage. To address this limitation, we propose DEEP, a DEnoising Entity Pre-tr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.07393v1-abstract-full').style.display = 'inline'; document.getElementById('2111.07393v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.07393v1-abstract-full" style="display: none;">
        It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus. Earlier named entity translation methods mainly focus on phonetic transliteration, which ignores the sentence context for translation and is limited in domain and language coverage. To address this limitation, we propose DEEP, a DEnoising Entity Pre-training method that leverages large amounts of monolingual data and a knowledge base to improve named entity translation accuracy within sentences. Besides, we investigate a multi-task learning strategy that finetunes a pre-trained neural machine translation model on both entity-augmented monolingual data and parallel data to further improve entity translation. Experimental results on three language pairs demonstrate that \method results in significant improvements over strong denoising auto-encoding baselines, with a gain of up to 1.3 BLEU and up to 9.2 entity accuracy points for English-Russian translation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.07393v1-abstract-full').style.display = 'none'; document.getElementById('2111.07393v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.02508">arXiv:2111.02508</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.02508">pdf</a>, <a href="https://arxiv.org/format/2111.02508">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AlphaD3M: Machine Learning Pipeline Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Drori%2C+I">Iddo Drori</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Krishnamurthy%2C+Y">Yamuna Krishnamurthy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rampin%2C+R">Remi Rampin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lourenco%2C+R+d+P">Raoni de Paula Lourenco</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ono%2C+J+P">Jorge Piazentin Ono</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Silva%2C+C">Claudio Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Freire%2C+J">Juliana Freire</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.02508v1-abstract-short" style="display: inline;">
        We introduce AlphaD3M, an automatic machine learning (AutoML) system based on meta reinforcement learning using sequence models with self play. AlphaD3M is based on edit operations performed over machine learning pipeline primitives providing explainability. We compare AlphaD3M with state-of-the-art AutoML systems: Autosklearn, Autostacker, and TPOT, on OpenML datasets. AlphaD3M achieves competiti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.02508v1-abstract-full').style.display = 'inline'; document.getElementById('2111.02508v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.02508v1-abstract-full" style="display: none;">
        We introduce AlphaD3M, an automatic machine learning (AutoML) system based on meta reinforcement learning using sequence models with self play. AlphaD3M is based on edit operations performed over machine learning pipeline primitives providing explainability. We compare AlphaD3M with state-of-the-art AutoML systems: Autosklearn, Autostacker, and TPOT, on OpenML datasets. AlphaD3M achieves competitive performance while being an order of magnitude faster, reducing computation time from hours to minutes, and is explainable by design.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.02508v1-abstract-full').style.display = 'none'; document.getElementById('2111.02508v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICML 2018 AutoML Workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.09646">arXiv:2110.09646</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.09646">pdf</a>, <a href="https://arxiv.org/format/2110.09646">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Monotonic Simultaneous Translation with Chunk-wise Reordering and Refinement
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+H">HyoJung Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ahn%2C+S">Seokchan Ahn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+Y">Yoonjung Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chung%2C+I">Insoo Chung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Sangha Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.09646v1-abstract-short" style="display: inline;">
        Recent work in simultaneous machine translation is often trained with conventional full sentence translation corpora, leading to either excessive latency or necessity to anticipate as-yet-unarrived words, when dealing with a language pair whose word orders significantly differ. This is unlike human simultaneous interpreters who produce largely monotonic translations at the expense of the grammatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09646v1-abstract-full').style.display = 'inline'; document.getElementById('2110.09646v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.09646v1-abstract-full" style="display: none;">
        Recent work in simultaneous machine translation is often trained with conventional full sentence translation corpora, leading to either excessive latency or necessity to anticipate as-yet-unarrived words, when dealing with a language pair whose word orders significantly differ. This is unlike human simultaneous interpreters who produce largely monotonic translations at the expense of the grammaticality of a sentence being translated. In this paper, we thus propose an algorithm to reorder and refine the target side of a full sentence translation corpus, so that the words/phrases between the source and target sentences are aligned largely monotonically, using word alignment and non-autoregressive neural machine translation. We then train a widely used wait-k simultaneous translation model on this reordered-and-refined corpus. The proposed approach improves BLEU scores and resulting translations exhibit enhanced monotonicity with source sentences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09646v1-abstract-full').style.display = 'none'; document.getElementById('2110.09646v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To be published in WMT2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.09888">arXiv:2109.09888</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.09888">pdf</a>, <a href="https://arxiv.org/format/2109.09888">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Chemical Physics">physics.chem-ph</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Chemical-Reaction-Aware Molecule Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hongwei Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Weijiang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+X">Xiaomeng Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+H">Heng Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jiawei Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burke%2C+M+D">Martin D. Burke</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.09888v2-abstract-short" style="display: inline;">
        Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, existing SMILES-based (Simplified Molecular-Input Line-Entry System) or GNN-based (Graph Neural Networks) MRL methods either take SMILES strings as input that have difficulty in encoding molecule structure information, or over-emphasize the importance of GNN architectures but neglect their gene&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.09888v2-abstract-full').style.display = 'inline'; document.getElementById('2109.09888v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.09888v2-abstract-full" style="display: none;">
        Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, existing SMILES-based (Simplified Molecular-Input Line-Entry System) or GNN-based (Graph Neural Networks) MRL methods either take SMILES strings as input that have difficulty in encoding molecule structure information, or over-emphasize the importance of GNN architectures but neglect their generalization ability. Here we propose using chemical reactions to assist learning molecule representation. The key idea of our approach is to preserve the equivalence of molecules with respect to chemical reactions in the embedding space, i.e., forcing the sum of reactant embeddings and the sum of product embeddings to be equal for each chemical equation. This constraint is proven effective to 1) keep the embedding space well-organized and 2) improve the generalization ability of molecule embeddings. Moreover, our model can use any GNN as the molecule encoder and is thus agnostic to GNN architectures. Experimental results demonstrate that our method achieves state-of-the-art performance in a variety of downstream tasks, e.g., 17.4% absolute Hit@1 gain in chemical reaction prediction, 2.3% absolute AUC gain in molecule property prediction, and 18.5% relative RMSE gain in graph-edit-distance prediction, respectively, over the best baseline method. The code is available at https://github.com/hwwang55/MolR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.09888v2-abstract-full').style.display = 'none'; document.getElementById('2109.09888v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 September, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.08227">arXiv:2109.08227</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.08227">pdf</a>, <a href="https://arxiv.org/format/2109.08227">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Brundyn%2C+A">Annika Brundyn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Swanson%2C+J">Jesse Swanson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kondziolka%2C+D">Doug Kondziolka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oermann%2C+E">Eric Oermann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.08227v1-abstract-short" style="display: inline;">
        We introduce the task of stereo video reconstruction or, equivalently, 2D-to-3D video conversion for minimally invasive surgical video. We design and implement a series of end-to-end U-Net-based solutions for this task by varying the input (single frame vs. multiple consecutive frames), loss function (MSE, MAE, or perceptual losses), and network architecture. We evaluate these solutions by surveyi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.08227v1-abstract-full').style.display = 'inline'; document.getElementById('2109.08227v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.08227v1-abstract-full" style="display: none;">
        We introduce the task of stereo video reconstruction or, equivalently, 2D-to-3D video conversion for minimally invasive surgical video. We design and implement a series of end-to-end U-Net-based solutions for this task by varying the input (single frame vs. multiple consecutive frames), loss function (MSE, MAE, or perceptual losses), and network architecture. We evaluate these solutions by surveying ten experts - surgeons who routinely perform endoscopic surgery. We run two separate reader studies: one evaluating individual frames and the other evaluating fully reconstructed 3D video played on a VR headset. In the first reader study, a variant of the U-Net that takes as input multiple consecutive video frames and outputs the missing view performs best. We draw two conclusions from this outcome. First, motion information coming from multiple past frames is crucial in recreating stereo vision. Second, the proposed U-Net variant can indeed exploit such motion information for solving this task. The result from the second study further confirms the effectiveness of the proposed U-Net variant. The surgeons reported that they could successfully perceive depth from the reconstructed 3D video clips. They also expressed a clear preference for the reconstructed 3D video over the original 2D video. These two reader studies strongly support the usefulness of the proposed task of stereo reconstruction for minimally invasive surgical video and indicate that deep learning is a promising approach to this task. Finally, we identify two automatic metrics, LPIPS and DISTS, that are strongly correlated with expert judgement and that could serve as proxies for the latter in future studies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.08227v1-abstract-full').style.display = 'none'; document.getElementById('2109.08227v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.02772">arXiv:2109.02772</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.02772">pdf</a>, <a href="https://arxiv.org/format/2109.02772">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Empirical Study on Few-shot Knowledge Probing for Pretrained Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+T">Tianxing He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Glass%2C+J">James Glass</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.02772v2-abstract-short" style="display: inline;">
        Prompt-based knowledge probing for 1-hop relations has been used to measure how much world knowledge is stored in pretrained language models. Existing work uses considerable amounts of data to tune the prompts for better performance. In this work, we compare a variety of approaches under a few-shot knowledge probing setting, where only a small number (e.g., 10 or 20) of example triples are availab&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.02772v2-abstract-full').style.display = 'inline'; document.getElementById('2109.02772v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.02772v2-abstract-full" style="display: none;">
        Prompt-based knowledge probing for 1-hop relations has been used to measure how much world knowledge is stored in pretrained language models. Existing work uses considerable amounts of data to tune the prompts for better performance. In this work, we compare a variety of approaches under a few-shot knowledge probing setting, where only a small number (e.g., 10 or 20) of example triples are available. In addition, we create a new dataset named TREx-2p, which contains 2-hop relations. We report that few-shot examples can strongly boost the probing performance for both 1-hop and 2-hop relations. In particular, we find that a simple-yet-effective approach of finetuning the bias vectors in the model outperforms existing prompt-engineering methods. Our dataset and code are available at \url{https://github.com/cloudygoose/fewshot_lama}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.02772v2-abstract-full').style.display = 'none'; document.getElementById('2109.02772v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 September, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.06897">arXiv:2108.06897</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.06897">pdf</a>, <a href="https://arxiv.org/format/2108.06897">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AutoChart: A Dataset for Chart-to-Text Generation Task
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+J">Jiawen Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ran%2C+J">Jinye Ran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+R+K">Roy Ka-wei Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choo%2C+K">Kenny Choo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhi Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.06897v1-abstract-short" style="display: inline;">
        The analytical description of charts is an exciting and important research area with many applications in academia and industry. Yet, this challenging task has received limited attention from the computational linguistics research community. This paper proposes \textsf{AutoChart}, a large dataset for the analytical description of charts, which aims to encourage more research into this important ar&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.06897v1-abstract-full').style.display = 'inline'; document.getElementById('2108.06897v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.06897v1-abstract-full" style="display: none;">
        The analytical description of charts is an exciting and important research area with many applications in academia and industry. Yet, this challenging task has received limited attention from the computational linguistics research community. This paper proposes \textsf{AutoChart}, a large dataset for the analytical description of charts, which aims to encourage more research into this important area. Specifically, we offer a novel framework that generates the charts and their analytical description automatically. We conducted extensive human and machine evaluations on the generated charts and descriptions and demonstrate that the generated texts are informative, coherent, and relevant to the corresponding charts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.06897v1-abstract-full').style.display = 'none'; document.getElementById('2108.06897v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.04800">arXiv:2108.04800</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.04800">pdf</a>, <a href="https://arxiv.org/format/2108.04800">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Meta-repository of screening mammography classifiers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Stadnick%2C+B">Benjamin Stadnick</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Witowski%2C+J">Jan Witowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rajiv%2C+V">Vishwaesh Rajiv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ch%C5%82%C4%99dowski%2C+J">Jakub Chłędowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shamout%2C+F+E">Farah E. Shamout</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Geras%2C+K+J">Krzysztof J. Geras</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.04800v3-abstract-short" style="display: inline;">
        Artificial intelligence (AI) is showing promise in improving clinical diagnosis. In breast cancer screening, recent studies show that AI has the potential to improve early cancer diagnosis and reduce unnecessary workup. As the number of proposed models and their complexity grows, it is becoming increasingly difficult to re-implement them. To enable reproducibility of research and to enable compari&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.04800v3-abstract-full').style.display = 'inline'; document.getElementById('2108.04800v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.04800v3-abstract-full" style="display: none;">
        Artificial intelligence (AI) is showing promise in improving clinical diagnosis. In breast cancer screening, recent studies show that AI has the potential to improve early cancer diagnosis and reduce unnecessary workup. As the number of proposed models and their complexity grows, it is becoming increasingly difficult to re-implement them. To enable reproducibility of research and to enable comparison between different methods, we release a meta-repository containing models for classification of screening mammograms. This meta-repository creates a framework that enables the evaluation of AI models on any screening mammography data set. At its inception, our meta-repository contains five state-of-the-art models with open-source implementations and cross-platform compatibility. We compare their performance on seven international data sets. Our framework has a flexible design that can be generalized to other medical image analysis tasks. The meta-repository is available at https://www.github.com/nyukat/mammography_metarepository.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.04800v3-abstract-full').style.display = 'none'; document.getElementById('2108.04800v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 2 figures. Meta-repository available at https://www.github.com/nyukat/mammography_metarepository ; v3 adds results on the CSAW-CC dataset</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.12329">arXiv:2107.12329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.12329">pdf</a>, <a href="https://arxiv.org/format/2107.12329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AASAE: Augmentation-Augmented Stochastic Autoencoders
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Falcon%2C+W">William Falcon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jha%2C+A+H">Ananya Harsh Jha</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Koker%2C+T">Teddy Koker</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.12329v2-abstract-short" style="display: inline;">
        Recent methods for self-supervised learning can be grouped into two paradigms: contrastive and non-contrastive approaches. Their success can largely be attributed to data augmentation pipelines which generate multiple views of a single input that preserve the underlying semantics. In this work, we introduce augmentation-augmented stochastic autoencoders (AASAE), yet another alternative to self-sup&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.12329v2-abstract-full').style.display = 'inline'; document.getElementById('2107.12329v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.12329v2-abstract-full" style="display: none;">
        Recent methods for self-supervised learning can be grouped into two paradigms: contrastive and non-contrastive approaches. Their success can largely be attributed to data augmentation pipelines which generate multiple views of a single input that preserve the underlying semantics. In this work, we introduce augmentation-augmented stochastic autoencoders (AASAE), yet another alternative to self-supervised learning, based on autoencoding. We derive AASAE starting from the conventional variational autoencoder (VAE), by replacing the KL divergence regularization, which is agnostic to the input domain, with data augmentations that explicitly encourage the internal representations to encode domain-specific invariances and equivariances. We empirically evaluate the proposed AASAE on image classification, similar to how recent contrastive and non-contrastive learning algorithms have been evaluated. Our experiments confirm the effectiveness of data augmentation as a replacement for KL divergence regularization. The AASAE outperforms the VAE by 30% on CIFAR-10, 40% on STL-10 and 45% on Imagenet. On CIFAR-10 and STL-10, the results for AASAE are largely comparable to the state-of-the-art algorithms for self-supervised learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.12329v2-abstract-full').style.display = 'none'; document.getElementById('2107.12329v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 5 figures, 3 table</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14273">arXiv:2106.14273</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14273">pdf</a>, <a href="https://arxiv.org/format/2106.14273">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ACCESS.2021.3093442">10.1109/ACCESS.2021.3093442 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Systematic Review of Bio-Cyber Interface Technologies and Security Issues for Internet of Bio-Nano Things
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zafar%2C+S">Sidra Zafar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nazir%2C+M">Mohsin Nazir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bakhshi%2C+T">Taimur Bakhshi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khattak%2C+H+A">Hasan Ali Khattak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+S">Sarmadullah Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bilal%2C+M">Muhammad Bilal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choo%2C+K+R">Kim-Kwang Raymond Choo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kwak7%2C+K">Kyung-Sup Kwak7</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sabah%2C+A">Aneeqa Sabah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14273v1-abstract-short" style="display: inline;">
        Advances in synthetic biology and nanotechnology have contributed to the design of tools that can be used to control, reuse, modify, and re-engineer cells&#39; structure, as well as enabling engineers to effectively use biological cells as programmable substrates to realize Bio-Nano Things (biological embedded computing devices). Bio-NanoThings are generally tiny, non-intrusive, and concealable device&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14273v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14273v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14273v1-abstract-full" style="display: none;">
        Advances in synthetic biology and nanotechnology have contributed to the design of tools that can be used to control, reuse, modify, and re-engineer cells&#39; structure, as well as enabling engineers to effectively use biological cells as programmable substrates to realize Bio-Nano Things (biological embedded computing devices). Bio-NanoThings are generally tiny, non-intrusive, and concealable devices that can be used for in-vivo applications such as intra-body sensing and actuation networks, where the use of artificial devices can be detrimental. Such (nano-scale) devices can be used in various healthcare settings such as continuous health monitoring, targeted drug delivery, and nano-surgeries. These services can also be grouped to form a collaborative network (i.e., nanonetwork), whose performance can potentially be improved when connected to higher bandwidth external networks such as the Internet, say via 5G. However, to realize the IoBNT paradigm, it is also important to seamlessly connect the biological environment with the technological landscape by having a dynamic interface design to convert biochemical signals from the human body into an equivalent electromagnetic signal (and vice versa). This, unfortunately, risks the exposure of internal biological mechanisms to cyber-based sensing and medical actuation, with potential security and privacy implications. This paper comprehensively reviews bio-cyber interface for IoBNT architecture, focusing on bio-cyber interfacing options for IoBNT like biologically inspired bio-electronic devices, RFID enabled implantable chips, and electronic tattoos. This study also identifies known and potential security and privacy vulnerabilities and mitigation strategies for consideration in future IoBNT designs and implementations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14273v1-abstract-full').style.display = 'none'; document.getElementById('2106.14273v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">41 pages, 9 tables, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14176">arXiv:2106.14176</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14176">pdf</a>, <a href="https://arxiv.org/ps/2106.14176">ps</a>, <a href="https://arxiv.org/format/2106.14176">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computational Geometry">cs.CG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Linear-Time Approximation Scheme for k-Means Clustering of Affine Subspaces
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyungjin Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oh%2C+E">Eunjin Oh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14176v1-abstract-short" style="display: inline;">
        In this paper, we present a linear-time approximation scheme for $k$-means clustering of \emph{incomplete} data points in $d$-dimensional Euclidean space. An \emph{incomplete} data point with $Δ&gt;0$ unspecified entries is represented as an axis-parallel affine subspaces of dimension $Δ$. The distance between two incomplete data points is defined as the Euclidean distance between two closest points&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14176v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14176v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14176v1-abstract-full" style="display: none;">
        In this paper, we present a linear-time approximation scheme for $k$-means clustering of \emph{incomplete} data points in $d$-dimensional Euclidean space. An \emph{incomplete} data point with $Δ&gt;0$ unspecified entries is represented as an axis-parallel affine subspaces of dimension $Δ$. The distance between two incomplete data points is defined as the Euclidean distance between two closest points in the axis-parallel affine subspaces corresponding to the data points. We present an algorithm for $k$-means clustering of axis-parallel affine subspaces of dimension $Δ$ that yields an $(1+ε)$-approximate solution in $O(nd)$ time. The constants hidden behind $O(\cdot)$ depend only on $Δ, ε$ and $k$. This improves the $O(n^2 d)$-time algorithm by Eiben et al.[SODA&#39;21] by a factor of $n$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14176v1-abstract-full').style.display = 'none'; document.getElementById('2106.14176v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.05459">arXiv:2106.05459</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.05459">pdf</a>, <a href="https://arxiv.org/format/2106.05459">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mode recovery in neural autoregressive sequence modeling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kulikov%2C+I">Ilia Kulikov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Welleck%2C+S">Sean Welleck</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.05459v1-abstract-short" style="display: inline;">
        Despite its wide use, recent studies have revealed unexpected and undesirable properties of neural autoregressive sequence models trained with maximum likelihood, such as an unreasonably high affinity to short sequences after training and to infinitely long sequences at decoding time. We propose to study these phenomena by investigating how the modes, or local maxima, of a distribution are maintai&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.05459v1-abstract-full').style.display = 'inline'; document.getElementById('2106.05459v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.05459v1-abstract-full" style="display: none;">
        Despite its wide use, recent studies have revealed unexpected and undesirable properties of neural autoregressive sequence models trained with maximum likelihood, such as an unreasonably high affinity to short sequences after training and to infinitely long sequences at decoding time. We propose to study these phenomena by investigating how the modes, or local maxima, of a distribution are maintained throughout the full learning chain of the ground-truth, empirical, learned and decoding-induced distributions, via the newly proposed mode recovery cost. We design a tractable testbed where we build three types of ground-truth distributions: (1) an LSTM based structured distribution, (2) an unstructured distribution where probability of a sequence does not depend on its content, and (3) a product of these two which we call a semi-structured distribution. Our study reveals both expected and unexpected findings. First, starting with data collection, mode recovery cost strongly relies on the ground-truth distribution and is most costly with the semi-structured distribution. Second, after learning, mode recovery cost from the ground-truth distribution may increase or decrease compared to data collection, with the largest cost degradation occurring with the semi-structured ground-truth distribution. Finally, the ability of the decoding-induced distribution to recover modes from the learned distribution is highly impacted by the choices made earlier in the learning chain. We conclude that future research must consider the entire learning chain in order to fully understand the potentials and perils and to further improve neural autoregressive sequence models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.05459v1-abstract-full').style.display = 'none'; document.getElementById('2106.05459v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL-IJCNLP 2021 5th Workshop on Structured Prediction for NLP</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.02207">arXiv:2106.02207</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.02207">pdf</a>, <a href="https://arxiv.org/format/2106.02207">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Barcode Method for Generative Model Evaluation driven by Topological Data Analysis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jang%2C+R">Ryoungwoo Jang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+M">Minjee Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eun%2C+D">Da-in Eun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyungjin Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seo%2C+J">Jiyeon Seo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+N">Namkug Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.02207v1-abstract-short" style="display: inline;">
        Evaluating the performance of generative models in image synthesis is a challenging task. Although the Fréchet Inception Distance is a widely accepted evaluation metric, it integrates different aspects (e.g., fidelity and diversity) of synthesized images into a single score and assumes the normality of embedded vectors. Recent methods such as precision-and-recall and its variants such as density-a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02207v1-abstract-full').style.display = 'inline'; document.getElementById('2106.02207v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.02207v1-abstract-full" style="display: none;">
        Evaluating the performance of generative models in image synthesis is a challenging task. Although the Fréchet Inception Distance is a widely accepted evaluation metric, it integrates different aspects (e.g., fidelity and diversity) of synthesized images into a single score and assumes the normality of embedded vectors. Recent methods such as precision-and-recall and its variants such as density-and-coverage have been developed to separate fidelity and diversity based on k-nearest neighborhood methods. In this study, we propose an algorithm named barcode, which is inspired by the topological data analysis and is almost free of assumption and hyperparameter selections. In extensive experiments on real-world datasets as well as theoretical approach on high-dimensional normal samples, it was found that the &#39;usual&#39; normality assumption of embedded vectors has several drawbacks. The experimental results demonstrate that barcode outperforms other methods in evaluating fidelity and diversity of GAN outputs. Official codes can be found in https://github.com/minjeekim00/Barcode.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02207v1-abstract-full').style.display = 'none'; document.getElementById('2106.02207v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.00840">arXiv:2106.00840</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.00840">pdf</a>, <a href="https://arxiv.org/format/2106.00840">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comparing Test Sets with Item Response Theory
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Vania%2C+C">Clara Vania</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Htut%2C+P+M">Phu Mon Htut</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+W">William Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mungra%2C+D">Dhara Mungra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pang%2C+R+Y">Richard Yuanzhe Pang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Phang%2C+J">Jason Phang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Haokun Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bowman%2C+S+R">Samuel R. Bowman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.00840v1-abstract-short" style="display: inline;">
        Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on natural language understanding tasks. Recent results from large pretrained models, though, show that many of these datasets are largely saturated and unlikely to be able to detect further progress. What kind of datasets are still effective at discriminating among strong models, and what kind&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00840v1-abstract-full').style.display = 'inline'; document.getElementById('2106.00840v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.00840v1-abstract-full" style="display: none;">
        Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on natural language understanding tasks. Recent results from large pretrained models, though, show that many of these datasets are largely saturated and unlikely to be able to detect further progress. What kind of datasets are still effective at discriminating among strong models, and what kind of datasets should we expect to be able to detect future improvements? To measure this uniformly across datasets, we draw on Item Response Theory and evaluate 29 datasets using predictions from 18 pretrained Transformer models on individual test examples. We find that Quoref, HellaSwag, and MC-TACO are best suited for distinguishing among state-of-the-art models, while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong models. We also observe span selection task format, which is used for QA datasets like QAMR or SQuAD2.0, is effective in differentiating between strong and weak models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00840v1-abstract-full').style.display = 'none'; document.getElementById('2106.00840v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.11447">arXiv:2105.11447</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.11447">pdf</a>, <a href="https://arxiv.org/format/2105.11447">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        True Few-Shot Learning with Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Perez%2C+E">Ethan Perez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kiela%2C+D">Douwe Kiela</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.11447v1-abstract-short" style="display: inline;">
        Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (&#34;prompts&#34;). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learnin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.11447v1-abstract-full').style.display = 'inline'; document.getElementById('2105.11447v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.11447v1-abstract-full" style="display: none;">
        Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (&#34;prompts&#34;). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model&#39;s true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.11447v1-abstract-full').style.display = 'none'; document.getElementById('2105.11447v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code at https://github.com/ethanjperez/true_few_shot</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.09680">arXiv:2105.09680</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.09680">pdf</a>, <a href="https://arxiv.org/format/2105.09680">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        KLUE: Korean Language Understanding Evaluation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Park%2C+S">Sungjoon Park</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moon%2C+J">Jihyung Moon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Sungdong Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+W+I">Won Ik Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jiyoon Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Park%2C+J">Jangwon Park</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+C">Chisung Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+J">Junseong Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yongsook Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oh%2C+T">Taehwan Oh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Joohong Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oh%2C+J">Juhyun Oh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+S">Sungwon Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jeong%2C+Y">Younghoon Jeong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+I">Inkwon Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seo%2C+S">Sangwoo Seo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+D">Dongjun Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H">Hyunwoo Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+M">Myeonghwa Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jang%2C+S">Seongbo Jang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Do%2C+S">Seungwon Do</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Sunkyoung Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lim%2C+K">Kyungtae Lim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Jongwon Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Park%2C+K">Kyumin Park</a>
      , et al. (6 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.09680v4-abstract-short" style="display: inline;">
        We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE is a collection of 8 Korean natural language understanding (NLU) tasks, including Topic Classification, SemanticTextual Similarity, Natural Language Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking. We build all of the tasks from scrat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.09680v4-abstract-full').style.display = 'inline'; document.getElementById('2105.09680v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.09680v4-abstract-full" style="display: none;">
        We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE is a collection of 8 Korean natural language understanding (NLU) tasks, including Topic Classification, SemanticTextual Similarity, Natural Language Inference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking. We build all of the tasks from scratch from diverse source corpora while respecting copyrights, to ensure accessibility for anyone without any restrictions. With ethical considerations in mind, we carefully design annotation protocols. Along with the benchmark tasks and data, we provide suitable evaluation metrics and fine-tuning recipes for pretrained language models for each task. We furthermore release the pretrained language models (PLM), KLUE-BERT and KLUE-RoBERTa, to help reproducing baseline models on KLUE and thereby facilitate future research. We make a few interesting observations from the preliminary experiments using the proposed KLUE benchmark suite, already demonstrating the usefulness of this new benchmark suite. First, we find KLUE-RoBERTa-large outperforms other baselines, including multilingual PLMs and existing open-source Korean PLMs. Second, we see minimal degradation in performance even when we replace personally identifiable information from the pretraining corpus, suggesting that privacy and NLU capability are not at odds with each other. Lastly, we find that using BPE tokenization in combination with morpheme-level pre-tokenization is effective in tasks involving morpheme-level tagging, detection and generation. In addition to accelerating Korean NLP research, our comprehensive documentation on creating KLUE will facilitate creating similar resources for other languages in the future. KLUE is available at https://klue-benchmark.com.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.09680v4-abstract-full').style.display = 'none'; document.getElementById('2105.09680v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">76 pages, 10 figures, 36 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.08876">arXiv:2105.08876</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.08876">pdf</a>, <a href="https://arxiv.org/format/2105.08876">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Lightweight Privacy-Preserving Scheme Using Label-based Pixel Block Mixing for Image Classification in Deep Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiang%2C+Y">Yuexin Xiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+T">Tiantian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+W">Wei Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+T">Tianqing Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choo%2C+K+R">Kim-Kwang Raymond Choo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.08876v1-abstract-short" style="display: inline;">
        To ensure the privacy of sensitive data used in the training of deep learning models, a number of privacy-preserving methods have been designed by the research community. However, existing schemes are generally designed to work with textual data, or are not efficient when a large number of images is used for training. Hence, in this paper we propose a lightweight and efficient approach to preserve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08876v1-abstract-full').style.display = 'inline'; document.getElementById('2105.08876v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.08876v1-abstract-full" style="display: none;">
        To ensure the privacy of sensitive data used in the training of deep learning models, a number of privacy-preserving methods have been designed by the research community. However, existing schemes are generally designed to work with textual data, or are not efficient when a large number of images is used for training. Hence, in this paper we propose a lightweight and efficient approach to preserve image privacy while maintaining the availability of the training set. Specifically, we design the pixel block mixing algorithm for image classification privacy preservation in deep learning. To evaluate its utility, we use the mixed training set to train the ResNet50, VGG16, InceptionV3 and DenseNet121 models on the WIKI dataset and the CNBC face dataset. Experimental findings on the testing set show that our scheme preserves image privacy while maintaining the availability of the training set in the deep learning models. Additionally, the experimental results demonstrate that we achieve good performance for the VGG16 model on the WIKI dataset and both ResNet50 and DenseNet121 on the CNBC dataset. The pixel block algorithm achieves fairly high efficiency in the mixing of the images, and it is computationally challenging for the attackers to restore the mixed training set to the original training set. Moreover, data augmentation can be applied to the mixed training set to improve the training&#39;s effectiveness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08876v1-abstract-full').style.display = 'none'; document.getElementById('2105.08876v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 16 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6; I.2.9
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.07360">arXiv:2105.07360</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.07360">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Investigating Protected Health Information Leakage from Android Medical Applications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Grispos%2C+G">George Grispos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Flynn%2C+T">Talon Flynn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Glisson%2C+W">William Glisson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choo%2C+K+R">Kim-Kwang Raymond Choo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.07360v1-abstract-short" style="display: inline;">
        As smartphones and smartphone applications are widely used in a healthcare context (e.g., remote healthcare), these devices and applications may need to comply with the Health Insurance Portability and Accountability Act (HIPAA) of 1996. In other words, adequate safeguards to protect the user&#39;s sensitive information (e.g., personally identifiable information and/or medical history) are required to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07360v1-abstract-full').style.display = 'inline'; document.getElementById('2105.07360v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.07360v1-abstract-full" style="display: none;">
        As smartphones and smartphone applications are widely used in a healthcare context (e.g., remote healthcare), these devices and applications may need to comply with the Health Insurance Portability and Accountability Act (HIPAA) of 1996. In other words, adequate safeguards to protect the user&#39;s sensitive information (e.g., personally identifiable information and/or medical history) are required to be enforced on such devices and applications. In this study, we forensically focus on the potential of recovering residual data from Android medical applications, with the objective of providing an initial risk assessment of such applications. Our findings (e.g., documentation of the artifacts) also contribute to a better understanding of the types and location of evidential artifacts that can, potentially, be recovered from these applications in a digital forensic investigation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07360v1-abstract-full').style.display = 'none'; document.getElementById('2105.07360v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at the 5th EAI International Conference on Future Access Enablers of Ubiquitous and Intelligent Infrastructures (EAI FABULOUS 2021), Zagreb, Croatia</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.06612">arXiv:2105.06612</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.06612">pdf</a>, <a href="https://arxiv.org/format/2105.06612">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Consumer, Commercial and Industrial IoT (In)Security: Attack Taxonomy and Case Studies
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xenofontos%2C+C">Christos Xenofontos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zografopoulos%2C+I">Ioannis Zografopoulos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Konstantinou%2C+C">Charalambos Konstantinou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jolfaei%2C+A">Alireza Jolfaei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khan%2C+M+K">Muhammad Khurram Khan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choo%2C+K+R">Kim-Kwang Raymond Choo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.06612v1-abstract-short" style="display: inline;">
        Internet of Things (IoT) devices are becoming ubiquitous in our lives, with applications spanning from the consumer domain to commercial and industrial systems. The steep growth and vast adoption of IoT devices reinforce the importance of sound and robust cybersecurity practices during the device development life-cycles. IoT-related vulnerabilities, if successfully exploited can affect, not only t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.06612v1-abstract-full').style.display = 'inline'; document.getElementById('2105.06612v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.06612v1-abstract-full" style="display: none;">
        Internet of Things (IoT) devices are becoming ubiquitous in our lives, with applications spanning from the consumer domain to commercial and industrial systems. The steep growth and vast adoption of IoT devices reinforce the importance of sound and robust cybersecurity practices during the device development life-cycles. IoT-related vulnerabilities, if successfully exploited can affect, not only the device itself, but also the application field in which the IoT device operates. Evidently, identifying and addressing every single vulnerability is an arduous, if not impossible, task. Attack taxonomies can assist in classifying attacks and their corresponding vulnerabilities. Security countermeasures and best practices can then be leveraged to mitigate threats and vulnerabilities before they emerge into catastrophic attacks and ensure overall secure IoT operation. Therefore, in this paper, we provide an attack taxonomy which takes into consideration the different layers of IoT stack, i.e., device, infrastructure, communication, and service, and each layer&#39;s designated characteristics which can be exploited by adversaries. Furthermore, using nine real-world cybersecurity incidents, that had targeted IoT devices deployed in the consumer, commercial, and industrial sectors, we describe the IoT-related vulnerabilities, exploitation procedures, attacks, impacts, and potential mitigation mechanisms and protection strategies. These (and many other) incidents highlight the underlying security concerns of IoT systems and demonstrate the potential attack impacts of such connected ecosystems, while the proposed taxonomy provides a systematic procedure to categorize attacks based on the affected layer and corresponding impact.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.06612v1-abstract-full').style.display = 'none'; document.getElementById('2105.06612v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE Internet of Things Journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.13568">arXiv:2104.13568</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.13568">pdf</a>, <a href="https://arxiv.org/format/2104.13568">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Interactive Visualization for Exploring Information Fragments in Software Repositories
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+Y">Youngtaek Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jeon%2C+H">Hyeon Jeon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choe%2C+K">Kiroong Choe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+H">Hyunjoo Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+B">Bohyoung Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seo%2C+J">Jinwook Seo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.13568v1-abstract-short" style="display: inline;">
        Software developers explore and inspect software repository data to obtain detailed information archived in the development history. However, developers who are not acquainted with the development context suffer from delving into the repositories with a handful of information; they have difficulty discovering and expanding information fragments considering the topological and sequential multi-dime&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.13568v1-abstract-full').style.display = 'inline'; document.getElementById('2104.13568v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.13568v1-abstract-full" style="display: none;">
        Software developers explore and inspect software repository data to obtain detailed information archived in the development history. However, developers who are not acquainted with the development context suffer from delving into the repositories with a handful of information; they have difficulty discovering and expanding information fragments considering the topological and sequential multi-dimensional structure of repositories. We introduce ExIF, an interactive visualization for exploring information fragments in software repositories. ExIF helps users discover new information fragments within clusters or topological neighbors and identify revisions incorporating user-collected fragments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.13568v1-abstract-full').style.display = 'none'; document.getElementById('2104.13568v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14th IEEE Pacific Visualization Symposium (PacificVis &#39;21) Poster</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.06344">arXiv:2104.06344</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.06344">pdf</a>, <a href="https://arxiv.org/format/2104.06344">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+M">Manling Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+S">Sha Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhenhailong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+L">Lifu Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+H">Heng Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jiawei Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Voss%2C+C">Clare Voss</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.06344v3-abstract-short" style="display: inline;">
        Event schemas encode knowledge of stereotypical structures of events and their connections. As events unfold, schemas are crucial to act as a scaffolding. Previous work on event schema induction focuses either on atomic events or linear temporal event sequences, ignoring the interplay between events via arguments and argument relations. We introduce a new concept of Temporal Complex Event Schema:&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.06344v3-abstract-full').style.display = 'inline'; document.getElementById('2104.06344v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.06344v3-abstract-full" style="display: none;">
        Event schemas encode knowledge of stereotypical structures of events and their connections. As events unfold, schemas are crucial to act as a scaffolding. Previous work on event schema induction focuses either on atomic events or linear temporal event sequences, ignoring the interplay between events via arguments and argument relations. We introduce a new concept of Temporal Complex Event Schema: a graph-based schema representation that encompasses events, arguments, temporal connections and argument relations. In addition, we propose a Temporal Event Graph Model that predicts event instances following the temporal complex event schema. To build and evaluate such schemas, we release a new schema learning corpus containing 6,399 documents accompanied with event graphs, and we have manually constructed gold-standard schemas. Intrinsic evaluations based on schema matching and instance graph perplexity, prove the superior quality of our probabilistic graph schema library compared to linear representations. Extrinsic evaluation on schema-guided future event prediction further demonstrates the predictive power of our event graph model, significantly outperforming human schemas and baselines by more than 23.8% on HITS@1.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.06344v3-abstract-full').style.display = 'none'; document.getElementById('2104.06344v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.05979">arXiv:2104.05979</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.05979">pdf</a>, <a href="https://arxiv.org/format/2104.05979">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Investigating Opportunities to Support Kids&#39; Agency and Well-being: A Review of Kids&#39; Wearables
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zehrung%2C+R">Rachael Zehrung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+L">Lily Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+B">Bongshin Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choe%2C+E+K">Eun Kyoung Choe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.05979v1-abstract-short" style="display: inline;">
        Wearable devices hold great potential for promoting children&#39;s health and well-being. However, research on kids&#39; wearables is sparse and often focuses on their use in the context of parental surveillance. To gain insight into the current landscape of kids&#39; wearables, we surveyed 47 wearable devices marketed for children. We collected rich data on the functionality of these devices and assessed how&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.05979v1-abstract-full').style.display = 'inline'; document.getElementById('2104.05979v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.05979v1-abstract-full" style="display: none;">
        Wearable devices hold great potential for promoting children&#39;s health and well-being. However, research on kids&#39; wearables is sparse and often focuses on their use in the context of parental surveillance. To gain insight into the current landscape of kids&#39; wearables, we surveyed 47 wearable devices marketed for children. We collected rich data on the functionality of these devices and assessed how different features satisfy parents&#39; information needs, and identified opportunities for wearables to support children&#39;s needs and interests. We found that many kids&#39; wearables are technologically sophisticated devices that focus on parents&#39; ability to communicate with their children and keep them safe, as well as encourage physical activity and nurture good habits. We discuss how our findings could inform the design of wearables that serve as more than monitoring devices, and instead support children and parents as equal stakeholders, providing implications for kids&#39; agency, long-term development, and overall well-being. Finally, we identify future research efforts related to designing for kids&#39; self-tracking and collaborative tracking with parents.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.05979v1-abstract-full').style.display = 'none'; document.getElementById('2104.05979v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 1 figure, 5 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.01112">arXiv:2104.01112</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.01112">pdf</a>, <a href="https://arxiv.org/format/2104.01112">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NaturalProofs: Mathematical Theorem Proving in Natural Language
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Welleck%2C+S">Sean Welleck</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiacheng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bras%2C+R+L">Ronan Le Bras</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+Y">Yejin Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.01112v2-abstract-short" style="display: inline;">
        Understanding and creating mathematics using natural mathematical language - the mixture of symbolic and natural language used by humans - is a challenging and important problem for driving progress in machine learning. As a step in this direction, we develop NaturalProofs, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NaturalProofs un&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.01112v2-abstract-full').style.display = 'inline'; document.getElementById('2104.01112v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.01112v2-abstract-full" style="display: none;">
        Understanding and creating mathematics using natural mathematical language - the mixture of symbolic and natural language used by humans - is a challenging and important problem for driving progress in machine learning. As a step in this direction, we develop NaturalProofs, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NaturalProofs unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization. Using NaturalProofs, we benchmark strong neural methods on mathematical reference retrieval and generation tasks which test a system&#39;s ability to determine key results that appear in a proof. Large-scale sequence models show promise compared to classical information retrieval methods, yet their performance and out-of-domain generalization leave substantial room for improvement. NaturalProofs opens many avenues for research on challenging mathematical tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.01112v2-abstract-full').style.display = 'none'; document.getElementById('2104.01112v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.09160">arXiv:2103.09160</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.09160">pdf</a>, <a href="https://arxiv.org/format/2103.09160">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/LRA.2021.3062607">10.1109/LRA.2021.3062607 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LRGNet: Learnable Region Growing for Class-Agnostic Point Cloud Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jingdao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+Y+K">Yong K. Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.09160v1-abstract-short" style="display: inline;">
        3D point cloud segmentation is an important function that helps robots understand the layout of their surrounding environment and perform tasks such as grasping objects, avoiding obstacles, and finding landmarks. Current segmentation methods are mostly class-specific, many of which are tuned to work with specific object categories and may not be generalizable to different types of scenes. This res&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09160v1-abstract-full').style.display = 'inline'; document.getElementById('2103.09160v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.09160v1-abstract-full" style="display: none;">
        3D point cloud segmentation is an important function that helps robots understand the layout of their surrounding environment and perform tasks such as grasping objects, avoiding obstacles, and finding landmarks. Current segmentation methods are mostly class-specific, many of which are tuned to work with specific object categories and may not be generalizable to different types of scenes. This research proposes a learnable region growing method for class-agnostic point cloud segmentation, specifically for the task of instance label prediction. The proposed method is able to segment any class of objects using a single deep neural network without any assumptions about their shapes and sizes. The deep neural network is trained to predict how to add or remove points from a point cloud region to morph it into incrementally more complete regions of an object instance. Segmentation results on the S3DIS and ScanNet datasets show that the proposed method outperforms competing methods by 1%-9% on 6 different evaluation metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09160v1-abstract-full').style.display = 'none'; document.getElementById('2103.09160v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Robotics and Automation Letters 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.03872">arXiv:2103.03872</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.03872">pdf</a>, <a href="https://arxiv.org/format/2103.03872">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rissanen Data Analysis: Examining Dataset Characteristics via Description Length
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Perez%2C+E">Ethan Perez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kiela%2C+D">Douwe Kiela</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.03872v1-abstract-short" style="display: inline;">
        We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.03872v1-abstract-full').style.display = 'inline'; document.getElementById('2103.03872v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.03872v1-abstract-full" style="display: none;">
        We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels&#39; minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.03872v1-abstract-full').style.display = 'none'; document.getElementById('2103.03872v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code at https://github.com/ethanjperez/rda along with a script to run RDA on your own dataset</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.07813">arXiv:2102.07813</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.07813">pdf</a>, <a href="https://arxiv.org/format/2102.07813">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Online hyperparameter optimization by real-time recurrent learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Im%2C+D+J">Daniel Jiwoong Im</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Savin%2C+C">Cristina Savin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.07813v2-abstract-short" style="display: inline;">
        Conventional hyperparameter optimization methods are computationally intensive and hard to generalize to scenarios that require dynamically adapting hyperparameters, such as life-long learning. Here, we propose an online hyperparameter optimization algorithm that is asymptotically exact and computationally tractable, both theoretically and practically. Our framework takes advantage of the analogy&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07813v2-abstract-full').style.display = 'inline'; document.getElementById('2102.07813v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.07813v2-abstract-full" style="display: none;">
        Conventional hyperparameter optimization methods are computationally intensive and hard to generalize to scenarios that require dynamically adapting hyperparameters, such as life-long learning. Here, we propose an online hyperparameter optimization algorithm that is asymptotically exact and computationally tractable, both theoretically and practically. Our framework takes advantage of the analogy between hyperparameter optimization and parameter learning in recurrent neural networks (RNNs). It adapts a well-studied family of online learning algorithms for RNNs to tune hyperparameters and network parameters simultaneously, without repeatedly rolling out iterative optimization. This procedure yields systematically better generalization performance compared to standard methods, at a fraction of wallclock time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07813v2-abstract-full').style.display = 'none'; document.getElementById('2102.07813v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.07511">arXiv:2102.07511</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.07511">pdf</a>, <a href="https://arxiv.org/format/2102.07511">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H">Heesu Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Park%2C+H">Hanmin Park</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+T">Taehyun Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kwanheum Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+E">Eojin Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ryu%2C+S">Soojung Ryu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+H">Hyuk-Jae Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+K">Kiyoung Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Jinho Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.07511v1-abstract-short" style="display: inline;">
        In this paper, we present GradPIM, a processing-in-memory architecture which accelerates parameter updates of deep neural networks training. As one of processing-in-memory techniques that could be realized in the near future, we propose an incremental, simple architectural design that does not invade the existing memory protocol. Extending DDR4 SDRAM to utilize bank-group parallelism makes our ope&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07511v1-abstract-full').style.display = 'inline'; document.getElementById('2102.07511v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.07511v1-abstract-full" style="display: none;">
        In this paper, we present GradPIM, a processing-in-memory architecture which accelerates parameter updates of deep neural networks training. As one of processing-in-memory techniques that could be realized in the near future, we propose an incremental, simple architectural design that does not invade the existing memory protocol. Extending DDR4 SDRAM to utilize bank-group parallelism makes our operation designs in processing-in-memory (PIM) module efficient in terms of hardware cost and performance. Our experimental results show that the proposed architecture can improve the performance of DNN training and greatly reduce memory bandwidth requirement while posing only a minimal amount of overhead to the protocol and DRAM area.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.07511v1-abstract-full').style.display = 'none'; document.getElementById('2102.07511v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to HPCA 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.06536">arXiv:2102.06536</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.06536">pdf</a>, <a href="https://arxiv.org/format/2102.06536">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CrossStack: A 3-D Reconfigurable RRAM Crossbar Inference Engine
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Eshraghian%2C+J+K">Jason K. Eshraghian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyoungrok Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+S+M">Sung Mo Kang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.06536v1-abstract-short" style="display: inline;">
        Deep neural network inference accelerators are rapidly growing in importance as we turn to massively parallelized processing beyond GPUs and ASICs. The dominant operation in feedforward inference is the multiply-and-accumlate process, where each column in a crossbar generates the current response of a single neuron. As a result, memristor crossbar arrays parallelize inference and image processing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.06536v1-abstract-full').style.display = 'inline'; document.getElementById('2102.06536v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.06536v1-abstract-full" style="display: none;">
        Deep neural network inference accelerators are rapidly growing in importance as we turn to massively parallelized processing beyond GPUs and ASICs. The dominant operation in feedforward inference is the multiply-and-accumlate process, where each column in a crossbar generates the current response of a single neuron. As a result, memristor crossbar arrays parallelize inference and image processing tasks very efficiently. In this brief, we present a 3-D active memristor crossbar array `CrossStack&#39;, which adopts stacked pairs of Al/TiO2/TiO2-x/Al devices with common middle electrodes. By designing CMOS-memristor hybrid cells used in the layout of the array, CrossStack can operate in one of two user-configurable modes as a reconfigurable inference engine: 1) expansion mode and 2) deep-net mode. In expansion mode, the resolution of the network is doubled by increasing the number of inputs for a given chip area, reducing IR drop by 22%. In deep-net mode, inference speed per-10-bit convolution is improved by 29\% by simultaneously using one TiO2/TiO2-x layer for read processes, and the other for write processes. We experimentally verify both modes on our $10\times10\times2$ array.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.06536v1-abstract-full').style.display = 'none'; document.getElementById('2102.06536v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.04883">arXiv:2102.04883</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.04883">pdf</a>, <a href="https://arxiv.org/format/2102.04883">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Disordered Systems and Neural Networks">cond-mat.dis-nn</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Introduction to Machine Learning for the Sciences
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Neupert%2C+T">Titus Neupert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fischer%2C+M+H">Mark H Fischer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Greplova%2C+E">Eliska Greplova</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choo%2C+K">Kenny Choo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Denner%2C+M">Michael Denner</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.04883v1-abstract-short" style="display: inline;">
        This is an introductory machine learning course specifically developed with STEM students in mind. We discuss supervised, unsupervised, and reinforcement learning. The notes start with an exposition of machine learning methods without neural networks, such as principle component analysis, t-SNE, and linear regression. We continue with an introduction to both basic and advanced neural network struc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.04883v1-abstract-full').style.display = 'inline'; document.getElementById('2102.04883v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.04883v1-abstract-full" style="display: none;">
        This is an introductory machine learning course specifically developed with STEM students in mind. We discuss supervised, unsupervised, and reinforcement learning. The notes start with an exposition of machine learning methods without neural networks, such as principle component analysis, t-SNE, and linear regression. We continue with an introduction to both basic and advanced neural network structures such as conventional neural networks, (variational) autoencoders, generative adversarial networks, restricted Boltzmann machines, and recurrent neural networks. Questions of interpretability are discussed using the examples of dreaming and adversarial attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.04883v1-abstract-full').style.display = 'none'; document.getElementById('2102.04883v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">80 pages, 36 figures. The content of these lecture notes together with exercises is available under http://www.ml-lectures.org. A shorter German version of the lecture notes is published in the Springer essential series, ISBN 978-3-658-32268-7, doi:10.1007/978-3-658-32268-7</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.00863">arXiv:2102.00863</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.00863">pdf</a>, <a href="https://arxiv.org/format/2102.00863">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Equivariant Scene Synthesis from Video
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Resnick%2C+C">Cinjon Resnick</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Litany%2C+O">Or Litany</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hei%C3%9F%2C+C">Cosmas Heiß</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Larochelle%2C+H">Hugo Larochelle</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bruna%2C+J">Joan Bruna</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.00863v1-abstract-short" style="display: inline;">
        We propose a self-supervised framework to learn scene representations from video that are automatically delineated into background, characters, and their animations. Our method capitalizes on moving characters being equivariant with respect to their transformation across frames and the background being constant with respect to that same transformation. After training, we can manipulate image encod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00863v1-abstract-full').style.display = 'inline'; document.getElementById('2102.00863v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.00863v1-abstract-full" style="display: none;">
        We propose a self-supervised framework to learn scene representations from video that are automatically delineated into background, characters, and their animations. Our method capitalizes on moving characters being equivariant with respect to their transformation across frames and the background being constant with respect to that same transformation. After training, we can manipulate image encodings in real time to create unseen combinations of the delineated components. As far as we know, we are the first method to perform unsupervised extraction and synthesis of interpretable background, character, and animation. We demonstrate results on three datasets: Moving MNIST with backgrounds, 2D video game sprites, and Fashion Modeling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00863v1-abstract-full').style.display = 'none'; document.getElementById('2102.00863v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2011.05787</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.09458">arXiv:2101.09458</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.09458">pdf</a>, <a href="https://arxiv.org/format/2101.09458">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Whitney%2C+W+F">William F. Whitney</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bloesch%2C+M">Michael Bloesch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Springenberg%2C+J+T">Jost Tobias Springenberg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abdolmaleki%2C+A">Abbas Abdolmaleki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Riedmiller%2C+M">Martin Riedmiller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.09458v2-abstract-short" style="display: inline;">
        Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09458v2-abstract-full').style.display = 'inline'; document.getElementById('2101.09458v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.09458v2-abstract-full" style="display: none;">
        Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09458v2-abstract-full').style.display = 'none'; document.getElementById('2101.09458v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.06283">arXiv:2101.06283</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.06283">pdf</a>, <a href="https://arxiv.org/format/2101.06283">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3411764.3445421">10.1145/3411764.3445421 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data@Hand: Fostering Visual Exploration of Personal Data on Smartphones Leveraging Speech and Touch Interaction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+Y">Young-Ho Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+B">Bongshin Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Srinivasan%2C+A">Arjun Srinivasan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choe%2C+E+K">Eun Kyoung Choe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.06283v1-abstract-short" style="display: inline;">
        Most mobile health apps employ data visualization to help people view their health and activity data, but these apps provide limited support for visual data exploration. Furthermore, despite its huge potential benefits, mobile visualization research in the personal data context is sparse. This work aims to empower people to easily navigate and compare their personal health data on smartphones by e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.06283v1-abstract-full').style.display = 'inline'; document.getElementById('2101.06283v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.06283v1-abstract-full" style="display: none;">
        Most mobile health apps employ data visualization to help people view their health and activity data, but these apps provide limited support for visual data exploration. Furthermore, despite its huge potential benefits, mobile visualization research in the personal data context is sparse. This work aims to empower people to easily navigate and compare their personal health data on smartphones by enabling flexible time manipulation with speech. We designed and developed Data@Hand, a mobile app that leverages the synergy of two complementary modalities: speech and touch. Through an exploratory study with 13 long-term Fitbit users, we examined how multimodal interaction helps participants explore their own health data. Participants successfully adopted multimodal interaction (i.e., speech and touch) for convenient and fluid data exploration. Based on the quantitative and qualitative findings, we discuss design implications and opportunities with multimodal interaction for better supporting visual data exploration on mobile devices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.06283v1-abstract-full').style.display = 'none'; document.getElementById('2101.06283v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in ACM CHI 2021 Conference on Human Factors in Computing Systems; 16 pages, 6 figures, 5 tables</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          H.5.2
        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        In CHI Conference on Human Factors in Computing Systems (CHI &#39;21), May 8-13, 2021, Yokohama, Japan
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.01502">arXiv:2101.01502</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.01502">pdf</a>, <a href="https://arxiv.org/format/2101.01502">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Control-Data Separation and Logical Condition Propagation for Efficient Inference on Probabilistic Programs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hasuo%2C+I">Ichiro Hasuo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oyabu%2C+Y">Yuichiro Oyabu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eberhart%2C+C">Clovis Eberhart</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Suenaga%2C+K">Kohei Suenaga</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kenta Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Katsumata%2C+S">Shin-ya Katsumata</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.01502v2-abstract-short" style="display: inline;">
        We introduce a novel sampling algorithm for Bayesian inference on imperative probabilistic programs. It features a hierarchical architecture that separates control flows from data: the top-level samples a control flow, and the bottom level samples data values along the control flow picked by the top level. This separation allows us to plug various language-based analysis techniques in probabilisti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01502v2-abstract-full').style.display = 'inline'; document.getElementById('2101.01502v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.01502v2-abstract-full" style="display: none;">
        We introduce a novel sampling algorithm for Bayesian inference on imperative probabilistic programs. It features a hierarchical architecture that separates control flows from data: the top-level samples a control flow, and the bottom level samples data values along the control flow picked by the top level. This separation allows us to plug various language-based analysis techniques in probabilistic program sampling; specifically, we use logical backward propagation of observations for sampling efficiency. We implemented our algorithm on top of Anglican. The experimental results demonstrate our algorithm&#39;s efficiency, especially for programs with while loops and rare observations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01502v2-abstract-full').style.display = 'none'; document.getElementById('2101.01502v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages with appendices</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=200"
            class="pagination-link "
            aria-label="Page 5"
            aria-current="page">5
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Cho%2C+K&amp;start=250"
            class="pagination-link "
            aria-label="Page 6"
            aria-current="page">6
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>