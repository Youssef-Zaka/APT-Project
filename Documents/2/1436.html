<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 2,210 results for author: <span class="mathjax">Zhang, X</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Zhang, X">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Zhang%2C+X&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Zhang, X">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01698">arXiv:2205.01698</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01698">pdf</a>, <a href="https://arxiv.org/format/2205.01698">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Disordered Systems and Neural Networks">cond-mat.dis-nn</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Circuit Depth Scaling For Quantum Approximate Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Akshay%2C+V">V. Akshay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Philathong%2C+H">H. Philathong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Campos%2C+E">E. Campos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rabinovich%2C+D">D. Rabinovich</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zacharov%2C+I">I. Zacharov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiao-Ming Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Biamonte%2C+J">J. Biamonte</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01698v1-abstract-short" style="display: inline;">
        Variational quantum algorithms are the centerpiece of modern quantum programming. These algorithms involve training parameterized quantum circuits using a classical co-processor, an approach adapted partly from classical machine learning. An important subclass of these algorithms, designed for combinatorial optimization on currrent quantum hardware, is the quantum approximate optimization algorith&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01698v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01698v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01698v1-abstract-full" style="display: none;">
        Variational quantum algorithms are the centerpiece of modern quantum programming. These algorithms involve training parameterized quantum circuits using a classical co-processor, an approach adapted partly from classical machine learning. An important subclass of these algorithms, designed for combinatorial optimization on currrent quantum hardware, is the quantum approximate optimization algorithm (QAOA). It is known that problem density - a problem constraint to variable ratio - induces under-parametrization in fixed depth QAOA. Density dependent performance has been reported in the literature, yet the circuit depth required to achieve fixed performance (henceforth called critical depth) remained unknown. Here, we propose a predictive model, based on a logistic saturation conjecture for critical depth scaling with respect to density. Focusing on random instances of MAX-2-SAT, we test our predictive model against simulated data with up to 15 qubits. We report the average critical depth, required to attain a success probability of 0.7, saturates at a value of 10 for densities beyond 4. We observe the predictive model to describe the simulated data within a $3σ$ confidence interval. Furthermore, based on the model, a linear trend for the critical depth with respect problem size is recovered for the range of 5 to 15 qubits.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01698v1-abstract-full').style.display = 'none'; document.getElementById('2205.01698v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">REVTeX, 6+2 pages, 4 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68Q12
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01051">arXiv:2205.01051</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01051">pdf</a>, <a href="https://arxiv.org/format/2205.01051">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RANG: A Residual-based Adaptive Node Generation Method for Physics-Informed Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+W">Wei Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+W">Weien Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiaoya Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+W">Wen Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zheliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01051v1-abstract-short" style="display: inline;">
        Learning solutions of partial differential equations (PDEs) with Physics-Informed Neural Networks(PINNs) is an attractive alternative approach to traditional solvers due to its flexibility and ease of incorporating observed data. Despite the success of PINNs in accurately solving a wide variety of PDEs, the method still requires improvements in terms of computational efficiency. One possible impro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01051v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01051v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01051v1-abstract-full" style="display: none;">
        Learning solutions of partial differential equations (PDEs) with Physics-Informed Neural Networks(PINNs) is an attractive alternative approach to traditional solvers due to its flexibility and ease of incorporating observed data. Despite the success of PINNs in accurately solving a wide variety of PDEs, the method still requires improvements in terms of computational efficiency. One possible improvement idea is to optimize the generation of training point sets. Residual-based adaptive sampling and quasi-uniform sampling approaches have been each applied to improve the training effects of PINNs, respectively. To benefit from both methods, we propose the Residual-based Adaptive Node Generation (RANG) approach for efficient training of PINNs, which is based on a variable density nodal distribution method for RBF-FD. The method is also enhanced by a memory mechanism to further improve training stability. We conduct experiments on three linear PDEs and three nonlinear PDEs with various node generation methods, through which the accuracy and efficiency of the proposed method compared to the predominant uniform sampling approach is verified numerically.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01051v1-abstract-full').style.display = 'none'; document.getElementById('2205.01051v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00904">arXiv:2205.00904</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00904">pdf</a>, <a href="https://arxiv.org/format/2205.00904">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+Z">Zhenwei Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pei%2C+S">Shichao Pei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yongchun Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuang%2C+F">Fuzhen Zhuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hoehndorf%2C+R">Robert Hoehndorf</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangliang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00904v1-abstract-short" style="display: inline;">
        Most real-world knowledge graphs (KG) are far from complete and comprehensive. This problem has motivated efforts in predicting the most plausible missing facts to complete a given KG, i.e., knowledge graph completion (KGC). However, existing KGC methods suffer from two main issues, 1) the false negative issue, i.e., the candidates for sampling negative training instances include potential true fa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00904v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00904v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00904v1-abstract-full" style="display: none;">
        Most real-world knowledge graphs (KG) are far from complete and comprehensive. This problem has motivated efforts in predicting the most plausible missing facts to complete a given KG, i.e., knowledge graph completion (KGC). However, existing KGC methods suffer from two main issues, 1) the false negative issue, i.e., the candidates for sampling negative training instances include potential true facts; and 2) the data sparsity issue, i.e., true facts account for only a tiny part of all possible facts. To this end, we propose positive-unlabeled learning with adversarial data augmentation (PUDA) for KGC. In particular, PUDA tailors positive-unlabeled risk estimator for the KGC task to deal with the false negative issue. Furthermore, to address the data sparsity issue, PUDA achieves a data augmentation strategy by unifying adversarial training and positive-unlabeled learning under the positive-unlabeled minimax game. Extensive experimental results demonstrate its effectiveness and compatibility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00904v1-abstract-full').style.display = 'none'; document.getElementById('2205.00904v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IJCAI 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00619">arXiv:2205.00619</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00619">pdf</a>, <a href="https://arxiv.org/format/2205.00619">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yujian Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X+F">Xinliang Frederick Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wegsman%2C+D">David Wegsman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Beauchamp%2C+N">Nick Beauchamp</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+L">Lu Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00619v1-abstract-short" style="display: inline;">
        Ideology is at the core of political science research. Yet, there still does not exist general-purpose tools to characterize and predict ideology across different genres of text. To this end, we study Pretrained Language Models using novel ideology-driven pretraining objectives that rely on the comparison of articles on the same story written by media of different ideologies. We further collect a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00619v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00619v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00619v1-abstract-full" style="display: none;">
        Ideology is at the core of political science research. Yet, there still does not exist general-purpose tools to characterize and predict ideology across different genres of text. To this end, we study Pretrained Language Models using novel ideology-driven pretraining objectives that rely on the comparison of articles on the same story written by media of different ideologies. We further collect a large-scale dataset, consisting of more than 3.6M political news articles, for pretraining. Our model POLITICS outperforms strong baselines and the previous state-of-the-art models on ideology prediction and stance detection tasks. Further analyses show that POLITICS is especially good at understanding long or formally written texts, and is also robust in few-shot learning scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00619v1-abstract-full').style.display = 'none'; document.getElementById('2205.00619v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Findings of NAACL&#39;22. The first two authors contribute equally</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00541">arXiv:2205.00541</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00541">pdf</a>, <a href="https://arxiv.org/format/2205.00541">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        COUCH: Towards Controllable Human-Chair Interactions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiaohan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bhatnagar%2C+B+L">Bharat Lal Bhatnagar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guzov%2C+V">Vladimir Guzov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Starke%2C+S">Sebastian Starke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pons-Moll%2C+G">Gerard Pons-Moll</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00541v1-abstract-short" style="display: inline;">
        Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00541v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00541v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00541v1-abstract-full" style="display: none;">
        Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework COUCH that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00541v1-abstract-full').style.display = 'none'; document.getElementById('2205.00541v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00463">arXiv:2205.00463</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00463">pdf</a>, <a href="https://arxiv.org/format/2205.00463">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+Q">Qiaoqiao Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+H">Hui Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Quan%2C+Y">Yuhui Quan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiaoqun Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00463v1-abstract-short" style="display: inline;">
        Low-dose CT (LDCT) imaging attracted a considerable interest for the reduction of the object&#39;s exposure to X-ray radiation. In recent years, supervised deep learning has been extensively studied for LDCT image reconstruction, which trains a network over a dataset containing many pairs of normal-dose and low-dose images. However, the challenge on collecting many such pairs in the clinical setup lim&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00463v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00463v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00463v1-abstract-full" style="display: none;">
        Low-dose CT (LDCT) imaging attracted a considerable interest for the reduction of the object&#39;s exposure to X-ray radiation. In recent years, supervised deep learning has been extensively studied for LDCT image reconstruction, which trains a network over a dataset containing many pairs of normal-dose and low-dose images. However, the challenge on collecting many such pairs in the clinical setup limits the application of such supervised-learning-based methods for LDCT image reconstruction in practice. Aiming at addressing the challenges raised by the collection of training dataset, this paper proposed a unsupervised deep learning method for LDCT image reconstruction, which does not require any external training data. The proposed method is built on a re-parametrization technique for Bayesian inference via deep network with random weights, combined with additional total variational (TV) regularization. The experiments show that the proposed method noticeably outperforms existing dataset-free image reconstruction methods on the test data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00463v1-abstract-full').style.display = 'none'; document.getElementById('2205.00463v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13874">arXiv:2204.13874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13874">pdf</a>, <a href="https://arxiv.org/format/2204.13874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3485447.3512035">10.1145/3485447.3512035 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OA-Mine: Open-World Attribute Mining for E-Commerce Products with Weak Supervision
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Chenwei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X+L">Xin Luna Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shang%2C+J">Jingbo Shang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Faloutsos%2C+C">Christos Faloutsos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jiawei Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13874v1-abstract-short" style="display: inline;">
        Automatic extraction of product attributes from their textual descriptions is essential for online shopper experience. One inherent challenge of this task is the emerging nature of e-commerce products -- we see new types of products with their unique set of new attributes constantly. Most prior works on this matter mine new values for a set of known attributes but cannot handle new attributes that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13874v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13874v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13874v1-abstract-full" style="display: none;">
        Automatic extraction of product attributes from their textual descriptions is essential for online shopper experience. One inherent challenge of this task is the emerging nature of e-commerce products -- we see new types of products with their unique set of new attributes constantly. Most prior works on this matter mine new values for a set of known attributes but cannot handle new attributes that arose from constantly changing data. In this work, we study the attribute mining problem in an open-world setting to extract novel attributes and their values. Instead of providing comprehensive training data, the user only needs to provide a few examples for a few known attribute types as weak supervision. We propose a principled framework that first generates attribute value candidates and then groups them into clusters of attributes. The candidate generation step probes a pre-trained language model to extract phrases from product titles. Then, an attribute-aware fine-tuning method optimizes a multitask objective and shapes the language model representation to be attribute-discriminative. Finally, we discover new attributes and values through the self-ensemble of our framework, which handles the open-world challenge. We run extensive experiments on a large distantly annotated development set and a gold standard human-annotated test set that we collected. Our model significantly outperforms strong baselines and can generalize to unseen attributes and product types.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13874v1-abstract-full').style.display = 'none'; document.getElementById('2204.13874v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">WWW 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13852">arXiv:2204.13852</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13852">pdf</a>, <a href="https://arxiv.org/format/2204.13852">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        H2H: Heterogeneous Model to Heterogeneous System Mapping with Computation and Communication Awareness
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinyi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hao%2C+C">Cong Hao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+P">Peipei Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jones%2C+A">Alex Jones</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+J">Jingtong Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13852v1-abstract-short" style="display: inline;">
        The complex nature of real-world problems calls for heterogeneity in both machine learning (ML) models and hardware systems. The heterogeneity in ML models comes from multi-sensor perceiving and multi-task learning, i.e., multi-modality multi-task (MMMT), resulting in diverse deep neural network (DNN) layers and computation patterns. The heterogeneity in systems comes from diverse processing compo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13852v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13852v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13852v1-abstract-full" style="display: none;">
        The complex nature of real-world problems calls for heterogeneity in both machine learning (ML) models and hardware systems. The heterogeneity in ML models comes from multi-sensor perceiving and multi-task learning, i.e., multi-modality multi-task (MMMT), resulting in diverse deep neural network (DNN) layers and computation patterns. The heterogeneity in systems comes from diverse processing components, as it becomes the prevailing method to integrate multiple dedicated accelerators into one system. Therefore, a new problem emerges: heterogeneous model to heterogeneous system mapping (H2H). While previous mapping algorithms mostly focus on efficient computations, in this work, we argue that it is indispensable to consider computation and communication simultaneously for better system efficiency. We propose a novel H2H mapping algorithm with both computation and communication awareness; by slightly trading computation for communication, the system overall latency and energy consumption can be largely reduced. The superior performance of our work is evaluated based on MAESTRO modeling, demonstrating 15%-74% latency reduction and 23%-64% energy reduction compared with existing computation-prioritized mapping algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13852v1-abstract-full').style.display = 'none'; document.getElementById('2204.13852v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13788">arXiv:2204.13788</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13788">pdf</a>, <a href="https://arxiv.org/format/2204.13788">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FPIRM: Floating-point Processing in Racetrack Memories
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ollivier%2C+S">Sébastien Ollivier</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinyi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+Y">Yue Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choudhuri%2C+C">Chayanika Choudhuri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+J">Jingtong Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jones%2C+A+K">Alex K. Jones</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13788v1-abstract-short" style="display: inline;">
        Convolutional neural networks (CNN) have become a ubiquitous algorithm with growing applications in mobile and edge settings. We describe a compute-in-memory (CIM) technique called FPIRM using Racetrack Memory (RM) to accelerate CNNs for edge systems. Using transverse read, a technique that can determine the number of &#39;1&#39;s multiple adjacent domains, FPIRM can efficiently implement multi-operand bu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13788v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13788v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13788v1-abstract-full" style="display: none;">
        Convolutional neural networks (CNN) have become a ubiquitous algorithm with growing applications in mobile and edge settings. We describe a compute-in-memory (CIM) technique called FPIRM using Racetrack Memory (RM) to accelerate CNNs for edge systems. Using transverse read, a technique that can determine the number of &#39;1&#39;s multiple adjacent domains, FPIRM can efficiently implement multi-operand bulk-bitwise and addition computations, and two-operand multiplication. We discuss how FPIRM can implement both variable precision integer and floating point arithmetic. This allows both CNN inference and on-device training without expensive data movement to the cloud. Based on these functions we demonstrate implementation of several CNNs with back propagation using RM CIM and compare these to state-of-the-art implementations of CIM inference and training in Field-Programmable Gate Arrays. During training FPIRM improves by 2$\times$ the efficiency, by reducing the energy consumption by at least 27% and increasing the throughput by at least 18% against FPGA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13788v1-abstract-full').style.display = 'none'; document.getElementById('2204.13788v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper is under review for possible publication by the IEEE</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13620">arXiv:2204.13620</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13620">pdf</a>, <a href="https://arxiv.org/format/2204.13620">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generative Adversarial Networks for Image Super-Resolution: A Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+C">Chunwei Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xuanyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+J+C">Jerry Chun-Wen Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zuo%2C+W">Wangmeng Zuo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yanning Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13620v1-abstract-short" style="display: inline;">
        Single image super-resolution (SISR) has played an important role in the field of image processing. Recent generative adversarial networks (GANs) can achieve excellent results on low-resolution images with small samples. However, there are little literatures summarizing different GANs in SISR. In this paper, we conduct a comparative study of GANs from different perspectives. We first take a look a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13620v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13620v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13620v1-abstract-full" style="display: none;">
        Single image super-resolution (SISR) has played an important role in the field of image processing. Recent generative adversarial networks (GANs) can achieve excellent results on low-resolution images with small samples. However, there are little literatures summarizing different GANs in SISR. In this paper, we conduct a comparative study of GANs from different perspectives. We first take a look at developments of GANs. Second, we present popular architectures for GANs in big and small samples for image applications. Then, we analyze motivations, implementations and differences of GANs based optimization methods and discriminative learning for image super-resolution in terms of supervised, semi-supervised and unsupervised manners. Next, we compare performance of these popular GANs on public datasets via quantitative and qualitative analysis in SISR. Finally, we highlight challenges of GANs and potential research points for SISR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13620v1-abstract-full').style.display = 'none'; document.getElementById('2204.13620v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13512">arXiv:2204.13512</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13512">pdf</a>, <a href="https://arxiv.org/format/2204.13512">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+R">Ruipeng Jia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xingxing Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+Y">Yanan Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Shi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Z">Zheng Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+F">Furu Wei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13512v2-abstract-short" style="display: inline;">
        In zero-shot multilingual extractive text summarization, a model is typically trained on English summarization dataset and then applied on summarization datasets of other languages. Given English gold summaries and documents, sentence-level labels for extractive summarization are usually generated using heuristics. However, these monolingual labels created on English datasets may not be optimal on&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13512v2-abstract-full').style.display = 'inline'; document.getElementById('2204.13512v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13512v2-abstract-full" style="display: none;">
        In zero-shot multilingual extractive text summarization, a model is typically trained on English summarization dataset and then applied on summarization datasets of other languages. Given English gold summaries and documents, sentence-level labels for extractive summarization are usually generated using heuristics. However, these monolingual labels created on English datasets may not be optimal on datasets of other languages, for that there is the syntactic or semantic discrepancy between different languages. In this way, it is possible to translate the English dataset to other languages and obtain different sets of labels again using heuristics. To fully leverage the information of these different sets of labels, we propose NLSSum (Neural Label Search for Summarization), which jointly learns hierarchical weights for these different sets of labels together with our summarization model. We conduct multilingual zero-shot summarization experiments on MLSUM and WikiLingua datasets, and we achieve state-of-the-art results using both human and automatic evaluations across these two datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13512v2-abstract-full').style.display = 'none'; document.getElementById('2204.13512v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13376">arXiv:2204.13376</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13376">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optics">physics.optics</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applied Physics">physics.app-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Machine learning for knowledge acquisition and accelerated inverse-design for non-Hermitian systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ahmed%2C+W+W">W. W. Ahmed</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Farhat%2C+M">M. Farhat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Staliunas%2C+K">K. Staliunas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">X. Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Y. Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13376v1-abstract-short" style="display: inline;">
        Non-Hermitian systems offer new platforms for unusual physical properties that can be flexibly manipulated by redistribution of the real and imaginary parts of refractive indices, whose presence breaks conventional wave propagation symmetries, leading to asymmetric reflection and symmetric transmission with respect to the wave propagation direction. Here, we use supervised and unsupervised learnin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13376v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13376v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13376v1-abstract-full" style="display: none;">
        Non-Hermitian systems offer new platforms for unusual physical properties that can be flexibly manipulated by redistribution of the real and imaginary parts of refractive indices, whose presence breaks conventional wave propagation symmetries, leading to asymmetric reflection and symmetric transmission with respect to the wave propagation direction. Here, we use supervised and unsupervised learning techniques for knowledge acquisition in non-Hermitian systems which accelerate the inverse design process. In particular, we construct a deep learning model that relates the transmission and asymmetric reflection in non-conservative settings and proposes sub-manifold learning to recognize non-Hermitian features from transmission spectra. The developed deep learning framework determines the feasibility of a desired spectral response for a given structure and uncovers the role of effective gain-loss parameters to tailor the spectral response. These findings pave the way for intelligent inverse design and shape our understanding of the physical mechanism in general non-Hermitian systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13376v1-abstract-full').style.display = 'none'; document.getElementById('2204.13376v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12948">arXiv:2204.12948</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12948">pdf</a>, <a href="https://arxiv.org/format/2204.12948">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3489517.3530501">10.1145/3489517.3530501 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Domain Knowledge-Infused Deep Learning for Automated Analog/Radio-Frequency Circuit Parameter Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+W">Weidong Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Benosman%2C+M">Mouhacine Benosman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+R">Rui Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12948v1-abstract-short" style="display: inline;">
        The design automation of analog circuits is a longstanding challenge. This paper presents a reinforcement learning method enhanced by graph learning to automate the analog circuit parameter optimization at the pre-layout stage, i.e., finding device parameters to fulfill desired circuit specifications. Unlike all prior methods, our approach is inspired by human experts who rely on domain knowledge&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12948v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12948v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12948v1-abstract-full" style="display: none;">
        The design automation of analog circuits is a longstanding challenge. This paper presents a reinforcement learning method enhanced by graph learning to automate the analog circuit parameter optimization at the pre-layout stage, i.e., finding device parameters to fulfill desired circuit specifications. Unlike all prior methods, our approach is inspired by human experts who rely on domain knowledge of analog circuit design (e.g., circuit topology and couplings between circuit specifications) to tackle the problem. By originally incorporating such key domain knowledge into policy training with a multimodal network, the method best learns the complex relations between circuit parameters and design targets, enabling optimal decisions in the optimization process. Experimental results on exemplary circuits show it achieves human-level design accuracy (99%) 1.5X efficiency of existing best-performing methods. Our method also shows better generalization ability to unseen specifications and optimality in circuit performance optimization. Moreover, it applies to design radio-frequency circuits on emerging semiconductor technologies, breaking the limitations of prior learning methods in designing conventional analog circuits.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12948v1-abstract-full').style.display = 'none'; document.getElementById('2204.12948v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 7 figures. arXiv admin note: substantial text overlap with arXiv:2202.13185</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12900">arXiv:2204.12900</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12900">pdf</a>, <a href="https://arxiv.org/format/2204.12900">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Global Trajectory Helps Person Retrieval in a Camera Network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xin Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+X">Xiaohua Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lai%2C+J">Jianhuang Lai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+W">Wei-Shi Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12900v1-abstract-short" style="display: inline;">
        We are concerned about retrieving a query person from the videos taken by a non-overlapping camera network. Existing methods often rely on pure visual matching or consider temporal constraint, but ignore the spatial information of the camera network. To address this problem, we propose a framework of person retrieval based on cross-camera trajectory generation which integrates both temporal and sp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12900v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12900v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12900v1-abstract-full" style="display: none;">
        We are concerned about retrieving a query person from the videos taken by a non-overlapping camera network. Existing methods often rely on pure visual matching or consider temporal constraint, but ignore the spatial information of the camera network. To address this problem, we propose a framework of person retrieval based on cross-camera trajectory generation which integrates both temporal and spatial information. To obtain the pedestrian trajectories, we propose a new cross-camera spatio-temporal model that integrates the walking habits of pedestrians and the path layout between cameras, forming a joint probability distribution. Such a spatio-temporal model among a camera network can be specified using sparsely sampled pedestrian data. Based on the spatio-temporal model, the cross-camera trajectories of a specific pedestrian can be extracted by the conditional random field model, and further optimized by the restricted nonnegative matrix factorization. Finally, a trajectory re-ranking technology is proposed to improve the person retrieval results. To verify the effectiveness of our approach, we build the first dataset of cross-camera pedestrian trajectories over an actual monitoring scenario, namely the Person Trajectory Dataset. Extensive experiments have verified the effectiveness and robustness of the proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12900v1-abstract-full').style.display = 'none'; document.getElementById('2204.12900v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12796">arXiv:2204.12796</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12796">pdf</a>, <a href="https://arxiv.org/format/2204.12796">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Supervised Contrastive CSI Representation Learning for Massive MIMO Positioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+J">Junquan Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+W">Wei Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jianzhao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xianyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Chuan Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12796v1-abstract-short" style="display: inline;">
        Similarity metric is crucial for massive MIMO positioning utilizing channel state information~(CSI). In this letter, we propose a novel massive MIMO CSI similarity learning method via deep convolutional neural network~(DCNN) and contrastive learning. A contrastive loss function is designed considering multiple positive and negative CSI samples drawn from a training dataset. The DCNN encoder is tra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12796v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12796v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12796v1-abstract-full" style="display: none;">
        Similarity metric is crucial for massive MIMO positioning utilizing channel state information~(CSI). In this letter, we propose a novel massive MIMO CSI similarity learning method via deep convolutional neural network~(DCNN) and contrastive learning. A contrastive loss function is designed considering multiple positive and negative CSI samples drawn from a training dataset. The DCNN encoder is trained using the loss so that positive samples are mapped to points close to the anchor&#39;s encoding, while encodings of negative samples are kept away from the anchor&#39;s in the representation space. Evaluation results of fingerprint-based positioning on a real-world CSI dataset show that the learned similarity metric improves positioning accuracy significantly compared with other known state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12796v1-abstract-full').style.display = 'none'; document.getElementById('2204.12796v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages,7 figures,submitted to IEEE Communications Letter</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12769">arXiv:2204.12769</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12769">pdf</a>, <a href="https://arxiv.org/format/2204.12769">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Registration: Joint Ego Motion Estimation and 3D Moving Object Detection in Dynamic Environment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wenyu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zijun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+S">Shichun Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiu%2C+N">Nan Qiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jun Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12769v1-abstract-short" style="display: inline;">
        Localization in a dynamic environment suffers from moving objects. Removing dynamic object is crucial in this situation but become tricky when ego-motion is coupled. In this paper, instead of proposing a new slam framework, we aim at a more general strategy for a localization scenario. In that case, Dynamic Registration is available for integrating with any lidar slam system. We utilize 3D object&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12769v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12769v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12769v1-abstract-full" style="display: none;">
        Localization in a dynamic environment suffers from moving objects. Removing dynamic object is crucial in this situation but become tricky when ego-motion is coupled. In this paper, instead of proposing a new slam framework, we aim at a more general strategy for a localization scenario. In that case, Dynamic Registration is available for integrating with any lidar slam system. We utilize 3D object detection to obtain potential moving objects and remove them temporarily. Then we proposed Dynamic Registration, to iteratively estimate ego-motion and segment moving objects until no static object generates. Static objects are merged with the environment. Finally, we successfully segment dynamic objects, static environments with static objects, and ego-motion estimation in a dynamic environment. We evaluate the performance of our proposed method on KITTI Tracking datasets. Results show stable and consistent improvements based on other classical registration algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12769v1-abstract-full').style.display = 'none'; document.getElementById('2204.12769v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submit to IV 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12663">arXiv:2204.12663</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12663">pdf</a>, <a href="https://arxiv.org/format/2204.12663">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding A Class of Decentralized and Federated Optimization Algorithms: A Multi-Rate Feedback Control Perspective
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinwei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+M">Mingyi Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Elia%2C+N">Nicola Elia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12663v1-abstract-short" style="display: inline;">
        Distributed algorithms have been playing an increasingly important role in many applications such as machine learning, signal processing, and control. Significant research efforts have been devoted to developing and analyzing new algorithms for various applications. In this work, we provide a fresh perspective to understand, analyze, and design distributed optimization algorithms. Through the lens&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12663v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12663v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12663v1-abstract-full" style="display: none;">
        Distributed algorithms have been playing an increasingly important role in many applications such as machine learning, signal processing, and control. Significant research efforts have been devoted to developing and analyzing new algorithms for various applications. In this work, we provide a fresh perspective to understand, analyze, and design distributed optimization algorithms. Through the lens of multi-rate feedback control, we show that a wide class of distributed algorithms, including popular decentralized/federated schemes, can be viewed as discretizing a certain continuous-time feedback control system, possibly with multiple sampling rates, such as decentralized gradient descent, gradient tracking, and federated averaging. This key observation not only allows us to develop a generic framework to analyze the convergence of the entire algorithm class. More importantly, it also leads to an interesting way of designing new distributed algorithms. We develop the theory behind our framework and provide examples to highlight how the framework can be used in practice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12663v1-abstract-full').style.display = 'none'; document.getElementById('2204.12663v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12463">arXiv:2204.12463</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12463">pdf</a>, <a href="https://arxiv.org/format/2204.12463">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Focal Sparse Convolutional Networks for 3D Object Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yukang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yanwei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+J">Jiaya Jia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12463v1-abstract-short" style="display: inline;">
        Non-uniformed 3D sparse data, e.g., point clouds or voxels in different spatial positions, make contribution to the task of 3D object detection in different ways. Existing basic components in sparse convolutional networks (Sparse CNNs) process all sparse data, regardless of regular or submanifold sparse convolution. In this paper, we introduce two new modules to enhance the capability of Sparse CN&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12463v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12463v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12463v1-abstract-full" style="display: none;">
        Non-uniformed 3D sparse data, e.g., point clouds or voxels in different spatial positions, make contribution to the task of 3D object detection in different ways. Existing basic components in sparse convolutional networks (Sparse CNNs) process all sparse data, regardless of regular or submanifold sparse convolution. In this paper, we introduce two new modules to enhance the capability of Sparse CNNs, both are based on making feature sparsity learnable with position-wise importance prediction. They are focal sparse convolution (Focals Conv) and its multi-modal variant of focal sparse convolution with fusion, or Focals Conv-F for short. The new modules can readily substitute their plain counterparts in existing Sparse CNNs and be jointly trained in an end-to-end fashion. For the first time, we show that spatially learnable sparsity in sparse convolution is essential for sophisticated 3D object detection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks validate the effectiveness of our approach. Without bells and whistles, our results outperform all existing single-model entries on the nuScenes test benchmark at the paper submission time. Code and models are at https://github.com/dvlab-research/FocalsConv.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12463v1-abstract-full').style.display = 'none'; document.getElementById('2204.12463v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2022 Oral. Code is at http://github.com/dvlab-research/FocalsConv</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12111">arXiv:2204.12111</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12111">pdf</a>, <a href="https://arxiv.org/format/2204.12111">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Function-words Enhanced Attention Networks for Few-Shot Inverse Relation Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dou%2C+C">Chunliu Dou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Shaojuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiaowang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+Z">Zhiyong Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+K">Kewen Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12111v1-abstract-short" style="display: inline;">
        The relation classification is to identify semantic relations between two entities in a given text. While existing models perform well for classifying inverse relations with large datasets, their performance is significantly reduced for few-shot learning. In this paper, we propose a function words adaptively enhanced attention framework (FAEA) for few-shot inverse relation classification, in which&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12111v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12111v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12111v1-abstract-full" style="display: none;">
        The relation classification is to identify semantic relations between two entities in a given text. While existing models perform well for classifying inverse relations with large datasets, their performance is significantly reduced for few-shot learning. In this paper, we propose a function words adaptively enhanced attention framework (FAEA) for few-shot inverse relation classification, in which a hybrid attention model is designed to attend class-related function words based on meta-learning. As the involvement of function words brings in significant intra-class redundancy, an adaptive message passing mechanism is introduced to capture and transfer inter-class differences.We mathematically analyze the negative impact of function words from dot-product measurement, which explains why message passing mechanism effectively reduces the impact. Our experimental results show that FAEA outperforms strong baselines, especially the inverse relation accuracy is improved by 14.33% under 1-shot setting in FewRel1.0.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12111v1-abstract-full').style.display = 'none'; document.getElementById('2204.12111v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.11849">arXiv:2204.11849</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.11849">pdf</a>, <a href="https://arxiv.org/format/2204.11849">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Risk Management">q-fin.RM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Heterogeneous Information Network based Default Analysis on Banking Micro and Small Enterprise Users
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zheng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+Y">Yingsheng Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+J">Jiachen Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+G">Guangwen Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.11849v2-abstract-short" style="display: inline;">
        Risk assessment is a substantial problem for financial institutions that has been extensively studied both for its methodological richness and its various practical applications. With the expansion of inclusive finance, recent attentions are paid to micro and small-sized enterprises (MSEs). Compared with large companies, MSEs present a higher exposure rate to default owing to their insecure financ&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11849v2-abstract-full').style.display = 'inline'; document.getElementById('2204.11849v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.11849v2-abstract-full" style="display: none;">
        Risk assessment is a substantial problem for financial institutions that has been extensively studied both for its methodological richness and its various practical applications. With the expansion of inclusive finance, recent attentions are paid to micro and small-sized enterprises (MSEs). Compared with large companies, MSEs present a higher exposure rate to default owing to their insecure financial stability. Conventional efforts learn classifiers from historical data with elaborate feature engineering. However, the main obstacle for MSEs involves severe deficiency in credit-related information, which may degrade the performance of prediction. Besides, financial activities have diverse explicit and implicit relations, which have not been fully exploited for risk judgement in commercial banks. In particular, the observations on real data show that various relationships between company users have additional power in financial risk analysis. In this paper, we consider a graph of banking data, and propose a novel HIDAM model for the purpose. Specifically, we attempt to incorporate heterogeneous information network with rich attributes on multi-typed nodes and links for modeling the scenario of business banking service. To enhance feature representation of MSEs, we extract interactive information through meta-paths and fully exploit path information. Furthermore, we devise a hierarchical attention mechanism respectively to learn the importance of contents inside each meta-path and the importance of different metapahs. Experimental results verify that HIDAM outperforms state-of-the-art competitors on real-world banking data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11849v2-abstract-full').style.display = 'none'; document.getElementById('2204.11849v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Corrected typos</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.11164">arXiv:2204.11164</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.11164">pdf</a>, <a href="https://arxiv.org/format/2204.11164">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Subgroup Fairness in Graph-based Spam Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiaxin Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+Y">Yuefei Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+S">Sihong Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.11164v1-abstract-short" style="display: inline;">
        Fake reviews are prevalent on review websites such as Amazon and Yelp. GNN is the state-of-the-art method that can detect suspicious reviewers by exploiting the topologies of the graph connecting reviewers, reviews, and target products. However, the discrepancy in the detection accuracy over different groups of reviewers causes discriminative treatment of different reviewers of the websites, leadi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11164v1-abstract-full').style.display = 'inline'; document.getElementById('2204.11164v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.11164v1-abstract-full" style="display: none;">
        Fake reviews are prevalent on review websites such as Amazon and Yelp. GNN is the state-of-the-art method that can detect suspicious reviewers by exploiting the topologies of the graph connecting reviewers, reviews, and target products. However, the discrepancy in the detection accuracy over different groups of reviewers causes discriminative treatment of different reviewers of the websites, leading to less engagement and trustworthiness of such websites. The complex dependencies over the review graph introduce difficulties in teasing out subgroups of reviewers that are hidden within larger groups and are treated unfairly. There is no previous study that defines and discovers the subtle subgroups to improve equitable treatment of reviewers. This paper addresses the challenges of defining, discovering, and utilizing subgroup memberships for fair spam detection. We first define a subgroup membership that can lead to discrepant accuracy in the subgroups. Since the subgroup membership is usually not observable while also important to guide the GNN detector to balance the treatment, we design a model that jointly infers the hidden subgroup memberships and exploits the membership for calibrating the target GNN&#39;s detection accuracy across subgroups. Comprehensive results on two large Yelp review datasets demonstrate that the proposed model can be trained to treat the subgroups more fairly.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11164v1-abstract-full').style.display = 'none'; document.getElementById('2204.11164v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.11053">arXiv:2204.11053</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.11053">pdf</a>, <a href="https://arxiv.org/format/2204.11053">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Uncertain Label Correction via Auxiliary Action Unit Graphs for Facial Expression Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xingming Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kauttonen%2C+J">Janne Kauttonen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+G">Guoying Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.11053v1-abstract-short" style="display: inline;">
        High-quality annotated images are significant to deep facial expression recognition (FER) methods. However, uncertain labels, mostly existing in large-scale public datasets, often mislead the training process. In this paper, we achieve uncertain label correction of facial expressions using auxiliary action unit (AU) graphs, called ULC-AG. Specifically, a weighted regularization module is introduce&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11053v1-abstract-full').style.display = 'inline'; document.getElementById('2204.11053v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.11053v1-abstract-full" style="display: none;">
        High-quality annotated images are significant to deep facial expression recognition (FER) methods. However, uncertain labels, mostly existing in large-scale public datasets, often mislead the training process. In this paper, we achieve uncertain label correction of facial expressions using auxiliary action unit (AU) graphs, called ULC-AG. Specifically, a weighted regularization module is introduced to highlight valid samples and suppress category imbalance in every batch. Based on the latent dependency between emotions and AUs, an auxiliary branch using graph convolutional layers is added to extract the semantic information from graph topologies. Finally, a re-labeling strategy corrects the ambiguous annotations by comparing their feature similarities with semantic templates. Experiments show that our ULC-AG achieves 89.31% and 61.57% accuracy on RAF-DB and AffectNet datasets, respectively, outperforming the baseline and state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11053v1-abstract-full').style.display = 'none'; document.getElementById('2204.11053v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 7 figures, accecpted by ICPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.10714">arXiv:2204.10714</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.10714">pdf</a>, <a href="https://arxiv.org/format/2204.10714">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Identifying Chinese Opinion Expressions with Extremely-Noisy Crowdsourcing Annotations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xin Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+G">Guangwei Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+Y">Yueheng Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Meishan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaobin Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Min Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.10714v1-abstract-short" style="display: inline;">
        Recent works of opinion expression identification (OEI) rely heavily on the quality and scale of the manually-constructed training corpus, which could be extremely difficult to satisfy. Crowdsourcing is one practical solution for this problem, aiming to create a large-scale but quality-unguaranteed corpus. In this work, we investigate Chinese OEI with extremely-noisy crowdsourcing annotations, con&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10714v1-abstract-full').style.display = 'inline'; document.getElementById('2204.10714v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.10714v1-abstract-full" style="display: none;">
        Recent works of opinion expression identification (OEI) rely heavily on the quality and scale of the manually-constructed training corpus, which could be extremely difficult to satisfy. Crowdsourcing is one practical solution for this problem, aiming to create a large-scale but quality-unguaranteed corpus. In this work, we investigate Chinese OEI with extremely-noisy crowdsourcing annotations, constructing a dataset at a very low cost. Following zhang et al. (2021), we train the annotator-adapter model by regarding all annotations as gold-standard in terms of crowd annotators, and test the model by using a synthetic expert, which is a mixture of all annotators. As this annotator-mixture for testing is never modeled explicitly in the training phase, we propose to generate synthetic training samples by a pertinent mixup strategy to make the training and testing highly consistent. The simulation experiments on our constructed dataset show that crowdsourcing is highly promising for OEI, and our proposed annotator-mixup can further enhance the crowdsourcing modeling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10714v1-abstract-full').style.display = 'none'; document.getElementById('2204.10714v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACL 2022 main conf</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.10176">arXiv:2204.10176</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.10176">pdf</a>, <a href="https://arxiv.org/format/2204.10176">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Probing Script Knowledge from Pre-Trained Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+Z">Zijian Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xingyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+M">Mo Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+L">Lifu Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.10176v1-abstract-short" style="display: inline;">
        Script knowledge is critical for humans to understand the broad daily tasks and routine activities in the world. Recently researchers have explored the large-scale pre-trained language models (PLMs) to perform various script related tasks, such as story generation, temporal ordering of event, future event prediction and so on. However, it&#39;s still not well studied in terms of how well the PLMs capt&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10176v1-abstract-full').style.display = 'inline'; document.getElementById('2204.10176v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.10176v1-abstract-full" style="display: none;">
        Script knowledge is critical for humans to understand the broad daily tasks and routine activities in the world. Recently researchers have explored the large-scale pre-trained language models (PLMs) to perform various script related tasks, such as story generation, temporal ordering of event, future event prediction and so on. However, it&#39;s still not well studied in terms of how well the PLMs capture the script knowledge. To answer this question, we design three probing tasks: inclusive sub-event selection, starting sub-event selection and temporal ordering to investigate the capabilities of PLMs with and without fine-tuning. The three probing tasks can be further used to automatically induce a script for each main event given all the possible sub-events. Taking BERT as a case study, by analyzing its performance on script induction as well as each individual probing task, we conclude that the stereotypical temporal knowledge among the sub-events is well captured in BERT, however the inclusive or starting sub-event knowledge is barely encoded.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10176v1-abstract-full').style.display = 'none'; document.getElementById('2204.10176v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.10107">arXiv:2204.10107</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.10107">pdf</a>, <a href="https://arxiv.org/format/2204.10107">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Industrial Experience Report on Retro-inspection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+L">Lanxin Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">He Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+F">Fuli Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiaodong Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rong%2C+G">Guoping Rong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.10107v2-abstract-short" style="display: inline;">
        To reinforce the quality of code delivery, especially to improve future coding quality, one global Information and Communication Technology (ICT) enterprise has institutionalized a retrospective style inspection (namely retro-inspection), which is similar to Fagan inspection but differs in terms of stage, participants, etc. This paper reports an industrial case study that aims to investigate the e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10107v2-abstract-full').style.display = 'inline'; document.getElementById('2204.10107v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.10107v2-abstract-full" style="display: none;">
        To reinforce the quality of code delivery, especially to improve future coding quality, one global Information and Communication Technology (ICT) enterprise has institutionalized a retrospective style inspection (namely retro-inspection), which is similar to Fagan inspection but differs in terms of stage, participants, etc. This paper reports an industrial case study that aims to investigate the experiences and lessons from this software practice. To this end, we collected and analyzed various empirical evidence for data triangulation. The results reflect that retro-inspection distinguishes itself from peer code review by identifying more complicated and underlying defects, providing more indicative and suggestive comments. Many experienced inspectors indicate defects together with their rationale behind and offer suggestions for correction and prevention. As a result, retro-inspection can benefit not only quality assurance (like Fagan inspection), but also internal audit, inter-division communication, and competence promotion. On the other side, we identify several lessons of retro-inspection at this stage, e.g., developers&#39; acceptance and organizers&#39; predicament, for next-step improvement of this practice. To be specific, some recommendations are discussed for retro-inspection, e.g., more adequate preparation and more careful publicity. This study concludes that most of the expected benefits of retro-inspection can be empirically confirmed in this enterprise and its value on the progress to continuous maturity can be recognized organization-wide. The experiences on executing this altered practice in a large enterprise provide reference value on code quality assurance to other software organizations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10107v2-abstract-full').style.display = 'none'; document.getElementById('2204.10107v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages. To be published in the Proceedings of the 44th IEEE/ACM International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP 2022)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.10090">arXiv:2204.10090</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.10090">pdf</a>, <a href="https://arxiv.org/format/2204.10090">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+D">Dihan Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiaowen Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+K">Kaisheng Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bao%2C+C">Chenglong Bao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.10090v1-abstract-short" style="display: inline;">
        Collecting paired training data is difficult in practice, but the unpaired samples broadly exist. Current approaches aim at generating synthesized training data from the unpaired samples by exploring the relationship between the corrupted and clean data. This work proposes LUD-VAE, a deep generative method to learn the joint probability density function from data sampled from marginal distribution&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10090v1-abstract-full').style.display = 'inline'; document.getElementById('2204.10090v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.10090v1-abstract-full" style="display: none;">
        Collecting paired training data is difficult in practice, but the unpaired samples broadly exist. Current approaches aim at generating synthesized training data from the unpaired samples by exploring the relationship between the corrupted and clean data. This work proposes LUD-VAE, a deep generative method to learn the joint probability density function from data sampled from marginal distributions. Our approach is based on a carefully designed probabilistic graphical model in which the clean and corrupted data domains are conditionally independent. Using variational inference, we maximize the evidence lower bound (ELBO) to estimate the joint probability density function. Furthermore, we show that the ELBO is computable without paired samples under the inference invariant assumption. This property provides the mathematical rationale of our approach in the unpaired setting. Finally, we apply our method to real-world image denoising and super-resolution tasks and train the models using the synthetic data generated by the LUD-VAE. Experimental results validate the advantages of our method over other learnable approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10090v1-abstract-full').style.display = 'none'; document.getElementById('2204.10090v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.10068">arXiv:2204.10068</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.10068">pdf</a>, <a href="https://arxiv.org/format/2204.10068">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Absolute Wrong Makes Better: Boosting Weakly Supervised Object Detection via Negative Deterministic Information
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Guanchun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangrong Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+Z">Zelin Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+X">Xu Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+H">Huiyu Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiao%2C+L">Licheng Jiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.10068v1-abstract-short" style="display: inline;">
        Weakly supervised object detection (WSOD) is a challenging task, in which image-level labels (e.g., categories of the instances in the whole image) are used to train an object detector. Many existing methods follow the standard multiple instance learning (MIL) paradigm and have achieved promising performance. However, the lack of deterministic information leads to part domination and missing insta&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10068v1-abstract-full').style.display = 'inline'; document.getElementById('2204.10068v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.10068v1-abstract-full" style="display: none;">
        Weakly supervised object detection (WSOD) is a challenging task, in which image-level labels (e.g., categories of the instances in the whole image) are used to train an object detector. Many existing methods follow the standard multiple instance learning (MIL) paradigm and have achieved promising performance. However, the lack of deterministic information leads to part domination and missing instances. To address these issues, this paper focuses on identifying and fully exploiting the deterministic information in WSOD. We discover that negative instances (i.e. absolutely wrong instances), ignored in most of the previous studies, normally contain valuable deterministic information. Based on this observation, we here propose a negative deterministic information (NDI) based method for improving WSOD, namely NDI-WSOD. Specifically, our method consists of two stages: NDI collecting and exploiting. In the collecting stage, we design several processes to identify and distill the NDI from negative instances online. In the exploiting stage, we utilize the extracted NDI to construct a novel negative contrastive learning mechanism and a negative guided instance selection strategy for dealing with the issues of part domination and missing instances, respectively. Experimental results on several public benchmarks including VOC 2007, VOC 2012 and MS COCO show that our method achieves satisfactory performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10068v1-abstract-full').style.display = 'none'; document.getElementById('2204.10068v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09931">arXiv:2204.09931</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09931">pdf</a>, <a href="https://arxiv.org/format/2204.09931">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-scale Knowledge Distillation for Unsupervised Person Re-Identification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lan%2C+L">Long Lan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Teng%2C+X">Xiao Teng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chi%2C+H">Haoang Chi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09931v1-abstract-short" style="display: inline;">
        Unsupervised person re-identification is a challenging and promising task in the computer vision. Nowadays unsupervised person re-identification methods have achieved great improvements by training with pseudo labels. However, the appearance and label noise are less explicitly studied in the unsupervised manner. To relieve the effects of appearance noise the global features involved, we also take&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09931v1-abstract-full').style.display = 'inline'; document.getElementById('2204.09931v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09931v1-abstract-full" style="display: none;">
        Unsupervised person re-identification is a challenging and promising task in the computer vision. Nowadays unsupervised person re-identification methods have achieved great improvements by training with pseudo labels. However, the appearance and label noise are less explicitly studied in the unsupervised manner. To relieve the effects of appearance noise the global features involved, we also take into account the features from two local views and produce multi-scale features. We explore the knowledge distillation to filter label noise, Specifically, we first train a teacher model from noisy pseudo labels in a iterative way, and then use the teacher model to guide the learning of our student model. In our setting, the student model could converge fast in the supervision of the teacher model thus reduce the interference of noisy labels as the teacher model greatly suffered. After carefully handling the noises in the feature learning, Our multi-scale knowledge distillation are proven to be very effective in the unsupervised re-identification. Extensive experiments on three popular person re-identification datasets demonstrate the superiority of our method. Especially, our approach achieves a state-of-the-art accuracy 85.7% @mAP or 94.3% @Rank-1 on the challenging Market-1501 benchmark with ResNet-50 under the fully unsupervised setting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09931v1-abstract-full').style.display = 'none'; document.getElementById('2204.09931v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09714">arXiv:2204.09714</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09714">pdf</a>, <a href="https://arxiv.org/format/2204.09714">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generative Pre-Trained Transformers for Biologically Inspired Design
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Q">Qihao Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+J">Jianxi Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09714v1-abstract-short" style="display: inline;">
        Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a novel form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineerin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09714v1-abstract-full').style.display = 'inline'; document.getElementById('2204.09714v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09714v1-abstract-full" style="display: none;">
        Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a novel form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a computational approach to bridge the gap. This paper proposes a generative design approach based on the pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely GPT-3, is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the correlation between the domains within the generated BID concepts. The approach is then tested via a case study in which the fine-tuned models are applied to generate and evaluate light-weighted flying car concepts inspired by nature. The results show our approach can generate BID concepts with good performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09714v1-abstract-full').style.display = 'none'; document.getElementById('2204.09714v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by the 34th ASME IDETC-CIE International Conference on Design Theory &amp; Methodology</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09456">arXiv:2204.09456</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09456">pdf</a>, <a href="https://arxiv.org/format/2204.09456">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        STAU: A SpatioTemporal-Aware Unit for Video Prediction and Beyond
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+Z">Zheng Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinfeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Shanshe Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+S">Siwei Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+W">Wen Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09456v1-abstract-short" style="display: inline;">
        Video prediction aims to predict future frames by modeling the complex spatiotemporal dynamics in videos. However, most of the existing methods only model the temporal information and the spatial information for videos in an independent manner but haven&#39;t fully explored the correlations between both terms. In this paper, we propose a SpatioTemporal-Aware Unit (STAU) for video prediction and beyond&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09456v1-abstract-full').style.display = 'inline'; document.getElementById('2204.09456v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09456v1-abstract-full" style="display: none;">
        Video prediction aims to predict future frames by modeling the complex spatiotemporal dynamics in videos. However, most of the existing methods only model the temporal information and the spatial information for videos in an independent manner but haven&#39;t fully explored the correlations between both terms. In this paper, we propose a SpatioTemporal-Aware Unit (STAU) for video prediction and beyond by exploring the significant spatiotemporal correlations in videos. On the one hand, the motion-aware attention weights are learned from the spatial states to help aggregate the temporal states in the temporal domain. On the other hand, the appearance-aware attention weights are learned from the temporal states to help aggregate the spatial states in the spatial domain. In this way, the temporal information and the spatial information can be greatly aware of each other in both domains, during which, the spatiotemporal receptive field can also be greatly broadened for more reliable spatiotemporal modeling. Experiments are not only conducted on traditional video prediction tasks but also other tasks beyond video prediction, including the early action recognition and object detection tasks. Experimental results show that our STAU can outperform other methods on all tasks in terms of performance and computation efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09456v1-abstract-full').style.display = 'none'; document.getElementById('2204.09456v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to TPAMI</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09314">arXiv:2204.09314</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09314">pdf</a>, <a href="https://arxiv.org/format/2204.09314">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+R">Ren Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+M">Meisong Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xing%2C+Q">Qunliang Xing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+M">Minglang Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Mai Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+L">Lai Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Huaida Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Ying Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ben%2C+Y">Youcheng Ben</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+X">Xiao Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+C">Chen Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+P">Pei Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+G">Gang Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Junyi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+R">Renlong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhilu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shang%2C+W">Wei Shang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Z">Zhengyao Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yunjin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+M">Mingcai Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+D">Dongwei Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+K">Kai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zuo%2C+W">Wangmeng Zuo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ostyakov%2C+P">Pavel Ostyakov</a>
      , et al. (54 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09314v2-abstract-short" style="display: inline;">
        This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video. In this challenge, we proposed the LDV 2.0 dataset, which includes the LDV dataset (240 videos) and 95 additional videos. This challenge includes three tracks. Track 1 aims at enhancing the videos compressed by HEVC at a fixed QP. Track 2 and Track 3 target both the super-resolution and qua&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09314v2-abstract-full').style.display = 'inline'; document.getElementById('2204.09314v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09314v2-abstract-full" style="display: none;">
        This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video. In this challenge, we proposed the LDV 2.0 dataset, which includes the LDV dataset (240 videos) and 95 additional videos. This challenge includes three tracks. Track 1 aims at enhancing the videos compressed by HEVC at a fixed QP. Track 2 and Track 3 target both the super-resolution and quality enhancement of HEVC compressed video. They require x2 and x4 super-resolution, respectively. The three tracks totally attract more than 600 registrations. In the test phase, 8 teams, 8 teams and 12 teams submitted the final results to Tracks 1, 2 and 3, respectively. The proposed methods and solutions gauge the state-of-the-art of super-resolution and quality enhancement of compressed video. The proposed LDV 2.0 dataset is available at https://github.com/RenYang-home/LDV_dataset. The homepage of this challenge (including open-sourced codes) is at https://github.com/RenYang-home/NTIRE22_VEnh_SR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09314v2-abstract-full').style.display = 'none'; document.getElementById('2204.09314v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09054">arXiv:2204.09054</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09054">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An unsupervised approach for semantic place annotation of trajectories based on the prior probability
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+J">Junyi Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xianfeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+P">Peng Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+J">Jie Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+J">Jianfeng Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09054v1-abstract-short" style="display: inline;">
        Semantic place annotation can provide individual semantics, which can be of great help in the field of trajectory data mining. Most existing methods rely on annotated or external data and require retraining following a change of region, thus preventing their large-scale applications. Herein, we propose an unsupervised method denoted as UPAPP for the semantic place annotation of trajectories using&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09054v1-abstract-full').style.display = 'inline'; document.getElementById('2204.09054v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09054v1-abstract-full" style="display: none;">
        Semantic place annotation can provide individual semantics, which can be of great help in the field of trajectory data mining. Most existing methods rely on annotated or external data and require retraining following a change of region, thus preventing their large-scale applications. Herein, we propose an unsupervised method denoted as UPAPP for the semantic place annotation of trajectories using spatiotemporal information. The Bayesian Criterion is specifically employed to decompose the spatiotemporal probability of the candidate place into spatial probability, duration probability, and visiting time probability. Spatial information in ROI and POI data is subsequently adopted to calculate the spatial probability. In terms of the temporal probabilities, the Term Frequency Inverse Document Frequency weighting algorithm is used to count the potential visits to different place types in the trajectories, and generates the prior probabilities of the visiting time and duration. The spatiotemporal probability of the candidate place is then combined with the importance of the place category to annotate the visited places. Validation with a trajectory dataset collected by 709 volunteers in Beijing showed that our method achieved an overall and average accuracy of 0.712 and 0.720, respectively, indicating that the visited places can be annotated accurately without any external data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09054v1-abstract-full').style.display = 'none'; document.getElementById('2204.09054v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">36 pages, 7 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08887">arXiv:2204.08887</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08887">pdf</a>, <a href="https://arxiv.org/format/2204.08887">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-Lingual Phrase Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+H">Heqi Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chi%2C+Z">Zewen Chi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+H">Heyan Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+T">Tan Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lan%2C+T">Tian Lan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+W">Wei Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mao%2C+X">Xian-Ling Mao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08887v1-abstract-short" style="display: inline;">
        Cross-lingual retrieval aims to retrieve relevant text across languages. Current methods typically achieve cross-lingual retrieval by learning language-agnostic text representations in word or sentence level. However, how to learn phrase representations for cross-lingual phrase retrieval is still an open problem. In this paper, we propose XPR, a cross-lingual phrase retriever that extracts phrase&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08887v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08887v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08887v1-abstract-full" style="display: none;">
        Cross-lingual retrieval aims to retrieve relevant text across languages. Current methods typically achieve cross-lingual retrieval by learning language-agnostic text representations in word or sentence level. However, how to learn phrase representations for cross-lingual phrase retrieval is still an open problem. In this paper, we propose XPR, a cross-lingual phrase retriever that extracts phrase representations from unlabeled example sentences. Moreover, we create a large-scale cross-lingual phrase retrieval dataset, which contains 65K bilingual phrase pairs and 4.2M example sentences in 8 English-centric language pairs. Experimental results show that XPR outperforms state-of-the-art baselines which utilize word-level or sentence-level representations. XPR also shows impressive zero-shot transferability that enables the model to perform retrieval in an unseen language pair during training. Our dataset, code, and trained models are publicly available at www.github.com/cwszz/XPR/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08887v1-abstract-full').style.display = 'none'; document.getElementById('2204.08887v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08689">arXiv:2204.08689</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08689">pdf</a>, <a href="https://arxiv.org/format/2204.08689">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lai%2C+S">Siyu Lai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Z">Zhen Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meng%2C+F">Fandong Meng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xue Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yufeng Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+J">Jinan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Jie Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08689v1-abstract-short" style="display: inline;">
        Generating adversarial examples for Neural Machine Translation (NMT) with single Round-Trip Translation (RTT) has achieved promising results by releasing the meaning-preserving restriction. However, a potential pitfall for this approach is that we cannot decide whether the generated examples are adversarial to the target NMT model or the auxiliary backward one, as the reconstruction error through&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08689v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08689v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08689v1-abstract-full" style="display: none;">
        Generating adversarial examples for Neural Machine Translation (NMT) with single Round-Trip Translation (RTT) has achieved promising results by releasing the meaning-preserving restriction. However, a potential pitfall for this approach is that we cannot decide whether the generated examples are adversarial to the target NMT model or the auxiliary backward one, as the reconstruction error through the RTT can be related to either. To remedy this problem, we propose a new criterion for NMT adversarial examples based on the Doubly Round-Trip Translation (DRTT). Specifically, apart from the source-target-source RTT, we also consider the target-source-target one, which is utilized to pick out the authentic adversarial examples for the target NMT model. Additionally, to enhance the robustness of the NMT model, we introduce the masked language models to construct bilingual adversarial pairs based on DRTT, which are used to train the NMT model directly. Extensive experiments on both the clean and noisy test sets (including the artificial and natural noise) show that our approach substantially improves the robustness of NMT models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08689v1-abstract-full').style.display = 'none'; document.getElementById('2204.08689v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at NAACL 2022 as a long paper of main conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08401">arXiv:2204.08401</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08401">pdf</a>, <a href="https://arxiv.org/format/2204.08401">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xuanyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Q">Qing Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+D">Dongliang Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08401v2-abstract-short" style="display: inline;">
        Knowledge graph embedding (KGE) aims to learn continuous vectors of relations and entities in knowledge graph. Recently, transition-based KGE methods have achieved promising performance, where the single relation vector learns to translate head entity to tail entity. However, this scoring pattern is not suitable for complex scenarios where the same entity pair has different relations. Previous mod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08401v2-abstract-full').style.display = 'inline'; document.getElementById('2204.08401v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08401v2-abstract-full" style="display: none;">
        Knowledge graph embedding (KGE) aims to learn continuous vectors of relations and entities in knowledge graph. Recently, transition-based KGE methods have achieved promising performance, where the single relation vector learns to translate head entity to tail entity. However, this scoring pattern is not suitable for complex scenarios where the same entity pair has different relations. Previous models usually focus on the improvement of entity representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single relation vector. In this paper, we propose a novel transition-based method, TranS, for knowledge graph embedding. The single relation vector in traditional scoring patterns is replaced with synthetic relation representation, which can solve these issues effectively and efficiently. Experiments on a large knowledge graph dataset, ogbl-wikikg2, show that our model achieves state-of-the-art results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08401v2-abstract-full').style.display = 'none'; document.getElementById('2204.08401v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08040">arXiv:2204.08040</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08040">pdf</a>, <a href="https://arxiv.org/format/2204.08040">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NICO++: Towards Better Benchmarking for Domain Generalization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xingxuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Y">Yue He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+R">Renzhe Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+H">Han Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Z">Zheyan Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cui%2C+P">Peng Cui</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08040v2-abstract-short" style="display: inline;">
        Despite the remarkable performance that modern deep neural networks have achieved on independent and identically distributed (I.I.D.) data, they can crash under distribution shifts. Most current evaluation methods for domain generalization (DG) adopt the leave-one-out strategy as a compromise on the limited number of domains. We propose a large-scale benchmark with extensive labeled domains named&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08040v2-abstract-full').style.display = 'inline'; document.getElementById('2204.08040v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08040v2-abstract-full" style="display: none;">
        Despite the remarkable performance that modern deep neural networks have achieved on independent and identically distributed (I.I.D.) data, they can crash under distribution shifts. Most current evaluation methods for domain generalization (DG) adopt the leave-one-out strategy as a compromise on the limited number of domains. We propose a large-scale benchmark with extensive labeled domains named NICO++ along with more rational evaluation methods for comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively. Two novel generalization bounds from the perspective of data construction are proposed to prove that limited concept shift and significant covariate shift favor the evaluation capability for generalization. Through extensive experiments, NICO++ shows its superior evaluation capability compared with current DG datasets and its contribution in alleviating unfairness caused by the leak of oracle knowledge in model selection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08040v2-abstract-full').style.display = 'none'; document.getElementById('2204.08040v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The NICO challenge based on NICO++ can be found at https://nicochallenge.com/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06892">arXiv:2204.06892</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06892">pdf</a>, <a href="https://arxiv.org/format/2204.06892">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Implicit Sample Extension for Unsupervised Person Re-Identification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+D">Dongdong Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhigang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+E">Errui Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+J+Q">Javen Qinfeng Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhaoxiang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jingdong Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06892v1-abstract-short" style="display: inline;">
        Most existing unsupervised person re-identification (Re-ID) methods use clustering to generate pseudo labels for model training. Unfortunately, clustering sometimes mixes different true identities together or splits the same identity into two or more sub clusters. Training on these noisy clusters substantially hampers the Re-ID accuracy. Due to the limited samples in each identity, we suppose ther&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06892v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06892v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06892v1-abstract-full" style="display: none;">
        Most existing unsupervised person re-identification (Re-ID) methods use clustering to generate pseudo labels for model training. Unfortunately, clustering sometimes mixes different true identities together or splits the same identity into two or more sub clusters. Training on these noisy clusters substantially hampers the Re-ID accuracy. Due to the limited samples in each identity, we suppose there may lack some underlying information to well reveal the accurate clusters. To discover these information, we propose an Implicit Sample Extension (\OurWholeMethod) method to generate what we call support samples around the cluster boundaries. Specifically, we generate support samples from actual samples and their neighbouring clusters in the embedding space through a progressive linear interpolation (PLI) strategy. PLI controls the generation with two critical factors, i.e., 1) the direction from the actual sample towards its K-nearest clusters and 2) the degree for mixing up the context information from the K-nearest clusters. Meanwhile, given the support samples, ISE further uses a label-preserving loss to pull them towards their corresponding actual samples, so as to compact each cluster. Consequently, ISE reduces the &#34;sub and mixed&#34; clustering errors, thus improving the Re-ID performance. Extensive experiments demonstrate that the proposed method is effective and achieves state-of-the-art performance for unsupervised person Re-ID. Code is available at: \url{https://github.com/PaddlePaddle/PaddleClas}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06892v1-abstract-full').style.display = 'none'; document.getElementById('2204.06892v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06307">arXiv:2204.06307</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06307">pdf</a>, <a href="https://arxiv.org/format/2204.06307">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-View Consistent Generative Adversarial Networks for 3D-aware Image Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xuanmeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Z">Zhedong Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+D">Daiheng Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+B">Bang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+P">Pan Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yi Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06307v1-abstract-short" style="display: inline;">
        3D-aware image synthesis aims to generate images of objects from multiple views by learning a 3D representation. However, one key challenge remains: existing approaches lack geometry constraints, hence usually fail to generate multi-view consistent images. To address this challenge, we propose Multi-View Consistent Generative Adversarial Networks (MVCGAN) for high-quality 3D-aware image synthesis&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06307v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06307v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06307v1-abstract-full" style="display: none;">
        3D-aware image synthesis aims to generate images of objects from multiple views by learning a 3D representation. However, one key challenge remains: existing approaches lack geometry constraints, hence usually fail to generate multi-view consistent images. To address this challenge, we propose Multi-View Consistent Generative Adversarial Networks (MVCGAN) for high-quality 3D-aware image synthesis with geometry constraints. By leveraging the underlying 3D geometry information of generated images, i.e., depth and camera transformation matrix, we explicitly establish stereo correspondence between views to perform multi-view joint optimization. In particular, we enforce the photometric consistency between pairs of views and integrate a stereo mixup mechanism into the training process, encouraging the model to reason about the correct 3D shape. Besides, we design a two-stage training strategy with feature-level multi-view joint optimization to improve the image quality. Extensive experiments on three datasets demonstrate that MVCGAN achieves the state-of-the-art performance for 3D-aware image synthesis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06307v1-abstract-full').style.display = 'none'; document.getElementById('2204.06307v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06117">arXiv:2204.06117</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06117">pdf</a>, <a href="https://arxiv.org/format/2204.06117">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Huili Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinqiao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+K">Ke Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Koushanfar%2C+F">Farinaz Koushanfar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06117v1-abstract-short" style="display: inline;">
        This paper proposes AdaTest, a novel adaptive test pattern generation framework for efficient and reliable Hardware Trojan (HT) detection. HT is a backdoor attack that tampers with the design of victim integrated circuits (ICs). AdaTest improves the existing HT detection techniques in terms of scalability and accuracy of detecting smaller Trojans in the presence of noise and variations. To achieve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06117v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06117v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06117v1-abstract-full" style="display: none;">
        This paper proposes AdaTest, a novel adaptive test pattern generation framework for efficient and reliable Hardware Trojan (HT) detection. HT is a backdoor attack that tampers with the design of victim integrated circuits (ICs). AdaTest improves the existing HT detection techniques in terms of scalability and accuracy of detecting smaller Trojans in the presence of noise and variations. To achieve high trigger coverage, AdaTest leverages Reinforcement Learning (RL) to produce a diverse set of test inputs. Particularly, we progressively generate test vectors with high reward values in an iterative manner. In each iteration, the test set is evaluated and adaptively expanded as needed. Furthermore, AdaTest integrates adaptive sampling to prioritize test samples that provide more information for HT detection, thus reducing the number of samples while improving the sample quality for faster exploration. We develop AdaTest with a Software/Hardware co-design principle and provide an optimized on-chip architecture solution. AdaTest&#39;s architecture minimizes the hardware overhead in two ways:(i) Deploying circuit emulation on programmable hardware to accelerate reward evaluation of the test input; (ii) Pipelining each computation stage in AdaTest by automatically constructing auxiliary circuit for test input generation, reward evaluation, and adaptive sampling. We evaluate AdaTest&#39;s performance on various HT benchmarks and compare it with two prior works that use logic testing for HT detection. Experimental results show that AdaTest engenders up to two orders of test generation speedup and two orders of test set size reduction compared to the prior works while achieving the same level or higher Trojan detection rate.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06117v1-abstract-full').style.display = 'none'; document.getElementById('2204.06117v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.05796">arXiv:2204.05796</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.05796">pdf</a>, <a href="https://arxiv.org/format/2204.05796">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A deep learning method for solving stochastic optimal control problems driven by fully-coupled FBSDEs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+S">Shaolin Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+S">Shige Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+Y">Ying Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xichuan Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.05796v1-abstract-short" style="display: inline;">
        In this paper, we mainly focus on the numerical solution of high-dimensional stochastic optimal control problem driven by fully-coupled forward-backward stochastic differential equations (FBSDEs in short) through deep learning. We first transform the problem into a stochastic Stackelberg differential game(leader-follower problem), then a cross-optimization method (CO method) is developed where the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05796v1-abstract-full').style.display = 'inline'; document.getElementById('2204.05796v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.05796v1-abstract-full" style="display: none;">
        In this paper, we mainly focus on the numerical solution of high-dimensional stochastic optimal control problem driven by fully-coupled forward-backward stochastic differential equations (FBSDEs in short) through deep learning. We first transform the problem into a stochastic Stackelberg differential game(leader-follower problem), then a cross-optimization method (CO method) is developed where the leader&#39;s cost functional and the follower&#39;s cost functional are optimized alternatively via deep neural networks. As for the numerical results, we compute two examples of the investment-consumption problem solved through stochastic recursive utility models, and the results of both examples demonstrate the effectiveness of our proposed algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05796v1-abstract-full').style.display = 'none'; document.getElementById('2204.05796v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.05596">arXiv:2204.05596</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.05596">pdf</a>, <a href="https://arxiv.org/format/2204.05596">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Equity of Nuclear Norm Maximization in Unsupervised Domain Adaptation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+W">Wenju Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+Q">Qing Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lan%2C+L">Long Lan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+M">Mengzhu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+B">Baoyun Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+Z">Zhengming Ding</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.05596v1-abstract-short" style="display: inline;">
        Nuclear norm maximization has shown the power to enhance the transferability of unsupervised domain adaptation model (UDA) in an empirical scheme. In this paper, we identify a new property termed equity, which indicates the balance degree of predicted classes, to demystify the efficacy of nuclear norm maximization for UDA theoretically. With this in mind, we offer a new discriminability-and-equity&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05596v1-abstract-full').style.display = 'inline'; document.getElementById('2204.05596v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.05596v1-abstract-full" style="display: none;">
        Nuclear norm maximization has shown the power to enhance the transferability of unsupervised domain adaptation model (UDA) in an empirical scheme. In this paper, we identify a new property termed equity, which indicates the balance degree of predicted classes, to demystify the efficacy of nuclear norm maximization for UDA theoretically. With this in mind, we offer a new discriminability-and-equity maximization paradigm built on squares loss, such that predictions are equalized explicitly. To verify its feasibility and flexibility, two new losses termed Class Weighted Squares Maximization (CWSM) and Normalized Squares Maximization (NSM), are proposed to maximize both predictive discriminability and equity, from the class level and the sample level, respectively. Importantly, we theoretically relate these two novel losses (i.e., CWSM and NSM) to the equity maximization under mild conditions, and empirically suggest the importance of the predictive equity in UDA. Moreover, it is very efficient to realize the equity constraints in both losses. Experiments of cross-domain image classification on three popular benchmark datasets show that both CWSM and NSM contribute to outperforming the corresponding counterparts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05596v1-abstract-full').style.display = 'none'; document.getElementById('2204.05596v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04954">arXiv:2204.04954</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04954">pdf</a>, <a href="https://arxiv.org/format/2204.04954">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reinforcement Re-ranking with 2D Grid-based Recommendation Panels
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+S">Sirui Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xu Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhiyu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yuan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Q">Quan Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+J">Jun Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04954v1-abstract-short" style="display: inline;">
        Modern recommender systems usually present items as one-dimensional ranking list. Recently there is a trend in e-commerce that the recommended items are organized as two-dimensional grid-based panels where users can view the items in both vertical and horizontal directions. Presenting items in grid-based result panels poses new challenges to recommender systems because existing models are all desi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04954v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04954v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04954v1-abstract-full" style="display: none;">
        Modern recommender systems usually present items as one-dimensional ranking list. Recently there is a trend in e-commerce that the recommended items are organized as two-dimensional grid-based panels where users can view the items in both vertical and horizontal directions. Presenting items in grid-based result panels poses new challenges to recommender systems because existing models are all designed to output sequential lists while the slots in a grid-based panel have no explicit order. Directly converting the item rankings into grids (e.g., pre-defining an order on the slots)overlooks the user-specific behavioral patterns on grid-based pan-els and inevitably hurts the user experiences. To address this issue, we propose a novel Markov decision process (MDP) to place the items in 2D grid-based result panels at the final re-ranking stage of the recommender systems. The model, referred to as Panel-MDP, takes an initial item ranking from the early stages as the input. Then, it defines the MDP discrete time steps as the ranks in the initial ranking list, and the actions as the slots in the grid-based panels, plus a NULL action. At each time step, Panel-MDP sequentially takes an action of selecting one slot for placing an item of the initial ranking list, or discarding the item if NULL action is selected. The process is continued until all of the slots are filled. The reinforcement learning algorithm of DQN is employed to implement and learn the parameters in the Panel-MDP. Experiments on a dataset collected from a widely-used e-commerce app demonstrated the superiority ofPanel-MDP in terms of recommending 2D grid-based result panels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04954v1-abstract-full').style.display = 'none'; document.getElementById('2204.04954v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04918">arXiv:2204.04918</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04918">pdf</a>, <a href="https://arxiv.org/format/2204.04918">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When NAS Meets Trees: An Efficient Algorithm for Neural Architecture Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+G">Guocheng Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xuanyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+G">Guohao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+C">Chen Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yukang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ghanem%2C+B">Bernard Ghanem</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04918v1-abstract-short" style="display: inline;">
        The key challenge in neural architecture search (NAS) is designing how to explore wisely in the huge search space. We propose a new NAS method called TNAS (NAS with trees), which improves search efficiency by exploring only a small number of architectures while also achieving a higher search accuracy. TNAS introduces an architecture tree and a binary operation tree, to factorize the search space a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04918v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04918v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04918v1-abstract-full" style="display: none;">
        The key challenge in neural architecture search (NAS) is designing how to explore wisely in the huge search space. We propose a new NAS method called TNAS (NAS with trees), which improves search efficiency by exploring only a small number of architectures while also achieving a higher search accuracy. TNAS introduces an architecture tree and a binary operation tree, to factorize the search space and substantially reduce the exploration size. TNAS performs a modified bi-level Breadth-First Search in the proposed trees to discover a high-performance architecture. Impressively, TNAS finds the global optimal architecture on CIFAR-10 with test accuracy of 94.37\% in four GPU hours in NAS-Bench-201. The average test accuracy is 94.35\%, which outperforms the state-of-the-art. Code is available at: \url{https://github.com/guochengqian/TNAS}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04918v1-abstract-full').style.display = 'none'; document.getElementById('2204.04918v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages, accepted at CVPR Workshop 2022 (ECV2022)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04867">arXiv:2204.04867</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04867">pdf</a>, <a href="https://arxiv.org/format/2204.04867">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Structured Graph Variational Autoencoders for Indoor Furniture layout Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chattopadhyay%2C+A">Aditya Chattopadhyay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wipf%2C+D+P">David Paul Wipf</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+H">Himanshu Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vidal%2C+R">Rene Vidal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04867v2-abstract-short" style="display: inline;">
        We present a structured graph variational autoencoder for generating the layout of indoor 3D scenes. Given the room type (e.g., living room or library) and the room layout (e.g., room elements such as floor and walls), our architecture generates a collection of objects (e.g., furniture items such as sofa, table and chairs) that is consistent with the room type and layout. This is a challenging pro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04867v2-abstract-full').style.display = 'inline'; document.getElementById('2204.04867v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04867v2-abstract-full" style="display: none;">
        We present a structured graph variational autoencoder for generating the layout of indoor 3D scenes. Given the room type (e.g., living room or library) and the room layout (e.g., room elements such as floor and walls), our architecture generates a collection of objects (e.g., furniture items such as sofa, table and chairs) that is consistent with the room type and layout. This is a challenging problem because the generated scene should satisfy multiple constrains, e.g., each object must lie inside the room and two objects cannot occupy the same volume. To address these challenges, we propose a deep generative model that encodes these relationships as soft constraints on an attributed graph (e.g., the nodes capture attributes of room and furniture elements, such as class, pose and size, and the edges capture geometric relationships such as relative orientation). The architecture consists of a graph encoder that maps the input graph to a structured latent space, and a graph decoder that generates a furniture graph, given a latent code and the room graph. The latent space is modeled with auto-regressive priors, which facilitates the generation of highly structured scenes. We also propose an efficient training procedure that combines matching and constrained learning. Experiments on the 3D-FRONT dataset show that our method produces scenes that are diverse and are adapted to the room layout.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04867v2-abstract-full').style.display = 'none'; document.getElementById('2204.04867v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04676">arXiv:2204.04676</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04676">pdf</a>, <a href="https://arxiv.org/format/2204.04676">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Simple Baselines for Image Restoration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+L">Liangyu Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+X">Xiaojie Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+J">Jian Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04676v1-abstract-short" style="display: inline;">
        Although there have been significant advances in the field of image restoration recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04676v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04676v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04676v1-abstract-full" style="display: none;">
        Although there have been significant advances in the field of image restoration recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that the nonlinear activation functions, e.g. Sigmoid, ReLU, GELU, Softmax, etc. are not necessary: they could be replaced by multiplication or removed. Thus, we derive a Nonlinear Activation Free Network, namely NAFNet, from the baseline. SOTA results are achieved on various challenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring), exceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs; 40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28 dB with less than half of its computational costs. The code and the pretrained models will be released at https://github.com/megvii-research/NAFNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04676v1-abstract-full').style.display = 'none'; document.getElementById('2204.04676v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Image Restoration</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04382">arXiv:2204.04382</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04382">pdf</a>, <a href="https://arxiv.org/format/2204.04382">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Federated Unsupervised Domain Adaptation for Face Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuang%2C+W">Weiming Zhuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gan%2C+X">Xin Gan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+Y">Yonggang Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xuesen Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shuai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yi%2C+S">Shuai Yi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04382v1-abstract-short" style="display: inline;">
        Given labeled data in a source domain, unsupervised domain adaptation has been widely adopted to generalize models for unlabeled data in a target domain, whose data distributions are different. However, existing works are inapplicable to face recognition under privacy constraints because they require sharing of sensitive face images between domains. To address this problem, we propose federated un&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04382v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04382v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04382v1-abstract-full" style="display: none;">
        Given labeled data in a source domain, unsupervised domain adaptation has been widely adopted to generalize models for unlabeled data in a target domain, whose data distributions are different. However, existing works are inapplicable to face recognition under privacy constraints because they require sharing of sensitive face images between domains. To address this problem, we propose federated unsupervised domain adaptation for face recognition, FedFR. FedFR jointly optimizes clustering-based domain adaptation and federated learning to elevate performance on the target domain. Specifically, for unlabeled data in the target domain, we enhance a clustering algorithm with distance constrain to improve the quality of predicted pseudo labels. Besides, we propose a new domain constraint loss (DCL) to regularize source domain training in federated learning. Extensive experiments on a newly constructed benchmark demonstrate that FedFR outperforms the baseline and classic methods on the target domain by 3% to 14% on different evaluation metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04382v1-abstract-full').style.display = 'none'; document.getElementById('2204.04382v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICME&#39;22. arXiv admin note: substantial text overlap with arXiv:2105.07606</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04329">arXiv:2204.04329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04329">pdf</a>, <a href="https://arxiv.org/format/2204.04329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinqiao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Huili Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+K">Ke Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Koushanfar%2C+F">Farinaz Koushanfar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04329v1-abstract-short" style="display: inline;">
        With the surge of Machine Learning (ML), An emerging amount of intelligent applications have been developed. Deep Neural Networks (DNNs) have demonstrated unprecedented performance across various fields such as medical diagnosis and autonomous driving. While DNNs are widely employed in security-sensitive fields, they are identified to be vulnerable to Neural Trojan (NT) attacks that are controlled&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04329v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04329v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04329v1-abstract-full" style="display: none;">
        With the surge of Machine Learning (ML), An emerging amount of intelligent applications have been developed. Deep Neural Networks (DNNs) have demonstrated unprecedented performance across various fields such as medical diagnosis and autonomous driving. While DNNs are widely employed in security-sensitive fields, they are identified to be vulnerable to Neural Trojan (NT) attacks that are controlled and activated by stealthy triggers. In this paper, we target to design a robust and adaptive Trojan detection scheme that inspects whether a pre-trained model has been Trojaned before its deployment. Prior works are oblivious of the intrinsic property of trigger distribution and try to reconstruct the trigger pattern using simple heuristics, i.e., stimulating the given model to incorrect outputs. As a result, their detection time and effectiveness are limited. We leverage the observation that the pixel trigger typically features spatial dependency and propose the first trigger approximation based black-box Trojan detection framework that enables a fast and scalable search of the trigger in the input space. Furthermore, our approach can also detect Trojans embedded in the feature space where certain filter transformations are used to activate the Trojan. We perform extensive experiments to investigate the performance of our approach across various datasets and ML models. Empirical results show that our approach achieves a ROC-AUC score of 0.93 on the public TrojAI dataset. Our code can be found at https://github.com/xinqiaozhang/adatrojan
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04329v1-abstract-full').style.display = 'none'; document.getElementById('2204.04329v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: substantial text overlap with arXiv:2102.01815</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04063">arXiv:2204.04063</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04063">pdf</a>, <a href="https://arxiv.org/format/2204.04063">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mao%2C+Y">Yuhao Mao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+C">Chong Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Saizhuo Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+S">Shouling Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xuhong Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhenguang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Jun Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+A+X">Alex X. Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Beyah%2C+R">Raheem Beyah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+T">Ting Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04063v1-abstract-short" style="display: inline;">
        One intriguing property of adversarial attacks is their &#34;transferability&#34; -- an adversarial example crafted with respect to one deep neural network (DNN) model is often found effective against other DNNs as well. Intensive research has been conducted on this phenomenon under simplistic controlled conditions. Yet, thus far, there is still a lack of comprehensive understanding about transferability-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04063v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04063v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04063v1-abstract-full" style="display: none;">
        One intriguing property of adversarial attacks is their &#34;transferability&#34; -- an adversarial example crafted with respect to one deep neural network (DNN) model is often found effective against other DNNs as well. Intensive research has been conducted on this phenomenon under simplistic controlled conditions. Yet, thus far, there is still a lack of comprehensive understanding about transferability-based attacks (&#34;transfer attacks&#34;) in real-world environments.
  To bridge this critical gap, we conduct the first large-scale systematic empirical study of transfer attacks against major cloud-based MLaaS platforms, taking the components of a real transfer attack into account. The study leads to a number of interesting findings which are inconsistent to the existing ones, including: (1) Simple surrogates do not necessarily improve real transfer attacks. (2) No dominant surrogate architecture is found in real transfer attacks. (3) It is the gap between posterior (output of the softmax layer) rather than the gap between logit (so-called $κ$ value) that increases transferability. Moreover, by comparing with prior works, we demonstrate that transfer attacks possess many previously unknown properties in real-world environments, such as (1) Model similarity is not a well-defined concept. (2) $L_2$ norm of perturbation can generate high transferability without usage of gradient and is a more powerful source than $L_\infty$ norm. We believe this work sheds light on the vulnerabilities of popular MLaaS platforms and points to a few promising research directions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04063v1-abstract-full').style.display = 'none'; document.getElementById('2204.04063v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE Security &amp; Privacy 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.03878">arXiv:2204.03878</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.03878">pdf</a>, <a href="https://arxiv.org/format/2204.03878">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Picture Fuzzy Interactional Aggregation Operators via Strict Triangular Norms and Applications to Multi-Criteria Decision Making
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">X. Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Z">Z. Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=%C3%87ayl%C4%B1%2C+G">G. Çaylı</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+P">P. Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">X. Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Z">Z. Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.03878v1-abstract-short" style="display: inline;">
        The picture fuzzy set, characterized by three membership degrees, is a helpful tool for multi-criteria decision making (MCDM). This paper investigates the structure of the closed operational laws in the picture fuzzy numbers (PFNs) and proposes efficient picture fuzzy MCDM methods. We first introduce an admissible order for PFNs and prove that all PFNs form a complete lattice under this order. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03878v1-abstract-full').style.display = 'inline'; document.getElementById('2204.03878v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.03878v1-abstract-full" style="display: none;">
        The picture fuzzy set, characterized by three membership degrees, is a helpful tool for multi-criteria decision making (MCDM). This paper investigates the structure of the closed operational laws in the picture fuzzy numbers (PFNs) and proposes efficient picture fuzzy MCDM methods. We first introduce an admissible order for PFNs and prove that all PFNs form a complete lattice under this order. Then, we give some specific examples to show the non-closeness of some existing picture fuzzy aggregation operators. To ensure the closeness of the operational laws in PFNs, we construct a new class of picture fuzzy operators based on strict triangular norms, which consider the interaction between the positive degrees (negative degrees) and the neutral degrees. Based on these new operators, we obtain the picture fuzzy interactional weighted average (PFIWA) operator and the picture fuzzy interactional weighted geometric (PFIWG) operator. They are proved to be monotonous, idempotent, bounded, shift-invariant, and homogeneous. We also establish a novel MCDM method under the picture fuzzy environment applying PFIWA and PFIWG operators. Furthermore, we present an illustrative example for a clear understanding of our method. We also give the comparative analysis among the operators induced by six classes of famous triangular norms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03878v1-abstract-full').style.display = 'none'; document.getElementById('2204.03878v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.03648">arXiv:2204.03648</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.03648">pdf</a>, <a href="https://arxiv.org/format/2204.03648">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SunStage: Portrait Reconstruction and Relighting using the Sun as a Light Stage
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yifan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Holynski%2C+A">Aleksander Holynski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiuming Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X+C">Xuaner Cecilia Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.03648v1-abstract-short" style="display: inline;">
        Outdoor portrait photographs are often marred by the harsh shadows cast under direct sunlight. To resolve this, one can use post-capture lighting manipulation techniques, but these methods either require complex hardware (e.g., a light stage) to capture each individual, or rely on image-based priors and thus fail to reconstruct many of the subtle facial details that vary from person to person. In&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03648v1-abstract-full').style.display = 'inline'; document.getElementById('2204.03648v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.03648v1-abstract-full" style="display: none;">
        Outdoor portrait photographs are often marred by the harsh shadows cast under direct sunlight. To resolve this, one can use post-capture lighting manipulation techniques, but these methods either require complex hardware (e.g., a light stage) to capture each individual, or rely on image-based priors and thus fail to reconstruct many of the subtle facial details that vary from person to person. In this paper, we present SunStage, a system for accurate, individually-tailored, and lightweight reconstruction of facial geometry and reflectance that can be used for general portrait relighting with cast shadows. Our method only requires the user to capture a selfie video outdoors, rotating in place, and uses the varying angles between the sun and the face as constraints in the joint reconstruction of facial geometry, reflectance properties, and lighting parameters. Aside from relighting, we show that our reconstruction can be used for applications like reflectance editing and view synthesis. Results and interactive demos are available at https://grail.cs.washington.edu/projects/sunstage/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03648v1-abstract-full').style.display = 'none'; document.getElementById('2204.03648v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://grail.cs.washington.edu/projects/sunstage/</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Zhang%2C+X&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>