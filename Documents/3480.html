<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 101 results for author: <span class="mathjax">Hsu, C</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Hsu%2C+C">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Hsu, C">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Hsu%2C+C&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Hsu, C">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+C&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Hsu%2C+C&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Hsu%2C+C&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Hsu%2C+C&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00780">arXiv:2205.00780</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00780">pdf</a>, <a href="https://arxiv.org/format/2205.00780">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ISCAS51556.2021.9401181">10.1109/ISCAS51556.2021.9401181 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VSA: Reconfigurable Vectorwise Spiking Neural Network Accelerator
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lien%2C+H">Hong-Han Lien</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chung-Wei Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+T">Tian-Sheuan Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00780v1-abstract-short" style="display: inline;">
        Spiking neural networks (SNNs) that enable low-power design on edge devices have recently attracted significant research. However, the temporal characteristic of SNNs causes high latency, high bandwidth and high energy consumption for the hardware. In this work, we propose a binary weight spiking model with IF-based Batch Normalization for small time steps and low hardware cost when direct trainin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00780v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00780v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00780v1-abstract-full" style="display: none;">
        Spiking neural networks (SNNs) that enable low-power design on edge devices have recently attracted significant research. However, the temporal characteristic of SNNs causes high latency, high bandwidth and high energy consumption for the hardware. In this work, we propose a binary weight spiking model with IF-based Batch Normalization for small time steps and low hardware cost when direct training with input encoding layer and spatio-temporal back propagation (STBP). In addition, we propose a vectorwise hardware accelerator that is reconfigurable for different models, inference time steps and even supports the encoding layer to receive multi-bit input. The required memory bandwidth is further reduced by two-layer fusion mechanism. The implementation result shows competitive accuracy on the MNIST and CIFAR-10 datasets with only 8 time steps, and achieves power efficiency of 25.9 TOPS/W.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00780v1-abstract-full').style.display = 'none'; document.getElementById('2205.00780v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 8 figures, published in IEEE ISCAS 2021</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          B.5.m
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00313">arXiv:2205.00313</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00313">pdf</a>, <a href="https://arxiv.org/format/2205.00313">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3495163">10.1145/3495163 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FairSR: Fairness-aware Sequential Recommendation through Multi-Task Learning with Preference Graph Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cheng-Te Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00313v1-abstract-short" style="display: inline;">
        Sequential recommendation (SR) learns from the temporal dynamics of user-item interactions to predict the next ones. Fairness-aware recommendation mitigates a variety of algorithmic biases in the learning of user preferences. This paper aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interacti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00313v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00313v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00313v1-abstract-full" style="display: none;">
        Sequential recommendation (SR) learns from the temporal dynamics of user-item interactions to predict the next ones. Fairness-aware recommendation mitigates a variety of algorithmic biases in the learning of user preferences. This paper aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interaction fairness, is defined to estimate how recommended items are fairly interacted by users with different protected attribute groups. We propose a multi-task learning based deep end-to-end model, FairSR, which consists of two parts. One is to learn and distill personalized sequential features from the given user and her item sequence for SR. The other is fairness-aware preference graph embedding (FPGE). The aim of FPGE is two-fold: incorporating the knowledge of users&#39; and items&#39; attributes and their correlation into entity representations, and alleviating the unfair distributions of user attributes on items. Extensive experiments conducted on three datasets show FairSR can outperform state-of-the-art SR models in recommendation performance. In addition, the recommended items by FairSR also exhibit promising interaction fairness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00313v1-abstract-full').style.display = 'none'; document.getElementById('2205.00313v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACM Trans. Intell. Syst. Technol. (TIST) 2022</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          H.3.3
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09915">arXiv:2204.09915</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09915">pdf</a>, <a href="https://arxiv.org/format/2204.09915">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Do Human Mobility Network Analyses Produced from Different Location-based Data Sources Yield Similar Results across Scales?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Wei Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Chenyue Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+K+M">Kiet Minh Nguyen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chien%2C+Y">Yu-Heng Chien</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mostafavi%2C+A">Ali Mostafavi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09915v1-abstract-short" style="display: inline;">
        The burgeoning availability of sensing technology and location-based data is driving the expansion of analysis of human mobility networks in science and engineering research, as well as in epidemic forecasting and mitigation, urban planning, traffic engineering, emergency response, and business development. However, studies employ datasets provided by different location-based data providers, and t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09915v1-abstract-full').style.display = 'inline'; document.getElementById('2204.09915v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09915v1-abstract-full" style="display: none;">
        The burgeoning availability of sensing technology and location-based data is driving the expansion of analysis of human mobility networks in science and engineering research, as well as in epidemic forecasting and mitigation, urban planning, traffic engineering, emergency response, and business development. However, studies employ datasets provided by different location-based data providers, and the extent to which the human mobility measures and results obtained from different datasets are comparable is not known. To address this gap, in this study, we examined three prominent location-based data sources: Spectus, X-Mode, and Veraset to analyze human mobility networks across metropolitan areas at different scales: global, sub-structure, and microscopic. Dissimilar results were obtained from the three datasets, suggesting the sensitivity of network models and measures to datasets. This finding has important implications for building generalized theories of human mobility and urban dynamics based on different datasets. The findings also highlighted the need for ground-truthed human movement datasets to serve as the benchmark for testing the representativeness of human mobility datasets. Researchers and decision-makers across different fields of science and technology should recognize the sensitivity of human mobility results to dataset choice and develop procedures for ground-truthing the selected datasets in terms of representativeness of data points and transferability of results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09915v1-abstract-full').style.display = 'none'; document.getElementById('2204.09915v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07316">arXiv:2204.07316</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07316">pdf</a>, <a href="https://arxiv.org/format/2204.07316">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chan-Jan Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsao%2C+Y">Yu Tsao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07316v3-abstract-short" style="display: inline;">
        Transformer-based models are widely used in natural language understanding (NLU) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders&#39; success in visual-language tasks while we alter the learning obj&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07316v3-abstract-full').style.display = 'inline'; document.getElementById('2204.07316v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07316v3-abstract-full" style="display: none;">
        Transformer-based models are widely used in natural language understanding (NLU) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders&#39; success in visual-language tasks while we alter the learning objective to cater to the language-heavy characteristics of NLU. After training with a small number of extra adapting steps and finetuned, the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in general language understanding evaluation (GLUE), situations with adversarial generations (SWAG) benchmarks, and readability benchmarks. We analyze the performance of XDBERT on GLUE to show that the improvement is likely visually grounded.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07316v3-abstract-full').style.display = 'none'; document.getElementById('2204.07316v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.09824">arXiv:2203.09824</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.09824">pdf</a>, <a href="https://arxiv.org/format/2203.09824">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Cho-Ying Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chin-Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Neumann%2C+U">Ulrich Neumann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.09824v1-abstract-short" style="display: inline;">
        This work digs into a root question in human perception: can face geometry be gleaned from one&#39;s voices? Previous works that study this question only adopt developments in image synthesis and convert voices into face images to show correlations, but working on the image domain unavoidably involves predicting attributes that voices cannot hint, including facial textures, hairstyles, and backgrounds&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09824v1-abstract-full').style.display = 'inline'; document.getElementById('2203.09824v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.09824v1-abstract-full" style="display: none;">
        This work digs into a root question in human perception: can face geometry be gleaned from one&#39;s voices? Previous works that study this question only adopt developments in image synthesis and convert voices into face images to show correlations, but working on the image domain unavoidably involves predicting attributes that voices cannot hint, including facial textures, hairstyles, and backgrounds. We instead investigate the ability to reconstruct 3D faces to concentrate on only geometry, which is much more physiologically grounded. We propose our analysis framework, Cross-Modal Perceptionist, under both supervised and unsupervised learning. First, we construct a dataset, Voxceleb-3D, which extends Voxceleb and includes paired voices and face meshes, making supervised learning possible. Second, we use a knowledge distillation mechanism to study whether face geometry can still be gleaned from voices without paired voices and 3D face data under limited availability of 3D face scans. We break down the core question into four parts and perform visual and numerical analyses as responses to the core question. Our findings echo those in physiology and neuroscience about the correlation between voices and facial structures. The work provides future human-centric cross-modal learning with explainable foundations. See our project page: https://choyingw.github.io/works/Voice2Mesh/index.html
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09824v1-abstract-full').style.display = 'none'; document.getElementById('2203.09824v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to CVPR 2022. Project page: https://choyingw.github.io/works/Voice2Mesh/index.html. This version supersedes arXiv:2104.10299</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.09565">arXiv:2203.09565</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.09565">pdf</a>, <a href="https://arxiv.org/format/2203.09565">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Strategic Maneuver and Disruption with Reinforcement Learning Approaches for Multi-Agent Coordination
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Asher%2C+D+E">Derrik E. Asher</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Basak%2C+A">Anjon Basak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fernandez%2C+R">Rolando Fernandez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sharma%2C+P+K">Piyush K. Sharma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaroukian%2C+E+G">Erin G. Zaroukian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C+D">Christopher D. Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dorothy%2C+M+R">Michael R. Dorothy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mahre%2C+T">Thomas Mahre</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Galindo%2C+G">Gerardo Galindo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Frerichs%2C+L">Luke Frerichs</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rogers%2C+J">John Rogers</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fossaceca%2C+J">John Fossaceca</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.09565v1-abstract-short" style="display: inline;">
        Reinforcement learning (RL) approaches can illuminate emergent behaviors that facilitate coordination across teams of agents as part of a multi-agent system (MAS), which can provide windows of opportunity in various military tasks. Technologically advancing adversaries pose substantial risks to a friendly nation&#39;s interests and resources. Superior resources alone are not enough to defeat adversari&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09565v1-abstract-full').style.display = 'inline'; document.getElementById('2203.09565v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.09565v1-abstract-full" style="display: none;">
        Reinforcement learning (RL) approaches can illuminate emergent behaviors that facilitate coordination across teams of agents as part of a multi-agent system (MAS), which can provide windows of opportunity in various military tasks. Technologically advancing adversaries pose substantial risks to a friendly nation&#39;s interests and resources. Superior resources alone are not enough to defeat adversaries in modern complex environments because adversaries create standoff in multiple domains against predictable military doctrine-based maneuvers. Therefore, as part of a defense strategy, friendly forces must use strategic maneuvers and disruption to gain superiority in complex multi-faceted domains such as multi-domain operations (MDO). One promising avenue for implementing strategic maneuver and disruption to gain superiority over adversaries is through coordination of MAS in future military operations. In this paper, we present overviews of prominent works in the RL domain with their strengths and weaknesses for overcoming the challenges associated with performing autonomous strategic maneuver and disruption in military contexts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09565v1-abstract-full').style.display = 'none'; document.getElementById('2203.09565v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, 3 figures, 60 references, Review Paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.08227">arXiv:2202.08227</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.08227">pdf</a>, <a href="https://arxiv.org/format/2202.08227">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ditto: Building Digital Twins of Articulated Objects from Interaction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+Z">Zhenyu Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Cheng-Chun Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yuke Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.08227v3-abstract-short" style="display: inline;">
        Digitizing physical objects into the virtual world has the potential to unlock new research and applications in embodied AI and mixed reality. This work focuses on recreating interactive digital twins of real-world articulated objects, which can be directly imported into virtual environments. We introduce Ditto to learn articulation model estimation and 3D geometry reconstruction of an articulated&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.08227v3-abstract-full').style.display = 'inline'; document.getElementById('2202.08227v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.08227v3-abstract-full" style="display: none;">
        Digitizing physical objects into the virtual world has the potential to unlock new research and applications in embodied AI and mixed reality. This work focuses on recreating interactive digital twins of real-world articulated objects, which can be directly imported into virtual environments. We introduce Ditto to learn articulation model estimation and 3D geometry reconstruction of an articulated object through interactive perception. Given a pair of visual observations of an articulated object before and after interaction, Ditto reconstructs part-level geometry and estimates the articulation model of the object. We employ implicit neural representations for joint geometry and articulation modeling. Our experiments show that Ditto effectively builds digital twins of articulated objects in a category-agnostic way. We also apply Ditto to real-world objects and deploy the recreated digital twins in physical simulation. Code and additional results are available at https://ut-austin-rpl.github.io/Ditto
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.08227v3-abstract-full').style.display = 'none'; document.getElementById('2202.08227v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2022 Oral. Code and additional results are available at https://ut-austin-rpl.github.io/Ditto</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.07983">arXiv:2202.07983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.07983">pdf</a>, <a href="https://arxiv.org/format/2202.07983">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fang%2C+H">Huihui Fang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+F">Fei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+H">Huazhu Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+X">Xu Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+X">Xingxing Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+F">Fengbin Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Son%2C+J">Jaemin Son</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+S">Sunho Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Quellec%2C+G">Gwenole Quellec</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Matta%2C+S">Sarah Matta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shankaranarayana%2C+S+M">Sharath M Shankaranarayana</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yi-Ting Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Chuen-heng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+C">Chia-Yen Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chih-Chung Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+H">Hai Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lei%2C+B">Baiying Lei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Baid%2C+U">Ujjwal Baid</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Innani%2C+S">Shubham Innani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dang%2C+K">Kang Dang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+W">Wenxiu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kamble%2C+R">Ravi Kamble</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Singhal%2C+N">Nitin Singhal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Orlando%2C+J+I">Jos√© Ignacio Orlando</a>
      , et al. (3 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.07983v2-abstract-short" style="display: inline;">
        Age-related macular degeneration (AMD) is the leading cause of visual impairment among elderly in the world. Early detection of AMD is of great importance as the vision loss caused by AMD is irreversible and permanent. Color fundus photography is the most cost-effective imaging modality to screen for retinal disorders. \textcolor{red}{Recently, some algorithms based on deep learning had been devel&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.07983v2-abstract-full').style.display = 'inline'; document.getElementById('2202.07983v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.07983v2-abstract-full" style="display: none;">
        Age-related macular degeneration (AMD) is the leading cause of visual impairment among elderly in the world. Early detection of AMD is of great importance as the vision loss caused by AMD is irreversible and permanent. Color fundus photography is the most cost-effective imaging modality to screen for retinal disorders. \textcolor{red}{Recently, some algorithms based on deep learning had been developed for fundus image analysis and automatic AMD detection. However, a comprehensive annotated dataset and a standard evaluation benchmark are still missing.} To deal with this issue, we set up the Automatic Detection challenge on Age-related Macular degeneration (ADAM) for the first time, held as a satellite event of the ISBI 2020 conference. The ADAM challenge consisted of four tasks which cover the main topics in detecting AMD from fundus images, including classification of AMD, detection and segmentation of optic disc, localization of fovea, and detection and segmentation of lesions. The ADAM challenge has released a comprehensive dataset of 1200 fundus images with the category labels of AMD, the pixel-wise segmentation masks of the full optic disc and lesions (drusen, exudate, hemorrhage, scar, and other), as well as the location coordinates of the macular fovea. A uniform evaluation framework has been built to make a fair comparison of different models. During the ADAM challenge, 610 results were submitted for online evaluation, and finally, 11 teams participated in the onsite challenge. This paper introduces the challenge, dataset, and evaluation methods, as well as summarizes the methods and analyzes the results of the participating teams of each task. In particular, we observed that ensembling strategy and clinical prior knowledge can better improve the performances of the deep learning models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.07983v2-abstract-full').style.display = 'none'; document.getElementById('2202.07983v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">29 pages, 17 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.02830">arXiv:2202.02830</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.02830">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/1122445.1122456">10.1145/1122445.1122456 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discovering Personalized Semantics for Soft Attributes in Recommender Systems using Concept Activation Vectors
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=G%C3%B6pfert%2C+C">Christina G√∂pfert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chow%2C+Y">Yinlam Chow</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chih-wei Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vendrov%2C+I">Ivan Vendrov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+T">Tyler Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ramachandran%2C+D">Deepak Ramachandran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boutilier%2C+C">Craig Boutilier</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.02830v2-abstract-short" style="display: inline;">
        Interactive recommender systems (RSs) allow users to express intent, preferences and contexts in a rich fashion, often using natural language. One challenge in using such feedback is inferring a user&#39;s semantic intent from the open-ended terms used to describe an item, and using it to refine recommendation results. Leveraging concept activation vectors (CAVs) [21], we develop a framework to learn&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.02830v2-abstract-full').style.display = 'inline'; document.getElementById('2202.02830v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.02830v2-abstract-full" style="display: none;">
        Interactive recommender systems (RSs) allow users to express intent, preferences and contexts in a rich fashion, often using natural language. One challenge in using such feedback is inferring a user&#39;s semantic intent from the open-ended terms used to describe an item, and using it to refine recommendation results. Leveraging concept activation vectors (CAVs) [21], we develop a framework to learn a representation that captures the semantics of such attributes and connects them to user preferences and behaviors in RSs. A novel feature of our approach is its ability to distinguish objective and subjective attributes and associate different senses with different users. Using synthetic and real-world datasets, we show that our CAV representation accurately interprets users&#39; subjective semantics, and can improve recommendations via interactive critiquing
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.02830v2-abstract-full').style.display = 'none'; document.getElementById('2202.02830v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.07490">arXiv:2201.07490</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.07490">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        POPPINS : A Population-Based Digital Spiking Neuromorphic Processor with Integer Quadratic Integrate-and-Fire Neurons
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yeh%2C+Z">Zuo-Wei Yeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Hua Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=White%2C+A">Alexander White</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yeh%2C+C">Chen-Fu Yeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+W">Wen-Chieh Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Cheng-Te Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lo%2C+C">Chung-Chuan Lo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+K">Kea-Tiong Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.07490v1-abstract-short" style="display: inline;">
        The inner operations of the human brain as a biological processing system remain largely a mystery. Inspired by the function of the human brain and based on the analysis of simple neural network systems in other species, such as Drosophila, neuromorphic computing systems have attracted considerable interest. In cellular-level connectomics research, we can identify the characteristics of biological&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.07490v1-abstract-full').style.display = 'inline'; document.getElementById('2201.07490v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.07490v1-abstract-full" style="display: none;">
        The inner operations of the human brain as a biological processing system remain largely a mystery. Inspired by the function of the human brain and based on the analysis of simple neural network systems in other species, such as Drosophila, neuromorphic computing systems have attracted considerable interest. In cellular-level connectomics research, we can identify the characteristics of biological neural network, called population, which constitute not only recurrent fullyconnection in network, also an external-stimulus and selfconnection in each neuron. Relying on low data bandwidth of spike transmission in network and input data, Spiking Neural Networks exhibit low-latency and low-power design. In this study, we proposed a configurable population-based digital spiking neuromorphic processor in 180nm process technology with two configurable hierarchy populations. Also, these neurons in the processor can be configured as novel models, integer quadratic integrate-and-fire neuron models, which contain an unsigned 8-bit membrane potential value. The processor can implement intelligent decision making for avoidance in real-time. Moreover, the proposed approach enables the developments of biomimetic neuromorphic system and various low-power, and low-latency inference processing applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.07490v1-abstract-full').style.display = 'none'; document.getElementById('2201.07490v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.08543">arXiv:2111.08543</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.08543">pdf</a>, <a href="https://arxiv.org/format/2111.08543">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WikiContradiction: Detecting Self-Contradiction Articles on Wikipedia
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cheng-Te Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saez-Trumper%2C+D">Diego Saez-Trumper</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yi-Zhan Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.08543v1-abstract-short" style="display: inline;">
        While Wikipedia has been utilized for fact-checking and claim verification to debunk misinformation and disinformation, it is essential to either improve article quality and rule out noisy articles. Self-contradiction is one of the low-quality article types in Wikipedia. In this work, we propose a task of detecting self-contradiction articles in Wikipedia. Based on the &#34;self-contradictory&#34; templat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08543v1-abstract-full').style.display = 'inline'; document.getElementById('2111.08543v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.08543v1-abstract-full" style="display: none;">
        While Wikipedia has been utilized for fact-checking and claim verification to debunk misinformation and disinformation, it is essential to either improve article quality and rule out noisy articles. Self-contradiction is one of the low-quality article types in Wikipedia. In this work, we propose a task of detecting self-contradiction articles in Wikipedia. Based on the &#34;self-contradictory&#34; template, we create a novel dataset for the self-contradiction detection task. Conventional contradiction detection focuses on comparing pairs of sentences or claims, but self-contradiction detection needs to further reason the semantics of an article and simultaneously learn the contradiction-aware comparison from all pairs of sentences. Therefore, we present the first model, Pairwise Contradiction Neural Network (PCNN), to not only effectively identify self-contradiction articles, but also highlight the most contradiction pairs of contradiction sentences. The main idea of PCNN is two-fold. First, to mitigate the effect of data scarcity on self-contradiction articles, we pre-train the module of pairwise contradiction learning using SNLI and MNLI benchmarks. Second, we select top-K sentence pairs with the highest contradiction probability values and model their correlation to determine whether the corresponding article belongs to self-contradiction. Experiments conducted on the proposed WikiContradiction dataset exhibit that PCNN can generate promising performance and comprehensively highlight the sentence pairs the contradiction locates.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08543v1-abstract-full').style.display = 'none'; document.getElementById('2111.08543v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at IEEE BigData 2021 (regular paper). Data and code can be access via: https://github.com/Wiki-Contradictory/Wiki-Self-Contradictory/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.03263">arXiv:2111.03263</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.03263">pdf</a>, <a href="https://arxiv.org/format/2111.03263">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning Based Near-Orthogonal Superposition Code for Short Message Transmission
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bian%2C+C">Chenghong Bian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Mingyu Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chin-Wei Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H">Hun-Seok Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.03263v2-abstract-short" style="display: inline;">
        Massive machine type communication (mMTC) has attracted new coding schemes optimized for reliable short message transmission. In this paper, a novel deep learning based near-orthogonal superposition (NOS) coding scheme is proposed for reliable transmission of short messages in the additive white Gaussian noise (AWGN) channel for mMTC applications. Similar to recent hyper-dimensional modulation (HD&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03263v2-abstract-full').style.display = 'inline'; document.getElementById('2111.03263v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.03263v2-abstract-full" style="display: none;">
        Massive machine type communication (mMTC) has attracted new coding schemes optimized for reliable short message transmission. In this paper, a novel deep learning based near-orthogonal superposition (NOS) coding scheme is proposed for reliable transmission of short messages in the additive white Gaussian noise (AWGN) channel for mMTC applications. Similar to recent hyper-dimensional modulation (HDM), the NOS encoder spreads the information bits to multiple near-orthogonal high dimensional vectors to be combined (superimposed) into a single vector for transmission. The NOS decoder first estimates the information vectors and then performs a cyclic redundancy check (CRC)-assisted K-best tree-search algorithm to further reduce the packet error rate. The proposed NOS encoder and decoder are deep neural networks (DNNs) jointly trained as an auto encoder and decoder pair to learn a new NOS coding scheme with near-orthogonal codewords. Simulation results show the proposed deep learning-based NOS scheme outperforms HDM and Polar code with CRC-aided list decoding for short(32-bit) message transmission.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03263v2-abstract-full').style.display = 'none'; document.getElementById('2111.03263v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.15122">arXiv:2110.15122</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.15122">pdf</a>, <a href="https://arxiv.org/format/2110.15122">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CAFE: Catastrophic Data Leakage in Vertical Federated Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+X">Xiao Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+P">Pin-Yu Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Yi Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+C">Chia-Mu Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+T">Tianyi Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.15122v4-abstract-short" style="display: inline;">
        Recent studies show that private training data can be leaked through the gradients sharing mechanism deployed in distributed machine learning systems, such as federated learning (FL). Increasing batch size to complicate data recovery is often viewed as a promising defense strategy against data leakage. In this paper, we revisit this defense premise and propose an advanced data leakage attack with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15122v4-abstract-full').style.display = 'inline'; document.getElementById('2110.15122v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.15122v4-abstract-full" style="display: none;">
        Recent studies show that private training data can be leaked through the gradients sharing mechanism deployed in distributed machine learning systems, such as federated learning (FL). Increasing batch size to complicate data recovery is often viewed as a promising defense strategy against data leakage. In this paper, we revisit this defense premise and propose an advanced data leakage attack with theoretical justification to efficiently recover batch data from the shared aggregated gradients. We name our proposed method as catastrophic data leakage in vertical federated learning (CAFE). Comparing to existing data leakage attacks, our extensive experimental results on vertical FL settings demonstrate the effectiveness of CAFE to perform large-batch data leakage attack with improved data recovery quality. We also propose a practical countermeasure to mitigate CAFE. Our results suggest that private data participated in standard FL, especially the vertical case, have a high risk of being leaked from the training gradients. Our analysis implies unprecedented and practical data leakage risks in those learning settings. The code of our work is available at https://github.com/DeRafael/CAFE.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15122v4-abstract-full').style.display = 'none'; document.getElementById('2110.15122v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.12501">arXiv:2110.12501</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.12501">pdf</a>, <a href="https://arxiv.org/format/2110.12501">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Abstractified Multi-instance Learning (AMIL) for Biomedical Relation Extraction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hogan%2C+W">William Hogan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+M">Molly Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Katsis%2C+Y">Yannis Katsis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Baldwin%2C+T">Tyler Baldwin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H">Ho-Cheol Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Baeza%2C+Y+V">Yoshiki Vazquez Baeza</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bartko%2C+A">Andrew Bartko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chun-Nan Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.12501v1-abstract-short" style="display: inline;">
        Relation extraction in the biomedical domain is a challenging task due to a lack of labeled data and a long-tail distribution of fact triples. Many works leverage distant supervision which automatically generates labeled data by pairing a knowledge graph with raw textual data. Distant supervision produces noisy labels and requires additional techniques, such as multi-instance learning (MIL), to de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.12501v1-abstract-full').style.display = 'inline'; document.getElementById('2110.12501v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.12501v1-abstract-full" style="display: none;">
        Relation extraction in the biomedical domain is a challenging task due to a lack of labeled data and a long-tail distribution of fact triples. Many works leverage distant supervision which automatically generates labeled data by pairing a knowledge graph with raw textual data. Distant supervision produces noisy labels and requires additional techniques, such as multi-instance learning (MIL), to denoise the training signal. However, MIL requires multiple instances of data and struggles with very long-tail datasets such as those found in the biomedical domain. In this work, we propose a novel reformulation of MIL for biomedical relation extraction that abstractifies biomedical entities into their corresponding semantic types. By grouping entities by types, we are better able to take advantage of the benefits of MIL and further denoise the training signal. We show this reformulation, which we refer to as abstractified multi-instance learning (AMIL), improves performance in biomedical relationship extraction. We also propose a novel relationship embedding architecture that further improves model performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.12501v1-abstract-full').style.display = 'none'; document.getElementById('2110.12501v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 3 figures, submitted to Automated Knowledge Base Construction (2021)</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          13
        

        

        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        3rd Conference on Automated Knowledge Base Construction (2021)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.10890">arXiv:2110.10890</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.10890">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mesoscale and Nanoscale Physics">cond-mat.mes-hall</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applied Physics">physics.app-ph</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/JXCDC.2022.3143130">10.1109/JXCDC.2022.3143130 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Physics-Based Models for Magneto-Electric Spin-Orbit Logic Circuits
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hai Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nikonov%2C+D+E">Dmitri E. Nikonov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+C">Chia-Ching Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Camsari%2C+K">Kerem Camsari</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+Y">Yu-Ching Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Sheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Naeemi%2C+A">Azad Naeemi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Young%2C+I+A">Ian A. Young</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.10890v1-abstract-short" style="display: inline;">
        Spintronic devices are a promising beyond-CMOS device option thanks to their energy efficiency and compatibility with CMOS. To accurately capture their multi-physics dynamics, a rigorous treatment of both spin and charge and their inter-conversion is required. Here we present physics-based device models based on 4x4 matrices for the spin-orbit coupling part of the magneto-electric spin-orbit (MESO&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.10890v1-abstract-full').style.display = 'inline'; document.getElementById('2110.10890v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.10890v1-abstract-full" style="display: none;">
        Spintronic devices are a promising beyond-CMOS device option thanks to their energy efficiency and compatibility with CMOS. To accurately capture their multi-physics dynamics, a rigorous treatment of both spin and charge and their inter-conversion is required. Here we present physics-based device models based on 4x4 matrices for the spin-orbit coupling part of the magneto-electric spin-orbit (MESO) device. Also, a more rigorous physics model of ferroelectric and magnetoelectric switching of ferromagnets, based on Landau-Lifshitz-Gilbert (LLG) and Landau-Khalatnikov (LK) equations, is presented. With the combined model implemented in a SPICE circuit simulator environment, simulation results were obtained which show feasibility of MESO implementation and functional operation of buffers, oscillators, and majority gates.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.10890v1-abstract-full').style.display = 'none'; document.getElementById('2110.10890v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 21 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Journal on Exploratory Solid-State Computational Devices and Circuits (2022)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.12242">arXiv:2109.12242</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.12242">pdf</a>, <a href="https://arxiv.org/format/2109.12242">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+A">An Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Z">Zexue He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+X">Xing Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+J">Jiang Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+E">Eric Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gentili%2C+A">Amilcare Gentili</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=McAuley%2C+J">Julian McAuley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chun-Nan Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.12242v1-abstract-short" style="display: inline;">
        Radiology report generation aims at generating descriptive text from radiology images automatically, which may present an opportunity to improve radiology reporting and interpretation. A typical setting consists of training encoder-decoder models on image-report pairs with a cross entropy loss, which struggles to generate informative sentences for clinical diagnoses since normal findings dominate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.12242v1-abstract-full').style.display = 'inline'; document.getElementById('2109.12242v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.12242v1-abstract-full" style="display: none;">
        Radiology report generation aims at generating descriptive text from radiology images automatically, which may present an opportunity to improve radiology reporting and interpretation. A typical setting consists of training encoder-decoder models on image-report pairs with a cross entropy loss, which struggles to generate informative sentences for clinical diagnoses since normal findings dominate the datasets. To tackle this challenge and encourage more clinically-accurate text outputs, we propose a novel weakly supervised contrastive loss for medical report generation. Experimental results demonstrate that our method benefits from contrasting target reports with incorrect but semantically-close ones. It outperforms previous work on both clinical correctness and text generation metrics for two public benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.12242v1-abstract-full').style.display = 'none'; document.getElementById('2109.12242v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Findings of EMNLP 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.06896">arXiv:2109.06896</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.06896">pdf</a>, <a href="https://arxiv.org/format/2109.06896">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decision-Focused Summarization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chao-Chun Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+C">Chenhao Tan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.06896v1-abstract-short" style="display: inline;">
        Relevance in summarization is typically defined based on textual information alone, without incorporating insights about a particular decision. As a result, to support risk analysis of pancreatic cancer, summaries of medical notes may include irrelevant information such as a knee injury. We propose a novel problem, decision-focused summarization, where the goal is to summarize relevant information&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.06896v1-abstract-full').style.display = 'inline'; document.getElementById('2109.06896v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.06896v1-abstract-full" style="display: none;">
        Relevance in summarization is typically defined based on textual information alone, without incorporating insights about a particular decision. As a result, to support risk analysis of pancreatic cancer, summaries of medical notes may include irrelevant information such as a knee injury. We propose a novel problem, decision-focused summarization, where the goal is to summarize relevant information for a decision. We leverage a predictive model that makes the decision based on the full text to provide valuable insights on how a decision can be inferred from text. To build a summary, we then select representative sentences that lead to similar model decisions as using the full text while accounting for textual non-redundancy. To evaluate our method (DecSum), we build a testbed where the task is to summarize the first ten reviews of a restaurant in support of predicting its future rating on Yelp. DecSum substantially outperforms text-only summarization methods and model-based explanation methods in decision faithfulness and representativeness. We further demonstrate that DecSum is the only method that enables humans to outperform random chance in predicting which restaurant will be better rated in the future.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.06896v1-abstract-full').style.display = 'none'; document.getElementById('2109.06896v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 10 figures, EMNLP 2021, code is available at https://github.com/ChicagoHAI/decsum</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.03329">arXiv:2109.03329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.03329">pdf</a>, <a href="https://arxiv.org/format/2109.03329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Real-World Adversarial Examples involving Makeup Application
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+C">Chang-Sheng Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Yi Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+P">Pin-Yu Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+C">Chia-Mu Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.03329v1-abstract-short" style="display: inline;">
        Deep neural networks have developed rapidly and have achieved outstanding performance in several tasks, such as image classification and natural language processing. However, recent studies have indicated that both digital and physical adversarial examples can fool neural networks. Face-recognition systems are used in various applications that involve security threats from physical adversarial exa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.03329v1-abstract-full').style.display = 'inline'; document.getElementById('2109.03329v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.03329v1-abstract-full" style="display: none;">
        Deep neural networks have developed rapidly and have achieved outstanding performance in several tasks, such as image classification and natural language processing. However, recent studies have indicated that both digital and physical adversarial examples can fool neural networks. Face-recognition systems are used in various applications that involve security threats from physical adversarial examples. Herein, we propose a physical adversarial attack with the use of full-face makeup. The presence of makeup on the human face is a reasonable possibility, which possibly increases the imperceptibility of attacks. In our attack framework, we combine the cycle-adversarial generative network (cycle-GAN) and a victimized classifier. The Cycle-GAN is used to generate adversarial makeup, and the architecture of the victimized classifier is VGG 16. Our experimental results show that our attack can effectively overcome manual errors in makeup application, such as color and position-related errors. We also demonstrate that the approaches used to train the models can influence physical attacks; the adversarial perturbations crafted from the pre-trained model are affected by the corresponding training data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.03329v1-abstract-full').style.display = 'none'; document.getElementById('2109.03329v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.13760">arXiv:2107.13760</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.13760">pdf</a>, <a href="https://arxiv.org/format/2107.13760">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Viewpoint-Invariant Exercise Repetition Counting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y+C">Yu Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Q">Qingpeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsougenis%2C+E">Efstratios Tsougenis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsui%2C+K">Kwok-Leung Tsui</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.13760v1-abstract-short" style="display: inline;">
        Counting the repetition of human exercise and physical rehabilitation is a common task in rehabilitation and exercise training. The existing vision-based repetition counting methods less emphasize the concurrent motions in the same video. This work presents a vision-based human motion repetition counting applicable to counting concurrent motions through the skeleton location extracted from various&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.13760v1-abstract-full').style.display = 'inline'; document.getElementById('2107.13760v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.13760v1-abstract-full" style="display: none;">
        Counting the repetition of human exercise and physical rehabilitation is a common task in rehabilitation and exercise training. The existing vision-based repetition counting methods less emphasize the concurrent motions in the same video. This work presents a vision-based human motion repetition counting applicable to counting concurrent motions through the skeleton location extracted from various pose estimation methods. The presented method was validated on the University of Idaho Physical Rehabilitation Movements Data Set (UI-PRMD), and MM-fit dataset. The overall mean absolute error (MAE) for mm-fit was 0.06 with off-by-one Accuracy (OBOA) 0.94. Overall MAE for UI-PRMD dataset was 0.06 with OBOA 0.95. We have also tested the performance in a variety of camera locations and concurrent motions with conveniently collected video with overall MAE 0.06 and OBOA 0.88. The proposed method provides a view-angle and motion agnostic concurrent motion counting. This method can potentially use in large-scale remote rehabilitation and exercise training with only one camera.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.13760v1-abstract-full').style.display = 'none'; document.getElementById('2107.13760v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.05334">arXiv:2107.05334</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.05334">pdf</a>, <a href="https://arxiv.org/format/2107.05334">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual Transformer with Statistical Test for COVID-19 Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chih-Chung Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+G">Guan-Lin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+M">Mei-Hsuan Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.05334v1-abstract-short" style="display: inline;">
        With the massive damage in the world caused by Coronavirus Disease 2019 SARS-CoV-2 (COVID-19), many related research topics have been proposed in the past two years. The Chest Computed Tomography (CT) scans are the most valuable materials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19 classification of Chest CT scan is based on a single-slice level, implying that the most cr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.05334v1-abstract-full').style.display = 'inline'; document.getElementById('2107.05334v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.05334v1-abstract-full" style="display: none;">
        With the massive damage in the world caused by Coronavirus Disease 2019 SARS-CoV-2 (COVID-19), many related research topics have been proposed in the past two years. The Chest Computed Tomography (CT) scans are the most valuable materials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19 classification of Chest CT scan is based on a single-slice level, implying that the most critical CT slice should be selected from the original CT scan volume manually. We simultaneously propose 2-D and 3-D models to predict the COVID-19 of CT scan to tickle this issue. In our 2-D model, we introduce the Deep Wilcoxon signed-rank test (DWCC) to determine the importance of each slice of a CT scan to overcome the issue mentioned previously. Furthermore, a Convolutional CT scan-Aware Transformer (CCAT) is proposed to discover the context of the slices fully. The frame-level feature is extracted from each CT slice based on any backbone network and followed by feeding the features to our within-slice-Transformer (WST) to discover the context information in the pixel dimension. The proposed Between-Slice-Transformer (BST) is used to aggregate the extracted spatial-context features of every CT slice. A simple classifier is then used to judge whether the Spatio-temporal features are COVID-19 or non-COVID-19. The extensive experiments demonstrated that the proposed CCAT and DWCC significantly outperform the state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.05334v1-abstract-full').style.display = 'none'; document.getElementById('2107.05334v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">this is a draft for MIA-Competition/ICCV2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.11589">arXiv:2106.11589</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.11589">pdf</a>, <a href="https://arxiv.org/format/2106.11589">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Part-Aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+H">Hau Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Jia-Hong Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+Y">Yao-Chih Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Ching-Hsien Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jia-Da Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Chu-Song Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.11589v1-abstract-short" style="display: inline;">
        This paper introduces an approach for multi-human 3D pose estimation and tracking based on calibrated multi-view. The main challenge lies in finding the cross-view and temporal correspondences correctly even when several human pose estimations are noisy. Compare to previous solutions that construct 3D poses from multiple views, our approach takes advantage of temporal consistency to match the 2D p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11589v1-abstract-full').style.display = 'inline'; document.getElementById('2106.11589v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.11589v1-abstract-full" style="display: none;">
        This paper introduces an approach for multi-human 3D pose estimation and tracking based on calibrated multi-view. The main challenge lies in finding the cross-view and temporal correspondences correctly even when several human pose estimations are noisy. Compare to previous solutions that construct 3D poses from multiple views, our approach takes advantage of temporal consistency to match the 2D poses estimated with previously constructed 3D skeletons in every view. Therefore cross-view and temporal associations are accomplished simultaneously. Since the performance suffers from mistaken association and noisy predictions, we design two strategies for aiming better correspondences and 3D reconstruction. Specifically, we propose a part-aware measurement for 2D-3D association and a filter that can cope with 2D outliers during reconstruction. Our approach is efficient and effective comparing to state-of-the-art methods; it achieves competitive results on two benchmarks: 96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus evaluation frames to be more challenging and our proposal also reach well-performed result.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.11589v1-abstract-full').style.display = 'none'; document.getElementById('2106.11589v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages with supplementary material; accepted to CVPR 2021 B-AMFG Workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.10859">arXiv:2106.10859</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.10859">pdf</a>, <a href="https://arxiv.org/format/2106.10859">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single Panorama
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Ching-Yu Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+C">Cheng Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Hwann-Tzong Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.10859v1-abstract-short" style="display: inline;">
        We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equ&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10859v1-abstract-full').style.display = 'inline'; document.getElementById('2106.10859v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.10859v1-abstract-full" style="display: none;">
        We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first method to the application of parallax-enabled novel panoramic view synthesis. Recent works for novel view synthesis focus on perspective images with limited field-of-view and require sufficient pictures captured in a specific condition. Conversely, OmniNeRF can generate panorama images for unknown viewpoints given a single equirectangular image as training data. To this end, we propose to augment the single RGB-D panorama by projecting back and forth between a 3D world and different 2D panoramic coordinates at different virtual camera positions. By doing so, we are able to optimize an Omnidirectional Neural Radiance Field with visible pixels collecting from omnidirectional viewing angles at a fixed center for the estimation of new viewing angles from varying camera positions. As a result, the proposed OmniNeRF achieves convincing renderings of novel panoramic views that exhibit the parallax effect. We showcase the effectiveness of each of our proposals on both synthetic and real-world datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10859v1-abstract-full').style.display = 'none'; document.getElementById('2106.10859v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.00955">arXiv:2106.00955</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.00955">pdf</a>, <a href="https://arxiv.org/ps/2106.00955">ps</a>, <a href="https://arxiv.org/format/2106.00955">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Answer Generation for Retrieval-based Question Answering Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chao-Chun Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lind%2C+E">Eric Lind</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Soldaini%2C+L">Luca Soldaini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moschitti%2C+A">Alessandro Moschitti</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.00955v1-abstract-short" style="display: inline;">
        Recent advancements in transformer-based models have greatly improved the ability of Question Answering (QA) systems to provide correct answers; in particular, answer sentence selection (AS2) models, core components of retrieval-based systems, have achieved impressive results. While generally effective, these models fail to provide a satisfying answer when all retrieved candidates are of poor qual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00955v1-abstract-full').style.display = 'inline'; document.getElementById('2106.00955v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.00955v1-abstract-full" style="display: none;">
        Recent advancements in transformer-based models have greatly improved the ability of Question Answering (QA) systems to provide correct answers; in particular, answer sentence selection (AS2) models, core components of retrieval-based systems, have achieved impressive results. While generally effective, these models fail to provide a satisfying answer when all retrieved candidates are of poor quality, even if they contain correct information. In AS2, models are trained to select the best answer sentence among a set of candidates retrieved for a given question. In this work, we propose to generate answers from a set of AS2 top candidates. Rather than selecting the best candidate, we train a sequence to sequence transformer model to generate an answer from a candidate set. Our tests on three English AS2 datasets show improvement up to 32 absolute points in accuracy over the state of the art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.00955v1-abstract-full').style.display = 'none'; document.getElementById('2106.00955v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Short paper, Accepted at Findings of ACL 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.13599">arXiv:2105.13599</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.13599">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/SMC52423.2021.9658607">10.1109/SMC52423.2021.9658607 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Short-Term Stock Price-Trend Prediction Using Meta-Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+S">Shin-Hung Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Cheng-Wen Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hsing-Ying Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+W">Wei-Sheng Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ho%2C+J">Jan-Ming Ho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.13599v2-abstract-short" style="display: inline;">
        Although conventional machine learning algorithms have been widely adopted for stock-price predictions in recent years, the massive volume of specific labeled data required are not always available. In contrast, meta-learning technology uses relatively small amounts of training data, called fast learners. Such methods are beneficial under conditions of limited data availability, which often obtain&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.13599v2-abstract-full').style.display = 'inline'; document.getElementById('2105.13599v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.13599v2-abstract-full" style="display: none;">
        Although conventional machine learning algorithms have been widely adopted for stock-price predictions in recent years, the massive volume of specific labeled data required are not always available. In contrast, meta-learning technology uses relatively small amounts of training data, called fast learners. Such methods are beneficial under conditions of limited data availability, which often obtain for trend prediction based on time-series data limited by sparse information. In this study, we consider short-term stock price prediction using a meta-learning framework with several convolutional neural networks, including the temporal convolution network, fully convolutional network, and residual neural network. We propose a sliding time horizon to label stocks according to their predicted price trends, referred to as called slope-detection labeling, using prediction labels including &#34;rise plus,&#34; &#34;rise,&#34; &#34;fall,&#34; and &#34;fall plus&#34;. The effectiveness of the proposed meta-learning framework was evaluated by application to the S&amp;P500. The experimental results show that the inclusion of the proposed meta-learning framework significantly improved both regular and balanced prediction accuracy and profitability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.13599v2-abstract-full').style.display = 'none'; document.getElementById('2105.13599v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">¬©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.07809">arXiv:2105.07809</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.07809">pdf</a>, <a href="https://arxiv.org/format/2105.07809">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learned Smartphone ISP on Mobile NPUs with Deep Learning, Mobile AI 2021 Challenge: Report
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ignatov%2C+A">Andrey Ignatov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chiang%2C+C">Cheng-Ming Chiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kuo%2C+H">Hsien-Kai Kuo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sycheva%2C+A">Anastasia Sycheva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M">Min-Hung Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+M">Man-Yu Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+Y">Yu-Syuan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tseng%2C+Y">Yu Tseng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+S">Shusong Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+J">Jin Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Chao-Hung Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsyu%2C+M">Ming-Chun Hsyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsai%2C+W">Wen-Chia Tsai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Chao-Wei Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Malivenko%2C+G">Grigory Malivenko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kwon%2C+M">Minsu Kwon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+M">Myungje Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yoo%2C+J">Jaeyoon Yoo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+C">Changbeom Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Shinjo Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shaolong%2C+Z">Zheng Shaolong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dejun%2C+H">Hao Dejun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fen%2C+X">Xie Fen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuang%2C+F">Feng Zhuang</a>
      , et al. (16 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.07809v1-abstract-short" style="display: inline;">
        As the quality of mobile cameras starts to play a crucial role in modern smartphones, more and more attention is now being paid to ISP algorithms used to improve various perceptual aspects of mobile photos. In this Mobile AI challenge, the target was to develop an end-to-end deep learning-based image signal processing (ISP) pipeline that can replace classical hand-crafted ISPs and achieve nearly r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07809v1-abstract-full').style.display = 'inline'; document.getElementById('2105.07809v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.07809v1-abstract-full" style="display: none;">
        As the quality of mobile cameras starts to play a crucial role in modern smartphones, more and more attention is now being paid to ISP algorithms used to improve various perceptual aspects of mobile photos. In this Mobile AI challenge, the target was to develop an end-to-end deep learning-based image signal processing (ISP) pipeline that can replace classical hand-crafted ISPs and achieve nearly real-time performance on smartphone NPUs. For this, the participants were provided with a novel learned ISP dataset consisting of RAW-RGB image pairs captured with the Sony IMX586 Quad Bayer mobile sensor and a professional 102-megapixel medium format camera. The runtime of all models was evaluated on the MediaTek Dimensity 1000+ platform with a dedicated AI processing unit capable of accelerating both floating-point and quantized neural networks. The proposed solutions are fully compatible with the above NPU and are capable of processing Full HD photos under 60-100 milliseconds while achieving high fidelity results. A detailed description of all models developed in this challenge is provided in this paper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07809v1-abstract-full').style.display = 'none'; document.getElementById('2105.07809v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Mobile AI 2021 Workshop and Challenges: https://ai-benchmark.com/workshops/mai/2021/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.06950">arXiv:2105.06950</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.06950">pdf</a>, <a href="https://arxiv.org/format/2105.06950">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Plot and Rework: Modeling Storylines for Visual Storytelling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chi-Yang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+Y">Yun-Wei Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T+%27">Ting-Hao &#39;Kenneth&#39; Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ku%2C+L">Lun-Wei Ku</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.06950v3-abstract-short" style="display: inline;">
        Writing a coherent and engaging story is not easy. Creative writers use their knowledge and worldview to put disjointed elements together to form a coherent storyline, and work and rework iteratively toward perfection. Automated visual storytelling (VIST) models, however, make poor use of external knowledge and iterative generation when attempting to create stories. This paper introduces PR-VIST,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.06950v3-abstract-full').style.display = 'inline'; document.getElementById('2105.06950v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.06950v3-abstract-full" style="display: none;">
        Writing a coherent and engaging story is not easy. Creative writers use their knowledge and worldview to put disjointed elements together to form a coherent storyline, and work and rework iteratively toward perfection. Automated visual storytelling (VIST) models, however, make poor use of external knowledge and iterative generation when attempting to create stories. This paper introduces PR-VIST, a framework that represents the input image sequence as a story graph in which it finds the best path to form a storyline. PR-VIST then takes this path and learns to generate the final story via an iterative training process. This framework produces stories that are superior in terms of diversity, coherence, and humanness, per both automatic and human evaluations. An ablation study shows that both plotting and reworking contribute to the model&#39;s superiority.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.06950v3-abstract-full').style.display = 'none'; document.getElementById('2105.06950v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, ACL-IJCNLP 2021 Findings</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.10299">arXiv:2104.10299</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.10299">pdf</a>, <a href="https://arxiv.org/format/2104.10299">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Cho-Ying Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+K">Ke Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chin-Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Neumann%2C+U">Ulrich Neumann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.10299v1-abstract-short" style="display: inline;">
        This work focuses on the analysis that whether 3D face models can be learned from only the speech inputs of speakers. Previous works for cross-modal face synthesis study image generation from voices. However, image synthesis includes variations such as hairstyles, backgrounds, and facial textures, that are arguably irrelevant to voice or without direct studies to show correlations. We instead inve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10299v1-abstract-full').style.display = 'inline'; document.getElementById('2104.10299v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.10299v1-abstract-full" style="display: none;">
        This work focuses on the analysis that whether 3D face models can be learned from only the speech inputs of speakers. Previous works for cross-modal face synthesis study image generation from voices. However, image synthesis includes variations such as hairstyles, backgrounds, and facial textures, that are arguably irrelevant to voice or without direct studies to show correlations. We instead investigate the ability to reconstruct 3D faces to concentrate on only geometry, which is more physiologically grounded. We propose both the supervised learning and unsupervised learning frameworks. Especially we demonstrate how unsupervised learning is possible in the absence of a direct voice-to-3D-face dataset under limited availability of 3D face scans when the model is equipped with knowledge distillation. To evaluate the performance, we also propose several metrics to measure the geometric fitness of two 3D faces based on points, lines, and regions. We find that 3D face shapes can be reconstructed from voices. Experimental results suggest that 3D faces can be reconstructed from voices, and our method can improve the performance over the baseline. The best performance gains (15% - 20%) on ear-to-ear distance ratio metric (ER) coincides with the intuition that one can roughly envision whether a speaker&#39;s face is overall wider or thinner only from a person&#39;s voice. See our project page for codes and data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10299v1-abstract-full').style.display = 'none'; document.getElementById('2104.10299v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://choyingw.github.io/works/Voice2Mesh/index.html</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02066">arXiv:2104.02066</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02066">pdf</a>, <a href="https://arxiv.org/format/2104.02066">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dopamine Transporter SPECT Image Classification for Neurodegenerative Parkinsonism via Diffusion Maps and Machine Learning Classifiers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+J">Jun-En Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+C">Chi-Hsiang Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+M+L">Mong-Na Lo Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chien-Ching Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02066v2-abstract-short" style="display: inline;">
        Neurodegenerative parkinsonism can be assessed by dopamine transporter single photon emission computed tomography (DaT-SPECT). Although generating images is time consuming, these images can show interobserver variability and they have been visually interpreted by nuclear medicine physicians to date. Accordingly, this study aims to provide an automatic and robust method based on Diffusion Maps and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02066v2-abstract-full').style.display = 'inline'; document.getElementById('2104.02066v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02066v2-abstract-full" style="display: none;">
        Neurodegenerative parkinsonism can be assessed by dopamine transporter single photon emission computed tomography (DaT-SPECT). Although generating images is time consuming, these images can show interobserver variability and they have been visually interpreted by nuclear medicine physicians to date. Accordingly, this study aims to provide an automatic and robust method based on Diffusion Maps and machine learning classifiers to classify the SPECT images into two types, namely Normal and Abnormal DaT-SPECT image groups. In the proposed method, the 3D images of N patients are mapped to an N by N pairwise distance matrix and are visualized in Diffusion Maps coordinates. The images of the training set are embedded into a low-dimensional space by using diffusion maps. Moreover, we use Nystr√∂m&#39;s out-of-sample extension, which embeds new sample points as the testing set in the reduced space. Testing samples in the embedded space are then classified into two types through the ensemble classifier with Linear Discriminant Analysis (LDA) and voting procedure through twenty-five-fold cross-validation results. The feasibility of the method is demonstrated via Parkinsonism Progression Markers Initiative (PPMI) dataset of 1097 subjects and a clinical cohort from Kaohsiung Chang Gung Memorial Hospital (KCGMH-TW) of 630 patients. We compare performances using Diffusion Maps with those of three alternative manifold methods for dimension reduction, namely Locally Linear Embedding (LLE), Isomorphic Mapping Algorithm (Isomap), and Kernel Principal Component Analysis (Kernel PCA). We also compare results using 2D and 3D CNN methods. The diffusion maps method has an average accuracy of 98% for the PPMI and 90% for the KCGMH-TW dataset with twenty-five fold cross-validation results. It outperforms the other three methods concerning the overall accuracy and the robustness in the training and testing samples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02066v2-abstract-full').style.display = 'none'; document.getElementById('2104.02066v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        24th Annual Conference, MIUA 2021, Oxford, UK, July 12-14, 2021, Proceedings
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.08176">arXiv:2103.08176</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.08176">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Biological Physics">physics.bio-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1177/0278364921989372">10.1177/0278364921989372 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Shape-induced obstacle attraction and repulsion during dynamic locomotion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+Y">Yuanfeng Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Othayoth%2C+R">Ratan Othayoth</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yulong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chun-Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Obert%2C+R+d+l+T">Rafael de la Tijera Obert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Francois%2C+E">Evains Francois</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chen Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.08176v1-abstract-short" style="display: inline;">
        Robots still struggle to dynamically traverse complex 3-D terrain with many large obstacles, an ability required for many critical applications. Body-obstacle interaction is often inevitable and induces perturbation and uncertainty in motion that challenges closed-form dynamic modeling. Here, inspired by recent discovery of a terradynamic streamlined shape, we studied how two body shapes interacti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.08176v1-abstract-full').style.display = 'inline'; document.getElementById('2103.08176v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.08176v1-abstract-full" style="display: none;">
        Robots still struggle to dynamically traverse complex 3-D terrain with many large obstacles, an ability required for many critical applications. Body-obstacle interaction is often inevitable and induces perturbation and uncertainty in motion that challenges closed-form dynamic modeling. Here, inspired by recent discovery of a terradynamic streamlined shape, we studied how two body shapes interacting with obstacles affect turning and pitching motions of an open-loop multi-legged robot and cockroaches during dynamic locomotion. With a common cuboidal body, the robot was attracted towards obstacles, resulting in pitching up and flipping-over. By contrast, with an elliptical body, the robot was repelled by obstacles and readily traversed. The animal displayed qualitatively similar turning and pitching motions induced by these two body shapes. However, unlike the cuboidal robot, the cuboidal animal was capable of escaping obstacle attraction and subsequent high pitching and flipping over, which inspired us to develop an empirical pitch-and-turn strategy for cuboidal robots. Considering the similarity of our self-propelled body-obstacle interaction with part-feeder interaction in robotic part manipulation, we developed a quasi-static potential energy landscape model to explain the dependence of dynamic locomotion on body shape. Our experimental and modeling results also demonstrated that obstacle attraction or repulsion is an inherent property of locomotor body shape and insensitive to obstacle geometry and size. Our study expanded the concept and usefulness of terradynamic shapes for passive control of robot locomotion to traverse large obstacles using physical interaction. Our study is also a step in establishing an energy landscape approach to locomotor transitions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.08176v1-abstract-full').style.display = 'none'; document.getElementById('2103.08176v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Journal of Robotics Research (2021)</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        International Journal of Robotics Research (2021), 40 (6-7), 939-956
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.08057">arXiv:2103.08057</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.08057">pdf</a>, <a href="https://arxiv.org/format/2103.08057">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RecSim NG: Toward Principled Uncertainty Modeling for Recommender Ecosystems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mladenov%2C+M">Martin Mladenov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chih-Wei Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jain%2C+V">Vihan Jain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ie%2C+E">Eugene Ie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Colby%2C+C">Christopher Colby</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mayoraz%2C+N">Nicolas Mayoraz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pham%2C+H">Hubert Pham</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tran%2C+D">Dustin Tran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vendrov%2C+I">Ivan Vendrov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boutilier%2C+C">Craig Boutilier</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.08057v1-abstract-short" style="display: inline;">
        The development of recommender systems that optimize multi-turn interaction with users, and model the interactions of different agents (e.g., users, content providers, vendors) in the recommender ecosystem have drawn increasing attention in recent years. Developing and training models and algorithms for such recommenders can be especially difficult using static datasets, which often fail to offer&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.08057v1-abstract-full').style.display = 'inline'; document.getElementById('2103.08057v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.08057v1-abstract-full" style="display: none;">
        The development of recommender systems that optimize multi-turn interaction with users, and model the interactions of different agents (e.g., users, content providers, vendors) in the recommender ecosystem have drawn increasing attention in recent years. Developing and training models and algorithms for such recommenders can be especially difficult using static datasets, which often fail to offer the types of counterfactual predictions needed to evaluate policies over extended horizons. To address this, we develop RecSim NG, a probabilistic platform for the simulation of multi-agent recommender systems. RecSim NG is a scalable, modular, differentiable simulator implemented in Edward2 and TensorFlow. It offers: a powerful, general probabilistic programming language for agent-behavior specification; tools for probabilistic inference and latent-variable model learning, backed by automatic differentiation and tracing; and a TensorFlow-based runtime for running simulations on accelerated hardware. We describe RecSim NG and illustrate how it can be used to create transparent, configurable, end-to-end models of a recommender ecosystem, complemented by a small set of simple use cases that demonstrate how RecSim NG can help both researchers and practitioners easily develop and train novel algorithms for recommender systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.08057v1-abstract-full').style.display = 'none'; document.getElementById('2103.08057v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.07833">arXiv:2103.07833</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.07833">pdf</a>, <a href="https://arxiv.org/format/2103.07833">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A `Sourceful&#39; Twist: Emoji Prediction Based on Sentiment, Hashtags and Application Source
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Venkit%2C+P">Pranav Venkit</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Karishma%2C+Z">Zeba Karishma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chi-Yang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Katiki%2C+R">Rahul Katiki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+K">Kenneth Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wilson%2C+S">Shomir Wilson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dudas%2C+P">Patrick Dudas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.07833v1-abstract-short" style="display: inline;">
        We widely use emojis in social networking to heighten, mitigate or negate the sentiment of the text. Emoji suggestions already exist in many cross-platform applications but an emoji is predicted solely based a few prominent words instead of understanding the subject and substance of the text. Through this paper, we showcase the importance of using Twitter features to help the model understand the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.07833v1-abstract-full').style.display = 'inline'; document.getElementById('2103.07833v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.07833v1-abstract-full" style="display: none;">
        We widely use emojis in social networking to heighten, mitigate or negate the sentiment of the text. Emoji suggestions already exist in many cross-platform applications but an emoji is predicted solely based a few prominent words instead of understanding the subject and substance of the text. Through this paper, we showcase the importance of using Twitter features to help the model understand the sentiment involved and hence to predict the most suitable emoji for the text. Hashtags and Application Sources like Android, etc. are two features which we found to be important yet underused in emoji prediction and Twitter sentiment analysis on the whole. To approach this shortcoming and to further understand emoji behavioral patterns, we propose a more balanced dataset by crawling additional Twitter data, including timestamp, hashtags, and application source acting as additional attributes to the tweet. Our data analysis and neural network model performance evaluations depict that using hashtags and application sources as features allows to encode different information and is effective in emoji prediction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.07833v1-abstract-full').style.display = 'none'; document.getElementById('2103.07833v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.04009">arXiv:2103.04009</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.04009">pdf</a>, <a href="https://arxiv.org/ps/2103.04009">ps</a>, <a href="https://arxiv.org/format/2103.04009">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning from Counting: Leveraging Temporal Classification for Weakly Supervised Object Localization and Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Yu Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wenwen Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.04009v1-abstract-short" style="display: inline;">
        This paper reports a new solution of leveraging temporal classification to support weakly supervised object detection (WSOD). Specifically, we introduce raster scan-order techniques to serialize 2D images into 1D sequence data, and then leverage a combined LSTM (Long, Short-Term Memory) and CTC (Connectionist Temporal Classification) network to achieve object localization based on a total count (o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.04009v1-abstract-full').style.display = 'inline'; document.getElementById('2103.04009v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.04009v1-abstract-full" style="display: none;">
        This paper reports a new solution of leveraging temporal classification to support weakly supervised object detection (WSOD). Specifically, we introduce raster scan-order techniques to serialize 2D images into 1D sequence data, and then leverage a combined LSTM (Long, Short-Term Memory) and CTC (Connectionist Temporal Classification) network to achieve object localization based on a total count (of interested objects). We term our proposed network LSTM-CCTC (Count-based CTC). This &#34;learning from counting&#34; strategy differs from existing WSOD methods in that our approach automatically identifies critical points on or near a target object. This strategy significantly reduces the need of generating a large number of candidate proposals for object localization. Experiments show that our method yields state-of-the-art performance based on an evaluation on PASCAL VOC datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.04009v1-abstract-full').style.display = 'none'; document.getElementById('2103.04009v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">31st British Machine Vision Conference (BMVC), oral presentation</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.02200">arXiv:2103.02200</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.02200">pdf</a>, <a href="https://arxiv.org/format/2103.02200">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Formalizing Generalization and Robustness of Neural Networks to Weight Perturbations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tsai%2C+Y">Yu-Lin Tsai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Yi Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+C">Chia-Mu Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+P">Pin-Yu Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.02200v2-abstract-short" style="display: inline;">
        Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forw&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.02200v2-abstract-full').style.display = 'inline'; document.getElementById('2103.02200v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.02200v2-abstract-full" style="display: none;">
        Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forward neural networks in terms of the robustness in pairwise class margin and its generalization behavior under weight perturbation. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.02200v2-abstract-full').style.display = 'none'; document.getElementById('2103.02200v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This version has been accepted for poster presentation at NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.01895">arXiv:2103.01895</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.01895">pdf</a>, <a href="https://arxiv.org/format/2103.01895">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Examples can be Effective Data Augmentation for Unsupervised Machine Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Yi Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+P">Pin-Yu Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+S">Songtao Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Sijia Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+C">Chia-Mu Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.01895v3-abstract-short" style="display: inline;">
        Adversarial examples causing evasive predictions are widely used to evaluate and improve the robustness of machine learning models. However, current studies focus on supervised learning tasks, relying on the ground-truth data label, a targeted objective, or supervision from a trained classifier. In this paper, we propose a framework of generating adversarial examples for unsupervised models and de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01895v3-abstract-full').style.display = 'inline'; document.getElementById('2103.01895v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.01895v3-abstract-full" style="display: none;">
        Adversarial examples causing evasive predictions are widely used to evaluate and improve the robustness of machine learning models. However, current studies focus on supervised learning tasks, relying on the ground-truth data label, a targeted objective, or supervision from a trained classifier. In this paper, we propose a framework of generating adversarial examples for unsupervised models and demonstrate novel applications to data augmentation. Our framework exploits a mutual information neural estimator as an information-theoretic similarity measure to generate adversarial examples without supervision. We propose a new MinMax algorithm with provable convergence guarantees for efficient generation of unsupervised adversarial examples. Our framework can also be extended to supervised adversarial examples. When using unsupervised adversarial examples as a simple plug-in data augmentation tool for model retraining, significant improvements are consistently observed across different unsupervised tasks and datasets, including data reconstruction, representation learning, and contrastive learning. Our results show novel methods and considerable advantages in studying and improving unsupervised machine learning via adversarial examples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01895v3-abstract-full').style.display = 'none'; document.getElementById('2103.01895v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This version has been accepted by AAAI-22</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.11935">arXiv:2102.11935</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.11935">pdf</a>, <a href="https://arxiv.org/format/2102.11935">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Non-Singular Adversarial Robustness of Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tsai%2C+Y">Yu-Lin Tsai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chia-Yi Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+C">Chia-Mu Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+P">Pin-Yu Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.11935v1-abstract-short" style="display: inline;">
        Adversarial robustness has become an emerging challenge for neural network owing to its over-sensitivity to small input perturbations. While being critical, we argue that solving this singular issue alone fails to provide a comprehensive robustness assessment. Even worse, the conclusions drawn from singular robustness may give a false sense of overall model robustness. Specifically, our findings s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11935v1-abstract-full').style.display = 'inline'; document.getElementById('2102.11935v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.11935v1-abstract-full" style="display: none;">
        Adversarial robustness has become an emerging challenge for neural network owing to its over-sensitivity to small input perturbations. While being critical, we argue that solving this singular issue alone fails to provide a comprehensive robustness assessment. Even worse, the conclusions drawn from singular robustness may give a false sense of overall model robustness. Specifically, our findings show that adversarially trained models that are robust to input perturbations are still (or even more) vulnerable to weight perturbations when compared to standard models. In this paper, we formalize the notion of non-singular adversarial robustness for neural networks through the lens of joint perturbations to data inputs as well as model weights. To our best knowledge, this study is the first work considering simultaneous input-weight adversarial perturbations. Based on a multi-layer feed-forward neural network model with ReLU activation functions and standard classification loss, we establish error analysis for quantifying the loss sensitivity subject to $\ell_\infty$-norm bounded perturbations on data inputs and model weights. Based on the error analysis, we propose novel regularization functions for robust training and demonstrate improved non-singular robustness against joint input-weight adversarial perturbations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11935v1-abstract-full').style.display = 'none'; document.getElementById('2102.11935v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ICASSP 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.06129">arXiv:2102.06129</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.06129">pdf</a>, <a href="https://arxiv.org/format/2102.06129">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Meta-Thompson Sampling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kveton%2C+B">Branislav Kveton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Konobeev%2C+M">Mikhail Konobeev</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaheer%2C+M">Manzil Zaheer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chih-wei Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mladenov%2C+M">Martin Mladenov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boutilier%2C+C">Craig Boutilier</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Szepesvari%2C+C">Csaba Szepesvari</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.06129v2-abstract-short" style="display: inline;">
        Efficient exploration in bandits is a fundamental online learning problem. We propose a variant of Thompson sampling that learns to explore better as it interacts with bandit instances drawn from an unknown prior. The algorithm meta-learns the prior and thus we call it MetaTS. We propose several efficient implementations of MetaTS and analyze it in Gaussian bandits. Our analysis shows the benefit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.06129v2-abstract-full').style.display = 'inline'; document.getElementById('2102.06129v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.06129v2-abstract-full" style="display: none;">
        Efficient exploration in bandits is a fundamental online learning problem. We propose a variant of Thompson sampling that learns to explore better as it interacts with bandit instances drawn from an unknown prior. The algorithm meta-learns the prior and thus we call it MetaTS. We propose several efficient implementations of MetaTS and analyze it in Gaussian bandits. Our analysis shows the benefit of meta-learning and is of a broader interest, because we derive a novel prior-dependent Bayes regret bound for Thompson sampling. Our theory is complemented by empirical evaluation, which shows that MetaTS quickly adapts to the unknown prior.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.06129v2-abstract-full').style.display = 'none'; document.getElementById('2102.06129v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 38th International Conference on Machine Learning</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.05449">arXiv:2102.05449</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.05449">pdf</a>, <a href="https://arxiv.org/format/2102.05449">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive Processor Frequency Adjustment for Mobile Edge Computing with Intermittent Energy Supply
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T">Tiansheng Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+W">Weiwei Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+X">Xiaobin Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiumin Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Q">Qingbo Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+R">Rui Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Ching-Hsien Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zomaya%2C+A+Y">Albert Y. Zomaya</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.05449v3-abstract-short" style="display: inline;">
        With astonishing speed, bandwidth, and scale, Mobile Edge Computing (MEC) has played an increasingly important role in the next generation of connectivity and service delivery. Yet, along with the massive deployment of MEC servers, the ensuing energy issue is now on an increasingly urgent agenda. In the current context, the large scale deployment of renewable-energy-supplied MEC servers is perhaps&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.05449v3-abstract-full').style.display = 'inline'; document.getElementById('2102.05449v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.05449v3-abstract-full" style="display: none;">
        With astonishing speed, bandwidth, and scale, Mobile Edge Computing (MEC) has played an increasingly important role in the next generation of connectivity and service delivery. Yet, along with the massive deployment of MEC servers, the ensuing energy issue is now on an increasingly urgent agenda. In the current context, the large scale deployment of renewable-energy-supplied MEC servers is perhaps the most promising solution for the incoming energy issue. Nonetheless, as a result of the intermittent nature of their power sources, these special design MEC server must be more cautious about their energy usage, in a bid to maintain their service sustainability as well as service standard. Targeting optimization on a single-server MEC scenario, we in this paper propose NAFA, an adaptive processor frequency adjustment solution, to enable an effective plan of the server&#39;s energy usage. By learning from the historical data revealing request arrival and energy harvest pattern, the deep reinforcement learning-based solution is capable of making intelligent schedules on the server&#39;s processor frequency, so as to strike a good balance between service sustainability and service quality. The superior performance of NAFA is substantiated by real-data-based experiments, wherein NAFA demonstrates up to 20% increase in average request acceptance ratio and up to 50% reduction in average request processing time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.05449v3-abstract-full').style.display = 'none'; document.getElementById('2102.05449v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.12457">arXiv:2101.12457</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.12457">pdf</a>, <a href="https://arxiv.org/format/2101.12457">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RetaGNN: Relational Temporal Attentive Graph Neural Networks for Holistic Sequential Recommendation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cheng-Te Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.12457v1-abstract-short" style="display: inline;">
        Sequential recommendation (SR) is to accurately recommend a list of items for a user based on her current accessed ones. While new-coming users continuously arrive in the real world, one crucial task is to have inductive SR that can produce embeddings of users and items without re-training. Given user-item interactions can be extremely sparse, another critical task is to have transferable SR that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.12457v1-abstract-full').style.display = 'inline'; document.getElementById('2101.12457v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.12457v1-abstract-full" style="display: none;">
        Sequential recommendation (SR) is to accurately recommend a list of items for a user based on her current accessed ones. While new-coming users continuously arrive in the real world, one crucial task is to have inductive SR that can produce embeddings of users and items without re-training. Given user-item interactions can be extremely sparse, another critical task is to have transferable SR that can transfer the knowledge derived from one domain with rich data to another domain. In this work, we aim to present the holistic SR that simultaneously accommodates conventional, inductive, and transferable settings. We propose a novel deep learning-based model, Relational Temporal Attentive Graph Neural Networks (RetaGNN), for holistic SR. The main idea of RetaGNN is three-fold. First, to have inductive and transferable capabilities, we train a relational attentive GNN on the local subgraph extracted from a user-item pair, in which the learnable weight matrices are on various relations among users, items, and attributes, rather than nodes or edges. Second, long-term and short-term temporal patterns of user preferences are encoded by a proposed sequential self-attention mechanism. Third, a relation-aware regularization term is devised for better training of RetaGNN. Experiments conducted on MovieLens, Instagram, and Book-Crossing datasets exhibit that RetaGNN can outperform state-of-the-art methods under conventional, inductive, and transferable settings. The derived attention weights also bring model explainability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.12457v1-abstract-full').style.display = 'none'; document.getElementById('2101.12457v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to The Web Conference (WWW) 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.03525">arXiv:2101.03525</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.03525">pdf</a>, <a href="https://arxiv.org/format/2101.03525">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/LRA.2021.3062011">10.1109/LRA.2021.3062011 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-Modal Contrastive Learning of Representations for Navigation using Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+J">Jui-Te Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+C">Chen-Lung Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+P">Po-Kai Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+C">Ching-I Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chao-Chun Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ewe%2C+Z+L">Zu Lin Ewe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+P">Po-Jui Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hsueh-Cheng Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.03525v1-abstract-short" style="display: inline;">
        Deep reinforcement learning (RL), where the agent learns from mistakes, has been successfully applied to a variety of tasks. With the aim of learning collision-free policies for unmanned vehicles, deep RL has been used for training with various types of data, such as colored images, depth images, and LiDAR point clouds, without the use of classic map--localize--plan approaches. However, existing m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.03525v1-abstract-full').style.display = 'inline'; document.getElementById('2101.03525v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.03525v1-abstract-full" style="display: none;">
        Deep reinforcement learning (RL), where the agent learns from mistakes, has been successfully applied to a variety of tasks. With the aim of learning collision-free policies for unmanned vehicles, deep RL has been used for training with various types of data, such as colored images, depth images, and LiDAR point clouds, without the use of classic map--localize--plan approaches. However, existing methods are limited by their reliance on cameras and LiDAR devices, which have degraded sensing under adverse environmental conditions (e.g., smoky environments). In response, we propose the use of single-chip millimeter-wave (mmWave) radar, which is lightweight and inexpensive, for learning-based autonomous navigation. However, because mmWave radar signals are often noisy and sparse, we propose a cross-modal contrastive learning for representation (CM-CLR) method that maximizes the agreement between mmWave radar data and LiDAR data in the training stage. We evaluated our method in real-world robot compared with 1) a method with two separate networks using cross-modal generative reconstruction and an RL policy and 2) a baseline RL policy without cross-modal representation. Our proposed end-to-end deep RL policy with contrastive learning successfully navigated the robot through smoke-filled maze environments and achieved better performance compared with generative reconstruction methods, in which noisy artifact walls or obstacles were produced. All pretrained models and hardware settings are open access for reproducing this study and can be obtained at https://arg-nctu.github.io/projects/deeprl-mmWave.html
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.03525v1-abstract-full').style.display = 'none'; document.getElementById('2101.03525v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">For further details, please visit https://arg-nctu.github.io/projects/deeprl-mmWave.html</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Robotics and Automation Letters, 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.08055">arXiv:2011.08055</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.08055">pdf</a>, <a href="https://arxiv.org/format/2011.08055">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scalable Reinforcement Learning Policies for Multi-Agent Control
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C+D">Christopher D. Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jeong%2C+H">Heejin Jeong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pappas%2C+G+J">George J. Pappas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chaudhari%2C+P">Pratik Chaudhari</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.08055v4-abstract-short" style="display: inline;">
        We develop a Multi-Agent Reinforcement Learning (MARL) method to learn scalable control policies for target tracking. Our method can handle an arbitrary number of pursuers and targets; we show results for tasks consisting up to 1000 pursuers tracking 1000 targets. We use a decentralized, partially-observable Markov Decision Process framework to model pursuers as agents receiving partial observatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.08055v4-abstract-full').style.display = 'inline'; document.getElementById('2011.08055v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.08055v4-abstract-full" style="display: none;">
        We develop a Multi-Agent Reinforcement Learning (MARL) method to learn scalable control policies for target tracking. Our method can handle an arbitrary number of pursuers and targets; we show results for tasks consisting up to 1000 pursuers tracking 1000 targets. We use a decentralized, partially-observable Markov Decision Process framework to model pursuers as agents receiving partial observations (range and bearing) about targets which move using fixed, unknown policies. An attention mechanism is used to parameterize the value function of the agents; this mechanism allows us to handle an arbitrary number of targets. Entropy-regularized off-policy RL methods are used to train a stochastic policy, and we discuss how it enables a hedging behavior between pursuers that leads to a weak form of cooperation in spite of completely decentralized control execution. We further develop a masking heuristic that allows training on smaller problems with few pursuers-targets and execution on much larger problems. Thorough simulation experiments, ablation studies, and comparisons to state of the art algorithms are performed to study the scalability of the approach and robustness of performance to varying numbers of agents and targets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.08055v4-abstract-full').style.display = 'none'; document.getElementById('2011.08055v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 10 figures, contributed paper at IROS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.06174">arXiv:2011.06174</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Theoretical Knowledge Graph Reasoning via Ending Anchored Rules
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Canlin Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Katsis%2C+Y">Yannis Katsis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vazquez-Baeza%2C+Y">Yoshiki Vazquez-Baeza</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bartko%2C+A">Andrew Bartko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+H">Ho-Cheol Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chun-Nan Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.06174v6-abstract-short" style="display: inline;">
        Discovering precise and specific rules from knowledge graphs is regarded as an essential challenge, which can improve the performances of many downstream tasks and even provide new ways to approach some Natural Language Processing research topics. In this paper, we provide a fundamental theory for knowledge graph reasoning based on the ending anchored rules. Our theory provides precise reasons exp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.06174v6-abstract-full').style.display = 'inline'; document.getElementById('2011.06174v6-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.06174v6-abstract-full" style="display: none;">
        Discovering precise and specific rules from knowledge graphs is regarded as an essential challenge, which can improve the performances of many downstream tasks and even provide new ways to approach some Natural Language Processing research topics. In this paper, we provide a fundamental theory for knowledge graph reasoning based on the ending anchored rules. Our theory provides precise reasons explaining why or why not a triple is correct. Then, we implement our theory by what we call the EARDict model. Results show that our EARDict model significantly outperforms all the benchmark models on three large datasets of knowledge graph completion. Especially, our model achieves a Hits@10 score of 96.6 percent on WN18RR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.06174v6-abstract-full').style.display = 'none'; document.getElementById('2011.06174v6-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">We made a mistake when ranking the test triples: Due to the rule-based nature, many triples r(s,t) have zero occurring probabilities. But since we always append the true test triple at the beginning, the true test triple is always ranked at the top. Due to this fact, our performance is severely over-estimated. As a result, we withdraw this paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.03574">arXiv:2010.03574</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.03574">pdf</a>, <a href="https://arxiv.org/format/2010.03574">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Characterizing the Value of Information in Medical Notes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chao-Chun Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Karnwal%2C+S">Shantanu Karnwal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mullainathan%2C+S">Sendhil Mullainathan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Obermeyer%2C+Z">Ziad Obermeyer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+C">Chenhao Tan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.03574v2-abstract-short" style="display: inline;">
        Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmission prediction and in-hospital mortality prediction, to characterize the value of information in medical notes. We show that as a whole, medical note&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.03574v2-abstract-full').style.display = 'inline'; document.getElementById('2010.03574v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.03574v2-abstract-full" style="display: none;">
        Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmission prediction and in-hospital mortality prediction, to characterize the value of information in medical notes. We show that as a whole, medical notes only provide additional predictive power over structured information in readmission prediction. We further propose a probing framework to select parts of notes that enable more accurate predictions than using all notes, despite that the selected information leads to a distribution shift from the training data (&#34;all notes&#34;). Finally, we demonstrate that models trained on the selected valuable information achieve even better predictive performance, with only 6.8% of all the tokens for readmission prediction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.03574v2-abstract-full').style.display = 'none'; document.getElementById('2010.03574v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 December, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 12 figures, Findings of EMNLP 2020, code is available at https://github.com/BoulderDS/value-of-medical-notes</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.02467">arXiv:2010.02467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.02467">pdf</a>, <a href="https://arxiv.org/format/2010.02467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ni%2C+J">Jianmo Ni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chun-Nan Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gentili%2C+A">Amilcare Gentili</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=McAuley%2C+J">Julian McAuley</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.02467v1-abstract-short" style="display: inline;">
        Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists&#39; workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g.~label imbalance) and face common issues inherent in text generation models (e.g.~repetition). In this work, we focus&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02467v1-abstract-full').style.display = 'inline'; document.getElementById('2010.02467v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.02467v1-abstract-full" style="display: none;">
        Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists&#39; workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g.~label imbalance) and face common issues inherent in text generation models (e.g.~repetition). In this work, we focus on reporting abnormal findings on radiology images; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02467v1-abstract-full').style.display = 'none'; document.getElementById('2010.02467v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 2 figures, to be published in Findings of EMNLP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.10897">arXiv:2009.10897</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.10897">pdf</a>, <a href="https://arxiv.org/format/2009.10897">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Revisiting Design Choices in Proximal Policy Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C+C">Chloe Ching-Yun Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mendler-D%C3%BCnner%2C+C">Celestine Mendler-D√ºnner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hardt%2C+M">Moritz Hardt</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.10897v1-abstract-short" style="display: inline;">
        Proximal Policy Optimization (PPO) is a popular deep policy gradient algorithm. In standard implementations, PPO regularizes policy updates with clipped probability ratios, and parameterizes policies with either continuous Gaussian distributions or discrete Softmax distributions. These design choices are widely accepted, and motivated by empirical performance comparisons on MuJoCo and Atari benchm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.10897v1-abstract-full').style.display = 'inline'; document.getElementById('2009.10897v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.10897v1-abstract-full" style="display: none;">
        Proximal Policy Optimization (PPO) is a popular deep policy gradient algorithm. In standard implementations, PPO regularizes policy updates with clipped probability ratios, and parameterizes policies with either continuous Gaussian distributions or discrete Softmax distributions. These design choices are widely accepted, and motivated by empirical performance comparisons on MuJoCo and Atari benchmarks.
  We revisit these practices outside the regime of current benchmarks, and expose three failure modes of standard PPO. We explain why standard design choices are problematic in these cases, and show that alternative choices of surrogate objectives and policy parameterizations can prevent the failure modes. We hope that our work serves as a reminder that many algorithmic design choices in reinforcement learning are tied to specific simulation environments. We should not implicitly accept these choices as a standard part of a more general algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.10897v1-abstract-full').style.display = 'none'; document.getElementById('2009.10897v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.08574">arXiv:2008.08574</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.08574">pdf</a>, <a href="https://arxiv.org/format/2008.08574">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Every Pixel Matters: Center-aware Feature Alignment for Domain Adaptive Object Detector
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Cheng-Chun Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsai%2C+Y">Yi-Hsuan Tsai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yen-Yu Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.08574v1-abstract-short" style="display: inline;">
        A domain adaptive object detector aims to adapt itself to unseen domains that may contain variations of object appearance, viewpoints or backgrounds. Most existing methods adopt feature alignment either on the image level or instance level. However, image-level alignment on global features may tangle foreground/background pixels at the same time, while instance-level alignment using proposals may&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.08574v1-abstract-full').style.display = 'inline'; document.getElementById('2008.08574v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.08574v1-abstract-full" style="display: none;">
        A domain adaptive object detector aims to adapt itself to unseen domains that may contain variations of object appearance, viewpoints or backgrounds. Most existing methods adopt feature alignment either on the image level or instance level. However, image-level alignment on global features may tangle foreground/background pixels at the same time, while instance-level alignment using proposals may suffer from the background noise. Different from existing solutions, we propose a domain adaptation framework that accounts for each pixel via predicting pixel-wise objectness and centerness. Specifically, the proposed method carries out center-aware alignment by paying more attention to foreground pixels, hence achieving better adaptation across domains. We demonstrate our method on numerous adaptation settings with extensive experimental results and show favorable performance against existing state-of-the-art algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.08574v1-abstract-full').style.display = 'none'; document.getElementById('2008.08574v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in ECCV&#39;20. Project page: https://chengchunhsu.github.io/EveryPixelMatters/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.07685">arXiv:2008.07685</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.07685">pdf</a>, <a href="https://arxiv.org/format/2008.07685">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.csl.2021.101199">10.1016/j.csl.2021.101199 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attack and Defense Strategies for Deep Speaker Recognition Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jati%2C+A">Arindam Jati</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chin-Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pal%2C+M">Monisankha Pal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peri%2C+R">Raghuveer Peri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=AbdAlmageed%2C+W">Wael AbdAlmageed</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Narayanan%2C+S">Shrikanth Narayanan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.07685v1-abstract-short" style="display: inline;">
        Robust speaker recognition, including in the presence of malicious attacks, is becoming increasingly important and essential, especially due to the proliferation of several smart speakers and personal agents that interact with an individual&#39;s voice commands to perform diverse, and even sensitive tasks. Adversarial attack is a recently revived domain which is shown to be effective in breaking deep&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07685v1-abstract-full').style.display = 'inline'; document.getElementById('2008.07685v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.07685v1-abstract-full" style="display: none;">
        Robust speaker recognition, including in the presence of malicious attacks, is becoming increasingly important and essential, especially due to the proliferation of several smart speakers and personal agents that interact with an individual&#39;s voice commands to perform diverse, and even sensitive tasks. Adversarial attack is a recently revived domain which is shown to be effective in breaking deep neural network-based classifiers, specifically, by forcing them to change their posterior distribution by only perturbing the input samples by a very small amount. Although, significant progress in this realm has been made in the computer vision domain, advances within speaker recognition is still limited. The present expository paper considers several state-of-the-art adversarial attacks to a deep speaker recognition system, employing strong defense methods as countermeasures, and reporting on several ablation studies to obtain a comprehensive understanding of the problem. The experiments show that the speaker recognition systems are vulnerable to adversarial attacks, and the strongest attacks can reduce the accuracy of the system from 94% to even 0%. The study also compares the performances of the employed defense methods in detail, and finds adversarial training based on Projected Gradient Descent (PGD) to be the best defense method in our setting. We hope that the experiments presented in this paper provide baselines that can be useful for the research community interested in further studying adversarial robustness of speaker recognition systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07685v1-abstract-full').style.display = 'none'; document.getElementById('2008.07685v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.05838">arXiv:2008.05838</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.05838">pdf</a>, <a href="https://arxiv.org/format/2008.05838">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.3390/s21062154">10.3390/s21062154 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Interpretable Detection of Partial Discharge in Power Lines with Deep Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Michau%2C+G">Gabriel Michau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chi-Ching Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fink%2C+O">Olga Fink</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.05838v3-abstract-short" style="display: inline;">
        Partial discharge (PD) is a common indication of faults in power systems, such as generators, and cables. These PD can eventually result in costly repairs and substantial power outages. PD detection traditionally relies on hand-crafted features and domain expertise to identify very specific pulses in the electrical current, and the performance declines in the presence of noise or of superposed pul&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.05838v3-abstract-full').style.display = 'inline'; document.getElementById('2008.05838v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.05838v3-abstract-full" style="display: none;">
        Partial discharge (PD) is a common indication of faults in power systems, such as generators, and cables. These PD can eventually result in costly repairs and substantial power outages. PD detection traditionally relies on hand-crafted features and domain expertise to identify very specific pulses in the electrical current, and the performance declines in the presence of noise or of superposed pulses. In this paper, we propose a novel end-to-end framework based on convolutional neural networks. The framework has two contributions. First, it does not require any feature extraction and enables robust PD detection. Second, we devise the pulse activation map. It provides interpretability of the results for the domain experts with the identification of the pulses that led to the detection of the PDs. The performance is evaluated on a public dataset for the detection of damaged power lines. An ablation study demonstrates the benefits of each part of the proposed framework.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.05838v3-abstract-full').style.display = 'none'; document.getElementById('2008.05838v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 4 figures, 2 tables</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          Sensors 2021, no. 6: 2154
        

        

        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Sensors 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.01937">arXiv:2008.01937</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.01937">pdf</a>, <a href="https://arxiv.org/format/2008.01937">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Biomolecules">q-bio.BM</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1371/journal.pcbi.1008967">10.1371/journal.pcbi.1008967 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Antibody Watch: Text Mining Antibody Specificity from the Literature
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chun-Nan Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+C">Chia-Hui Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Poopradubsil%2C+T">Thamolwan Poopradubsil</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lo%2C+A">Amanda Lo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=William%2C+K+A">Karen A. William</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+K">Ko-Wei Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bandrowski%2C+A">Anita Bandrowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ozyurt%2C+I+B">Ibrahim Burak Ozyurt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grethe%2C+J+S">Jeffrey S. Grethe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Martone%2C+M+E">Maryann E. Martone</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.01937v2-abstract-short" style="display: inline;">
        Antibodies are widely used reagents to test for expression of proteins and other antigens. However, they might not always reliably produce results when they do not specifically bind to the target proteins that their providers designed them for, leading to unreliable research results. While many proposals have been developed to deal with the problem of antibody specificity, it is still challenging&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.01937v2-abstract-full').style.display = 'inline'; document.getElementById('2008.01937v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.01937v2-abstract-full" style="display: none;">
        Antibodies are widely used reagents to test for expression of proteins and other antigens. However, they might not always reliably produce results when they do not specifically bind to the target proteins that their providers designed them for, leading to unreliable research results. While many proposals have been developed to deal with the problem of antibody specificity, it is still challenging to cover the millions of antibodies that are available to researchers. In this study, we investigate the feasibility of automatically generating alerts to users of problematic antibodies by extracting statements about antibody specificity reported in the literature. The extracted alerts can be used to construct an &#34;Antibody Watch&#34; knowledge base containing supporting statements of problematic antibodies. We developed a deep neural network system and tested its performance with a corpus of more than two thousand articles that reported uses of antibodies. We divided the problem into two tasks. Given an input article, the first task is to identify snippets about antibody specificity and classify if the snippets report that any antibody exhibits non-specificity, and thus is problematic. The second task is to link each of these snippets to one or more antibodies mentioned in the snippet. The experimental evaluation shows that our system can accurately perform both classification and linking tasks with weighted F-scores over 0.925 and 0.923, respectively, and 0.914 overall when combined to complete the joint task. We leveraged Research Resource Identifiers (RRID) to precisely identify antibodies linked to the extracted specificity snippets. The result shows that it is feasible to construct a reliable knowledge base about problematic antibodies by text mining.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.01937v2-abstract-full').style.display = 'none'; document.getElementById('2008.01937v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 1 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        PLOS Computational Biology, 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.08919">arXiv:2007.08919</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.08919">pdf</a>, <a href="https://arxiv.org/format/2007.08919">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Edge-Preserving Guided Semantic Segmentation for VIPriors Challenge
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chih-Chung Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+H">Hsin-Ti Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.08919v1-abstract-short" style="display: inline;">
        Semantic segmentation is one of the most attractive research fields in computer vision. In the VIPriors challenge, only very limited numbers of training samples are allowed, leading to that the current state-of-the-art and deep learning-based semantic segmentation techniques are hard to train well. To overcome this shortcoming, therefore, we propose edge-preserving guidance to obtain the extra pri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.08919v1-abstract-full').style.display = 'inline'; document.getElementById('2007.08919v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.08919v1-abstract-full" style="display: none;">
        Semantic segmentation is one of the most attractive research fields in computer vision. In the VIPriors challenge, only very limited numbers of training samples are allowed, leading to that the current state-of-the-art and deep learning-based semantic segmentation techniques are hard to train well. To overcome this shortcoming, therefore, we propose edge-preserving guidance to obtain the extra prior information, to avoid the overfitting under small-scale training dataset. First, a two-channeled convolutional layer is concatenated to the last layer of the conventional semantic segmentation network. Then, an edge map is calculated from the ground truth by Sobel operation and followed by concatenating a hard-thresholding operation to indicate whether the pixel is the edge or not. Then, the two-dimensional cross-entropy loss is adopted to calculate the loss between the predicted edge map and its ground truth, termed as an edge-preserving loss. In this way, the continuity of boundaries between different instances can be forced by the proposed edge-preserving loss. Experiments demonstrate that the proposed method can achieve excellent performance under small-scale training set, compared to state-of-the-art semantic segmentation techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.08919v1-abstract-full').style.display = 'none'; document.getElementById('2007.08919v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical report for VIPChallenge</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.05094">arXiv:2006.05094</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.05094">pdf</a>, <a href="https://arxiv.org/format/2006.05094">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Meta-Learning Bandit Policies by Gradient Ascent
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kveton%2C+B">Branislav Kveton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mladenov%2C+M">Martin Mladenov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Chih-Wei Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaheer%2C+M">Manzil Zaheer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Szepesvari%2C+C">Csaba Szepesvari</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boutilier%2C+C">Craig Boutilier</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.05094v2-abstract-short" style="display: inline;">
        Most bandit policies are designed to either minimize regret in any problem instance, making very few assumptions about the underlying environment, or in a Bayesian sense, assuming a prior distribution over environment parameters. The former are often too conservative in practical settings, while the latter require assumptions that are hard to verify in practice. We study bandit problems that fall&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.05094v2-abstract-full').style.display = 'inline'; document.getElementById('2006.05094v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.05094v2-abstract-full" style="display: none;">
        Most bandit policies are designed to either minimize regret in any problem instance, making very few assumptions about the underlying environment, or in a Bayesian sense, assuming a prior distribution over environment parameters. The former are often too conservative in practical settings, while the latter require assumptions that are hard to verify in practice. We study bandit problems that fall between these two extremes, where the learning agent has access to sampled bandit instances from an unknown prior distribution $\mathcal{P}$ and aims to achieve high reward on average over the bandit instances drawn from $\mathcal{P}$. This setting is of a particular importance because it lays foundations for meta-learning of bandit policies and reflects more realistic assumptions in many practical domains. We propose the use of parameterized bandit policies that are differentiable and can be optimized using policy gradients. This provides a broadly applicable framework that is easy to implement. We derive reward gradients that reflect the structure of bandit problems and policies, for both non-contextual and contextual settings, and propose a number of interesting policies that are both differentiable and have low regret. Our algorithmic and theoretical contributions are supported by extensive experiments that show the importance of baseline subtraction, learned biases, and the practicality of our approach on a range problems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.05094v2-abstract-full').style.display = 'none'; document.getElementById('2006.05094v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+C&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Hsu%2C+C&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Hsu%2C+C&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Hsu%2C+C&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>