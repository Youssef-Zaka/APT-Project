<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;41 of 41 results for author: <span class="mathjax">Shi, K</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Shi%2C+K">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Shi, K">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Shi%2C+K&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Shi, K">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.03758">arXiv:2204.03758</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.03758">pdf</a>, <a href="https://arxiv.org/format/2204.03758">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compositional Generalization and Decomposition in Neural Program Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+J">Joey Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaheer%2C+M">Manzil Zaheer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yin%2C+P">Pengcheng Yin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sutton%2C+C">Charles Sutton</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.03758v1-abstract-short" style="display: inline;">
        When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, what we can measure is whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03758v1-abstract-full').style.display = 'inline'; document.getElementById('2204.03758v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.03758v1-abstract-full" style="display: none;">
        When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, what we can measure is whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we focus on measuring the ability of learned program synthesizers to compositionally generalize. We first characterize several different axes along which program synthesis methods would be desired to generalize, e.g., length generalization, or the ability to combine known subroutines in new ways that do not occur in the training data. Based on this characterization, we introduce a benchmark suite of tasks to assess these abilities based on two popular existing datasets, SCAN and RobustFill. Finally, we make first attempts to improve the compositional generalization ability of Transformer models along these axes through novel attention mechanisms that draw inspiration from a human-like decomposition strategy. Empirically, we find our modified Transformer models generally perform better than natural baselines, but the tasks remain challenging.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03758v1-abstract-full').style.display = 'none'; document.getElementById('2204.03758v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at the Deep Learning for Code (DL4C) Workshop at ICLR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.02311">arXiv:2204.02311</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.02311">pdf</a>, <a href="https://arxiv.org/format/2204.02311">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PaLM: Scaling Language Modeling with Pathways
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Narang%2C+S">Sharan Narang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Devlin%2C+J">Jacob Devlin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bosma%2C+M">Maarten Bosma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mishra%2C+G">Gaurav Mishra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roberts%2C+A">Adam Roberts</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barham%2C+P">Paul Barham</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chung%2C+H+W">Hyung Won Chung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sutton%2C+C">Charles Sutton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gehrmann%2C+S">Sebastian Gehrmann</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schuh%2C+P">Parker Schuh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsvyashchenko%2C+S">Sasha Tsvyashchenko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Maynez%2C+J">Joshua Maynez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rao%2C+A">Abhishek Rao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barnes%2C+P">Parker Barnes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tay%2C+Y">Yi Tay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shazeer%2C+N">Noam Shazeer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Prabhakaran%2C+V">Vinodkumar Prabhakaran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reif%2C+E">Emily Reif</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+N">Nan Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hutchinson%2C+B">Ben Hutchinson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pope%2C+R">Reiner Pope</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bradbury%2C+J">James Bradbury</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Austin%2C+J">Jacob Austin</a>
      , et al. (42 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.02311v3-abstract-short" style="display: inline;">
        Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Tran&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.02311v3-abstract-full').style.display = 'inline'; document.getElementById('2204.02311v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.02311v3-abstract-full" style="display: none;">
        Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.02311v3-abstract-full').style.display = 'none'; document.getElementById('2204.02311v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10452">arXiv:2203.10452</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10452">pdf</a>, <a href="https://arxiv.org/format/2203.10452">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CrossBeam: Learning to Search in Bottom-Up Program Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+H">Hanjun Dai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ellis%2C+K">Kevin Ellis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sutton%2C+C">Charles Sutton</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10452v1-abstract-short" style="display: inline;">
        Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10452v1-abstract-full').style.display = 'inline'; document.getElementById('2203.10452v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10452v1-abstract-full" style="display: none;">
        Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CrossBeam, uses the neural model to choose how to combine previously-explored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CrossBeam is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CrossBeam in two very different domains, string manipulation and logic programming. We observe that CrossBeam learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10452v1-abstract-full').style.display = 'none'; document.getElementById('2203.10452v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at ICLR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.06358">arXiv:2202.06358</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.06358">pdf</a>, <a href="https://arxiv.org/format/2202.06358">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Diverse facial inpainting guided by exemplars
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+W">Wanglong Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Hanli Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+X">Xianta Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+X">Xiaogang Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+M">Min Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+J">Jiankai Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kaijie Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.06358v2-abstract-short" style="display: inline;">
        Facial image inpainting is a task of filling visually realistic and semantically meaningful contents for missing or masked pixels in a face image. Although existing methods have made significant progress in achieving high visual quality, the controllable diversity of facial image inpainting remains an open problem in this field. This paper introduces EXE-GAN, a novel diverse and interactive facial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.06358v2-abstract-full').style.display = 'inline'; document.getElementById('2202.06358v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.06358v2-abstract-full" style="display: none;">
        Facial image inpainting is a task of filling visually realistic and semantically meaningful contents for missing or masked pixels in a face image. Although existing methods have made significant progress in achieving high visual quality, the controllable diversity of facial image inpainting remains an open problem in this field. This paper introduces EXE-GAN, a novel diverse and interactive facial inpainting framework, which can not only preserve the high-quality visual effect of the whole image but also complete the face image with exemplar-like facial attributes. The proposed facial inpainting is achieved based on generative adversarial networks by leveraging the global style of input image, the stochastic style, and the exemplar style of exemplar image. A novel attribute similarity metric is introduced to encourage networks to learn the style of facial attributes from the exemplar in a self-supervised way. To guarantee the natural transition across the boundary of inpainted regions, a novel spatial variant gradient backpropagation technique is designed to adjust the loss gradients based on the spatial location. A variety of experimental results and comparisons on public CelebA-HQ and FFHQ datasets are presented to demonstrate the superiority of the proposed method in terms of both the quality and diversity in facial inpainting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.06358v2-abstract-full').style.display = 'none'; document.getElementById('2202.06358v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">There are 13 pages, 11 figures in this paper</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.4.9; I.4.5
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.07684">arXiv:2111.07684</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.07684">pdf</a>, <a href="https://arxiv.org/format/2111.07684">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AutoGMap: Learning to Map Large-scale Sparse Graphs on Memristive Crossbars
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+B">Bo Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Shengbo Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+S">Shiping Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kaibo Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yin Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T">Tingwen Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.07684v1-abstract-short" style="display: inline;">
        The sparse representation of graphs has shown its great potential for accelerating the computation of the graph applications (e.g. Social Networks, Knowledge Graphs) on traditional computing architectures (CPU, GPU, or TPU). But the exploration of the large-scale sparse graph computing on processing-in-memory (PIM) platforms (typically with memristive crossbars) is still in its infancy. As we look&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.07684v1-abstract-full').style.display = 'inline'; document.getElementById('2111.07684v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.07684v1-abstract-full" style="display: none;">
        The sparse representation of graphs has shown its great potential for accelerating the computation of the graph applications (e.g. Social Networks, Knowledge Graphs) on traditional computing architectures (CPU, GPU, or TPU). But the exploration of the large-scale sparse graph computing on processing-in-memory (PIM) platforms (typically with memristive crossbars) is still in its infancy. As we look to implement the computation or storage of large-scale or batch graphs on memristive crossbars, a natural assumption would be that we need a large-scale crossbar, but with low utilization. Some recent works have questioned this assumption to avoid the waste of the storage and computational resource by &#34;block partition&#34;, which is fixed-size, progressively scheduled, or coarse-grained, thus is not effectively sparsity-aware in our view. This work proposes the dynamic sparsity-aware mapping scheme generating method that models the problem as a sequential decision-making problem which is solved by reinforcement learning (RL) algorithm (REINFORCE). Our generating model (LSTM, combined with our dynamic-fill mechanism) generates remarkable mapping performance on a small-scale typical graph/matrix data (43% area of the original matrix with fully mapping), and two large-scale matrix data (22.5% area on qh882, and 17.1% area on qh1484). Moreover, our coding framework of the scheme is intuitive and has promising adaptability with the deployment or compilation system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.07684v1-abstract-full').style.display = 'none'; document.getElementById('2111.07684v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.03892">arXiv:2111.03892</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.03892">pdf</a>, <a href="https://arxiv.org/format/2111.03892">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TND-NAS: Towards Non-Differentiable Objectives in Differentiable Neural Architecture Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+B">Bo Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+S">Shiping Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+Z">Zheng Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kaibo Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+K">Ke Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T">Tingwen Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.03892v2-abstract-short" style="display: inline;">
        Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tack&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03892v2-abstract-full').style.display = 'inline'; document.getElementById('2111.03892v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.03892v2-abstract-full" style="display: none;">
        Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tackling the non-differentiable objectives, e.g., energy, resource-constrained efficiency, and other metrics, let alone the multi-objective search demands. Researches in the multi-objective NAS field target this but requires vast computational resources cause of the sole optimization of each candidate architecture. In light of this discrepancy, we propose the TND-NAS, which is with the merits of the high efficiency in differentiable NAS framework and the compatibility among non-differentiable metrics in Multi-objective NAS. Under the differentiable NAS framework, with the continuous relaxation of the search space, TND-NAS has the architecture parameters ($α$) been optimized in discrete space, while resorting to the progressive search space shrinking by $α$. Our representative experiment takes two objectives (Parameters, Accuracy) as an example, we achieve a series of high-performance compact architectures on CIFAR10 (1.09M/3.3\%, 2.4M/2.95\%, 9.57M/2.54\%) and CIFAR100 (2.46M/18.3\%, 5.46/16.73\%, 12.88/15.20\%) datasets. Favorably, compared with other multi-objective NAS methods, TND-NAS is less time-consuming (1.3 GPU-days on NVIDIA 1080Ti, 1/6 of that in NSGA-Net), and can be conveniently adapted to real-world NAS scenarios (resource-constrained, platform-specialized).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03892v2-abstract-full').style.display = 'none'; document.getElementById('2111.03892v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.04518">arXiv:2110.04518</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.04518">pdf</a>, <a href="https://arxiv.org/format/2110.04518">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhengyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Ke Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+N+F">Nancy F. Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.04518v1-abstract-short" style="display: inline;">
        Text discourse parsing weighs importantly in understanding information flow and argumentative structure in natural language, making it beneficial for downstream tasks. While previous work significantly improves the performance of RST discourse parsing, they are not readily applicable to practical use cases: (1) EDU segmentation is not integrated into most existing tree parsing frameworks, thus it&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.04518v1-abstract-full').style.display = 'inline'; document.getElementById('2110.04518v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.04518v1-abstract-full" style="display: none;">
        Text discourse parsing weighs importantly in understanding information flow and argumentative structure in natural language, making it beneficial for downstream tasks. While previous work significantly improves the performance of RST discourse parsing, they are not readily applicable to practical use cases: (1) EDU segmentation is not integrated into most existing tree parsing frameworks, thus it is not straightforward to apply such models on newly-coming data. (2) Most parsers cannot be used in multilingual scenarios, because they are developed only in English. (3) Parsers trained from single-domain treebanks do not generalize well on out-of-domain inputs. In this work, we propose a document-level multilingual RST discourse parsing framework, which conducts EDU segmentation and discourse tree parsing jointly. Moreover, we propose a cross-translation augmentation strategy to enable the framework to support multilingual parsing and improve its domain generality. Experimental results show that our model achieves state-of-the-art performance on document-level multilingual RST parsing in all sub-tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.04518v1-abstract-full').style.display = 'none'; document.getElementById('2110.04518v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in CODI@EMNLP 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.13989">arXiv:2108.13989</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.13989">pdf</a>, <a href="https://arxiv.org/format/2108.13989">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DeepTaskAPT: Insider APT detection using Task-tree based Deep Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mamun%2C+M">Mohammad Mamun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kevin Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.13989v1-abstract-short" style="display: inline;">
        APT, known as Advanced Persistent Threat, is a difficult challenge for cyber defence. These threats make many traditional defences ineffective as the vulnerabilities exploited by these threats are insiders who have access to and are within the network. This paper proposes DeepTaskAPT, a heterogeneous task-tree based deep learning method to construct a baseline model based on sequences of tasks usi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13989v1-abstract-full').style.display = 'inline'; document.getElementById('2108.13989v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.13989v1-abstract-full" style="display: none;">
        APT, known as Advanced Persistent Threat, is a difficult challenge for cyber defence. These threats make many traditional defences ineffective as the vulnerabilities exploited by these threats are insiders who have access to and are within the network. This paper proposes DeepTaskAPT, a heterogeneous task-tree based deep learning method to construct a baseline model based on sequences of tasks using a Long Short-Term Memory (LSTM) neural network that can be applied across different users to identify anomalous behaviour. Rather than applying the model to sequential log entries directly, as most current approaches do, DeepTaskAPT applies a process tree based task generation method to generate sequential log entries for the deep learning model. To assess the performance of DeepTaskAPT, we use a recently released synthetic dataset, DARPA Operationally Transparent Computing (OpTC) dataset and a real-world dataset, Los Alamos National Laboratory (LANL) dataset. Both of them are composed of host-based data collected from sensors. Our results show that DeepTaskAPT outperforms similar approaches e.g. DeepLog and the DeepTaskAPT baseline model demonstrate its capability to detect malicious traces in various attack scenarios while having high accuracy and low false-positive rates. To the best of knowledge this is the very first attempt of using recently introduced OpTC dataset for cyber threat detection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13989v1-abstract-full').style.display = 'none'; document.getElementById('2108.13989v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Conference: IEEE Trustcom 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.08441">arXiv:2108.08441</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.08441">pdf</a>, <a href="https://arxiv.org/format/2108.08441">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Chaos Engineering For Understanding Consensus Algorithms Performance in Permissioned Blockchains
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sondhi%2C+S">Shiv Sondhi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saad%2C+S">Sherif Saad</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kevin Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mamun%2C+M">Mohammad Mamun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Traore%2C+I">Issa Traore</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.08441v1-abstract-short" style="display: inline;">
        A critical component of any blockchain or distributed ledger technology (DLT) platform is the consensus algorithm. Blockchain consensus algorithms are the primary vehicle for the nodes within a blockchain network to reach an agreement. In recent years, many blockchain consensus algorithms have been proposed mainly for private and permissioned blockchain networks. However, the performance of these&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.08441v1-abstract-full').style.display = 'inline'; document.getElementById('2108.08441v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.08441v1-abstract-full" style="display: none;">
        A critical component of any blockchain or distributed ledger technology (DLT) platform is the consensus algorithm. Blockchain consensus algorithms are the primary vehicle for the nodes within a blockchain network to reach an agreement. In recent years, many blockchain consensus algorithms have been proposed mainly for private and permissioned blockchain networks. However, the performance of these algorithms and their reliability in hostile environments or the presence of byzantine and other network failures are not well understood. In addition, the testing and validation of blockchain applications come with many technical challenges. In this paper, we apply chaos engineering and testing to understand the performance of consensus algorithms in the presence of different loads, byzantine failure and other communication failure scenarios. We apply chaos engineering to evaluate the performance of three different consensus algorithms (PBFT, Clique, Raft) and their respective blockchain platforms. We measure the blockchain network&#39;s throughput, latency, and success rate while executing chaos and load tests. We develop lightweight blockchain applications to execute our test in a semi-production environment. Our results show that using chaos engineering helps understand how different consensus algorithms perform in a hostile or unreliable environment and the limitations of blockchain platforms. Our work demonstrates the benefits of using chaos engineering in testing complex distributed systems such as blockchain networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.08441v1-abstract-full').style.display = 'none'; document.getElementById('2108.08441v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">to appear in The 19th IEEE International Conference on Dependable, Autonomic &amp; Secure Computing (DASC 2021)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.03675">arXiv:2107.03675</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.03675">pdf</a>, <a href="https://arxiv.org/format/2107.03675">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Huayun Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Ke Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+N+F">Nancy F. Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.03675v1-abstract-short" style="display: inline;">
        Speech evaluation is an essential component in computer-assisted language learning (CALL). While speech evaluation on English has been popular, automatic speech scoring on low resource languages remains challenging. Work in this area has focused on monolingual specific designs and handcrafted features stemming from resource-rich languages like English. Such approaches are often difficult to genera&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03675v1-abstract-full').style.display = 'inline'; document.getElementById('2107.03675v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.03675v1-abstract-full" style="display: none;">
        Speech evaluation is an essential component in computer-assisted language learning (CALL). While speech evaluation on English has been popular, automatic speech scoring on low resource languages remains challenging. Work in this area has focused on monolingual specific designs and handcrafted features stemming from resource-rich languages like English. Such approaches are often difficult to generalize to other languages, especially if we also want to consider suprasegmental qualities such as rhythm. In this work, we examine three different languages that possess distinct rhythm patterns: English (stress-timed), Malay (syllable-timed), and Tamil (mora-timed). We exploit robust feature representations inspired by music processing and vector representation learning. Empirical validations show consistent gains for all three languages when predicting pronunciation, rhythm and intonation performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03675v1-abstract-full').style.display = 'none'; document.getElementById('2107.03675v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at INTERSPEECH 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.03216">arXiv:2107.03216</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.03216">pdf</a>, <a href="https://arxiv.org/format/2107.03216">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+H">Haiwei Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+S">Shuning He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+K">Kejia Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qu%2C+B">Bo Qu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Chunling Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kun Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.03216v1-abstract-short" style="display: inline;">
        Medical Visual Question Answering (VQA) is a multi-modal challenging task widely considered by research communities of the computer vision and natural language processing. Since most current medical VQA models focus on visual content, ignoring the importance of text, this paper proposes a multi-view attention-based model(MuVAM) for medical visual question answering which integrates the high-level&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03216v1-abstract-full').style.display = 'inline'; document.getElementById('2107.03216v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.03216v1-abstract-full" style="display: none;">
        Medical Visual Question Answering (VQA) is a multi-modal challenging task widely considered by research communities of the computer vision and natural language processing. Since most current medical VQA models focus on visual content, ignoring the importance of text, this paper proposes a multi-view attention-based model(MuVAM) for medical visual question answering which integrates the high-level semantics of medical images on the basis of text description. Firstly, different methods are utilized to extract the features of the image and the question for the two modalities of vision and text. Secondly, this paper proposes a multi-view attention mechanism that include Image-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view attention can correlate the question with image and word in order to better analyze the question and get an accurate answer. Thirdly, a composite loss is presented to predict the answer accurately after multi-modal feature fusion and improve the similarity between visual and textual cross-modal features. It consists of classification loss and image-question complementary (IQC) loss. Finally, for data errors and missing labels in the VQA-RAD dataset, we collaborate with medical experts to correct and complete this dataset and then construct an enhanced dataset, VQA-RADPh. The experiments on these two datasets show that the effectiveness of MuVAM surpasses the state-of-the-art method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03216v1-abstract-full').style.display = 'none'; document.getElementById('2107.03216v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.08556">arXiv:2106.08556</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.08556">pdf</a>, <a href="https://arxiv.org/format/2106.08556">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Coreference-Aware Dialogue Summarization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhengyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Ke Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+N+F">Nancy F. Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.08556v2-abstract-short" style="display: inline;">
        Summarizing conversations via neural approaches has been gaining research traction lately, yet it is still challenging to obtain practical solutions. Examples of such challenges include unstructured information exchange in dialogues, informal interactions between speakers, and dynamic role changes of speakers as the dialogue evolves. Many of such challenges result in complex coreference links. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08556v2-abstract-full').style.display = 'inline'; document.getElementById('2106.08556v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.08556v2-abstract-full" style="display: none;">
        Summarizing conversations via neural approaches has been gaining research traction lately, yet it is still challenging to obtain practical solutions. Examples of such challenges include unstructured information exchange in dialogues, informal interactions between speakers, and dynamic role changes of speakers as the dialogue evolves. Many of such challenges result in complex coreference links. Therefore, in this work, we investigate different approaches to explicitly incorporate coreference information in neural abstractive dialogue summarization models to tackle the aforementioned challenges. Experimental results show that the proposed approaches achieve state-of-the-art performance, implying it is useful to utilize coreference information in dialogue summarization. Evaluation results on factual correctness suggest such coreference-aware models are better at tracing the information flow among interlocutors and associating accurate status/actions with the corresponding interlocutors and person mentions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08556v2-abstract-full').style.display = 'none'; document.getElementById('2106.08556v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for presentation at SIGDIAL-2021. Version2: add BART-Large results/fix typos</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.09386">arXiv:2105.09386</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.09386">pdf</a>, <a href="https://arxiv.org/format/2105.09386">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Surprisingly Popular Voting Recovers Rankings, Surprisingly!
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hosseini%2C+H">Hadi Hosseini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mandal%2C+D">Debmalya Mandal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kevin Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.09386v1-abstract-short" style="display: inline;">
        The wisdom of the crowd has long become the de facto approach for eliciting information from individuals or experts in order to predict the ground truth. However, classical democratic approaches for aggregating individual \emph{votes} only work when the opinion of the majority of the crowd is relatively accurate. A clever recent approach, \emph{surprisingly popular voting}, elicits additional info&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.09386v1-abstract-full').style.display = 'inline'; document.getElementById('2105.09386v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.09386v1-abstract-full" style="display: none;">
        The wisdom of the crowd has long become the de facto approach for eliciting information from individuals or experts in order to predict the ground truth. However, classical democratic approaches for aggregating individual \emph{votes} only work when the opinion of the majority of the crowd is relatively accurate. A clever recent approach, \emph{surprisingly popular voting}, elicits additional information from the individuals, namely their \emph{prediction} of other individuals&#39; votes, and provably recovers the ground truth even when experts are in minority. This approach works well when the goal is to pick the correct option from a small list, but when the goal is to recover a true ranking of the alternatives, a direct application of the approach requires eliciting too much information. We explore practical techniques for extending the surprisingly popular algorithm to ranked voting by partial votes and predictions and designing robust aggregation rules. We experimentally demonstrate that even a little prediction information helps surprisingly popular voting outperform classical approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.09386v1-abstract-full').style.display = 'none'; document.getElementById('2105.09386v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Forthcoming at IJCAI-2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.10781">arXiv:2104.10781</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.10781">pdf</a>, <a href="https://arxiv.org/format/2104.10781">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NTIRE 2021 Challenge on Quality Enhancement of Compressed Video: Methods and Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+R">Ren Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jing Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+Y">Yi Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinjian Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+M">Minyi Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+S">Shuigeng Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chan%2C+K+C+K">Kelvin C. K. Chan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+S">Shangchen Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+X">Xiangyu Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Loy%2C+C+C">Chen Change Loy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+F">Fanglong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+H">He Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+L">Lielin Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Q">Qi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+D">Dongliang He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+F">Fu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dang%2C+Q">Qingqing Dang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yibin Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Maggioni%2C+M">Matteo Maggioni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+Z">Zhongqian Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+S">Shuai Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=li%2C+C">Cheng li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tanay%2C+T">Thomas Tanay</a>
      , et al. (47 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.10781v5-abstract-short" style="display: inline;">
        This paper reviews the first NTIRE challenge on quality enhancement of compressed video, with a focus on the proposed methods and results. In this challenge, the new Large-scale Diverse Video (LDV) dataset is employed. The challenge has three tracks. Tracks 1 and 2 aim at enhancing the videos compressed by HEVC at a fixed QP, while Track 3 is designed for enhancing the videos compressed by x265 at&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10781v5-abstract-full').style.display = 'inline'; document.getElementById('2104.10781v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.10781v5-abstract-full" style="display: none;">
        This paper reviews the first NTIRE challenge on quality enhancement of compressed video, with a focus on the proposed methods and results. In this challenge, the new Large-scale Diverse Video (LDV) dataset is employed. The challenge has three tracks. Tracks 1 and 2 aim at enhancing the videos compressed by HEVC at a fixed QP, while Track 3 is designed for enhancing the videos compressed by x265 at a fixed bit-rate. Besides, the quality enhancement of Tracks 1 and 3 targets at improving the fidelity (PSNR), and Track 2 targets at enhancing the perceptual quality. The three tracks totally attract 482 registrations. In the test phase, 12 teams, 8 teams and 11 teams submitted the final results of Tracks 1, 2 and 3, respectively. The proposed methods and solutions gauge the state-of-the-art of video quality enhancement. The homepage of the challenge: https://github.com/RenYang-home/NTIRE21_VEnh
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.10781v5-abstract-full').style.display = 'none'; document.getElementById('2104.10781v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Corrected the MOS values in Table 2</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.11169">arXiv:2012.11169</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.11169">pdf</a>, <a href="https://arxiv.org/format/2012.11169">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An End-to-End Document-Level Neural Discourse Parser Exploiting Multi-Granularity Representations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Ke Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhengyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+N+F">Nancy F. Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.11169v1-abstract-short" style="display: inline;">
        Document-level discourse parsing, in accordance with the Rhetorical Structure Theory (RST), remains notoriously challenging. Challenges include the deep structure of document-level discourse trees, the requirement of subtle semantic judgments, and the lack of large-scale training corpora. To address such challenges, we propose to exploit robust representations derived from multiple levels of granu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.11169v1-abstract-full').style.display = 'inline'; document.getElementById('2012.11169v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.11169v1-abstract-full" style="display: none;">
        Document-level discourse parsing, in accordance with the Rhetorical Structure Theory (RST), remains notoriously challenging. Challenges include the deep structure of document-level discourse trees, the requirement of subtle semantic judgments, and the lack of large-scale training corpora. To address such challenges, we propose to exploit robust representations derived from multiple levels of granularity across syntax and semantics, and in turn incorporate such representations in an end-to-end encoder-decoder neural architecture for more resourceful discourse processing. In particular, we first use a pre-trained contextual language model that embodies high-order and long-range dependency to enable finer-grain semantic, syntactic, and organizational representations. We further encode such representations with boundary and hierarchical information to obtain more refined modeling for document-level discourse processing. Experimental results show that our parser achieves the state-of-the-art performance, approaching human-level performance on the benchmarked RST dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.11169v1-abstract-full').style.display = 'none'; document.getElementById('2012.11169v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 3 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.01704">arXiv:2012.01704</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.01704">pdf</a>, <a href="https://arxiv.org/format/2012.01704">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multilingual Neural RST Discourse Parsing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhengyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Ke Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+N+F">Nancy F. Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.01704v1-abstract-short" style="display: inline;">
        Text discourse parsing plays an important role in understanding information flow and argumentative structure in natural language. Previous research under the Rhetorical Structure Theory (RST) has mostly focused on inducing and evaluating models from the English treebank. However, the parsing tasks for other languages such as German, Dutch, and Portuguese are still challenging due to the shortage o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.01704v1-abstract-full').style.display = 'inline'; document.getElementById('2012.01704v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.01704v1-abstract-full" style="display: none;">
        Text discourse parsing plays an important role in understanding information flow and argumentative structure in natural language. Previous research under the Rhetorical Structure Theory (RST) has mostly focused on inducing and evaluating models from the English treebank. However, the parsing tasks for other languages such as German, Dutch, and Portuguese are still challenging due to the shortage of annotated data. In this work, we investigate two approaches to establish a neural, cross-lingual discourse parser via: (1) utilizing multilingual vector representations; and (2) adopting segment-level translation of the source content. Experiment results show that both methods are effective even with limited training data, and achieve state-of-the-art performance on cross-lingual, document-level discourse parsing on all sub-tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.01704v1-abstract-full').style.display = 'none'; document.getElementById('2012.01704v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in COLING2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.10710">arXiv:2008.10710</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.10710">pdf</a>, <a href="https://arxiv.org/format/2008.10710">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TIP.2021.3049974">10.1109/TIP.2021.3049974 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploit Camera Raw Data for Video Super-Resolution via Hidden Markov Model Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xiaohong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kangdi Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhe Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jun Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.10710v2-abstract-short" style="display: inline;">
        To the best of our knowledge, the existing deep-learning-based Video Super-Resolution (VSR) methods exclusively make use of videos produced by the Image Signal Processor (ISP) of the camera system as inputs. Such methods are 1) inherently suboptimal due to information loss incurred by non-invertible operations in ISP, and 2) inconsistent with the real imaging pipeline where VSR in fact serves as a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.10710v2-abstract-full').style.display = 'inline'; document.getElementById('2008.10710v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.10710v2-abstract-full" style="display: none;">
        To the best of our knowledge, the existing deep-learning-based Video Super-Resolution (VSR) methods exclusively make use of videos produced by the Image Signal Processor (ISP) of the camera system as inputs. Such methods are 1) inherently suboptimal due to information loss incurred by non-invertible operations in ISP, and 2) inconsistent with the real imaging pipeline where VSR in fact serves as a pre-processing unit of ISP. To address this issue, we propose a new VSR method that can directly exploit camera sensor data, accompanied by a carefully built Raw Video Dataset (RawVD) for training, validation, and testing. This method consists of a Successive Deep Inference (SDI) module and a reconstruction module, among others. The SDI module is designed according to the architectural principle suggested by a canonical decomposition result for Hidden Markov Model (HMM) inference; it estimates the target high-resolution frame by repeatedly performing pairwise feature fusion using deformable convolutions. The reconstruction module, built with elaborately designed Attention-based Residual Dense Blocks (ARDBs), serves the purpose of 1) refining the fused feature and 2) learning the color information needed to generate a spatial-specific transformation for accurate color correction. Extensive experiments demonstrate that owing to the informativeness of the camera raw data, the effectiveness of the network architecture, and the separation of super-resolution and color correction processes, the proposed method achieves superior VSR results compared to the state-of-the-art and can be adapted to any specific camera-ISP. Code and dataset are available at https://github.com/proteus1991/RawVSR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.10710v2-abstract-full').style.display = 'none'; document.getElementById('2008.10710v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 14 figures, accepted in IEEE Transactions on Image Processing</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.10680">arXiv:2008.10680</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.10680">pdf</a>, <a href="https://arxiv.org/format/2008.10680">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Video Frame Interpolation via Generalized Deformable Convolution
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+Z">Zhihao Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+X">Xiaohong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kangdi Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+L">Linhui Dai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Jun Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.10680v3-abstract-short" style="display: inline;">
        Video frame interpolation aims at synthesizing intermediate frames from nearby source frames while maintaining spatial and temporal consistencies. The existing deep-learning-based video frame interpolation methods can be roughly divided into two categories: flow-based methods and kernel-based methods. The performance of flow-based methods is often jeopardized by the inaccuracy of flow map estimati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.10680v3-abstract-full').style.display = 'inline'; document.getElementById('2008.10680v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.10680v3-abstract-full" style="display: none;">
        Video frame interpolation aims at synthesizing intermediate frames from nearby source frames while maintaining spatial and temporal consistencies. The existing deep-learning-based video frame interpolation methods can be roughly divided into two categories: flow-based methods and kernel-based methods. The performance of flow-based methods is often jeopardized by the inaccuracy of flow map estimation due to oversimplified motion models, while that of kernel-based methods tends to be constrained by the rigidity of kernel shape. To address these performance-limiting issues, a novel mechanism named generalized deformable convolution is proposed, which can effectively learn motion information in a data-driven manner and freely select sampling points in space-time. We further develop a new video frame interpolation method based on this mechanism. Our extensive experiments demonstrate that the new method performs favorably against the state-of-the-art, especially when dealing with complex motions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.10680v3-abstract-full').style.display = 'none'; document.getElementById('2008.10680v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13pages, journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.00335">arXiv:2008.00335</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.00335">pdf</a>, <a href="https://arxiv.org/format/2008.00335">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        V2I Connectivity-Based Dynamic Queue-Jump Lane for Emergency Vehicles: A Deep Reinforcement Learning Approach
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+H">Haoran Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kejian Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+L">Li Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chow%2C+J+Y+J">Joseph Y. J. Chow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.00335v2-abstract-short" style="display: inline;">
        Emergency vehicle (EMV) service is a key function of cities and is exceedingly challenging due to urban traffic congestion. A main reason behind EMV service delay is the lack of communication and cooperation between vehicles blocking EMVs. In this paper, we study the improvement of EMV service under V2I connectivity. We consider the establishment of dynamic queue jump lanes (DQJLs) based on real-t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.00335v2-abstract-full').style.display = 'inline'; document.getElementById('2008.00335v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.00335v2-abstract-full" style="display: none;">
        Emergency vehicle (EMV) service is a key function of cities and is exceedingly challenging due to urban traffic congestion. A main reason behind EMV service delay is the lack of communication and cooperation between vehicles blocking EMVs. In this paper, we study the improvement of EMV service under V2I connectivity. We consider the establishment of dynamic queue jump lanes (DQJLs) based on real-time coordination of connected vehicles. We develop a novel Markov decision process formulation for the DQJL problem, which explicitly accounts for the uncertainty of drivers&#39; reaction to approaching EMVs. We propose a deep neural network-based reinforcement learning algorithm that efficiently computes the optimal coordination instructions. We also validate our approach on a micro-simulation testbed using Simulation of Urban Mobility (SUMO). Validation results show that with our proposed methodology, the centralized control system saves approximately 15\% EMV passing time than the benchmark system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.00335v2-abstract-full').style.display = 'none'; document.getElementById('2008.00335v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.14381">arXiv:2007.14381</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.14381">pdf</a>, <a href="https://arxiv.org/format/2007.14381">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bieber%2C+D">David Bieber</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Singh%2C+R">Rishabh Singh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sutton%2C+C">Charles Sutton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+H">Hanjun Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.14381v3-abstract-short" style="display: inline;">
        Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up sear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.14381v3-abstract-full').style.display = 'inline'; document.getElementById('2007.14381v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.14381v3-abstract-full" style="display: none;">
        Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.14381v3-abstract-full').style.display = 'none'; document.getElementById('2007.14381v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.09503">arXiv:2006.09503</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.09503">pdf</a>, <a href="https://arxiv.org/format/2006.09503">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Memory-Efficient Pipeline-Parallel DNN Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Narayanan%2C+D">Deepak Narayanan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Phanishayee%2C+A">Amar Phanishayee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kaiyu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xie Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaharia%2C+M">Matei Zaharia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.09503v3-abstract-short" style="display: inline;">
        Many state-of-the-art ML results have been obtained by scaling up the number of parameters in existing models. However, parameters and activations for such large models often do not fit in the memory of a single accelerator device; this means that it is necessary to distribute training of large models over multiple accelerators. In this work, we propose PipeDream-2BW, a system that supports memory&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.09503v3-abstract-full').style.display = 'inline'; document.getElementById('2006.09503v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.09503v3-abstract-full" style="display: none;">
        Many state-of-the-art ML results have been obtained by scaling up the number of parameters in existing models. However, parameters and activations for such large models often do not fit in the memory of a single accelerator device; this means that it is necessary to distribute training of large models over multiple accelerators. In this work, we propose PipeDream-2BW, a system that supports memory-efficient pipeline parallelism. PipeDream-2BW uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint, and weight update semantics similar to data parallelism. In addition, PipeDream-2BW automatically partitions the model over the available hardware resources, while respecting hardware constraints such as memory capacities of accelerators and interconnect topologies. PipeDream-2BW can accelerate the training of large GPT and BERT language models by up to 20$\times$ with similar final model accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.09503v3-abstract-full').style.display = 'none'; document.getElementById('2006.09503v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICML 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.13983">arXiv:2004.13983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.13983">pdf</a>, <a href="https://arxiv.org/format/2004.13983">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Conditional Neural Generation using Sub-Aspect Functions for Extractive News Summarization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhengyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Ke Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+N+F">Nancy F. Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.13983v3-abstract-short" style="display: inline;">
        Much progress has been made in text summarization, fueled by neural architectures using large-scale training corpora. However, in the news domain, neural models easily overfit by leveraging position-related features due to the prevalence of the inverted pyramid writing style. In addition, there is an unmet need to generate a variety of summaries for different users. In this paper, we propose a neu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.13983v3-abstract-full').style.display = 'inline'; document.getElementById('2004.13983v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.13983v3-abstract-full" style="display: none;">
        Much progress has been made in text summarization, fueled by neural architectures using large-scale training corpora. However, in the news domain, neural models easily overfit by leveraging position-related features due to the prevalence of the inverted pyramid writing style. In addition, there is an unmet need to generate a variety of summaries for different users. In this paper, we propose a neural framework that can flexibly control summary generation by introducing a set of sub-aspect functions (i.e. importance, diversity, position). These sub-aspect functions are regulated by a set of control codes to decide which sub-aspect to focus on during summary generation. We demonstrate that extracted summaries with minimal position bias is comparable with those generated by standard models that take advantage of position preference. We also show that news summaries generated with a focus on diversity can be more preferred by human raters. These results suggest that a more flexible neural summarization framework providing more control options could be desirable in tailoring to different user preferences, which is useful since it is often impractical to articulate such preferences for different applications a priori.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.13983v3-abstract-full').style.display = 'none'; document.getElementById('2004.13983v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Findings of EMNLP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.11521">arXiv:2003.11521</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.11521">pdf</a>, <a href="https://arxiv.org/format/2003.11521">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Matching Text with Deep Mutual Information Estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+X">Xixi Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chengxi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bu%2C+J">Jiajun Bu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+C">Chengwei Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Keyue Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Z">Zhi Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Z">Zhou Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.11521v1-abstract-short" style="display: inline;">
        Text matching is a core natural language processing research problem. How to retain sufficient information on both content and structure information is one important challenge. In this paper, we present a neural approach for general-purpose text matching with deep mutual information estimation incorporated. Our approach, Text matching with Deep Info Max (TIM), is integrated with a procedure of uns&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.11521v1-abstract-full').style.display = 'inline'; document.getElementById('2003.11521v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.11521v1-abstract-full" style="display: none;">
        Text matching is a core natural language processing research problem. How to retain sufficient information on both content and structure information is one important challenge. In this paper, we present a neural approach for general-purpose text matching with deep mutual information estimation incorporated. Our approach, Text matching with Deep Info Max (TIM), is integrated with a procedure of unsupervised learning of representations by maximizing the mutual information between text matching neural network&#39;s input and output. We use both global and local mutual information to learn text representations. We evaluate our text matching approach on several tasks including natural language inference, paraphrase identification, and answer selection. Compared to the state-of-the-art approaches, the experiments show that our method integrated with mutual information estimation learns better text representation and achieves better experimental results of text matching tasks without exploiting pretraining on external data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.11521v1-abstract-full').style.display = 'none'; document.getElementById('2003.11521v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.09040">arXiv:2003.09040</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.09040">pdf</a>, <a href="https://arxiv.org/format/2003.09040">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3517034">10.1145/3517034 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TF-Coder: Program Synthesis for Tensor Manipulations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bieber%2C+D">David Bieber</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Singh%2C+R">Rishabh Singh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.09040v4-abstract-short" style="display: inline;">
        The success and popularity of deep learning is on the rise, partially due to powerful deep learning frameworks such as TensorFlow and PyTorch that make it easier to develop deep learning models. However, these libraries also come with steep learning curves, since programming in these frameworks is quite different from traditional imperative programming with explicit loops and conditionals. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.09040v4-abstract-full').style.display = 'inline'; document.getElementById('2003.09040v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.09040v4-abstract-full" style="display: none;">
        The success and popularity of deep learning is on the rise, partially due to powerful deep learning frameworks such as TensorFlow and PyTorch that make it easier to develop deep learning models. However, these libraries also come with steep learning curves, since programming in these frameworks is quite different from traditional imperative programming with explicit loops and conditionals. In this work, we present a tool called TF-Coder for programming by example in TensorFlow. TF-Coder uses a bottom-up weighted enumerative search, with value-based pruning of equivalent expressions and flexible type- and value-based filtering to ensure that expressions adhere to various requirements imposed by the TensorFlow library. We train models to predict TensorFlow operations from features of the input and output tensors and natural language descriptions of tasks, to prioritize relevant operations during search. TF-Coder solves 63 of 70 real-world tasks within 5 minutes, sometimes finding simpler solutions in less time compared to experienced human programmers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.09040v4-abstract-full').style.display = 'none'; document.getElementById('2003.09040v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in ACM Transactions on Programming Languages and Systems (TOPLAS) with presentation at PLDI 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.01025">arXiv:2003.01025</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.01025">pdf</a>, <a href="https://arxiv.org/format/2003.01025">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Queue-Jump Lane for Emergency Vehicles under Partially Connected Settings: A Multi-Agent Deep Reinforcement Learning Approach
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+H">Haoran Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kejian Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chow%2C+J+Y+J">Joseph. Y. J. Chow</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+L">Li Jin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.01025v3-abstract-short" style="display: inline;">
        Emergency vehicle (EMV) service is a key function of cities and is exceedingly challenging due to urban traffic congestion. The main reason behind EMV service delay is the lack of communication and cooperation between vehicles blocking EMVs. In this paper, we study the improvement of EMV service under V2X connectivity. We consider the establishment of dynamic queue jump lanes (DQJLs) based on real&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.01025v3-abstract-full').style.display = 'inline'; document.getElementById('2003.01025v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.01025v3-abstract-full" style="display: none;">
        Emergency vehicle (EMV) service is a key function of cities and is exceedingly challenging due to urban traffic congestion. The main reason behind EMV service delay is the lack of communication and cooperation between vehicles blocking EMVs. In this paper, we study the improvement of EMV service under V2X connectivity. We consider the establishment of dynamic queue jump lanes (DQJLs) based on real-time coordination of connected vehicles in the presence of non-connected human-driven vehicles. We develop a novel Markov decision process formulation for the DQJL coordination strategies, which explicitly accounts for the uncertainty of drivers&#39; yielding pattern to approaching EMVs. Based on pairs of neural networks representing actors and critics for agent vehicles, we develop a multi-agent actor-critic deep reinforcement learning algorithm that handles a varying number of vehicles and a random proportion of connected vehicles in the traffic. Approaching the optimal coordination strategies via indirect and direct reinforcement learning, we present two schemata to address multi-agent reinforcement learning on this connected vehicle application. Both approaches are validated, on a micro-simulation testbed SUMO, to establish a DQJL fast and safely. Validation results reveal that, with DQJL coordination strategies, it saves up to 30% time for EMVs to pass a link-level intelligent urban roadway than the baseline scenario.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.01025v3-abstract-full').style.display = 'none'; document.getElementById('2003.01025v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">42 pages, 13 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.09067">arXiv:2002.09067</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.09067">pdf</a>, <a href="https://arxiv.org/format/2002.09067">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Incremental Sampling Without Replacement for Sequence Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bieber%2C+D">David Bieber</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sutton%2C+C">Charles Sutton</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.09067v2-abstract-short" style="display: inline;">
        Sampling is a fundamental technique, and sampling without replacement is often desirable when duplicate samples are not beneficial. Within machine learning, sampling is useful for generating diverse outputs from a trained model. We present an elegant procedure for sampling without replacement from a broad class of randomized programs, including generative neural models that construct outputs seque&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.09067v2-abstract-full').style.display = 'inline'; document.getElementById('2002.09067v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.09067v2-abstract-full" style="display: none;">
        Sampling is a fundamental technique, and sampling without replacement is often desirable when duplicate samples are not beneficial. Within machine learning, sampling is useful for generating diverse outputs from a trained model. We present an elegant procedure for sampling without replacement from a broad class of randomized programs, including generative neural models that construct outputs sequentially. Our procedure is efficient even for exponentially-large output spaces. Unlike prior work, our approach is incremental, i.e., samples can be drawn one at a time, allowing for increased flexibility. We also present a new estimator for computing expectations from samples drawn without replacement. We show that incremental sampling without replacement is applicable to many domains, e.g., program synthesis and combinatorial optimization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.09067v2-abstract-full').style.display = 'none'; document.getElementById('2002.09067v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 February, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.00059">arXiv:2001.00059</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.00059">pdf</a>, <a href="https://arxiv.org/format/2001.00059">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning and Evaluating Contextual Embedding of Source Code
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kanade%2C+A">Aditya Kanade</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Maniatis%2C+P">Petros Maniatis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Balakrishnan%2C+G">Gogul Balakrishnan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.00059v3-abstract-short" style="display: inline;">
        Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as BERT, which can be fine-tuned for downstream tasks with less labeled data and training budget,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.00059v3-abstract-full').style.display = 'inline'; document.getElementById('2001.00059v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.00059v3-abstract-full" style="display: none;">
        Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as BERT, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4M Python files from GitHub, which we use to pre-train CuBERT, an open-sourced code-understanding BERT model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune CuBERT on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, BiLSTM and Transformer models, as well as published state-of-the-art models, showing that CuBERT outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against CuBERT models as a strong baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.00059v3-abstract-full').style.display = 'none'; document.getElementById('2001.00059v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 August, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 December, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in ICML 2020. This version (v.3) is the final camera-ready version of the paper. It contains the re-computed results, based on the open-sourced datasets</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.09826">arXiv:1911.09826</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.09826">pdf</a>, <a href="https://arxiv.org/format/1911.09826">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Factorized Multimodal Transformer for Multimodal Sequential Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zadeh%2C+A">Amir Zadeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mao%2C+C">Chengfeng Mao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kelly Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yiwei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+P+P">Paul Pu Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Poria%2C+S">Soujanya Poria</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Morency%2C+L">Louis-Philippe Morency</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.09826v1-abstract-short" style="display: inline;">
        The complex world around us is inherently multimodal and sequential (continuous). Information is scattered across different modalities and requires multiple continuous sensors to be captured. As machine learning leaps towards better generalization to real world, multimodal sequential learning becomes a fundamental research area. Arguably, modeling arbitrarily distributed spatio-temporal dynamics w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.09826v1-abstract-full').style.display = 'inline'; document.getElementById('1911.09826v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.09826v1-abstract-full" style="display: none;">
        The complex world around us is inherently multimodal and sequential (continuous). Information is scattered across different modalities and requires multiple continuous sensors to be captured. As machine learning leaps towards better generalization to real world, multimodal sequential learning becomes a fundamental research area. Arguably, modeling arbitrarily distributed spatio-temporal dynamics within and across modalities is the biggest challenge in this research area. In this paper, we present a new transformer model, called the Factorized Multimodal Transformer (FMT) for multimodal sequential learning. FMT inherently models the intramodal and intermodal (involving two or more modalities) dynamics within its multimodal input in a factorized manner. The proposed factorization allows for increasing the number of self-attentions to better model the multimodal phenomena at hand; without encountering difficulties during training (e.g. overfitting) even on relatively low-resource setups. All the attention mechanisms within FMT have a full time-domain receptive field which allows them to asynchronously capture long-range multimodal dynamics. In our experiments we focus on datasets that contain the three commonly studied modalities of language, vision and acoustic. We perform a wide range of experiments, spanning across 3 well-studied datasets and 21 distinct labels. FMT shows superior performance over previously proposed models, setting new state of the art in the studied datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.09826v1-abstract-full').style.display = 'none'; document.getElementById('1911.09826v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.05896">arXiv:1906.05896</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.05896">pdf</a>, <a href="https://arxiv.org/format/1906.05896">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Instance Occlusion for Panoptic Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lazarow%2C+J">Justin Lazarow</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+K">Kwonjoon Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kunyu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+Z">Zhuowen Tu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.05896v4-abstract-short" style="display: inline;">
        Panoptic segmentation requires segments of both &#34;things&#34; (countable object instances) and &#34;stuff&#34; (uncountable and amorphous regions) within a single output. A common approach involves the fusion of instance segmentation (for &#34;things&#34;) and semantic segmentation (for &#34;stuff&#34;) into a non-overlapping placement of segments, and resolves overlaps. However, instance ordering with detection confidence do&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.05896v4-abstract-full').style.display = 'inline'; document.getElementById('1906.05896v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.05896v4-abstract-full" style="display: none;">
        Panoptic segmentation requires segments of both &#34;things&#34; (countable object instances) and &#34;stuff&#34; (uncountable and amorphous regions) within a single output. A common approach involves the fusion of instance segmentation (for &#34;things&#34;) and semantic segmentation (for &#34;stuff&#34;) into a non-overlapping placement of segments, and resolves overlaps. However, instance ordering with detection confidence do not correlate well with natural occlusion relationship. To resolve this issue, we propose a branch that is tasked with modeling how two instance masks should overlap one another as a binary relation. Our method, named OCFusion, is lightweight but particularly effective in the instance fusion process. OCFusion is trained with the ground truth relation derived automatically from the existing dataset annotations. We obtain state-of-the-art results on COCO and show competitive results on the Cityscapes panoptic segmentation benchmark.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.05896v4-abstract-full').style.display = 'none'; document.getElementById('1906.05896v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 June, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to CVPR 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.03231">arXiv:1906.03231</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.03231">pdf</a>, <a href="https://arxiv.org/ps/1906.03231">ps</a>, <a href="https://arxiv.org/format/1906.03231">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A cryptographic approach to black box adversarial machine learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kevin Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+D">Daniel Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bishop%2C+A">Allison Bishop</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.03231v2-abstract-short" style="display: inline;">
        We propose a new randomized ensemble technique with a provable security guarantee against black-box transfer attacks. Our proof constructs a new security problem for random binary classifiers which is easier to empirically verify and a reduction from the security of this new model to the security of the ensemble classifier. We provide experimental evidence of the security of our random binary clas&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.03231v2-abstract-full').style.display = 'inline'; document.getElementById('1906.03231v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.03231v2-abstract-full" style="display: none;">
        We propose a new randomized ensemble technique with a provable security guarantee against black-box transfer attacks. Our proof constructs a new security problem for random binary classifiers which is easier to empirically verify and a reduction from the security of this new model to the security of the ensemble classifier. We provide experimental evidence of the security of our random binary classifiers, as well as empirical results of the adversarial accuracy of the overall ensemble to black-box attacks. Our construction crucially leverages hidden randomness in the multiclass-to-binary reduction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.03231v2-abstract-full').style.display = 'none'; document.getElementById('1906.03231v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 February, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 June, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.09364">arXiv:1903.09364</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.09364">pdf</a>, <a href="https://arxiv.org/format/1903.09364">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Differentially Private Nonparametric Hypothesis Testing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Couch%2C+S">Simon Couch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kazan%2C+Z">Zeki Kazan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kaiyan Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bray%2C+A">Andrew Bray</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Groce%2C+A">Adam Groce</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.09364v1-abstract-short" style="display: inline;">
        Hypothesis tests are a crucial statistical tool for data mining and are the workhorse of scientific research in many fields. Here we study differentially private tests of independence between a categorical and a continuous variable. We take as our starting point traditional nonparametric tests, which require no distributional assumption (e.g., normality) about the data distribution. We present pri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.09364v1-abstract-full').style.display = 'inline'; document.getElementById('1903.09364v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.09364v1-abstract-full" style="display: none;">
        Hypothesis tests are a crucial statistical tool for data mining and are the workhorse of scientific research in many fields. Here we study differentially private tests of independence between a categorical and a continuous variable. We take as our starting point traditional nonparametric tests, which require no distributional assumption (e.g., normality) about the data distribution. We present private analogues of the Kruskal-Wallis, Mann-Whitney, and Wilcoxon signed-rank tests, as well as the parametric one-sample t-test. These tests use novel test statistics developed specifically for the private setting. We compare our tests to prior work, both on parametric and nonparametric tests. We find that in all cases our new nonparametric tests achieve large improvements in statistical power, even when the assumptions of parametric tests are met.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.09364v1-abstract-full').style.display = 'none'; document.getElementById('1903.09364v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.02009">arXiv:1812.02009</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.02009">pdf</a>, <a href="https://arxiv.org/format/1812.02009">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s11390-020-9638-7">10.1007/s11390-020-9638-7 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Research on the Security of Blockchain Data: A Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+L">Liehuang Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+B">Baokun Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+M">Meng Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+S">Shui Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+F">Feng Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hongyu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kexin Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gai%2C+K">Keke Gai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.02009v2-abstract-short" style="display: inline;">
        With the more and more extensive application of blockchain, blockchain security has been widely concerned by the society and deeply studied by scholars. Moreover, the security of blockchain data directly affects the security of various applications of blockchain. In this survey, we perform a comprehensive classification and summary of the security of blockchain data. First, we present classificati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.02009v2-abstract-full').style.display = 'inline'; document.getElementById('1812.02009v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.02009v2-abstract-full" style="display: none;">
        With the more and more extensive application of blockchain, blockchain security has been widely concerned by the society and deeply studied by scholars. Moreover, the security of blockchain data directly affects the security of various applications of blockchain. In this survey, we perform a comprehensive classification and summary of the security of blockchain data. First, we present classification of blockchain data attacks. Subsequently, we present the attacks and defenses of blockchain data in terms of privacy, availability, integrity and controllability. Data privacy attacks present data leakage or data obtained by attackers through analysis. Data availability attacks present abnormal or incorrect access to blockchain data. Data integrity attacks present blockchain data being tampered. Data controllability attacks present blockchain data accidentally manipulated by smart contract vulnerability. Finally, we present several important open research directions to identify follow-up studies in this area.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.02009v2-abstract-full').style.display = 'none'; document.getElementById('1812.02009v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 December, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 December, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Journal of Computer Science and Technology, 2020, 35(4): 843-862
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.05175">arXiv:1811.05175</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.05175">pdf</a>, <a href="https://arxiv.org/format/1811.05175">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3290386">10.1145/3290386 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FrAngel: Component-Based Synthesis with Control Structures
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Steinhardt%2C+J">Jacob Steinhardt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+P">Percy Liang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.05175v2-abstract-short" style="display: inline;">
        In component-based program synthesis, the synthesizer generates a program given a library of components (functions). Existing component-based synthesizers have difficulty synthesizing loops and other control structures, and they often require formal specifications of the components, which can be expensive to generate. We present FrAngel, a new approach to component-based synthesis that can synthes&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.05175v2-abstract-full').style.display = 'inline'; document.getElementById('1811.05175v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.05175v2-abstract-full" style="display: none;">
        In component-based program synthesis, the synthesizer generates a program given a library of components (functions). Existing component-based synthesizers have difficulty synthesizing loops and other control structures, and they often require formal specifications of the components, which can be expensive to generate. We present FrAngel, a new approach to component-based synthesis that can synthesize short Java functions with control structures when given a desired signature, a set of input-output examples, and a collection of libraries (without formal specifications). FrAngel aims to discover programs with many distinct behaviors by combining two main ideas. First, it mines code fragments from partially-successful programs that only pass some of the examples. These extracted fragments are often useful for synthesis due to a property that we call special-case similarity. Second, FrAngel uses angelic conditions as placeholders for control structure conditions and optimistically evaluates the resulting program sketches. Angelic conditions decompose the synthesis process: FrAngel first finds promising partial programs and later fills in their missing conditions. We demonstrate that FrAngel can synthesize a variety of interesting programs with combinations of control structures within seconds, significantly outperforming prior state-of-the-art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.05175v2-abstract-full').style.display = 'none'; document.getElementById('1811.05175v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 November, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">30 pages, 12 figures, to appear in the 46th ACM SIGPLAN Symposium on Principles of Programming Languages (POPL 2019)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.05733">arXiv:1810.05733</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.05733">pdf</a>, <a href="https://arxiv.org/format/1810.05733">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-030-00889-5_26">10.1007/978-3-030-00889-5_26 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Optimal Deep Projection of $^{18}$F-FDG PET Imaging for Early Differential Diagnosis of Parkinsonian Syndromes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+S">Shubham Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roy%2C+A+G">Abhijit Guha Roy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+P">Ping Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Conjeti%2C+S">Sailesh Conjeti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anand%2C+R+S">R. S. Anand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yakushev%2C+I">Igor Yakushev</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=F%C3%B6rster%2C+S">Stefan Förster</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schwaiger%2C+M">Markus Schwaiger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+S">Sung-Cheng Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rominger%2C+A">Axel Rominger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zuo%2C+C">Chuantao Zuo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kuangyu Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.05733v1-abstract-short" style="display: inline;">
        Several diseases of parkinsonian syndromes present similar symptoms at early stage and no objective widely used diagnostic methods have been approved until now. Positron emission tomography (PET) with $^{18}$F-FDG was shown to be able to assess early neuronal dysfunction of synucleinopathies and tauopathies. Tensor factorization (TF) based approaches have been applied to identify characteristic me&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.05733v1-abstract-full').style.display = 'inline'; document.getElementById('1810.05733v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.05733v1-abstract-full" style="display: none;">
        Several diseases of parkinsonian syndromes present similar symptoms at early stage and no objective widely used diagnostic methods have been approved until now. Positron emission tomography (PET) with $^{18}$F-FDG was shown to be able to assess early neuronal dysfunction of synucleinopathies and tauopathies. Tensor factorization (TF) based approaches have been applied to identify characteristic metabolic patterns for differential diagnosis. However, these conventional dimension-reduction strategies assume linear or multi-linear relationships inside data, and are therefore insufficient to distinguish nonlinear metabolic differences between various parkinsonian syndromes. In this paper, we propose a Deep Projection Neural Network (DPNN) to identify characteristic metabolic pattern for early differential diagnosis of parkinsonian syndromes. We draw our inspiration from the existing TF methods. The network consists of a (i) compression part: which uses a deep network to learn optimal 2D projections of 3D scans, and a (ii) classification part: which maps the 2D projections to labels. The compression part can be pre-trained using surplus unlabelled datasets. Also, as the classification part operates on these 2D projections, it can be trained end-to-end effectively with limited labelled data, in contrast to 3D approaches. We show that DPNN is more effective in comparison to existing state-of-the-art and plausible baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.05733v1-abstract-full').style.display = 'none'; document.getElementById('1810.05733v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 3 figures, conference, MICCAI DLMIA, 2018</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.10; I.2.4; I.4.10; I.2.1
        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Kumar, Shubham, et al. DLMIA, Springer, Cham, 2018. 227-235
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.01635">arXiv:1809.01635</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.01635">pdf</a>, <a href="https://arxiv.org/format/1809.01635">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Differentially Private Wilcoxon Signed-Rank Test
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Couch%2C+S">Simon Couch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kazan%2C+Z">Zeki Kazan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kaiyan Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bray%2C+A">Andrew Bray</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Groce%2C+A">Adam Groce</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.01635v1-abstract-short" style="display: inline;">
        Hypothesis tests are a crucial statistical tool for data mining and are the workhorse of scientific research in many fields. Here we present a differentially private analogue of the classic Wilcoxon signed-rank hypothesis test, which is used when comparing sets of paired (e.g., before-and-after) data values. We present not only a private estimate of the test statistic, but a method to accurately c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.01635v1-abstract-full').style.display = 'inline'; document.getElementById('1809.01635v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.01635v1-abstract-full" style="display: none;">
        Hypothesis tests are a crucial statistical tool for data mining and are the workhorse of scientific research in many fields. Here we present a differentially private analogue of the classic Wilcoxon signed-rank hypothesis test, which is used when comparing sets of paired (e.g., before-and-after) data values. We present not only a private estimate of the test statistic, but a method to accurately compute a p-value and assess statistical significance. We evaluate our test on both simulated and real data. Compared to the only existing private test for this situation, that of Task and Clifton, we find that our test requires less than half as much data to achieve the same statistical power.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.01635v1-abstract-full').style.display = 'none'; document.getElementById('1809.01635v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.11475">arXiv:1806.11475</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.11475">pdf</a>, <a href="https://arxiv.org/format/1806.11475">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SynNet: Structure-Preserving Fully Convolutional Networks for Medical Image Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gunashekar%2C+D">Deepa Gunashekar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Conjeti%2C+S">Sailesh Conjeti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roy%2C+A+G">Abhijit Guha Roy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Navab%2C+N">Nassir Navab</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kuangyu Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.11475v1-abstract-short" style="display: inline;">
        Cross modal image syntheses is gaining significant interests for its ability to estimate target images of a different modality from a given set of source images,like estimating MR to MR, MR to CT, CT to PET etc, without the need for an actual acquisition.Though they show potential for applications in radiation therapy planning,image super resolution, atlas construction, image segmentation etc.The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11475v1-abstract-full').style.display = 'inline'; document.getElementById('1806.11475v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.11475v1-abstract-full" style="display: none;">
        Cross modal image syntheses is gaining significant interests for its ability to estimate target images of a different modality from a given set of source images,like estimating MR to MR, MR to CT, CT to PET etc, without the need for an actual acquisition.Though they show potential for applications in radiation therapy planning,image super resolution, atlas construction, image segmentation etc.The synthesis results are not as accurate as the actual acquisition.In this paper,we address the problem of multi modal image synthesis by proposing a fully convolutional deep learning architecture called the SynNet.We extend the proposed architecture for various input output configurations. And finally, we propose a structure preserving custom loss function for cross-modal image synthesis.We validate the proposed SynNet and its extended framework on BRATS dataset with comparisons against three state-of-the art methods.And the results of the proposed custom loss function is validated against the traditional loss function used by the state-of-the-art methods for cross modal image synthesis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11475v1-abstract-full').style.display = 'none'; document.getElementById('1806.11475v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.01023">arXiv:1806.01023</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.01023">pdf</a>, <a href="https://arxiv.org/format/1806.01023">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Differential Diagnosis for Pancreatic Cysts in CT Scans Using Densely-Connected Convolutional Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hongwei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+K">Kanru Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reichert%2C+M">Maximilian Reichert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+L">Lina Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Braren%2C+R">Rickmer Braren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+D">Deliang Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schmid%2C+R">Roland Schmid</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Ji Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Menze%2C+B">Bjoern Menze</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kuangyu Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.01023v3-abstract-short" style="display: inline;">
        The lethal nature of pancreatic ductal adenocarcinoma (PDAC) calls for early differential diagnosis of pancreatic cysts, which are identified in up to 16% of normal subjects, and some of which may develop into PDAC. Previous computer-aided developments have achieved certain accuracy for classification on segmented cystic lesions in CT. However, pancreatic cysts have a large variation in size and s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.01023v3-abstract-full').style.display = 'inline'; document.getElementById('1806.01023v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.01023v3-abstract-full" style="display: none;">
        The lethal nature of pancreatic ductal adenocarcinoma (PDAC) calls for early differential diagnosis of pancreatic cysts, which are identified in up to 16% of normal subjects, and some of which may develop into PDAC. Previous computer-aided developments have achieved certain accuracy for classification on segmented cystic lesions in CT. However, pancreatic cysts have a large variation in size and shape, and the precise segmentation of them remains rather challenging, which restricts the computer-aided interpretation of CT images acquired for differential diagnosis. We propose a computer-aided framework for early differential diagnosis of pancreatic cysts without pre-segmenting the lesions using densely-connected convolutional networks (Dense-Net). The Dense-Net learns high-level features from whole abnormal pancreas and builds mappings between medical imaging appearance to different pathological types of pancreatic cysts. To enhance the clinical applicability, we integrate saliency maps in the framework to assist the physicians to understand the decision of the deep learning method. The test on a cohort of 206 patients with 4 pathologically confirmed subtypes of pancreatic cysts has achieved an overall accuracy of 72.8%, which is significantly higher than the baseline accuracy of 48.1%, which strongly supports the clinical potential of our developed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.01023v3-abstract-full').style.display = 'none'; document.getElementById('1806.01023v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 June, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to miccai 2017, *corresponding author: liji@huashan.org.cn</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1705.07048">arXiv:1705.07048</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1705.07048">pdf</a>, <a href="https://arxiv.org/ps/1705.07048">ps</a>, <a href="https://arxiv.org/format/1705.07048">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistics Theory">math.ST</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Linear regression without correspondence
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+D">Daniel Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kevin Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+X">Xiaorui Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1705.07048v2-abstract-short" style="display: inline;">
        This article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. First, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. Next, in an average-case and noise-free setting where the responses exactly correspond to a linear funct&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07048v2-abstract-full').style.display = 'inline'; document.getElementById('1705.07048v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1705.07048v2-abstract-full" style="display: none;">
        This article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. First, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. Next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. Finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07048v2-abstract-full').style.display = 'none'; document.getElementById('1705.07048v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 November, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 May, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1505.03227">arXiv:1505.03227</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1505.03227">pdf</a>, <a href="https://arxiv.org/format/1505.03227">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TIP.2015.2432712">10.1109/TIP.2015.2432712 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Edge-Preserving Coherence
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+K">Keze Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+L">Liang Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+J">Jiangbo Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenglong Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Keyang Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1505.03227v1-abstract-short" style="display: inline;">
        Driven by recent vision and graphics applications such as image segmentation and object recognition, computing pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly important. In this paper, we propose a unified framework called PISA, which stands for Pixelwise Image Saliency Aggregating various bottom-up cues and priors. It generates spatially coherent yet&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1505.03227v1-abstract-full').style.display = 'inline'; document.getElementById('1505.03227v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1505.03227v1-abstract-full" style="display: none;">
        Driven by recent vision and graphics applications such as image segmentation and object recognition, computing pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly important. In this paper, we propose a unified framework called PISA, which stands for Pixelwise Image Saliency Aggregating various bottom-up cues and priors. It generates spatially coherent yet detail-preserving, pixel-accurate and fine-grained saliency, and overcomes the limitations of previous methods which use homogeneous superpixel-based and color only treatment. PISA aggregates multiple saliency cues in a global context such as complementary color and structure contrast measures with their spatial priors in the image domain. The saliency confidence is further jointly modeled with a neighborhood consistence constraint into an energy minimization formulation, in which each pixel will be evaluated with multiple hypothetical saliency levels. Instead of using global discrete optimization methods, we employ the cost-volume filtering technique to solve our formulation, assigning the saliency levels smoothly while preserving the edge-aware structure details. In addition, a faster version of PISA is developed using a gradient-driven image sub-sampling strategy to greatly improve the runtime efficiency while keeping comparable detection accuracy. Extensive experiments on a number of public datasets suggest that PISA convincingly outperforms other state-of-the-art approaches. In addition, with this work we also create a new dataset containing $800$ commodity images for evaluating saliency detection. The dataset and source code of PISA can be downloaded at http://vision.sysu.edu.cn/project/PISA/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1505.03227v1-abstract-full').style.display = 'none'; document.getElementById('1505.03227v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 May, 2015; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 14 figures, 1 table, to appear in IEEE Transactions on Image Processing</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68U10
        

        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Transactions on Image Processing (TIP), volume. 24, Issue. 10, page. 3019 - 3033, Oct. 2015
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1305.2801">arXiv:1305.2801</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1305.2801">pdf</a>, <a href="https://arxiv.org/format/1305.2801">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantization Noise Shaping for Information Maximizing ADCs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Redfern%2C+A+J">Arthur J. Redfern</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kun Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1305.2801v1-abstract-short" style="display: inline;">
        ADCs sit at the interface of the analog and digital worlds and fundamentally determine what information is available in the digital domain for processing. This paper shows that a configurable ADC can be designed for signals with non constant information as a function of frequency such that within a fixed power budget the ADC maximizes the information in the converted signal by frequency shaping th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1305.2801v1-abstract-full').style.display = 'inline'; document.getElementById('1305.2801v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1305.2801v1-abstract-full" style="display: none;">
        ADCs sit at the interface of the analog and digital worlds and fundamentally determine what information is available in the digital domain for processing. This paper shows that a configurable ADC can be designed for signals with non constant information as a function of frequency such that within a fixed power budget the ADC maximizes the information in the converted signal by frequency shaping the quantization noise. Quantization noise shaping can be realized via loop filter design for a single channel delta sigma ADC and extended to common time and frequency interleaved multi channel structures. Results are presented for example wireline and wireless style channels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1305.2801v1-abstract-full').style.display = 'none'; document.getElementById('1305.2801v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2013; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2013.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages, 6 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          94A15
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1201.6278">arXiv:1201.6278</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1201.6278">pdf</a>, <a href="https://arxiv.org/ps/1201.6278">ps</a>, <a href="https://arxiv.org/format/1201.6278">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Data Analysis, Statistics and Probability">physics.data-an</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1103/PhysRevE.85.016118">10.1103/PhysRevE.85.016118 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Solving the accuracy-diversity dilemma via directed random walks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jian-Guo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kerui Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+Q">Qiang Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1201.6278v1-abstract-short" style="display: inline;">
        Random walks have been successfully used to measure user or object similarities in collaborative filtering (CF) recommender systems, which is of high accuracy but low diversity. A key challenge of CF system is that the reliably accurate results are obtained with the help of peers&#39; recommendation, but the most useful individual recommendations are hard to be found among diverse niche objects. In th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1201.6278v1-abstract-full').style.display = 'inline'; document.getElementById('1201.6278v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1201.6278v1-abstract-full" style="display: none;">
        Random walks have been successfully used to measure user or object similarities in collaborative filtering (CF) recommender systems, which is of high accuracy but low diversity. A key challenge of CF system is that the reliably accurate results are obtained with the help of peers&#39; recommendation, but the most useful individual recommendations are hard to be found among diverse niche objects. In this paper we investigate the direction effect of the random walk on user similarity measurements and find that the user similarity, calculated by directed random walks, is reverse to the initial node&#39;s degree. Since the ratio of small-degree users to large-degree users is very large in real data sets, the large-degree users&#39; selections are recommended extensively by traditional CF algorithms. By tuning the user similarity direction from neighbors to the target user, we introduce a new algorithm specifically to address the challenge of diversity of CF and show how it can be used to solve the accuracy-diversity dilemma. Without relying on any context-specific information, we are able to obtain accurate and diverse recommendations, which outperforms the state-of-the-art CF methods. This work suggests that the random walk direction is an important factor to improve the personalized recommendation performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1201.6278v1-abstract-full').style.display = 'none'; document.getElementById('1201.6278v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 January, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 6 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        PhysRevE.85.016118 (2012)
      </p>
    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>