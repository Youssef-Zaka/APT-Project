<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="applicable-device" content="pc,mobile">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <title>A large scale study of reader interactions with images on Wikipedia | EPJ Data Science | Full Text</title>

        
    
    <meta name="citation_abstract" content="Wikipedia is the largest source of free encyclopedic knowledge and one of the most visited sites on the Web. To increase reader understanding of the article, Wikipedia editors add images within the text of the article&#8217;s body. However, despite their widespread usage on web platforms and the huge volume of visual content on Wikipedia, little is known about the importance of images in the context of free knowledge environments. To bridge this gap, we collect data about English Wikipedia reader interactions with images during one month and perform the first large-scale analysis of how interactions with images happen on Wikipedia. First, we quantify the overall engagement with images, finding that one in 29 pageviews results in a click on at least one image, one order of magnitude higher than interactions with other types of article content. Second, we study what factors associate with image engagement and observe that clicks on images occur more often in shorter articles and articles about visual arts or transports and biographies of less well-known people. Third, we look at interactions with Wikipedia article previews and find that images help support reader information need when navigating through the site, especially for more popular pages. The findings in this study deepen our understanding of the role of images for free knowledge and provide a guide for Wikipedia editors and web user communities to enrich the world&#8217;s largest source of encyclopedic knowledge."/>
    <meta name="journal_id" content="13688"/>
    <meta name="dc.title" content="A large scale study of reader interactions with images on Wikipedia"/>
    <meta name="dc.source" content="EPJ Data Science 2021 11:1"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="SpringerOpen"/>
    <meta name="dc.date" content="2022-01-03"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2021 The Author(s)"/>
    <meta name="dc.rights" content="2021 The Author(s)"/>
    <meta name="dc.rightsAgent" content="reprints@biomedcentral.com"/>
    <meta name="dc.description" content="Wikipedia is the largest source of free encyclopedic knowledge and one of the most visited sites on the Web. To increase reader understanding of the article, Wikipedia editors add images within the text of the article&#8217;s body. However, despite their widespread usage on web platforms and the huge volume of visual content on Wikipedia, little is known about the importance of images in the context of free knowledge environments. To bridge this gap, we collect data about English Wikipedia reader interactions with images during one month and perform the first large-scale analysis of how interactions with images happen on Wikipedia. First, we quantify the overall engagement with images, finding that one in 29 pageviews results in a click on at least one image, one order of magnitude higher than interactions with other types of article content. Second, we study what factors associate with image engagement and observe that clicks on images occur more often in shorter articles and articles about visual arts or transports and biographies of less well-known people. Third, we look at interactions with Wikipedia article previews and find that images help support reader information need when navigating through the site, especially for more popular pages. The findings in this study deepen our understanding of the role of images for free knowledge and provide a guide for Wikipedia editors and web user communities to enrich the world&#8217;s largest source of encyclopedic knowledge."/>
    <meta name="prism.issn" content="2193-1127"/>
    <meta name="prism.publicationName" content="EPJ Data Science"/>
    <meta name="prism.publicationDate" content="2022-01-03"/>
    <meta name="prism.volume" content="11"/>
    <meta name="prism.number" content="1"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1"/>
    <meta name="prism.endingPage" content="29"/>
    <meta name="prism.copyright" content="2021 The Author(s)"/>
    <meta name="prism.rightsAgent" content="reprints@biomedcentral.com"/>
    <meta name="prism.url" content="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00312-8"/>
    <meta name="prism.doi" content="doi:10.1140/epjds/s13688-021-00312-8"/>
    <meta name="citation_pdf_url" content="https://epjdatascience.springeropen.com/track/pdf/10.1140/epjds/s13688-021-00312-8"/>
    <meta name="citation_fulltext_html_url" content="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00312-8"/>
    <meta name="citation_journal_title" content="EPJ Data Science"/>
    <meta name="citation_journal_abbrev" content="EPJ Data Sci."/>
    <meta name="citation_publisher" content="SpringerOpen"/>
    <meta name="citation_issn" content="2193-1127"/>
    <meta name="citation_title" content="A large scale study of reader interactions with images on Wikipedia"/>
    <meta name="citation_volume" content="11"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_publication_date" content="2022/12"/>
    <meta name="citation_online_date" content="2022/01/03"/>
    <meta name="citation_firstpage" content="1"/>
    <meta name="citation_lastpage" content="29"/>
    <meta name="citation_article_type" content="Regular article"/>
    <meta name="citation_fulltext_world_readable" content=""/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1140/epjds/s13688-021-00312-8"/>
    <meta name="DOI" content="10.1140/epjds/s13688-021-00312-8"/>
    <meta name="citation_doi" content="10.1140/epjds/s13688-021-00312-8"/>
    <meta name="description" content="Wikipedia is the largest source of free encyclopedic knowledge and one of the most visited sites on the Web. To increase reader understanding of the article, Wikipedia editors add images within the text of the article&#8217;s body. However, despite their widespread usage on web platforms and the huge volume of visual content on Wikipedia, little is known about the importance of images in the context of free knowledge environments. To bridge this gap, we collect data about English Wikipedia reader interactions with images during one month and perform the first large-scale analysis of how interactions with images happen on Wikipedia. First, we quantify the overall engagement with images, finding that one in 29 pageviews results in a click on at least one image, one order of magnitude higher than interactions with other types of article content. Second, we study what factors associate with image engagement and observe that clicks on images occur more often in shorter articles and articles about visual arts or transports and biographies of less well-known people. Third, we look at interactions with Wikipedia article previews and find that images help support reader information need when navigating through the site, especially for more popular pages. The findings in this study deepen our understanding of the role of images for free knowledge and provide a guide for Wikipedia editors and web user communities to enrich the world&#8217;s largest source of encyclopedic knowledge."/>
    <meta name="dc.creator" content="Rama, Daniele"/>
    <meta name="dc.creator" content="Piccardi, Tiziano"/>
    <meta name="dc.creator" content="Redi, Miriam"/>
    <meta name="dc.creator" content="Schifanella, Rossano"/>
    <meta name="dc.subject" content="Computer Appl. in Social and Behavioral Sciences"/>
    <meta name="dc.subject" content="Data-driven Science, Modeling and Theory Building"/>
    <meta name="dc.subject" content="Complexity"/>
    <meta name="citation_reference" content="citation_journal_title=Ration Soc; citation_title=Reputation and reliability in collective goods: the case of the online encyclopedia Wikipedia; citation_author=D Anthony, SW Smith, T Williamson; citation_volume=21; citation_issue=3; citation_publication_date=2009; citation_pages=283-306; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=ECTJ; citation_title=Effects of text illustrations: a review of research; citation_author=WH Levie, R Lentz; citation_volume=30; citation_issue=4; citation_publication_date=1982; citation_pages=195-232; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=PLoS ONE; citation_title=Dynamics of conflicts in Wikipedia; citation_author=T Yasseri, R Sumi, A Rung, A Kornai, J Kert&#233;sz; citation_volume=7; citation_issue=6; citation_publication_date=2012; citation_id=CR3"/>
    <meta name="citation_reference" content="citation_title=Why the world reads Wikipedia: beyond English speakers; citation_inbook_title=Proc. International conference on web search and data mining (WSDM); citation_publication_date=2019; citation_id=CR4; citation_author=F Lemmerich; citation_author=D S&#225;ez-Trumper; citation_author=R West; citation_author=L Zia"/>
    <meta name="citation_reference" content="
					Halfaker A, Geiger RS (2020) Ores: lowering barriers with participatory machine learning in wikipedia. Proc Human-Computer Interaction (HCI)
				"/>
    <meta name="citation_reference" content="citation_title=The_tower_of_babel. jpg: diversity of visual encyclopedic knowledge across Wikipedia language editions; citation_inbook_title=Proc. International conference on web and social media (ICWSM); citation_publication_date=2018; citation_id=CR6; citation_author=S He; citation_author=AY Lin; citation_author=E Adar; citation_author=BJ Hecht"/>
    <meta name="citation_reference" content="citation_title=The visual side of Wikipedia; citation_inbook_title=Proc. Hawaii international conference on system sciences (HICSS); citation_publication_date=2007; citation_id=CR7; citation_author=FB Viegas"/>
    <meta name="citation_reference" content="citation_title=Faces engage us: photos with faces attract more likes and comments on Instagram; citation_inbook_title=Proc. Conference on human factors in computing systems (SIGCHI); citation_publication_date=2014; citation_id=CR8; citation_author=S Bakhshi; citation_author=DA Shamma; citation_author=E Gilbert"/>
    <meta name="citation_reference" content="citation_title=A large-scale study of user image search behavior on the web; citation_inbook_title=Proc. Conference on human factors in computing systems (SIGCHI); citation_publication_date=2015; citation_id=CR9; citation_author=JY Park; citation_author=N O&#8217;Hare; citation_author=R Schifanella; citation_author=A Jaimes; citation_author=C-W Chung"/>
    <meta name="citation_reference" content="citation_journal_title=ACM Comput Surv; citation_title=Computational understanding of visual interestingness beyond semantics: literature survey and analysis of covariates; citation_author=MG Constantin, M Redi, G Zen, B Ionescu; citation_volume=52; citation_issue=2; citation_publication_date=2019; citation_pages=1-37; citation_id=CR10"/>
    <meta name="citation_reference" content="citation_journal_title=AERA Open; citation_title=Do you get the picture? A meta-analysis of the effect of graphics on reading comprehension; citation_author=D Guo, S Zhang, KL Wright, EM McTigue; citation_volume=6; citation_issue=1; citation_publication_date=2020; citation_id=CR11"/>
    <meta name="citation_reference" content="citation_title=Multimedia learning; citation_inbook_title=Psychology of learning and motivation; citation_publication_date=2002; citation_pages=85-139; citation_id=CR12; citation_author=RE Mayer"/>
    <meta name="citation_reference" content="citation_title=Impact of interactive multimedia in e-learning technologies: role of multimedia in e-learning; citation_inbook_title=Digital multimedia: concepts, methodologies, tools, and applications; citation_publication_date=2018; citation_pages=1087-1110; citation_id=CR13; citation_author=A Khamparia; citation_author=B Pandey"/>
    <meta name="citation_reference" content="citation_journal_title=J Online Higher Educ; citation_title=Cognitive theory of multimedia learning; citation_author=M Rudolph; citation_volume=1; citation_issue=2; citation_publication_date=2017; citation_pages=1-10; citation_id=CR14"/>
    <meta name="citation_reference" content="citation_journal_title=Coll Res Libr; citation_title=Multimedia learning theories and online instruction; citation_author=N Tempelman-Kluit; citation_volume=67; citation_issue=4; citation_publication_date=2006; citation_pages=364-369; citation_id=CR15"/>
    <meta name="citation_reference" content="citation_journal_title=Learn Instr; citation_title=Increasing picture effects in learning from illustrated text; citation_author=J Peeck; citation_volume=3; citation_issue=3; citation_publication_date=1993; citation_pages=227-238; citation_id=CR16"/>
    <meta name="citation_reference" content="citation_journal_title=Br J Educ Technol; citation_title=Using extended captions to improve learning from instructional illustrations; citation_author=RM Bernard; citation_volume=21; citation_issue=3; citation_publication_date=1990; citation_pages=215-225; citation_id=CR17"/>
    <meta name="citation_reference" content="citation_title=The interestingness of images; citation_inbook_title=Proc. International conference on computer vision (ICCV); citation_publication_date=2013; citation_id=CR18; citation_author=M Gygli; citation_author=H Grabner; citation_author=H Riemenschneider; citation_author=F Nater; citation_author=L Gool"/>
    <meta name="citation_reference" content="citation_title=What makes an image popular?; citation_inbook_title=Proc. International world wide web conference (WWW); citation_publication_date=2014; citation_id=CR19; citation_author=A Khosla; citation_author=A Das Sarma; citation_author=R Hamid"/>
    <meta name="citation_reference" content="citation_title=Intrinsic image popularity assessment; citation_inbook_title=Proc. International conference on multimedia (MM); citation_publication_date=2019; citation_id=CR20; citation_author=K Ding; citation_author=K Ma; citation_author=S Wang"/>
    <meta name="citation_reference" content="citation_title=User-guided hierarchical attention network for multi-modal social image popularity prediction; citation_inbook_title=Proc. International world wide web conference (WWW); citation_publication_date=2018; citation_id=CR21; citation_author=W Zhang; citation_author=W Wang; citation_author=J Wang; citation_author=H Zha"/>
    <meta name="citation_reference" content="citation_title=Multi-evidence user group discovery in professional image search; citation_inbook_title=Proc. European conference on information retrieval (ECIR); citation_publication_date=2014; citation_id=CR22; citation_author=T Tsikrika; citation_author=C Diou"/>
    <meta name="citation_reference" content="
					Jansen BJ (2008) Searching for digital images on the web. J Doc
				"/>
    <meta name="citation_reference" content="citation_title=Analyzing and evaluating query reformulation strategies in web search logs; citation_inbook_title=Proc. Conference on information and knowledge management (CIKM); citation_publication_date=2009; citation_id=CR24; citation_author=J Huang; citation_author=EN Efthimiadis"/>
    <meta name="citation_reference" content="citation_journal_title=J Web Eng; citation_title=The effect of specialized multimedia collections on web searching; citation_author=BJ Jansen, A Spink, JO Pedersen; citation_volume=3; citation_issue=3&#8211;4; citation_publication_date=2004; citation_pages=182-199; citation_id=CR25"/>
    <meta name="citation_reference" content="citation_journal_title=Harv JL &amp; Tech; citation_title=The valuation of unprotected works: a case study of public domain images on Wikipedia; citation_author=P Heald, K Erickson, M Kretschmer; citation_volume=29; citation_issue=1; citation_publication_date=2015; citation_pages=1-31; citation_id=CR26"/>
    <meta name="citation_reference" content="citation_title=What is the commons worth? Estimating the value of wikimedia imagery by observing downstream use; citation_inbook_title=Proc. International symposium on open collaboration (OpenSym); citation_publication_date=2018; citation_id=CR27; citation_author=K Erickson; citation_author=FR Perez; citation_author=JR Perez"/>
    <meta name="citation_reference" content="citation_title=Dbpedia commons: structured multimedia metadata from the wikimedia commons; citation_inbook_title=Proc. International semantic web conference (ISWC); citation_publication_date=2015; citation_id=CR28; citation_author=G Vaidya; citation_author=D Kontokostas; citation_author=M Knuth; citation_author=J Lehmann; citation_author=S Hellmann"/>
    <meta name="citation_reference" content="citation_title=Imgpedia: a linked dataset with content-based analysis of wikimedia images; citation_inbook_title=Proc. International semantic web conference (ISWC); citation_publication_date=2017; citation_id=CR29; citation_author=S Ferrada; citation_author=B Bustos; citation_author=A Hogan"/>
    <meta name="citation_reference" content="
					Navarrete T, Villaespesa E (2020) Image-based information: paintings in Wikipedia. J Doc
				"/>
    <meta name="citation_reference" content="citation_title=Reader preferences and behavior on Wikipedia; citation_inbook_title=Proc. Conference on hypertext and social media (HT); citation_publication_date=2014; citation_id=CR31; citation_author=J Lehmann; citation_author=C M&#252;ller-Birn; citation_author=D Laniado; citation_author=M Lalmas; citation_author=A Kaltenbrunner"/>
    <meta name="citation_reference" content="citation_journal_title=First Monday; citation_title=What is popular on Wikipedia and why?; citation_author=A Spoerri; citation_volume=12; citation_issue=4; citation_publication_date=2007; citation_pages=1-6; citation_id=CR32"/>
    <meta name="citation_reference" content="citation_title=A large-scale study of Wikipedia users&#8217; quality of experience; citation_inbook_title=Proc. International world wide web conference (WWW); citation_publication_date=2019; citation_id=CR33; citation_author=F Salutari; citation_author=D Hora; citation_author=G Dubuc; citation_author=D Rossi"/>
    <meta name="citation_reference" content="citation_title=Why we read Wikipedia; citation_inbook_title=Proc. International world wide web conference (WWW); citation_publication_date=2017; citation_id=CR34; citation_author=P Singer; citation_author=F Lemmerich; citation_author=R West; citation_author=L Zia; citation_author=E Wulczyn; citation_author=M Strohmaier; citation_author=J Leskovec"/>
    <meta name="citation_reference" content="citation_title=Quantifying engagement with citations on Wikipedia; citation_inbook_title=Proc. The web conference (WWW); citation_publication_date=2020; citation_id=CR35; citation_author=T Piccardi; citation_author=M Redi; citation_author=G Colavizza; citation_author=R West"/>
    <meta name="citation_reference" content="citation_title=On the value of Wikipedia as a gateway to the web; citation_inbook_title=Proc. The web conference (WWW); citation_publication_date=2021; citation_id=CR36; citation_author=T Piccardi; citation_author=M Redi; citation_author=G Colavizza; citation_author=R West"/>
    <meta name="citation_reference" content="citation_title=Modeling delayed feedback in display advertising; citation_inbook_title=Proc. Conference on knowledge discovery and data mining (SIGKDD); citation_publication_date=2014; citation_id=CR37; citation_author=O Chapelle"/>
    <meta name="citation_reference" content="citation_title=Post-click conversion modeling and analysis for non-guaranteed delivery display advertising; citation_inbook_title=Proc. Conference on web search and data mining (WSDM); citation_publication_date=2012; citation_id=CR38; citation_author=R Rosales; citation_author=H Cheng; citation_author=E Manavoglu"/>
    <meta name="citation_reference" content="citation_title=Factorization machines with follow-the-regularized-leader for ctr prediction in display advertising; citation_inbook_title=Proc. International conference on big data (big data); citation_publication_date=2015; citation_id=CR39; citation_author=A-P Ta"/>
    <meta name="citation_reference" content="citation_title=Predicting clicks: estimating the click-through rate for new ads; citation_inbook_title=Proc. International world wide web conference (WWW); citation_publication_date=2007; citation_id=CR40; citation_author=M Richardson; citation_author=E Dominowska; citation_author=R Ragno"/>
    <meta name="citation_reference" content="citation_title=Deep character-level click-through rate prediction for sponsored search; citation_inbook_title=Proc. Conference on research and development in information retrieval (SIGIR); citation_publication_date=2017; citation_id=CR41; citation_author=B Edizel; citation_author=A Mantrach; citation_author=X Bai"/>
    <meta name="citation_reference" content="citation_title=Why the world reads Wikipedia: beyond English speakers; citation_inbook_title=Proc. International conference on web search and data mining (WSDM); citation_publication_date=2019; citation_id=CR42; citation_author=F Lemmerich; citation_author=D S&#225;ez-Trumper; citation_author=R West; citation_author=L Zia"/>
    <meta name="citation_reference" content="citation_journal_title=J Appl Psychol; citation_title=A new readability yardstick; citation_author=R Flesch; citation_volume=32; citation_issue=3; citation_publication_date=1948; citation_id=CR43"/>
    <meta name="citation_reference" content="citation_title=Improving website hyperlink structure using server logs; citation_inbook_title=Proc. Conference on web search and data mining (WSDM); citation_publication_date=2016; citation_id=CR44; citation_author=A Paranjape; citation_author=R West; citation_author=L Zia; citation_author=J Leskovec"/>
    <meta name="citation_reference" content="citation_title=An image is worth more than a thousand favorites: surfacing the hidden beauty of Flickr pictures; citation_inbook_title=International conference on web and social media (ICWSM); citation_publication_date=2015; citation_id=CR45; citation_author=R Schifanella; citation_author=M Redi; citation_author=LM Aiello"/>
    <meta name="citation_reference" content="citation_title=Rethinking the inception architecture for computer vision; citation_inbook_title=Proc. Conference on computer vision and pattern recognition (CVPR); citation_publication_date=2016; citation_id=CR46; citation_author=C Szegedy; citation_author=V Vanhoucke; citation_author=S Ioffe; citation_author=J Shlens; citation_author=Z Wojna"/>
    <meta name="citation_reference" content="citation_title=Imagenet: a large-scale hierarchical image database; citation_inbook_title=Proc. Conference on computer vision and pattern recognition (CVPR); citation_publication_date=2009; citation_id=CR47; citation_author=J Deng; citation_author=W Dong; citation_author=R Socher; citation_author=L-J Li; citation_author=K Li; citation_author=L Fei-Fei"/>
    <meta name="citation_reference" content="
					Huh M, Agrawal P, Efros AA (2016) What makes imagenet good for transfer learning? arXiv preprint 
                  1608.08614
                  
                "/>
    <meta name="citation_reference" content="citation_title=Multilingual visual sentiment concept matching; citation_inbook_title=Proc. International conference on multimedia retrieval (ICMR); citation_publication_date=2016; citation_id=CR49; citation_author=N Pappas; citation_author=M Redi; citation_author=M Topkara; citation_author=B Jou; citation_author=H Liu; citation_author=T Chen; citation_author=S-F Chang"/>
    <meta name="citation_reference" content="citation_title=A discriminative feature learning approach for deep face recognition; citation_inbook_title=Proc. European conference on computer vision (ECCV); citation_publication_date=2016; citation_id=CR50; citation_author=Y Wen; citation_author=K Zhang; citation_author=Z Li; citation_author=Y Qiao"/>
    <meta name="citation_reference" content="
					Zagoruyko S, Komodakis N (2016) Wide residual networks. arXiv preprint 
                  1605.07146
                  
                "/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Places: a 10 million image database for scene recognition; citation_author=B Zhou, A Lapedriza, A Khosla, A Oliva, A Torralba; citation_volume=40; citation_issue=6; citation_publication_date=2017; citation_pages=1452-1464; citation_id=CR52"/>
    <meta name="citation_reference" content="citation_title=Applied linear statistical models; citation_publication_date=2005; citation_id=CR53; citation_author=MH Kutner; citation_author=CJ Nachtsheim; citation_author=J Neter; citation_author=W Li"/>
    <meta name="citation_reference" content="citation_title=Density-based clustering based on hierarchical density estimates; citation_inbook_title=Proc. Pacific-Asia conference on knowledge discovery and data mining (PAKDD); citation_publication_date=2013; citation_id=CR54; citation_author=RJ Campello; citation_author=D Moulavi; citation_author=J Sander"/>
    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Inf Theory; citation_title=Least squares quantization in pcm; citation_author=S Lloyd; citation_volume=28; citation_issue=2; citation_publication_date=1982; citation_pages=129-137; citation_id=CR55"/>
    <meta name="citation_reference" content="citation_title=A density-based algorithm for discovering clusters in large spatial databases with noise; citation_inbook_title=International conference on knowledge discovery and data mining (KDD); citation_publication_date=1996; citation_id=CR56; citation_author=M Ester; citation_author=H-P Kriegel; citation_author=J Sander; citation_author=X Xu"/>
    <meta name="citation_reference" content="citation_journal_title=J Open Sour Softw; citation_title=hdbscan: hierarchical density based clustering; citation_author=L McInnes, J Healy, S Astels; citation_volume=2; citation_issue=11; citation_publication_date=2017; citation_id=CR57"/>
    <meta name="citation_reference" content="citation_title=Density-based clustering validation; citation_inbook_title=Proc. SIAM international conference on data mining (SDM); citation_publication_date=2014; citation_id=CR58; citation_author=D Moulavi; citation_author=PA Jaskowiak; citation_author=RJ Campello; citation_author=A Zimek; citation_author=J Sander"/>
    <meta name="citation_reference" content="citation_journal_title=Econometrica; citation_title=Large sample properties of matching estimators for average treatment effects; citation_author=A Abadie, GW Imbens; citation_volume=74; citation_issue=1; citation_publication_date=2006; citation_pages=235-267; citation_id=CR59"/>
    <meta name="citation_reference" content="citation_title=Extracting event-related information from article updates in Wikipedia; citation_inbook_title=Proc. European conference on information retrieval (ECIR); citation_publication_date=2013; citation_id=CR60; citation_author=M Georgescu; citation_author=N Kanhabua; citation_author=D Krause; citation_author=W Nejdl; citation_author=S Siersdorfer"/>
    <meta name="citation_reference" content="citation_journal_title=Psychol Rev; citation_title=Conspec and conlern: a two-process theory of infant face recognition; citation_author=J Morton, MH Johnson; citation_volume=98; citation_issue=2; citation_publication_date=1991; citation_id=CR61"/>
    <meta name="citation_reference" content="
					Redi M, Gerlach M, Johnson I, Morgan J, Zia L (2020) A taxonomy of knowledge gaps for wikimedia projects (second draft). arXiv preprint 
                  2008.12314
                  
                "/>
    <meta name="citation_reference" content="citation_title=Cultural diffusion and trends in Facebook photographs; citation_inbook_title=Proc. International conference on web and social media (ICWSM); citation_publication_date=2017; citation_id=CR63; citation_author=Q You; citation_author=D Garc&#237;a-Garc&#237;a; citation_author=M Paluri; citation_author=J Luo; citation_author=J Joo"/>
    <meta name="citation_reference" content="citation_title=Crosslingual topic modeling with WikiPDA; citation_inbook_title=Proc. The web conference (WWW); citation_publication_date=2021; citation_id=CR64; citation_author=T Piccardi; citation_author=R West"/>
    <meta name="citation_author" content="Rama, Daniele"/>
    <meta name="citation_author_institution" content="University of Turin, Turin, Italy"/>
    <meta name="citation_author" content="Piccardi, Tiziano"/>
    <meta name="citation_author_institution" content="&#201;cole polytechnique f&#233;d&#233;rale de Lausanne (EPFL), Lausanne, Switzerland"/>
    <meta name="citation_author" content="Redi, Miriam"/>
    <meta name="citation_author_institution" content="Wikimedia Foundation, San Francisco, USA"/>
    <meta name="citation_author" content="Schifanella, Rossano"/>
    <meta name="citation_author_institution" content="University of Turin, Turin, Italy"/>
    <meta name="citation_author_institution" content="ISI Foundation, Turin, Italy"/>
    


        <meta name="format-detection" content="telephone=no">

        
<link rel="apple-touch-icon" sizes="180x180" href=/static/img/favicons/springer/apple-touch-icon-1ee298e6a2.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/img/favicons/springer/favicon-32x32-da5cbe81a8.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/img/favicons/springer/favicon-16x16-b154d15d9a.png>
<link rel="manifest" href=/static/app-springeropen/manifest-f52de3c2a4.json>
<link rel="mask-icon" href=/static/img/favicons/springer/safari-pinned-tab-d9da0ac5a2.svg color="#e6e6e6">
<link rel="shortcut icon" data-test="shortcut-icon" href=/static/img/favicons/springer/favicon-d1430c5053.ico>
<meta name="msapplication-config" content=/static/app-springeropen/browserconfig-9445b29a7c.xml>
<meta name="msapplication-TileColor"
      content="#e6e6e6">
<meta name="msapplication-TileImage"
      content=/static/img/favicons/springer/app-icon-144x144.png>
<meta name="theme-color"
      content="#e6e6e6">


        
        <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

        


    
        <link rel="stylesheet" media="screen" href=/static/app-springeropen/css/core-article-945374050f.css>
    

    <link rel="stylesheet" media="screen" href=/static/app-springeropen/css/core-0f01bd1b5a.css>
    <link rel="stylesheet" media="print" href=/static/app-springeropen/css/print-b8af42253b.css>
    

    

    
        <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) { h1{font-size:2.25rem;font-style:normal;margin-bottom:1em}h1,h2{font-family:Georgia,Palatino,serif;font-weight:400}h2{font-size:1.75rem}.u-h3,h2{font-style:normal;margin-bottom:1em}.u-h3{line-height:1.4}.u-h3,h3{font-family:Georgia,Palatino,serif;font-size:1.5rem;font-weight:400}.u-h4{line-height:1.4}.u-h4,h3{font-style:normal;margin-bottom:1em}.u-h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:700}button{line-height:inherit}html,label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}html{-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}*{box-sizing:inherit}body{background:#fcfcfc;margin:0;max-width:100%;min-height:100%}button,div,form,input{margin:0;padding:0}body,p{padding:0}a{color:#004b83;text-decoration:underline;text-decoration-skip-ink:auto}a>img{vertical-align:middle}h1,h2,h3{line-height:1.4}p{margin:0}h1,h2,h3,ul{margin-top:0}p{margin-bottom:1.5em}p:last-child{margin-bottom:0}p:empty{display:none}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label,.c-banner{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-ad__label{color:#333;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-banner{background-color:#f7fbfe;border-bottom:1px solid #003b84;font-size:1rem;margin:0 auto;padding:16px 12px;position:relative;text-align:center}.c-banner--marketing .c-banner__link{color:#fff}.c-banner--marketing{background-color:#004b83;border-bottom:0;color:#fff}.c-banner__link{color:#004b83;text-decoration:underline}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-dropdown__button:after{border-color:transparent transparent transparent #fff;border-style:solid;border-width:4px 0 4px 14px;content:"";display:block;height:0;margin-left:3px;width:0}.c-dropdown{display:inline-block;position:relative}.c-dropdown__button{background-color:transparent;border:0;display:inline-block;padding:0;white-space:nowrap}.c-dropdown__button:after{border-color:currentcolor transparent transparent;border-width:5px 4px 0 5px;display:inline-block;margin-left:8px;vertical-align:middle}.c-dropdown__menu{background-color:#fff;border:1px solid #d9d9d9;border-radius:3px;box-shadow:0 2px 6px rgba(0,0,0,.1);font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;line-height:1.4;list-style:none;margin:0;padding:8px 0;position:absolute;top:100%;transform:translateY(8px);width:180px;z-index:100}.c-dropdown__menu:after,.c-dropdown__menu:before{border-style:solid;bottom:100%;content:"";display:block;height:0;left:16px;position:absolute;width:0}.c-dropdown__menu:before{border-color:transparent transparent #d9d9d9;border-width:0 9px 9px;transform:translateX(-1px)}.c-dropdown__menu:after{border-color:transparent transparent #fff;border-width:0 8px 8px}.c-dropdown__menu--right{left:auto;right:0}.c-dropdown__menu--right:after,.c-dropdown__menu--right:before{left:auto;right:16px}.c-dropdown__menu--right:before{transform:translateX(1px)}.c-dropdown__link{background-color:transparent;color:#004b83;display:block;padding:4px 16px;text-decoration:none}.c-header{background-color:#fff;border-bottom:4px solid #00285a;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;padding:16px 0}.c-header__container,.c-header__menu{align-items:center;display:flex;flex-wrap:wrap}@supports (gap:2em){.c-header__container,.c-header__menu{gap:2em 2em}}.c-header__menu{list-style:none;margin:0;padding:0}.c-header__item{color:inherit}@supports not (gap:2em){.c-header__item{margin-left:24px}}.c-header__container{justify-content:space-between;margin:0 auto;max-width:1280px;padding:0 16px}@supports not (gap:2em){.c-header__brand{margin-right:32px}}.c-header__brand a{display:block;text-decoration:none}.c-header__link{color:inherit;text-decoration:none}.c-form-field{margin-bottom:1em}.c-form-field__label{color:#666;display:block;font-size:.875rem;margin-bottom:.4em}.c-form-field__input{border:1px solid #b3b3b3;border-radius:3px;box-shadow:inset 0 1px 3px 0 rgba(0,0,0,.21);font-size:.875rem;line-height:1.28571;padding:.75em 1em;vertical-align:middle;width:100%}.c-nav{display:flex;flex-wrap:wrap;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.42857;list-style:none;margin:0;padding:0;word-break:normal}.c-nav__link{display:block;padding:.4em 1em;text-decoration:none}.c-journal-header__title>a,.c-nav__link{color:inherit}.c-nav__link.is-active{font-weight:700}.c-nav--tabs{border-bottom:1px solid #ccc;flex-wrap:nowrap;text-align:center}.c-nav--tabs .c-nav__item:not(:first-child){margin-left:-1px}.c-nav--tabs .c-nav__link{background-color:#e6e6e6;border-bottom-color:initial;border-bottom-style:none;border-bottom-width:0;border-image:initial;border-left:1px solid #e6e6e6;border-right:1px solid #e6e6e6;border-top:1px solid #e6e6e6;height:100%;padding:.7em 1.8em;position:relative;transition:border-left-color .2s ease 0s,border-top-color .2s ease 0s,border-right-color .2s ease 0s,background-color .2s ease 0s}.c-nav--tabs .c-nav__link.is-active{background-color:#fcfcfc;border-color:#ee7d11 #ee7d11 transparent;position:relative;z-index:1}.c-nav--tabs .c-nav__link.is-active:after{background-color:#fcfcfc;content:"";display:block;height:1px;left:0;position:absolute;right:0;top:100%}.c-popup-search{background-color:#f2f2f2;box-shadow:0 3px 3px -3px rgba(0,0,0,.21);padding:16px 0;position:relative;z-index:10}@media only screen and (min-width:1024px){.js .c-popup-search{position:absolute;top:100%;width:100%}.c-popup-search__container{margin:auto;max-width:70%}}.ctx-search .c-form-field{margin-bottom:0}.ctx-search .c-form-field__input{border-bottom-right-radius:0;border-top-right-radius:0;margin-right:0}.c-journal-header{background-color:#f2f2f2;padding-top:16px}.c-journal-header__title{font-size:1.3125rem;margin:0 0 16px}.c-journal-header__grid{column-gap:1.25rem;display:grid;grid-template-areas:"main" "side";grid-template-columns:1fr;width:100%}@media only screen and (min-width:768px){.c-journal-header__grid{column-gap:1.25rem;grid-template-areas:"main side";grid-template-columns:1fr 160px}}@media only screen and (min-width:1024px){.c-journal-header__grid{column-gap:3.125rem;grid-template-areas:"main side";grid-template-columns:1fr 190px}}@media only screen and (min-width:768px){.c-journal-header__grid-main{margin:0!important;width:auto!important}}.c-journal-header__grid-main{grid-area:main/main/main/main}.ctx-journal-header .c-nav--tabs{border-bottom:0}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button svg,.u-button--primary svg{fill:currentcolor}.u-button{color:#004b83}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-display-flex{display:flex;width:100%}.u-align-items-center{align-items:center}.u-align-items-baseline{align-items:baseline}.u-justify-content-space-between{justify-content:space-between}.u-flex-static{flex:0 0 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-position-relative{position:relative}.u-mt-32{margin-top:32px}.u-mr-24{margin-right:24px}.u-mr-48{margin-right:48px}.u-mb-32{margin-bottom:32px}.u-ml-8{margin-left:8px}.u-button-reset{background-color:transparent;border:0;padding:0}.u-text-sm{font-size:1rem}.u-vh-full{min-height:100vh}.u-hide{display:none;visibility:hidden}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}}@media only screen and (max-width:1023px){.u-hide-at-lt-lg{display:none;visibility:hidden}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}body{font-size:1.125rem}.c-header__navigation{display:flex;gap:.5rem .5rem} }</style>

        <link rel="stylesheet" data-inline-css-source="critical-css" href="/static/app-springeropen/css/enhanced-873f8344ac.css"
              media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">

        <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) { h1{margin-bottom:1em}.c-article-associated-content__container .c-article-associated-content__collection-label,h1,h2{font-family:Georgia,Palatino,serif;font-style:normal}.u-h3,h2{margin-bottom:1em}.u-h3{font-family:Georgia,Palatino,serif;font-style:normal;line-height:1.4}.u-h4,h3{margin-bottom:1em}h3{font-family:Georgia,Palatino,serif;font-style:normal}.c-article-associated-content__container .c-article-associated-content__collection-label a,.c-article-change-list__item>ins,.c-article-editorial-summary__container .c-article-editorial-summary__article-title a,.c-card__title a,.c-reading-companion__figure-title a,.u-h3 a,.u-h4 a,abbr[title],h3 a,h4 a,h5 a,h6 a{text-decoration:none}.c-article-associated-content__container .c-article-associated-content__collection-label a.active,.c-article-associated-content__container .c-article-associated-content__collection-label a.hover,.c-article-associated-content__container .c-article-associated-content__collection-label a:active,.c-article-associated-content__container .c-article-associated-content__collection-label a:hover,.c-article-editorial-summary__container .c-article-editorial-summary__article-title a.active,.c-article-editorial-summary__container .c-article-editorial-summary__article-title a.hover,.c-article-editorial-summary__container .c-article-editorial-summary__article-title a:active,.c-article-editorial-summary__container .c-article-editorial-summary__article-title a:hover,.c-card__title a.active,.c-card__title a.hover,.c-card__title a:active,.c-card__title a:hover,.c-reading-companion__figure-title a.active,.c-reading-companion__figure-title a.hover,.c-reading-companion__figure-title a:active,.c-reading-companion__figure-title a:hover,.u-h3 a.active,.u-h3 a.hover,.u-h3 a:active,.u-h3 a:hover,.u-h4 a.active,.u-h4 a.hover,.u-h4 a:active,.u-h4 a:hover,h3 a.active,h3 a.hover,h3 a:active,h3 a:hover,h4 a.active,h4 a.hover,h4 a:active,h4 a:hover,h5 a.active,h5 a.hover,h5 a:active,h5 a:hover,h6 a.active,h6 a.hover,h6 a:active,h6 a:hover{text-decoration:underline;text-decoration-skip-ink:auto}.u-h4{line-height:1.4}.c-reading-companion__figure-title,.u-h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-style:normal}.c-reading-companion__figure-title{line-height:1.4}button{line-height:inherit}html,label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}html{-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}*{box-sizing:inherit}body{background:#fcfcfc;margin:0;max-width:100%;min-height:100%}button,div,form,input{margin:0;padding:0}body,p{padding:0}a{color:#004b83;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}a>img{vertical-align:middle}h1,h2,h3{line-height:1.4}p{margin:0}h1,h2,h3,ul{margin-top:0}p{margin-bottom:1.5em;overflow-wrap:break-word;word-break:break-word}p:last-child{margin-bottom:0}p:empty{display:none}h1{font-size:2.25rem}h1,h2{font-weight:700}h2{font-size:1.75rem}.c-article-associated-content__container .c-article-associated-content__collection-label,.u-h3,h3{font-weight:700}.u-h3,h3{font-size:1.5rem}.c-reading-companion__figure-title,.u-h4{font-size:1.25rem;font-weight:700}body{font-size:1.125rem}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-identifiers__open{color:#b74616}.c-article-title{font-size:1.5rem;line-height:1.25;margin-bottom:16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{margin-left:0}.c-article-author-list li,.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list__item svg{margin-left:4px}.c-article-author-list__show-more{margin-right:4px}@media only screen and (max-width:539px){html.js .js-small-screen-show-inline{display:inline!important;visibility:visible!important}.js .js-smaller-author-etal{display:none;visibility:hidden}}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-button-author-list svg{margin:1px 4px 0 0}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:0 0 16px}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;line-height:1.3;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article__sub-heading{color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;font-style:normal;font-weight:400;line-height:1.3;margin:0 0 8px}@media only screen and (min-width:768px){.c-article__sub-heading{font-size:1.5rem;line-height:1.24}}.c-reading-companion__figure-full-link svg{margin-left:4px}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;display:inline-block;font-size:.875rem;font-weight:700;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__figures-list,.c-reading-companion__references-list,.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{display:inline-block}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-reading-companion__panel--active{display:block}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%;padding:13px 24px!important}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.hide{display:none;visibility:hidden}@media only screen and (max-width:30em){.js .js-smaller-author-etal{display:none;visibility:hidden}}.c-journal-header__title>a{color:inherit}.c-article-associated-content__container .c-article-associated-content__collection.collection~.c-article-associated-content__collection.collection .c-article-associated-content__collection-label,.c-article-associated-content__container .c-article-associated-content__collection.section~.c-article-associated-content__collection.section .c-article-associated-content__collection-label,.c-article-associated-content__container .c-article-associated-content__title{display:none}.c-article-associated-content__container a{text-decoration:underline}.c-article-associated-content__container .c-article-associated-content__collection.collection .c-article-associated-content__collection-label,.c-article-associated-content__container .c-article-associated-content__collection.section .c-article-associated-content__collection-label{display:block}.c-article-associated-content__container .c-article-associated-content__collection.collection,.c-article-associated-content__container .c-article-associated-content__collection.section{margin-bottom:5px}.c-article-associated-content__container .c-article-associated-content__collection.section~.c-article-associated-content__collection.collection{margin-top:28px}.c-article-associated-content__container .c-article-associated-content__collection:first-child{margin-top:0}.c-article-associated-content__container .c-article-associated-content__collection-label{color:#1b3051;margin-bottom:8px}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.0625rem;font-weight:400} }</style>
        <link rel="stylesheet" data-inline-css-source="critical-css" href="/static/app-springeropen/css/enhanced-article-b9540b0172.css"
              media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
    



        

        
    <script type="text/javascript">
        config = {
            env: 'live',
            site: 'epjdatascience.springeropen.com',
            siteWithPath: 'epjdatascience.springeropen.com' + window.location.pathname,
            twitterHashtag: '',
            cmsPrefix: 'https://studio-cms.springernature.com/studio/',
            
            doi: '10.1140/epjds/s13688-021-00312-8',
            figshareScriptUrl: 'https://widgets.figshare.com/static/figshare.js',
            hasFigshareInvoked: false,
            publisherBrand: 'SpringerOpen',
            mustardcut: false
        };
    </script>

        
    <script type="text/javascript" data-test="dataLayer">
        window.dataLayer = [{"content":{"article":{"doi":"10.1140/epjds/s13688-021-00312-8","articleType":"Regular article","peerReviewType":"Closed","supplement":null,"keywords":"Wikipedia;Images;Computer vision;User behavior"},"contentInfo":{"imprint":"SpringerOpen","title":"A large scale study of reader interactions with images on Wikipedia","publishedAt":1641168000000,"publishedAtDate":"2022-01-03","author":["Daniele Rama","Tiziano Piccardi","Miriam Redi","Rossano Schifanella"],"collection":[]},"attributes":{"deliveryPlatform":"oscar","template":"classic","cms":null,"copyright":{"creativeCommonsType":"CC BY","openAccess":true},"environment":"live"},"journal":{"siteKey":"epjdatascience.springeropen.com","volume":"11","issue":"1","title":"EPJ Data Science","type":null,"journalID":13688,"section":[]},"category":{"pmc":{"primarySubject":"Computer Science"},"contentType":"Regular article","publishingSegment":"PhysApp2"}},"session":{"authentication":{"authenticationID":[]}},"version":"1.0.0","page":{"category":{"pageType":"article"},"attributes":{"featureFlags":[],"environment":"live"}},"event":"dataLayerCreated"}];
    </script>

        

        
            <script>
                window.dataLayer.push({
                    cmpAndNewGtmFeatureFlag: true
                });
            </script>
        

        
        
        
            <script>
                window.dataLayer.push({
                    ga4MeasurementId: 'G-PJCTJWPV25',
                    ga360TrackingId: 'UA-54492316-9',
                    twitterId: 'o47a2',
                    ga4ServerUrl: 'https://collect.biomedcentral.com',
                    imprint: 'springeropen'
                });
            </script>
        

        
    <script>
        (function(w, d) {
            w.config = w.config || {};
            w.config.mustardcut = false;

            
            if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
                w.config.mustardcut = true;
                d.classList.add('js');
                d.classList.remove('grade-c');
            }
        })(window, document.documentElement);
    </script>



        
<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>

        
    <script class="js-entry">
        if (window.config.mustardcut) {
            (function(w, d) {
                
                
                
                    window.Component = {};
                    window.suppressShareButton = true;
                

                var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

                
                function catchNoModuleSupport() {
                    var scriptEl = d.createElement('script');
                    return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
                }

                var headScripts = [
                    {'src': '/static/js/polyfill-es5-bundle-78e0640118.js', 'async': false},
                    {'src': '/static/js/airbrake-es5-bundle-e17dddfbcf.js', 'async': false},
                ];

                var bodyScripts = [
                    {'src': '/static/js/app-es5-bundle-121e04b11b.js', 'async': false, 'module': false},
                    {'src': '/static/js/app-es6-bundle-aaec8ca904.js', 'async': false, 'module': true}
                    
                    
                        , {'src': '/static/js/global-article-es5-bundle-e1b712d921.js', 'async': false, 'module': false},
                        {'src': '/static/js/global-article-es6-bundle-f3b4f7e32b.js', 'async': false, 'module': true}
                    
                ];

                function createScript(script) {
                    var scriptEl = d.createElement('script');
                    scriptEl.src = script.src;
                    scriptEl.async = script.async;
                    if (script.module === true) {
                        scriptEl.type = "module";
                        if (catchNoModuleSupport()) {
                            scriptEl.src = '';
                        }
                    } else if (script.module === false) {
                        scriptEl.setAttribute('nomodule', true)
                    }
                    if (script.charset) {
                        scriptEl.setAttribute('charset', script.charset);
                    }

                    return scriptEl;
                }

                for (var i = 0; i < headScripts.length; ++i) {
                    var scriptEl = createScript(headScripts[i]);
                    currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
                }

                d.addEventListener('DOMContentLoaded', function() {
                    for (var i = 0; i < bodyScripts.length; ++i) {
                        var scriptEl = createScript(bodyScripts[i]);
                        d.body.appendChild(scriptEl);
                    }
                });

                // Webfont repeat view
                var config = w.config;
                if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                    d.documentElement.className += ' webfonts-loaded';
                }
            })(window, document);
        }
    </script>




        
            
            
            
                <script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                
                    j.src = 'https://collect.biomedcentral.com/gtm.js?id=' + i + dl;
                
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
            
        

        <script>
    (function (w, d, t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
            s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                e.src = 'https://push-content.springernature.io/pcf_sb_5_1617714720898560639/production_live/consent-bundle-17-9.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else if (h.indexOf('biomedcentral.com') > -1) {
                e.src = 'https://push-content.springernature.io/pcf_sb_5_1617714720898560639/production_live/consent-bundle-15-9.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else if (h.indexOf('springeropen.com') > -1) {
                e.src = 'https://push-content.springernature.io/pcf_sb_5_1617714720898560639/production_live/consent-bundle-16-9.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-2b0f06c1e4.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window, document, 'script');
</script>


        
    
        
    


        
    
    <link rel="canonical" href="//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00312-8"/>
    

        
        
        
    
    <meta property="og:url" content="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00312-8"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="SpringerOpen"/>
    <meta property="og:title" content="A large scale study of reader interactions with images on Wikipedia - EPJ Data Science"/>
    <meta property="og:description" content="Wikipedia is the largest source of free encyclopedic knowledge and one of the most visited sites on the Web. To increase reader understanding of the article, Wikipedia editors add images within the text of the article’s body. However, despite their widespread usage on web platforms and the huge volume of visual content on Wikipedia, little is known about the importance of images in the context of free knowledge environments. To bridge this gap, we collect data about English Wikipedia reader interactions with images during one month and perform the first large-scale analysis of how interactions with images happen on Wikipedia. First, we quantify the overall engagement with images, finding that one in 29 pageviews results in a click on at least one image, one order of magnitude higher than interactions with other types of article content. Second, we study what factors associate with image engagement and observe that clicks on images occur more often in shorter articles and articles about visual arts or transports and biographies of less well-known people. Third, we look at interactions with Wikipedia article previews and find that images help support reader information need when navigating through the site, especially for more popular pages. The findings in this study deepen our understanding of the role of images for free knowledge and provide a guide for Wikipedia editors and web user communities to enrich the world’s largest source of encyclopedic knowledge."/>
    <meta property="og:image" content="https://media.springernature.com/w200/springer-static/cover/journal/13688.jpg"/>
    

        

        
    <script type="application/ld+json">{"mainEntity":{"headline":"A large scale study of reader interactions with images on Wikipedia","description":"Wikipedia is the largest source of free encyclopedic knowledge and one of the most visited sites on the Web. To increase reader understanding of the article, Wikipedia editors add images within the text of the article’s body. However, despite their widespread usage on web platforms and the huge volume of visual content on Wikipedia, little is known about the importance of images in the context of free knowledge environments. To bridge this gap, we collect data about English Wikipedia reader interactions with images during one month and perform the first large-scale analysis of how interactions with images happen on Wikipedia. First, we quantify the overall engagement with images, finding that one in 29 pageviews results in a click on at least one image, one order of magnitude higher than interactions with other types of article content. Second, we study what factors associate with image engagement and observe that clicks on images occur more often in shorter articles and articles about visual arts or transports and biographies of less well-known people. Third, we look at interactions with Wikipedia article previews and find that images help support reader information need when navigating through the site, especially for more popular pages. The findings in this study deepen our understanding of the role of images for free knowledge and provide a guide for Wikipedia editors and web user communities to enrich the world’s largest source of encyclopedic knowledge.","datePublished":"2022-01-03","dateModified":"2022-01-03","pageStart":"1","pageEnd":"29","license":"http://creativecommons.org/licenses/by/4.0/","sameAs":"https://doi.org/10.1140/epjds/s13688-021-00312-8","keywords":"Computer Appl. in Social and Behavioral Sciences,Data-driven Science,Modeling and Theory Building,Complexity","image":"https://static-content.springer.com/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig1_HTML.jpg","isPartOf":{"name":"EPJ Data Science","issn":["2193-1127"],"volumeNumber":"11","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Springer Berlin Heidelberg","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Rama, Daniele","url":"http://orcid.org/0000-0001-9419-7315","affiliation":[{"name":"University of Turin","address":{"name":"University of Turin, Turin, Italy","@type":"PostalAddress"},"@type":"Organization"}],"email":"daniele.rama@unito.it","@type":"Person"},{"name":"Piccardi, Tiziano","url":"http://orcid.org/0000-0001-9314-1440","affiliation":[{"name":"École polytechnique fédérale de Lausanne (EPFL)","address":{"name":"École polytechnique fédérale de Lausanne (EPFL), Lausanne, Switzerland","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Redi, Miriam","url":"http://orcid.org/0000-0002-0581-0251","affiliation":[{"name":"Wikimedia Foundation","address":{"name":"Wikimedia Foundation, San Francisco, USA","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Schifanella, Rossano","url":"http://orcid.org/0000-0002-3745-5792","affiliation":[{"name":"University of Turin","address":{"name":"University of Turin, Turin, Italy","@type":"PostalAddress"},"@type":"Organization"},{"name":"ISI Foundation","address":{"name":"ISI Foundation, Turin, Italy","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>

        
    </head>

    <body class="journal journal-fulltext"
    
          >
        
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript>
                <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                        height="0" width="0" style="display:none;visibility:hidden"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    

        <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m1.00087166 7h5.99912834v-5.99912834c0-.55276615.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v5.99912834h5.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-5.9991283v5.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-5.9991283h-5.99912834c-.55276615 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-facebook-bordered" viewBox="463.812 263.868 32 32"><path d="M479.812,263.868c-8.837,0-16,7.163-16,16s7.163,16,16,16s16-7.163,16-16S488.649,263.868,479.812,263.868z
	 M479.812,293.868c-7.732,0-14-6.269-14-14s6.268-14,14-14s14,6.269,14,14S487.545,293.868,479.812,293.868z"/><path d="M483.025,280.48l0.32-2.477h-2.453v-1.582c0-0.715,0.199-1.207,1.227-1.207h1.311v-2.213
	c-0.227-0.029-1.003-0.098-1.907-0.098c-1.894,0-3.186,1.154-3.186,3.271v1.826h-2.142v2.477h2.142v6.354h2.557v-6.354
	L483.025,280.48L483.025,280.48z"/></symbol><symbol id="icon-twitter-bordered" viewBox="463.812 263.868 32 32"><g><path d="M486.416,276.191c-0.483,0.215-1.007,0.357-1.554,0.429c0.558-0.338,0.991-0.868,1.19-1.502
		c-0.521,0.308-1.104,0.536-1.72,0.657c-0.494-0.526-1.2-0.854-1.979-0.854c-1.496,0-2.711,1.213-2.711,2.71
		c0,0.212,0.023,0.419,0.069,0.616c-2.252-0.111-4.25-1.19-5.586-2.831c-0.231,0.398-0.365,0.866-0.365,1.361
		c0,0.94,0.479,1.772,1.204,2.257c-0.441-0.015-0.861-0.138-1.227-0.339v0.031c0,1.314,0.937,2.41,2.174,2.656
		c-0.227,0.062-0.47,0.098-0.718,0.098c-0.171,0-0.343-0.018-0.511-0.049c0.35,1.074,1.347,1.859,2.531,1.883
		c-0.928,0.726-2.095,1.16-3.366,1.16c-0.22,0-0.433-0.014-0.644-0.037c1.2,0.768,2.621,1.215,4.155,1.215
		c4.983,0,7.71-4.129,7.71-7.711c0-0.115-0.004-0.232-0.006-0.351C485.592,277.212,486.054,276.734,486.416,276.191z"/></g><path d="M479.812,263.868c-8.837,0-16,7.163-16,16s7.163,16,16,16s16-7.163,16-16S488.649,263.868,479.812,263.868z
	 M479.812,293.868c-7.732,0-14-6.269-14-14s6.268-14,14-14s14,6.269,14,14S487.545,293.868,479.812,293.868z"/></symbol><symbol id="icon-weibo-bordered" viewBox="463.812 263.868 32 32"><path d="M479.812,263.868c-8.838,0-16,7.163-16,16s7.162,16,16,16c8.837,0,16-7.163,16-16S488.649,263.868,479.812,263.868z
	 M479.812,293.868c-7.732,0-14-6.269-14-14s6.268-14,14-14c7.731,0,14,6.269,14,14S487.545,293.868,479.812,293.868z"/><g><path d="M478.552,285.348c-2.616,0.261-4.876-0.926-5.044-2.649c-0.167-1.722,1.814-3.33,4.433-3.588
		c2.609-0.263,4.871,0.926,5.041,2.647C483.147,283.479,481.164,285.089,478.552,285.348 M483.782,279.63
		c-0.226-0.065-0.374-0.109-0.259-0.403c0.25-0.639,0.276-1.188,0.005-1.581c-0.515-0.734-1.915-0.693-3.521-0.021
		c0,0-0.508,0.224-0.378-0.181c0.247-0.798,0.209-1.468-0.178-1.852c-0.87-0.878-3.194,0.032-5.183,2.027
		c-1.489,1.494-2.357,3.082-2.357,4.453c0,2.619,3.354,4.213,6.631,4.213c4.297,0,7.154-2.504,7.154-4.493
		C485.697,280.594,484.689,279.911,483.782,279.63"/><path d="M486.637,274.833c-1.039-1.154-2.57-1.592-3.982-1.291l0,0c-0.325,0.068-0.532,0.391-0.465,0.72
		c0.068,0.328,0.391,0.537,0.72,0.466c1.005-0.215,2.092,0.104,2.827,0.92c0.736,0.818,0.938,1.939,0.625,2.918l0,0
		c-0.102,0.318,0.068,0.661,0.39,0.762c0.32,0.104,0.658-0.069,0.763-0.391v-0.001C487.953,277.558,487.674,275.985,486.637,274.833
		"/><path d="M485.041,276.276c-0.504-0.562-1.25-0.774-1.938-0.63c-0.279,0.06-0.461,0.339-0.396,0.621
		c0.062,0.281,0.335,0.461,0.617,0.398l0,0c0.336-0.071,0.702,0.03,0.947,0.307s0.312,0.649,0.207,0.979l0,0
		c-0.089,0.271,0.062,0.565,0.336,0.654c0.274,0.09,0.564-0.062,0.657-0.336C485.686,277.604,485.549,276.837,485.041,276.276"/><path d="M478.694,282.227c-0.09,0.156-0.293,0.233-0.451,0.166c-0.151-0.062-0.204-0.235-0.115-0.389
		c0.093-0.155,0.284-0.229,0.44-0.168C478.725,281.892,478.782,282.071,478.694,282.227 M477.862,283.301
		c-0.253,0.405-0.795,0.58-1.202,0.396c-0.403-0.186-0.521-0.655-0.27-1.051c0.248-0.39,0.771-0.566,1.176-0.393
		C477.979,282.423,478.109,282.889,477.862,283.301 M478.812,280.437c-1.244-0.326-2.65,0.294-3.19,1.396
		c-0.553,1.119-0.021,2.369,1.236,2.775c1.303,0.42,2.84-0.225,3.374-1.436C480.758,281.989,480.1,280.77,478.812,280.437"/></g></symbol><symbol id="icon-arrow-left-bullet" viewBox="0 0 8 16"><path d="M3 8l5 5v3L0 8l8-8v3L3 8z"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 3)"/></symbol><symbol id="icon-download-rounded"><path d="M0 13c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H1.002A1.006 1.006 0 010 13zM7 1v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L1.115 6.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L5 7.8V1c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z"/></symbol><symbol id="icon-ext-link" viewBox="0 0 16 16"><path d="M12.9 16H3.1C1.4 16 0 14.6 0 12.9V3.2C0 1.4 1.4 0 3.1 0h3.7v1H3.1C2 1 1 2 1 3.2v9.7C1 14 2 15 3.1 15h9.7c1.2 0 2.1-1 2.1-2.1V8.7h1v4.2c.1 1.7-1.3 3.1-3 3.1z"/><path d="M12.8 2.5l.7.7-9 8.9-.7-.7 9-8.9z"/><path d="M9.7 0L16 6.2V0z"/></symbol><symbol id="icon-remove" viewBox="-296 388 18 18"><path d="M-291.7 396.1h9v2h-9z"/><path d="M-287 405.5c-4.7 0-8.5-3.8-8.5-8.5s3.8-8.5 8.5-8.5 8.5 3.8 8.5 8.5-3.8 8.5-8.5 8.5zm0-16c-4.1 0-7.5 3.4-7.5 7.5s3.4 7.5 7.5 7.5 7.5-3.4 7.5-7.5-3.4-7.5-7.5-7.5z"/></symbol><symbol id="icon-rss" viewBox="0 0 18 18"><path d="m.97480857 6.01583891.11675372.00378391c5.75903295.51984988 10.34261021 5.10537458 10.85988231 10.86480098.0494035.5500707-.3564674 1.0360406-.906538 1.0854441-.5500707.0494036-1.0360406-.3564673-1.08544412-.906538-.43079083-4.7965248-4.25151132-8.61886853-9.04770289-9.05180573-.55004837-.04965115-.95570047-.53580366-.90604933-1.08585203.04610464-.5107592.46858035-.89701345.96909831-.90983323zm1.52519143 6.95474179c1.38071187 0 2.5 1.1192881 2.5 2.5s-1.11928813 2.5-2.5 2.5-2.5-1.1192881-2.5-2.5 1.11928813-2.5 2.5-2.5zm-1.43253846-12.96884168c9.09581416.53242539 16.37540296 7.8163886 16.90205336 16.91294558.0319214.5513615-.389168 1.0242056-.9405294 1.056127-.5513615.0319214-1.0242057-.389168-1.0561271-.9405294-.4679958-8.08344784-6.93949306-14.55883389-15.02226722-15.03196077-.55134101-.03227286-.97212889-.50538538-.93985602-1.05672639.03227286-.551341.50538538-.97212888 1.05672638-.93985602z" fill-rule="evenodd"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-springer-collections" viewBox="3 3 32 32"><path fill-rule="evenodd" d="M25.583333,30.1249997 L25.583333,7.1207574 C25.583333,7.10772495 25.579812,7.10416665 25.5859851,7.10416665 L6.10400517,7.10571021 L6.10400517,30.1355179 C6.10400517,31.1064087 6.89406744,31.8958329 7.86448169,31.8958329 L26.057145,31.8958329 C25.7558021,31.374901 25.583333,30.7700915 25.583333,30.1249997 Z M4.33333333,30.1355179 L4.33333333,7.10571021 C4.33333333,6.12070047 5.12497502,5.33333333 6.10151452,5.33333333 L25.5859851,5.33333333 C26.5617372,5.33333333 27.3541664,6.13359035 27.3541664,7.1207574 L27.3541664,12.4166666 L32.6666663,12.4166666 L32.6666663,30.1098941 C32.6666663,32.0694626 31.0857174,33.6666663 29.1355179,33.6666663 L7.86448169,33.6666663 C5.91736809,33.6666663 4.33333333,32.0857174 4.33333333,30.1355179 Z M27.3541664,14.1874999 L27.3541664,30.1249997 C27.3541664,31.1030039 28.1469954,31.8958329 29.1249997,31.8958329 C30.1030039,31.8958329 30.8958329,31.1030039 30.8958329,30.1249997 L30.8958329,14.1874999 L27.3541664,14.1874999 Z M9.64583326,10.6458333 L22.0416665,10.6458333 L22.0416665,17.7291665 L9.64583326,17.7291665 L9.64583326,10.6458333 Z M11.4166666,12.4166666 L11.4166666,15.9583331 L20.2708331,15.9583331 L20.2708331,12.4166666 L11.4166666,12.4166666 Z M9.64583326,19.4999998 L22.0416665,19.4999998 L22.0416665,21.2708331 L9.64583326,21.2708331 L9.64583326,19.4999998 Z M9.64583326,23.0416665 L22.0416665,23.0416665 L22.0416665,24.8124997 L9.64583326,24.8124997 L9.64583326,23.0416665 Z M9.64583326,26.583333 L22.0416665,26.583333 L22.0416665,28.3541664 L9.64583326,28.3541664 L9.64583326,26.583333 Z"/></symbol><symbol id="icon-springer-download" viewBox="-301 390 9 14"><path d="M-301 395.6l4.5 5.1 4.5-5.1h-3V390h-3v5.6h-3zm0 6.5h9v1.9h-9z"/></symbol><symbol id="icon-springer-info" viewBox="0 0 24 24"><!--Generator: Sketch 63.1 (92452) - https://sketch.com--><g id="V&amp;I" stroke="none" stroke-width="1" fill-rule="evenodd"><g id="info" fill-rule="nonzero"><path d="M12,0 C18.627417,0 24,5.372583 24,12 C24,18.627417 18.627417,24 12,24 C5.372583,24 0,18.627417 0,12 C0,5.372583 5.372583,0 12,0 Z M12.5540543,9.1 L11.5540543,9.1 C11.0017696,9.1 10.5540543,9.54771525 10.5540543,10.1 L10.5540543,10.1 L10.5540543,18.1 C10.5540543,18.6522847 11.0017696,19.1 11.5540543,19.1 L11.5540543,19.1 L12.5540543,19.1 C13.1063391,19.1 13.5540543,18.6522847 13.5540543,18.1 L13.5540543,18.1 L13.5540543,10.1 C13.5540543,9.54771525 13.1063391,9.1 12.5540543,9.1 L12.5540543,9.1 Z M12,5 C11.5356863,5 11.1529412,5.14640523 10.8517647,5.43921569 C10.5505882,5.73202614 10.4,6.11546841 10.4,6.58954248 C10.4,7.06361656 10.5505882,7.45054466 10.8517647,7.7503268 C11.1529412,8.05010893 11.5356863,8.2 12,8.2 C12.4768627,8.2 12.8627451,8.05010893 13.1576471,7.7503268 C13.452549,7.45054466 13.6,7.06361656 13.6,6.58954248 C13.6,6.11546841 13.452549,5.73202614 13.1576471,5.43921569 C12.8627451,5.14640523 12.4768627,5 12,5 Z" id="Combined-Shape"/></g></g></symbol><symbol id="icon-springer-tick-circle" viewBox="0 0 24 24"><g id="Page-1" stroke="none" stroke-width="1" fill-rule="evenodd"><g id="springer-tick-circle" fill-rule="nonzero"><path d="M12,24 C5.372583,24 0,18.627417 0,12 C0,5.372583 5.372583,0 12,0 C18.627417,0 24,5.372583 24,12 C24,18.627417 18.627417,24 12,24 Z M7.657,10.79 C7.45285634,10.6137568 7.18569967,10.5283283 6.91717333,10.5534259 C6.648647,10.5785236 6.40194824,10.7119794 6.234,10.923 C5.87705269,11.3666969 5.93445559,12.0131419 6.364,12.387 L10.261,15.754 C10.6765468,16.112859 11.3037113,16.0695601 11.666,15.657 L17.759,8.713 C18.120307,8.27302248 18.0695334,7.62621189 17.644,7.248 C17.4414817,7.06995024 17.1751516,6.9821166 16.9064461,7.00476032 C16.6377406,7.02740404 16.3898655,7.15856958 16.22,7.368 L10.768,13.489 L7.657,10.79 Z" id="path-1"/></g></g></symbol><symbol id="icon-updates" viewBox="0 0 18 18"><g fill-rule="nonzero"><path d="M16.98 3.484h-.48c-2.52-.058-5.04 1.161-7.44 2.903-2.46-1.8-4.74-2.903-8.04-2.903-.3 0-.54.29-.54.58v9.813c0 .29.24.523.54.581 2.76.348 4.86 1.045 7.62 2.903.24.116.54.116.72 0 2.76-1.858 4.86-2.555 7.62-2.903.3-.058.54-.29.54-.58V4.064c0-.29-.24-.523-.54-.581zm-15.3 1.22c2.34 0 4.86 1.509 6.72 2.786v8.478c-2.34-1.394-4.38-2.09-6.72-2.439V4.703zm14.58 8.767c-2.34.348-4.38 1.045-6.72 2.439V7.374C12 5.632 14.1 4.645 16.26 4.645v8.826z"/><path d="M9 .058c-1.56 0-2.76 1.22-2.76 2.671C6.24 4.181 7.5 5.4 9 5.4c1.5 0 2.76-1.22 2.76-2.671 0-1.452-1.2-2.67-2.76-2.67zm0 4.413c-.96 0-1.8-.755-1.8-1.742C7.2 1.8 7.98.987 9 .987s1.8.755 1.8 1.742c0 .93-.84 1.742-1.8 1.742z"/></g></symbol></svg>
</div>


        

        

    <div class="u-vh-full">
        <a class="c-skip-link" href="#main-content">Skip to main content</a>
        
            
    
        <div class="u-hide u-show-following-ad"></div>
    
    <aside class="adsbox c-ad c-ad--728x90" data-component-mpu>
        <div class="c-ad__inner" >
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1"
                data-ad-type="LB1"
                data-pa11y-ignore
                data-gpt
                data-gpt-unitpath="/270604982/springer_open/epjdatascience/articles"
                data-gpt-sizes="728x90,970x90"
                data-gpt-targeting="pos=LB1;doi=10.1140/epjds/s13688-021-00312-8;type=article;kwrd=Wikipedia,Images,Computer vision,User behavior;pmc=I23028,P33030,T11022;"
            >
                <noscript>
                    <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/270604982/springer_open/epjdatascience/articles&amp;sz=728x90,970x90&amp;pos=LB1&amp;doi=10.1140/epjds/s13688-021-00312-8&amp;type=article&amp;kwrd=Wikipedia,Images,Computer vision,User behavior&amp;pmc=I23028,P33030,T11022&amp;">
                        <img data-test="gpt-advert-fallback-img"
                            src="//pubads.g.doubleclick.net/gampad/ad?iu=/270604982/springer_open/epjdatascience/articles&amp;sz=728x90,970x90&amp;pos=LB1&amp;doi=10.1140/epjds/s13688-021-00312-8&amp;type=article&amp;kwrd=Wikipedia,Images,Computer vision,User behavior&amp;pmc=I23028,P33030,T11022&amp;"
                            alt="Advertisement"
                            width="728"
                            height="90">
                    </a>
                </noscript>
            </div>
        </div>
    </aside>

        
        

         
    
        <div id="membership-message-loader-desktop" class="placeholder" data-placeholder="/placeholder/v1/membership/message"></div>
    
    
        <div id="top" class="u-position-relative">
    <header class="c-header" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand u-mr-48" itemscope itemtype="http://schema.org/Organization" data-test="navbar-logo-header">
                <a href="https://www.springeropen.com" itemprop="url">
    <img alt="SpringerOpen" itemprop="logo" width="160" height="30" role="img" src=/static/images/springeropen/logo-springer-open-f45a49bbb5.svg>
</a>
            </div>
            <div class="c-header__navigation">
                <button
                    type="button"
                    class="c-header__link u-button-reset u-mr-24"
                    data-expander
                    data-expander-target="#publisher-header-search"
                    data-expander-autofocus="firstTabbable"
                    data-test="header-search-button"
                    aria-controls="publisher-header-search"
                    aria-expanded="false">
                    <span class="u-display-flex u-align-items-center">
                        <span class="u-text-sm">Search</span>
                        <svg class="u-icon u-flex-static u-ml-8" aria-hidden="true" focusable="false">
                            <use xlink:href="#global-icon-search"></use>
                        </svg>
                    </span>
                </button>
                <nav>
                    <ul class="c-header__menu" data-enhanced-menu data-test="publisher-navigation">
                        
                            
                                <li class="c-header__item u-hide-at-lt-lg">
                                    <a class="c-header__link"
                                       href="//www.springeropen.com/get-published">
                                        Get published
                                    </a>
                                </li>
                            
                                <li class="c-header__item u-hide-at-lt-lg">
                                    <a class="c-header__link"
                                       href="//www.springeropen.com/journals">
                                        Explore Journals
                                    </a>
                                </li>
                            
                                <li class="c-header__item u-hide-at-lt-lg">
                                    <a class="c-header__link"
                                       href="https://www.springer.com/gp/open-access/books">
                                        Books
                                    </a>
                                </li>
                            
                                <li class="c-header__item u-hide-at-lt-lg">
                                    <a class="c-header__link"
                                       href="//www.springeropen.com/about">
                                        About
                                    </a>
                                </li>
                            
                        
                        <li class="c-header__item">
                            <a data-enhanced-account
                               class="c-header__link"
                               href="https://www.springeropen.com/account"
                               data-test="login-link">
                                My account
                            </a>
                        </li>
                    </ul>
                </nav>
            </div>
        </div>
    </header>
    <div class="c-popup-search u-js-hide" id="publisher-header-search">
        <div class="u-container">
            <div class="c-popup-search__container">
                <div class="ctx-search">
    <form role="search" class="c-form-field" method="GET" action="//www.springeropen.com/search" data-track="submit"
        data-track-category="Search and Results" data-track-action="Submit search" data-dynamic-track-label data-track-label="" data-test="global-search">
        <label for="publisherSearch" class="c-form-field__label">Search all SpringerOpen articles</label>
        <div class="u-display-flex">
            <input id="publisherSearch" class="c-form-field__input" data-search-input autocomplete="off" role="textbox" data-test="search-input" name="query" type="text" value=""/>
            <div>
                <button class="u-button u-button--primary" type="submit" data-test="search-submit-button">
    <span class="u-visually-hidden">Search</span>
    <svg class="u-icon u-flex-static" width="16" height="16" aria-hidden="true" focusable="false">
        <use xlink:href="#global-icon-search"></use>
    </svg>
</button>


            </div>
        </div>
        <input type="hidden" name="searchType" value="publisherSearch"/>
    </form>
</div>

            </div>
        </div>
    </div>
</div>

    

        
            

            <header class="c-journal-header ctx-journal-header">
                <div class="u-container">
                    <div class="c-journal-header__grid">
                        <div class="c-journal-header__grid-main">
                            
                            <div class="h2 c-journal-header__title" id="journalTitle">
                                <a href="/">EPJ Data Science</a>
                            </div>
                            
                            
                            <div class="u-display-flex u-justify-content-space-between u-align-items-baseline">
                                <nav>
                                    <ul class="c-nav c-nav--tabs" id="siteNavigation" role="menu">
                                        
                                            <li class="c-nav__item" role="menuitem">
                                                <a class="c-nav__link" href='/about'>About</a>
                                            </li>
                                        
                                            <li class="c-nav__item" role="menuitem">
                                                <a class="c-nav__link is-active" href='/articles'>Articles</a>
                                            </li>
                                        
                                            <li class="c-nav__item" role="menuitem">
                                                <a class="c-nav__link" href='/submission-guidelines'>Submission Guidelines</a>
                                            </li>
                                        
                                    </ul>
                                </nav>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </header>

        

        <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">

            <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
                <div class="c-pdf-button__container u-hide-at-lg">
                    <div class="c-pdf-container">
                        
    <div class="c-pdf-download u-clear-both">
        <a href="//epjdatascience.springeropen.com/track/pdf/10.1140/epjds/s13688-021-00312-8.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true"  data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external>
            <span class="c-pdf-download__text">Download PDF</span>
            <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
        </a>
    </div>

                        
                    </div>
                </div>
                <article lang="en">
                    <div class="c-article-header">
                        
    


                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Regular article</li>
    
    
        <li class="c-article-identifiers__item">
            <span class="c-article-identifiers__open" data-test="open-access">Open Access</span>
        </li>
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2022-01-03">03 January 2022</time></a></li>
                        </ul>

                        <h1 class="c-article-title" data-test="article-title" data-article-title="">A large scale study of reader interactions with images on Wikipedia</h1>
                        <ul class="c-article-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daniele-Rama" aria-label="Read more about Daniele Rama" data-author-popup="auth-Daniele-Rama" data-corresp-id="c1">Daniele Rama<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0001-9419-7315"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0001-9419-7315</a></span><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tiziano-Piccardi" aria-label="Read more about Tiziano Piccardi" data-author-popup="auth-Tiziano-Piccardi">Tiziano Piccardi</a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0001-9314-1440"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0001-9314-1440</a></span><sup class="u-js-hide"><a href="#Aff2">2</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Miriam-Redi" aria-label="Read more about Miriam Redi" data-author-popup="auth-Miriam-Redi">Miriam Redi</a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0002-0581-0251"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-0581-0251</a></span><sup class="u-js-hide"><a href="#Aff3">3</a></sup> &amp; </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Rossano-Schifanella" aria-label="Read more about Rossano Schifanella" data-author-popup="auth-Rossano-Schifanella">Rossano Schifanella</a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0002-3745-5792"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-3745-5792</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff4">4</a></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/"><i data-test="journal-title">EPJ Data Science</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 11</b>, Article number: <span data-test="article-number">1</span> (<span data-test="article-publication-year">2022</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    <div class="c-article-metrics-bar__wrapper u-clear-both">
        <ul class="c-article-metrics-bar u-list-reset">
            
                <li class=" c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__count">1130 <span class="c-article-metrics-bar__label">Accesses</span></p>
                </li>
            
            
            
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">29 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                    </li>
                
            
            <li class="c-article-metrics-bar__item">
                <p class="c-article-metrics-bar__details"><a href="/articles/10.1140/epjds/s13688-021-00312-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
            </li>
        </ul>
    </div>

                        
                        
    

    

                        
                    </div>

                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Wikipedia is the largest source of free encyclopedic knowledge and one of the most visited sites on the Web. To increase reader understanding of the article, Wikipedia editors add images within the text of the article’s body. However, despite their widespread usage on web platforms and the huge volume of visual content on Wikipedia, little is known about the importance of images in the context of free knowledge environments. To bridge this gap, we collect data about English Wikipedia reader interactions with images during one month and perform the first large-scale analysis of how interactions with images happen on Wikipedia. First, we quantify the overall engagement with images, finding that one in 29 pageviews results in a click on at least one image, one order of magnitude higher than interactions with other types of article content. Second, we study what factors associate with image engagement and observe that clicks on images occur more often in shorter articles and articles about visual arts or transports and biographies of less well-known people. Third, we look at interactions with Wikipedia article previews and find that images help support reader information need when navigating through the site, especially for more popular pages. The findings in this study deepen our understanding of the role of images for free knowledge and provide a guide for Wikipedia editors and web user communities to enrich the world’s largest source of encyclopedic knowledge.</p></div></div></section>
                    
    


                    <section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Almost 20 years after its birth, Wikipedia has become the reference for online diffusion of free encyclopedic knowledge, reaching 54M articles in 313 language editions.<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> Its content is generated by the collaborative effort of a large community of editors, and provides a reliable source of information for web users [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Anthony D, Smith SW, Williamson T (2009) Reputation and reliability in collective goods: the case of the online encyclopedia Wikipedia. Ration Soc 21(3):283–306&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR1" id="ref-link-section-d26201386e477">1</a>]. Knowledge on Wikipedia is mainly conveyed under the form of written text, but also through other types of content, such as references and images.</p><p>The space of visual content on Wikipedia is vast. English Wikipedia alone contains more than 5M distinct images, the majority of which is hosted by Wikipedia’s sister project Wikimedia Commons,<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> the world’s largest free visual knowledge repository. The genesis of Wikipedia images involves not only the contribution of Wikipedia editors, but also the participation of visual content <i>creators</i>. Visual content on Commons and Wikipedia originates from individual photographers, artists, web users and cultural institutions in the GLAM spaces,<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> who actively release their works of art for free and public use.</p><p>Given the crucial role of Wikipedia as a central hub for knowledge sharing and learning, understanding how images are used on Wikipedia is particularly important. A vast body of literature in experimental psychology has shown the impact of images for learning and engaging with knowledge. Images positively affect comprehension and increase attention on the textual material [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Levie WH, Lentz R (1982) Effects of text illustrations: a review of research. ECTJ 30(4):195–232&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR2" id="ref-link-section-d26201386e508">2</a>]. Despite their importance, while other aspects of Wikipedia have been widely studied [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Yasseri T, Sumi R, Rung A, Kornai A, Kertész J (2012) Dynamics of conflicts in Wikipedia. PLoS ONE 7(6):38869&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR3" id="ref-link-section-d26201386e511">3</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Halfaker A, Geiger RS (2020) Ores: lowering barriers with participatory machine learning in wikipedia. Proc Human-Computer Interaction (HCI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR5" id="ref-link-section-d26201386e514">5</a>], little is known about visual content and its usage, with only a few studies looking at cross-language image diversity [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;He S, Lin AY, Adar E, Hecht BJ (2018) The_tower_of_babel. jpg: diversity of visual encyclopedic knowledge across Wikipedia language editions. In: Proc. International conference on web and social media (ICWSM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR6" id="ref-link-section-d26201386e517">6</a>], and the communities of Wikipedia “image” editors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Viegas FB (2007) The visual side of Wikipedia. In: Proc. Hawaii international conference on system sciences (HICSS)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR7" id="ref-link-section-d26201386e520">7</a>].</p><p>In this paper, we fill this gap in the literature by providing for the first time a comprehensive overview of how readers interact with images in (English) Wikipedia. We quantify and characterize reader engagement with images when browsing the encyclopedia using traffic data and we explore the role played by images in the exploration of free knowledge. To operationalize reader engagement, we adopt the most widely-used metrics in web user studies [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Bakhshi S, Shamma DA, Gilbert E (2014) Faces engage us: photos with faces attract more likes and comments on Instagram. In: Proc. Conference on human factors in computing systems (SIGCHI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR8" id="ref-link-section-d26201386e526">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Park JY, O’Hare N, Schifanella R, Jaimes A, Chung C-W (2015) A large-scale study of user image search behavior on the web. In: Proc. Conference on human factors in computing systems (SIGCHI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR9" id="ref-link-section-d26201386e529">9</a>]: we compute click-through rate on images, and conversion rate on illustrated and unillustrated page previews. While only partially representing the complex, multifaceted notion of interest [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Constantin MG, Redi M, Zen G, Ionescu B (2019) Computational understanding of visual interestingness beyond semantics: literature survey and analysis of covariates. ACM Comput Surv 52(2):1–37&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR10" id="ref-link-section-d26201386e532">10</a>], these implicit signals do reflect an expression of engagement with visual content and they provide a solid baseline for an initial overview of readers’ interactions with images. More specifically, we address three major research questions: </p><dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term u-text-bold u-float-left u-pr-16"><dfn>RQ1::</dfn></dt><dd class="c-abbreviation_list__description u-mb-24">
                    <p><i>To what extent are readers interacting with images on Wikipedia?</i> And what is the relation with engagement values on other types of content?</p>
                  </dd><dt class="c-abbreviation_list__term u-text-bold u-float-left u-pr-16"><dfn>RQ2::</dfn></dt><dd class="c-abbreviation_list__description u-mb-24">
                    <p><i>What drives reader’s engagement with images when reading Wikipedia articles?</i> What are the visual and contextual factors that influence image interactions?</p>
                  </dd><dt class="c-abbreviation_list__term u-text-bold u-float-left u-pr-16"><dfn>RQ3::</dfn></dt><dd class="c-abbreviation_list__description u-mb-24">
                    <p><i>Do images support reader’s need for additional information when navigating Wikipedia?</i> Are images helpful to delve into contextual information provided by the article?</p>
                  </dd></dl><p>In addressing these questions, we make the following contributions: </p><ul class="u-list-style-bullet">
                <li>
                  <p><i>RQ1:</i> We collect a large dataset of reader interactions with images in English Wikipedia over one month and characterize the landscape of Wikipedia images with several features inspired by experimental psychology and web user studies (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec13">4.3</a>). We quantify reader engagement with images and find that, on average, readers click with images 1 in every 29 pageviews on English Wikipedia, ten times more often than with references (RQ1, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec17">5</a>).</p>
                </li>
                <li>
                  <p><i>RQ2:</i> To visualize the factors impacting reader engagement with Wikipedia images, we perform a set of multivariate analyses on the image features extracted and find that readers interact more often with images of monuments, maps, vehicles, and unfamiliar faces (RQ2, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec20">6</a>).</p>
                </li>
                <li>
                  <p><i>RQ3:</i> To understand whether images support readers’ need for additional contextual information when navigating Wikipedia, we design a matched observational study based on page previews, i.e., the short article summaries that are displayed when users hover on links to other Wikipedia pages (RQ3, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec25">7</a>). We find a negative effect of the presence of images on the proportion of articles’ page previews that convert into a visualization of the full article page.</p>
                </li>
              </ul><p>We conclude (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec26">8</a>) that the visual preferences of Wikipedia readers are radically different compared to web users in photo-sharing platforms or image search engines, where images of people and celebrities largely predominate. We also find that images on Wikipedia appear to fulfill part of the <i>cognitive</i> function typical of illustrations in instructional settings supporting readers’ information need. Finally, we discuss theoretical implication of this research and its important repercussions on how the Wikipedia communities organize and prioritize the inclusion of visual content and how the broader web and content creators could contribute to the web with free visual knowledge.</p></div></div></section><section data-title="Related work"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Our work is highly related to research from experimental psychology, computer vision, information retrieval, and computational social science, looking at how readers navigate knowledge.</p><h3 class="c-article__sub-heading" id="Sec3">The role of text illustrations</h3><p>A substantial body of literature from experimental and educational psychology studied the role of images for knowledge understanding and learning. Researchers have found that, very often, images in association with text help to support learning in instructional contexts [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Guo D, Zhang S, Wright KL, McTigue EM (2020) Do you get the picture? A meta-analysis of the effect of graphics on reading comprehension. AERA Open 6(1):2332858420901696&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR11" id="ref-link-section-d26201386e642">11</a>], also in online settings [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Mayer RE (2002) Multimedia learning. In: Psychology of learning and motivation, vol 41, pp 85–139&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR12" id="ref-link-section-d26201386e645">12</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Tempelman-Kluit N (2006) Multimedia learning theories and online instruction. Coll Res Libr 67(4):364–369&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR15" id="ref-link-section-d26201386e648">15</a>], especially when images are carefully curated, described, and positioned in the text [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Peeck J (1993) Increasing picture effects in learning from illustrated text. Learn Instr 3(3):227–238&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR16" id="ref-link-section-d26201386e651">16</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Bernard RM (1990) Using extended captions to improve learning from instructional illustrations. Br J Educ Technol 21(3):215–225&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR17" id="ref-link-section-d26201386e654">17</a>]. Beyond this <i>cognitive</i> purpose of facilitating content comprehension and providing complementary information, textual illustrations can have many other functions: the <i>attentional</i> function, meaning that images can help to attract attention to the information in the textual form; the <i>affective</i> function—images help enhance emotions and enjoyment when reading a text; and the <i>compensatory</i> purpose of supporting poor readers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Levie WH, Lentz R (1982) Effects of text illustrations: a review of research. ECTJ 30(4):195–232&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR2" id="ref-link-section-d26201386e670">2</a>]. While testing the role of images for knowledge understanding is beyond the scope of this paper, we borrow some ideas from these works to analyze reader interactions with images and design features and experiments aimed at replicating some of their findings.</p><h3 class="c-article__sub-heading" id="Sec4">Image interestingness</h3><p>Several studies in computer science have looked at what makes images interesting from a computational perspective. Researchers have typically described interestingness in two ways. <i>Visual</i> interestingness is the extent to which an image can hold or catch the viewer’s attention due to its intrinsic visual qualities (see Constantin et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Constantin MG, Redi M, Zen G, Ionescu B (2019) Computational understanding of visual interestingness beyond semantics: literature survey and analysis of covariates. ACM Comput Surv 52(2):1–37&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR10" id="ref-link-section-d26201386e684">10</a>] for a review of the most recent works in this space). Researchers have found that, for example, images are more interesting when they are more aesthetically pleasing or when their content is visually complex or unfamiliar [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Gygli M, Grabner H, Riemenschneider H, Nater F, Van Gool L (2013) The interestingness of images. In: Proc. International conference on computer vision (ICCV)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR18" id="ref-link-section-d26201386e687">18</a>]. <i>Social</i> interestingness is often also called <i>popularity</i> and corresponds to the extent to which an image is liked by a large number of people in a community. Social interestingness depends on the social dynamics of the platforms where images are shared, the pictures visual content [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Bakhshi S, Shamma DA, Gilbert E (2014) Faces engage us: photos with faces attract more likes and comments on Instagram. In: Proc. Conference on human factors in computing systems (SIGCHI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR8" id="ref-link-section-d26201386e697">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Khosla A, Das Sarma A, Hamid R (2014) What makes an image popular?. In: Proc. International world wide web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR19" id="ref-link-section-d26201386e700">19</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Ding K, Ma K, Wang S (2019) Intrinsic image popularity assessment. In: Proc. International conference on multimedia (MM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR20" id="ref-link-section-d26201386e703">20</a>] and the text associated with them [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Zhang W, Wang W, Wang J, Zha H (2018) User-guided hierarchical attention network for multi-modal social image popularity prediction. In: Proc. International world wide web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR21" id="ref-link-section-d26201386e706">21</a>]. Most of these previous works focus on predicting image popularity in photo sharing platforms such as Flickr [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Khosla A, Das Sarma A, Hamid R (2014) What makes an image popular?. In: Proc. International world wide web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR19" id="ref-link-section-d26201386e709">19</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Zhang W, Wang W, Wang J, Zha H (2018) User-guided hierarchical attention network for multi-modal social image popularity prediction. In: Proc. International world wide web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR21" id="ref-link-section-d26201386e712">21</a>] or Instagram [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Bakhshi S, Shamma DA, Gilbert E (2014) Faces engage us: photos with faces attract more likes and comments on Instagram. In: Proc. Conference on human factors in computing systems (SIGCHI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR8" id="ref-link-section-d26201386e716">8</a>], specifically designed to increase social interestingness in images. Unlike these existing works, we analyze here for the first time how readers engage with images in the context of online free knowledge spaces. We model the complex interplay between encyclopedic knowledge, pictorial representations, and reader engagement and explore the role of images informational support for Wikipedia articles.</p><h3 class="c-article__sub-heading" id="Sec5">Image search behavior</h3><p>Related works have investigated web user behavior in image search engines. Researchers have found that, in general, the most popular queries in image search engines are about people, celebrities, and entertainment [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Tsikrika T, Diou C (2014) Multi-evidence user group discovery in professional image search. In: Proc. European conference on information retrieval (ECIR)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR22" id="ref-link-section-d26201386e727">22</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Huang J, Efthimiadis EN (2009) Analyzing and evaluating query reformulation strategies in web search logs. In: Proc. Conference on information and knowledge management (CIKM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR24" id="ref-link-section-d26201386e730">24</a>]. By comparing image search behavior with text search behavior, several studies have found that image search sessions are heavier in interaction and exploration than the more “focused” textual sessions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Jansen BJ, Spink A, Pedersen JO (2004) The effect of specialized multimedia collections on web searching. J Web Eng 3(3–4):182–199&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR25" id="ref-link-section-d26201386e733">25</a>], although, in a later study, O’Hare et al. found that web image search behavior is nonuniform across query types [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Park JY, O’Hare N, Schifanella R, Jaimes A, Chung C-W (2015) A large-scale study of user image search behavior on the web. In: Proc. Conference on human factors in computing systems (SIGCHI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR9" id="ref-link-section-d26201386e736">9</a>]. While the scope of this work is different from this body of research, we will factor into our analysis findings from this area.</p><h3 class="c-article__sub-heading" id="Sec6">Images on Wikipedia</h3><p>Recent works have quantified the monetary value and underlined the social contribution of Wikipedia’s visual side [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Heald P, Erickson K, Kretschmer M (2015) The valuation of unprotected works: a case study of public domain images on Wikipedia. Harv JL &amp; Tech 29(1):1–31&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR26" id="ref-link-section-d26201386e748">26</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Erickson K, Perez FR, Perez JR (2018) What is the commons worth? Estimating the value of wikimedia imagery by observing downstream use. In: Proc. International symposium on open collaboration (OpenSym)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR27" id="ref-link-section-d26201386e751">27</a>]. However, despite their important role, the space of images on the encyclopedia has rarely been investigated. Given the richness of their semantics, researchers have worked on building structured datasets from images in Wikimedia Commons, and Wikipedia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Vaidya G, Kontokostas D, Knuth M, Lehmann J, Hellmann S (2015) Dbpedia commons: structured multimedia metadata from the wikimedia commons. In: Proc. International semantic web conference (ISWC)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR28" id="ref-link-section-d26201386e754">28</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Ferrada S, Bustos B, Hogan A (2017) Imgpedia: a linked dataset with content-based analysis of wikimedia images. In: Proc. International semantic web conference (ISWC)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR29" id="ref-link-section-d26201386e757">29</a>]. However, only a few works have focused on understanding editing behavior concerning images. A seminal study looked at understanding communities of editors who curate the visual content of the encyclopedia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Viegas FB (2007) The visual side of Wikipedia. In: Proc. Hawaii international conference on system sciences (HICSS)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR7" id="ref-link-section-d26201386e760">7</a>]. More recently, He et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;He S, Lin AY, Adar E, Hecht BJ (2018) The_tower_of_babel. jpg: diversity of visual encyclopedic knowledge across Wikipedia language editions. In: Proc. International conference on web and social media (ICWSM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR6" id="ref-link-section-d26201386e764">6</a>] measured the visual diversity of Wikipedia, finding that cross-language image diversity is higher than the diversity of textual content and that many images are unique to specific language editions. Moreover, Navarrete et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Navarrete T, Villaespesa E (2020) Image-based information: paintings in Wikipedia. J Doc&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR30" id="ref-link-section-d26201386e767">30</a>] investigated the role of image paintings on Wikipedia, finding that they are extensively used to illustrate also non-art-related topic and that their audience is even larger than that of art-related articles. While most of these works focus on the image <i>content</i> or the Wikipedia <i>editor</i> communities, we study here the complementary aspect of how <i>readers</i> interact with the visual content.</p><h3 class="c-article__sub-heading" id="Sec7">Studying Wikipedia readers</h3><p>A few studies have focused on Wikipedia reader behavior, including reader article topic preferences [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Lehmann J, Müller-Birn C, Laniado D, Lalmas M, Kaltenbrunner A (2014) Reader preferences and behavior on Wikipedia. In: Proc. Conference on hypertext and social media (HT)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR31" id="ref-link-section-d26201386e787">31</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Spoerri A (2007) What is popular on Wikipedia and why? First Monday 12(4):1–6&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR32" id="ref-link-section-d26201386e790">32</a>], reader perception of the site performances [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Salutari F, Da Hora D, Dubuc G, Rossi D (2019) A large-scale study of Wikipedia users’ quality of experience. In: Proc. International world wide web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR33" id="ref-link-section-d26201386e793">33</a>], or reader informational need [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Lemmerich F, Sáez-Trumper D, West R, Zia L (2019) Why the world reads Wikipedia: beyond English speakers. In: Proc. International conference on web search and data mining (WSDM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR4" id="ref-link-section-d26201386e796">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Singer P, Lemmerich F, West R, Zia L, Wulczyn E, Strohmaier M, Leskovec J (2017) Why we read Wikipedia. In: Proc. International world wide web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR34" id="ref-link-section-d26201386e799">34</a>]. More recently, Piccardi et al. worked on quantifying reader engagement with citations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR35" id="ref-link-section-d26201386e803">35</a>] and external links [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2021) On the value of Wikipedia as a gateway to the web. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR36" id="ref-link-section-d26201386e806">36</a>]. The authors collected a large dataset of reader interactions with footnotes and references and showed that only a tiny portion of readers engage with citations on Wikipedia. In this direction, this work gives an additional perspective on Wikipedia readers’ behavior, focusing on the volume and characteristics of reader interactions with visual content.</p><h3 class="c-article__sub-heading" id="Sec8">User engagement metrics</h3><p>To quantify user engagement with images when reading Wikipedia, we borrow metrics used by several studies in the computational advertising field and user engagement studies [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Lehmann J, Müller-Birn C, Laniado D, Lalmas M, Kaltenbrunner A (2014) Reader preferences and behavior on Wikipedia. In: Proc. Conference on hypertext and social media (HT)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR31" id="ref-link-section-d26201386e817">31</a>]. While these works aim to predict engagement metrics such as conversion rate [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Chapelle O (2014) Modeling delayed feedback in display advertising. In: Proc. Conference on knowledge discovery and data mining (SIGKDD)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR37" id="ref-link-section-d26201386e820">37</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Rosales R, Cheng H, Manavoglu E (2012) Post-click conversion modeling and analysis for non-guaranteed delivery display advertising. In: Proc. Conference on web search and data mining (WSDM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR38" id="ref-link-section-d26201386e823">38</a>], namely the percentage of landing page visits that result in a target action, or the click-through rate [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Ta A-P (2015) Factorization machines with follow-the-regularized-leader for ctr prediction in display advertising. In: Proc. International conference on big data (big data)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR39" id="ref-link-section-d26201386e826">39</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Edizel B, Mantrach A, Bai X (2017) Deep character-level click-through rate prediction for sponsored search. In: Proc. Conference on research and development in information retrieval (SIGIR)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR41" id="ref-link-section-d26201386e829">41</a>], namely the ratio between clicks and impressions, we use these metrics here as a means to decode Wikipedia reader behavior.</p></div></div></section><section data-title="Images on Wikipedia"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Images on Wikipedia</h2><div class="c-article-section__content" id="Sec9-content"><p>Images are a core component to help readers interpret knowledge in the encyclopedia and complement the textual information on Wikipedia articles. As English Wikipedia guidelines put it, “The purpose of an image is to increase reader understanding of the article’s subject matter, usually by directly depicting people, things, activities, and concepts described in the article.”<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup></p><p>Images are added to Wikipedia by hundreds of thousands of editors from all around the world, following a Manual of Style maintained by the Wikipedia community.<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> In essence, images added to Wikipedia articles have to be relevant to the article’s content and of high photographic quality. The majority of images in the encyclopedia are hosted in Wikimedia Commons, the largest free visual knowledge repository. Images in Wikimedia Commons must be either of the public domain or licensed under a free license allowing anyone to reuse the material for any purpose, including commercial purposes.</p><p>Images can be placed in different parts of an article (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig1">1</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig2">2</a>). Readers can find images in the <i>infobox</i>, a table summarizing the main facts about the article’s subject, usually placed in the top-right corner of the page on desktop browsers or at the top of the screen in mobile browsers. Images can also be added <i>inline</i>, namely individually near the relevant text in the article body. Finally, when images are too many to be placed within the text body, they can be collected into <i>galleries</i>, generally added at the bottom of the articles. On desktop devices, images are also available in article <i>previews</i>, i.e., the pop-up containing the article summary and an image (when available) which gets displayed whenever a reader hovers over on a link to another Wikipedia article. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Figure 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig1_HTML.jpg?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig1_HTML.jpg" alt="figure 1" loading="lazy" width="685" height="307"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Examples of the two types of images visualization that we investigate in this study. The Media Viewer is opened when images are clicked, while Page Previews are shown when the reader hovers over a link to another page</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Figure 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig2_HTML.jpg?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig2_HTML.jpg" alt="figure 2" loading="lazy" width="685" height="303"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Examples of the three types of positions (<i>infobox</i>, <i>inline</i>, and <i>gallery</i>) of images within a Wikipedia article</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>For readers and editors who are interested in exploring Wikipedia visual content in further detail, images in articles are <i>clickable</i>: when clicked, an image is previewed in a visualization tool called Media Viewer.<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> The Media Viewer overlays on the article and displays the image in a larger size, and additional metadata below.</p><p>But how much visual content is available for readers to explore? If we take English Wikipedia, the largest language edition of the online encyclopedia, as of March 2021, it counted 6.2M articles, for a total of 5M unique images. As many other quantities on the Web, the distribution of images across Wikipedia articles follows a power law: as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig3">3</a>(A), only <span class="mathjax-tex">\(\approx 44\%\)</span> of pages in English Wikipedia are illustrated. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Figure 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig3_HTML.jpg?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig3_HTML.jpg" alt="figure 3" loading="lazy" width="685" height="329"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Cumulative distributions of (<b>A</b>) number of images per article and (<b>B</b>) the number of articles per image</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>One reason for the number of missing images is the effort needed to illustrate articles. Wikipedia editors need to find the right image match for an article by searching through millions of images in Wikimedia Commons. However, when relevant images are not present in Commons, editors will have to search other sources. First, the right pictures for an article need to <i>exist</i> somewhere on the Web (or in the world): otherwise, someone, Wikipedia editors, photographers, GLAM institutions, or other users, must create or retrieve them. Second, images have to be free to reuse. If images are not free-licensed, editors’ and authors’ efforts will be needed to make them publicly available. Only then images can be hosted in Wikimedia Commons and finally added to Wikipedia articles. To help with these efforts, the Wikimedia movement organizes several initiatives, e.g., Wiki Loves Monuments, encouraging photographers to add free images of monuments,<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> or the #WPWP campaign, which helps editors add images to unillustrated Wikipedia articles.<sup><a href="#Fn8"><span class="u-visually-hidden">Footnote </span>8</a></sup></p><p>Given the central role of Wikipedia and its diverse content nature, knowing whether and how readers use visual information could help prioritize efforts around the visual enrichment of Wikipedia.</p></div></div></section><section data-title="Data collection and methods"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Data collection and methods</h2><div class="c-article-section__content" id="Sec10-content"><p>To answer our research questions, we first need to estimate the volume of Wikipedia articles and their images, collect data about reader interactions with those, and characterize them through feature extraction.</p><h3 class="c-article__sub-heading" id="Sec11">Collecting article and image counts data</h3><p>To measure the number of articles and images, we used the HTML version of English Wikipedia at the end of March 2021. We collected 6.2M documents, and we parsed them to extract the images’ URLs, caption text, resolution, and position on the page. Using the CSS class in the HTML code, we exclude all images that appear as icons (for example, portals or Wikiprojects). Additionally, for each page, we also record the article length as the number of characters.</p><p>Out of the 6.2M articles, 2.7M (44%) contained at least one image, for a total of 5M unique images across all English Wikipedia articles. The vast majority of the articles (91%) contain two images or less, while only 1.5% has more than eight images (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig3">3</a>(A)). On average, there are 2.3 images per illustrated article. Around 84% of images is unique to the article where it appears, while 16% of the images appear in more than one article (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig3">3</a>(B)).</p><h3 class="c-article__sub-heading" id="Sec12">Collecting article and image traffic data</h3><p>We obtained the reader interactions with images for desktop and mobile browsers by processing the server access logs<sup><a href="#Fn9"><span class="u-visually-hidden">Footnote </span>9</a></sup> collected from 1st to 28th of March 2021. We restricted our analysis to only human interactions by ignoring traffic from bots thanks to a set of heuristics developed by Wikimedia’s Analytics team.<sup><a href="#Fn10"><span class="u-visually-hidden">Footnote </span>10</a></sup> For privacy reasons, we worked with an anonymized version without sensitive information. Since the logs do not contain any explicit identifier for the user, before the anonymization, we assigned a random id based on IP and user-agent similar to previous work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Lemmerich F, Sáez-Trumper D, West R, Zia L (2019) Why the world reads Wikipedia: beyond English speakers. In: Proc. International conference on web search and data mining (WSDM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR42" id="ref-link-section-d26201386e1099">42</a>]. In addition, we discarded all the events coming from logged-in users, the events of any user that edited a page, and the events originated from countries where not all days have more than 500 pageviews consistently. This filtering ensures more privacy for the Wikipedia readers by dropping around 3% of the data.</p><p>Over the considered period, we selected from the web logs all requests that reflect three types of actions: </p><ul class="u-list-style-bullet">
                  <li>
                    <p><i>Imageviews:</i> these requests correspond to image visualizations in the Media Viewer after a user clicks on an article image.</p>
                  </li>
                  <li>
                    <p><i>Pageviews:</i> these are requests logged every time a user visits a Wikipedia page. For the scope of this study, we select only pageviews of articles with at least one image.</p>
                  </li>
                  <li>
                    <p><i>Page previews:</i> these requests are logged whenever a user hovers over a link to an article. To remove the effect of casually generated page previews, we only keep those previews that are shown for at least one second. Note that page previews are generated only on desktop devices.</p>
                  </li>
                </ul><p>We aggregated these image-related events at the user level by using the previously assigned id to obtain sorted sequences of actions from the same user, which we refer to as <i>sessions</i>.</p><p>In our analysis, we do not consider exogenous time-dependent events’ impact and the role that external image search engines may play in directing users to Wikipedia. For these reasons, we filter out all the incoming traffic generated from Google Image Search, which represents by far the most used image retrieval engine from which people access Wikipedia’s visual content. Nevertheless, the pageviews originating from Google Image search account for 0.006% of the total, making their impact negligible.</p><p>In our data collection, we extracted interactions for 1.5B sessions. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig4">4</a> we report the distributions of sessions by the number of imageviews, pageviews, and previews. The distributions are heavily skewed, with 91% and 94% of sessions having less than 10 pageviews on desktop and mobile devices, respectively, and 99% of sessions having less than 10 imageviews both on desktop and mobile devices during our data collection period. Similarly, 79% of sessions have generated less than 10 previews. Users with extensive sessions (i.e. “power users”), that may be over-represented, are therefore limited in our analysis. Over one month, 100% of the illustrated articles have been loaded at least once, accounting for a total of 7.1B pageviews, 461M imageviews, and 49M previews events in our dataset. We find that most pageviews are generated from mobile devices (59% from the mobile site), while most imageviews are generated from desktop (58% from desktop). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Figure 4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig4_HTML.jpg?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig4_HTML.jpg" alt="figure 4" loading="lazy" width="685" height="226"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Cumulative distributions of number of sessions by (<b>A</b>) imageviews, (<b>B</b>) pageviews, and (<b>C</b>) previews partitioned by desktop (in blue) and mobile device (in orange). Page previews are available only on desktop devices</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec13">Mining image content and context</h3><p>To investigate the factors that make images engaging when reading Wikipedia, we characterize the pictures in our dataset with several features related to the visual context and content. Our choice of features is largely inspired by the literature around the cognitive perception of images in instructional or web environments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec14">Contextual factors</h4><p>Images on Wikipedia are not isolated items. Instead, they exist in <i>context</i>, providing epistemic support to the article they are illustrating. To extract features from the image context, we resort to previous literature on Wikipedia reader behavior and experimental psychology studies on the role of images in instructional settings. Note that 16% of the images appear in multiple articles. Since the same image may appear in very different articles, thus belonging to very different contexts, we treat such images as distinct.</p>
                    <h3 class="c-article__sub-heading" id="FPar1">Page topic</h3>
                    <p>In a previous study, Piccardi et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR35" id="ref-link-section-d26201386e1199">35</a>] found that Wikipedia reader engagement with references varies with the article topic. To test whether the reader’s need for visual support similarly varies across subject matters, we extract, for each page in our dataset, a <i>topic vector</i>, using the Wikidata topic model.<sup><a href="#Fn11"><span class="u-visually-hidden">Footnote </span>11</a></sup> The classifier takes as input the Wikidata item of a Wikipedia page, and it returns a 64-dimensional vector containing the probability that the article belongs to the topics of the Wikiproject hierarchy.<sup><a href="#Fn12"><span class="u-visually-hidden">Footnote </span>12</a></sup> To reduce the dimensionality of the topic vector, we consider the second level of the topic taxonomy accounting for 31 topics. We then rearranged some of the topics into coarse-grained topics, namely media, internet culture, and performing arts into <i>entertainment</i>, chemistry and biology into <i>biology</i>, computing and libraries &amp; information into <i>computer science</i>, mathematics and physics into <i>maths &amp; physics</i>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig5">5</a>(A) shows the distribution of images by article topic. Geographic articles are the most illustrated, containing <span class="mathjax-tex">\(1/4\)</span> of the images in our dataset. Biographies, making up 30% of the articles on Wikipedia, also contain around 15% of the images. Topics such as entertainment (movies, plays, books), visual arts, transportation, military, biology, and sports follow, covering together another third of the images in English Wikipedia. A summary of the numerical values can be found in the Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#MOESM1">1</a> (Supplementary Table). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Figure 5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="403"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>(<b>A</b>) Fraction of images by topic (in blue) and fraction of images with faces (in orange). (<b>B</b>) Image-specific CTR by article topic</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  
                    <h3 class="c-article__sub-heading" id="FPar2">Page length</h3>
                    <p>One of the possible functions of text illustrations in learning contexts is to enrich and complement the textual content with additional material [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Levie WH, Lentz R (1982) Effects of text illustrations: a review of research. ECTJ 30(4):195–232&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR2" id="ref-link-section-d26201386e1305">2</a>]. To investigate to which extent images are used to complement the lack of textual information, we measure the textual richness as the <i>length</i> of each article in characters. The distribution of the number of images by text length as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig6">6</a> is log-normal, with most images in English Wikipedia being found in articles between 1k and 100k characters long. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Figure 6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig6_HTML.png?as=webp"><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="398"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Feature distributions. Spearman’s rank correlation coefficient <i>ρ</i> between the numerical features and the iCTR at the top of each panel (<span class="mathjax-tex">\(p&lt;0.001\)</span> for each feature)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  
                    <h3 class="c-article__sub-heading" id="FPar3">Page popularity</h3>
                    <p>Previous work analyzing reader behavior with respect to Wikipedia citations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR35" id="ref-link-section-d26201386e1364">35</a>] found that there is an inverse relation between article popularity and reference click-through rate. To test whether this relation is valid also in the case of interactions with images, we compute a page <i>popularity</i> feature for each image by computing the total monthly pageviews for the page where an image appears. As in Piccardi et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR35" id="ref-link-section-d26201386e1370">35</a>], the page popularity follows a power-law distribution (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig7">7</a>), with 70% images having average monthly pageviews in the range between 50 and 10k. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7" data-title="Figure 7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig7_HTML.png?as=webp"><img aria-describedby="Fig7" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig7_HTML.png" alt="figure 7" loading="lazy" width="685" height="319"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Distributions of (<b>A</b>) values of iCTR by page popularity partitioned by device and (<b>B</b>) number of images per page popularity. Spearman’s rank correlation coefficient <span class="mathjax-tex">\(\rho _{iCTR}\)</span> between iCTR and pageviews in the inset (<span class="mathjax-tex">\(p&lt;0.001\)</span>). The axes are in log scale</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/7" data-track-dest="link:Figure7 Full size image" aria-label="Full size image figure 7" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  
                    <h3 class="c-article__sub-heading" id="FPar4">Readability</h3>
                    <p>Although not completely verified, another function of images in textual knowledge is to facilitate text comprehension, especially in the case of reading difficulties [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Levie WH, Lentz R (1982) Effects of text illustrations: a review of research. ECTJ 30(4):195–232&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR2" id="ref-link-section-d26201386e1461">2</a>]. To take into account this function in our study, we quantify the reading ease by computing the Flesch <i>readbility</i> score, reflecting the “comprehension difficulty of written material” [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Flesch R (1948) A new readability yardstick. J Appl Psychol 32(3):221&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR43" id="ref-link-section-d26201386e1467">43</a>], on the text of each article in our dataset. We compute the readability score for all the pages containing an image, and plot the resulting distribution in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig6">6</a>: most of the images on Wikipedia are in articles detected as “Fairly difficult to read” (score 50–60), or “Difficult to read” (score 30–50).</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar5">Length of the image caption</h3>
                    <p>Studies in educational technologies have found that the usage of captions marginally enhances the usefulness of text illustrations [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Bernard RM (1990) Using extended captions to improve learning from instructional illustrations. Br J Educ Technol 21(3):215–225&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR17" id="ref-link-section-d26201386e1481">17</a>]. To operationalize the presence of captions as a contextual feature of the images in our dataset, we store the average number of words used to caption each image when appearing in a Wikipedia article. We can see from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig6">6</a> the caption length following a Tweedie distribution with a large fraction of the images without a description and the majority of existing captions centered around ten words.</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar6">Image placement</h3>
                    <p>How images are placed in the text can play a crucial role in the knowledge exploration experience [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Peeck J (1993) Increasing picture effects in learning from illustrated text. Learn Instr 3(3):227–238&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR16" id="ref-link-section-d26201386e1495">16</a>], and researchers investigating Wikipedia reader behavior showed that people tend to engage more with content (in this case, internal hyperlinks) which lies at the top of the article [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Paranjape A, West R, Zia L, Leskovec J (2016) Improving website hyperlink structure using server logs. In: Proc. Conference on web search and data mining (WSDM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR44" id="ref-link-section-d26201386e1498">44</a>]. At the same time, Wikipedia editors follow specific placement guidelines when illustrating an article. To investigate the role of image placement on Wikipedia article consumption, we extract the image’s <i>text offset</i>, i.e., the relative position of the image with respect to the length of the article, as well as the image <i>position</i>, a categorical feature which can take the values <span class="mathjax-tex">\(\{\mathit{infobox}, \mathit{inline}, \mathit{gallery}\}\)</span>, depending on the template used to add the image to the article. From the plots in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig6">6</a> we can see that only 36% of the images in our dataset is generally placed in infoboxes, while only 16% can be found in galleries, and that the majority of inline images are generally placed at the top of the article (see <i>offset</i>). A summary of the numerical values can be found in the Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#MOESM1">1</a> (Supplementary Table).</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar7">Image resolution</h3>
                    <p>In addition to their position, the viewer’s attention may also be driven by the size of an image. According to the Wikipedia’s Image Size guidelines,<sup><a href="#Fn13"><span class="u-visually-hidden">Footnote </span>13</a></sup> editors should choose the appropriate image size in proportion of its level of details. However, readers may still tend to click on small images that are inherently difficult to observe. To investigate the role of the image size, we compute the image <i>resolution</i> in pixels for each image. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig6">6</a>, image resolutions vary across different scales, mostly ranging from 10k to 100k pixels.</p>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec15">Visual factors</h4><p>The content of pictures plays a key role in driving readers’ attention to both the images [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Gygli M, Grabner H, Riemenschneider H, Nater F, Van Gool L (2013) The interestingness of images. In: Proc. International conference on computer vision (ICCV)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR18" id="ref-link-section-d26201386e1588">18</a>] and the text on the page [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Levie WH, Lentz R (1982) Effects of text illustrations: a review of research. ECTJ 30(4):195–232&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR2" id="ref-link-section-d26201386e1591">2</a>]. To understand the type of visuals that elicit higher levels of interactions with Wikipedia images, we run a set of computer vision-based classifiers. Since training a classifier to detect every concept in Wikipedia’s visual knowledge would be practically infeasible, we instead focus on three main indicators, based on extensive literature from visual and social interestingness prediction.</p>
                    <h3 class="c-article__sub-heading" id="FPar8">Image quality</h3>
                    <p>Visual aesthetics, or image quality, is one of the top visual factors driving the viewer’s attention to an image [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Gygli M, Grabner H, Riemenschneider H, Nater F, Van Gool L (2013) The interestingness of images. In: Proc. International conference on computer vision (ICCV)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR18" id="ref-link-section-d26201386e1601">18</a>]. At the same time, researchers have found that not all images which receive much attention from web communities are actually of high quality [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Schifanella R, Redi M, Aiello LM (2015) An image is worth more than a thousand favorites: surfacing the hidden beauty of Flickr pictures. In: International conference on web and social media (ICWSM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR45" id="ref-link-section-d26201386e1604">45</a>], and that a lot of socially uninteresting pictures are very beautiful. We investigate here whether the quality of an image plays an important role in eliciting Wikipedia reader attention. To do so, we design a <i>Wikipedia Image Quality</i> classifier, as follows. </p><ul class="u-list-style-bullet">
                      <li>
                        <p>We collect a training set of images annotated with a binary (high/low) image quality score. To annotate images, we resort to the highly curated categories that Wikimedia Commons editors assign to images. We download <span class="mathjax-tex">\(141{,}984\)</span> images from the <i>Quality images</i> category from Commons:<sup><a href="#Fn14"><span class="u-visually-hidden">Footnote </span>14</a></sup> these are high-quality images that have to meet Commons’ quality guidelines<sup><a href="#Fn15"><span class="u-visually-hidden">Footnote </span>15</a></sup> before being voted and promoted as Quality images by the community through a highly selective process. Only a few images make it to the “image quality” category: there is, therefore, a large consensus on the quality of the images in that category. To collect low-quality images, we simply randomly sample an approximately equal number of pictures (<span class="mathjax-tex">\(169{,}310\)</span>) from the large pool of Commons images. These are very likely to be low quality, as images randomly drawn from Commons tend to have a small resolution, and they are rarely used to illustrate Wikipedia articles [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Erickson K, Perez FR, Perez JR (2018) What is the commons worth? Estimating the value of wikimedia imagery by observing downstream use. In: Proc. International symposium on open collaboration (OpenSym)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR27" id="ref-link-section-d26201386e1688">27</a>].</p>
                      </li>
                      <li>
                        <p>We next train a deep neural network using transfer learning: we fine-tune a pre-trained model, originally designed to classify image objects, using the image quality data collected. We use the Inception-v3 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016) Rethinking the inception architecture for computer vision. In: Proc. Conference on computer vision and pattern recognition (CVPR)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR46" id="ref-link-section-d26201386e1697">46</a>] deep network pre-trained on the 1000-classes ImageNet dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L (2009) Imagenet: a large-scale hierarchical image database. In: Proc. Conference on computer vision and pattern recognition (CVPR)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR47" id="ref-link-section-d26201386e1700">47</a>], as it was proved to be a good starting dataset for transfer learning tasks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Huh M, Agrawal P, Efros AA (2016) What makes imagenet good for transfer learning? arXiv preprint &#xA;                  1608.08614&#xA;                  &#xA;                " href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR48" id="ref-link-section-d26201386e1703">48</a>]. We use 90% of the data for training and the rest for validation, and we train the last layer of the network over 10,000 iterations with the data collected. The fine-tuned classifier achieves 77% accuracy on a balanced test set.</p>
                      </li>
                    </ul>
                    <p>The resulting image quality classifier, given any image, outputs a <i>quality score</i> in the range <span class="mathjax-tex">\([0,1]\)</span> which corresponds to the probability that the image belongs to the “High Quality” class. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig6">6</a>, most images in our dataset have a very low-quality score.</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar9">Presence of faces</h3>
                    <p>In line with several studies showing the importance of faces for web users’ positive reactions and engagement with images [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Bakhshi S, Shamma DA, Gilbert E (2014) Faces engage us: photos with faces attract more likes and comments on Instagram. In: Proc. Conference on human factors in computing systems (SIGCHI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR8" id="ref-link-section-d26201386e1752">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Pappas N, Redi M, Topkara M, Jou B, Liu H, Chen T, Chang S-F (2016) Multilingual visual sentiment concept matching. In: Proc. International conference on multimedia retrieval (ICMR)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR49" id="ref-link-section-d26201386e1755">49</a>], we also extract information about the presence of faces of people in the image. We use MTCNN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Wen Y, Zhang K, Li Z, Qiao Y (2016) A discriminative feature learning approach for deep face recognition. In: Proc. European conference on computer vision (ECCV)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR50" id="ref-link-section-d26201386e1758">50</a>] to detect faces and their bounding box in an image. For a given image, we then output a binary feature indicating whether it contains at least one face or not. We find that around <span class="mathjax-tex">\(1/3\)</span> of the images on Wikipedia have at least one face (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig6">6</a>), and most of those are in articles about biographies, entertainment, and sports (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig5">5</a>(A)).</p>
                  
                    <h3 class="c-article__sub-heading" id="FPar10">Outdoor setting</h3>
                    <p>Literature around image interestingness and aesthetics [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Gygli M, Grabner H, Riemenschneider H, Nater F, Van Gool L (2013) The interestingness of images. In: Proc. International conference on computer vision (ICCV)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR18" id="ref-link-section-d26201386e1798">18</a>] has shown that outdoor images tend to elicit the viewer’s interest more than indoor images do. To extract the information about the image scene setting, we use a Wide Residual Network [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Zagoruyko S, Komodakis N (2016) Wide residual networks. arXiv preprint &#xA;                  1605.07146&#xA;                  &#xA;                " href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR51" id="ref-link-section-d26201386e1801">51</a>] trained on MIT’s Places [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Zhou B, Lapedriza A, Khosla A, Oliva A, Torralba A (2017) Places: a 10 million image database for scene recognition. IEEE Trans Pattern Anal Mach Intell 40(6):1452–1464&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR52" id="ref-link-section-d26201386e1804">52</a>], an image dataset with 10M images annotated with 365 scene types, and indoor/outdoor labels. This classifier, given an image, outputs an <i>outdoor</i> score which reflects the probability of the image being an outdoor scenery. When the feature is ≤0.5, the image is likely to be an indoor scenery. In our dataset, indoor and outdoor images are almost equally distributed, with a slight prevalence of outdoor pictures.</p>
                  <h3 class="c-article__sub-heading" id="Sec16">Engagement metrics</h3><p>To quantify the volume of readers’ interactions with visual content, we introduce the following metrics:</p>
                  <h3 class="c-article__sub-heading" id="FPar11">Global click-through rate</h3>
                  <p>The global click-through rate (gCTR) measures the overall reader engagement with images. It is defined as the fraction of reading sessions with at least one interaction with an image. Formally, for each session <i>s</i>, let <span class="mathjax-tex">\(C(s,p)\)</span> be the indicator function that is 1 if at least one image was clicked on page <i>p</i> by the respective reader, and 0 otherwise. Moreover, let <span class="mathjax-tex">\(N(p)\)</span> be the number of distinct reading sessions during which page <i>p</i> was loaded. We define the global click-through rate as 
			</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathrm{gCTR} = \frac{\sum_{s} \sum_{p} C(s,p)}{\sum_{p} N(p)}, \end{aligned}$$ </span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> where <i>p</i> ranges over the set of pages that contain at least one image.</p>
                
                  <h3 class="c-article__sub-heading" id="FPar12">Image-specific click-through rate</h3>
                  <p>The image-specific click-through rate (iCTR) measures how much engagement a Wikipedia image elicits. It is defined as the ratio of clicks to impressions. Formally, let <span class="mathjax-tex">\(N(i)\)</span> be the number of distinct sessions with clicks on image <i>i</i> and <span class="mathjax-tex">\(N(p_{i},i)\)</span> the number of distinct sessions that viewed page <span class="mathjax-tex">\(p_{i}\)</span> where the image is placed, the image-specific click-through rate is 
			</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathrm{iCTR}(i) = \frac{N(i)}{\sum_{p_{i} \in P_{i}} N(p_{i},i)}, \end{aligned}$$ </span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p> where <span class="mathjax-tex">\(p_{i}\)</span> ranges over the set <span class="mathjax-tex">\(P_{i}\)</span> of pages containing <i>i</i>.</p>
                
                  <h3 class="c-article__sub-heading" id="FPar13">Conversion rate</h3>
                  <p>The conversion rate (CR) quantifies the probability of clicking on an article link after its preview is shown in another article. Formally, for each page <i>p</i> and session <i>s</i>, we denote by <span class="mathjax-tex">\({C(s,p)}\)</span> the indicator function that is one if session <i>s</i> has clicked on a link to page <i>p</i> after seeing its preview. Moreover, we denote by <span class="mathjax-tex">\(N(p)\)</span> the total number of distinct sessions that loaded a preview of <i>p</i>. The conversion rate for page <i>p</i> can be written as: 
			</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex"> $$\begin{aligned} \mathrm{CR}(p) = \frac{\sum_{s} C(s,p)}{N(p)} \end{aligned}$$ </span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                  <p>In the following sections, we restrict our analyses to images visualized by at least 50 readers during the period of our data collection in order to reduce the effect of rarely viewed articles and obtain a reliable estimate of the quantities above. This results in a set of 3.2M unique images displayed in 2.7M articles.</p>
                </div></div></section><section data-title="RQ1: to what extent are readers interacting with images in Wikipedia?"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">RQ1: to what extent are readers interacting with images in Wikipedia?</h2><div class="c-article-section__content" id="Sec17-content"><p>The first step of our analysis is to quantify the volume of readers’ interactions towards visual content when reading Wikipedia. To this aim, we compute the global click-through rate and image-specific click-through rate on our data and find the following.</p><h3 class="c-article__sub-heading" id="Sec18">Overall engagement with images: the global click-through rate</h3><p>We find that the gCTR across all pages in English Wikipedia with at least one image is 3.5%, meaning that around 3.5 out of 100 times readers visit a page, they also click on an image. This metric is higher for desktop (5.0%) and lower for mobile web users (2.6%), probably due to differences in the way readers navigate Wikipedia on the two devices and the better Media Viewer experience on desktop. Over time, the behavior also changes depending on the device used. For example, on desktop, readers tend to click more often on images during weekdays (Monday to Friday), with an increase of 5.5% over weekends. However, on mobile, there is no significant difference between week and weekends. To understand whether these values represent a high or low level of engagement, we can compare them with engagement metrics on another type of article content, namely article’s references. According to Piccardi et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR35" id="ref-link-section-d26201386e2390">35</a>], the gCTR on citations in English Wikipedia is 0.29%, thus around ten times lower than for images. This observation suggests that images tend to elicit a different level of engagement than those on references for English Wikipedia.</p><h3 class="c-article__sub-heading" id="Sec19">Average engagement with individual images: image-specific click-through rate</h3><p>On average, an image in a Wikipedia article gets clicked 2.6 times every 100 impressions. Again, the iCTR is higher (3.2%) for desktop than for mobile users (2.2%). In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig8">8</a> we report examples of highly engaging and less engaging images. By visually inspecting these results, we can see some visual trends: highly engaging images seem to depict outdoor environments. In contrast, among the images with low levels of iCTR, we can find human faces. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8" data-title="Figure 8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig8_HTML.jpg?as=webp"><img aria-describedby="Fig8" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig8_HTML.jpg" alt="figure 8" loading="lazy" width="685" height="410"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Examples of <i>high</i> and <i>low</i> image-specific CTR images by page popularity (left) and image quality (right). We ranked images by iCTR, popularity and quality, and picked examples from the top-100 (“high”) and bottom-100 (“low”) for each dimension</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/8" data-track-dest="link:Figure8 Full size image" aria-label="Full size image figure 8" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section data-title="RQ2: what drives reader’s engagement with images when reading Wikipedia articles?"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">RQ2: what drives reader’s engagement with images when reading Wikipedia articles?</h2><div class="c-article-section__content" id="Sec20-content"><p>To address RQ2, we now model reader interaction with images on Wikipedia using the factors listed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec13">4.3</a>.</p><h3 class="c-article__sub-heading" id="Sec21">Exploratory analysis</h3><p>We start our analysis by seeking a relationship between our target metric, the iCTR, and each of the contextual and visual factors in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec13">4.3</a>. We report the Spearman’s rank correlation coefficients <span class="mathjax-tex">\(\rho _{ctr}\)</span> between the iCTR and the scalar predictors in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig6">6</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig7">7</a>. Considering the contextual factors, we observe a negative correlation with article length and popularity (<span class="mathjax-tex">\(\rho =-0.31\)</span> and <span class="mathjax-tex">\(\rho =-0.21\)</span>, respectively). When further investigating the relationship with article popularity (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig7">7</a>), we find that it seems non-linear: engagement with images is low for highly unpopular pages. It becomes higher for pages in a mid-level bucket of popularity and drops again for highly viewed pages. Regarding the image size, despide images are displayed in different resolutions, this does not have a clear relation with the iCTR (<span class="mathjax-tex">\(\rho =-0.002\)</span>). When considering the position in the page instead, the median iCTR is higher for images in galleries (median <span class="mathjax-tex">\(\mathrm{iCTR}=0.024\)</span>) than for images in the infobox (median <span class="mathjax-tex">\(\mathrm{iCTR}=0.019\)</span>) and inline (median <span class="mathjax-tex">\(\mathrm{iCTR}=0.016\)</span>). Moreover, we see signals of reader visual preferences in terms of article topics (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig5">5</a>(B)): the topics with the highest median value are transportation (0.037) and visual arts (0.037), while politics and sports show the lowest level of interaction with a median iCTR of 0.008. Finally, the correlation analysis of the visual factors confirms our initial intuitions from the visual analysis. There is a positive correlation between the iCTR and outdoor scenery (<span class="mathjax-tex">\(\rho =0.23\)</span>) and a negative relation between the presence of faces and readers’ engagement (<span class="mathjax-tex">\(\rho =-0.14\)</span>). A complete summary of the numerical values discussed can be found in the Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#MOESM1">1</a> (Supplementary Table).</p><h3 class="c-article__sub-heading" id="Sec22">Regression analysis</h3><p>Next, we aim to understand how much these features are predictive of reader engagement with images. To do so, we perform a logistic regression analysis that classifies images according to their iCTR.</p>
                  <h3 class="c-article__sub-heading" id="FPar14">Study design</h3>
                  <p>We build the training set as follows. We take the median value of iCTR and label the images in our dataset with two classes of <i>high</i> and <i>low</i> iCTR according to whether their iCTR is above or below the median.<sup><a href="#Fn16"><span class="u-visually-hidden">Footnote </span>16</a></sup> We use the contextual and visual factors described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec13">4.3</a> as predictors and the binary iCTR as the target variable. Moreover, we split the predictors into two sets of features and train two separate logistic regression models. The first set of features consists of the topic vectors, while the second consists of the remaining other factors. In the second set of features, we log-transform variables that span over different scales, such as page popularity, text length, caption length, and the number of faces. Moreover, to reduce the amount of multicollinearity among the predictors, we manually inspect the correlation table and compute the Variance Inflation Factor [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Kutner MH, Nachtsheim CJ, Neter J, Li W et al. (2005) Applied linear statistical models, vol 5&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR53" id="ref-link-section-d26201386e2706">53</a>] for each variable. We decide to exclude the <i>inline</i> variable, as it shows strong collinearity with <i>gallery</i> and <i>infobox</i>. Finally, we standardize each predictor in the two sets of features.</p>
                
                  <h3 class="c-article__sub-heading" id="FPar15">Impact of image resolution</h3>
                  <p>We found that images on Wikipedia are displayed in different resolutions. Before running the regression analysis, we test the hypothesis that the image size could be decisive in attracting clicks, i. e. readers may tend to click on smaller images as it may be harder to see the details. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec21">6.1</a> we found the correlation coefficient to be −0.002 (with <span class="mathjax-tex">\(p &lt; 0.001\)</span>), indicating no clear relationship between the two variables. Moreover, we observe that image resolution is highly related to its position within the page: the median resolution is about 46, 36, and 11 megapixels respectively for images in the infobox, inline, and in galleries. Also, image resolution is highly correlated with some topics, e. g. it has large positive correlation with biography and entertainment, and large negative correlation with geography and visual arts. Since the image resolution does not seem to be directly related to the iCTR, while it seems to be influenced by some other independent variables, and thus may act as a confounder, we decide not to take it into account in the subsequent analyses.</p>
                
                  <h3 class="c-article__sub-heading" id="FPar16">Controlling for page length and popularity</h3>
                  <p>Similar to what was described in previous work on engagement with Wikipedia content [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR35" id="ref-link-section-d26201386e2759">35</a>], we found that the page popularity and the text length have strong negative correlations with the target. Since page popularity and text length show large variations across the other predictors, especially across topics, we remove the effect of these two confounding variables with a matched study. We build a bipartite graph with images of low and high iCTR as nodes of the two halves. We split the log-transformed page popularity and text length ranges into 100 bins of equal size each, and assign the nodes to these bins, linking two nodes of opposite iCTR when falling into the same bins of popularity and length. Finally, we use min-weight matching on the bipartite graph to find pairs of high/low iCTR samples that minimize the Euclidean distance between all pairs. This procedure succesfully balanced the dataset, with the standardized mean difference of text length and pape popularity across the two classes dropping from −0.54 and −0.51 to −0.010 and −0.007, respectively.</p>
                
                  <h3 class="c-article__sub-heading" id="FPar17">Results</h3>
                  <p>The resulting regression models have an area under the ROC curve (AUC) of 0.67 and 0.62 for the model trained on the topics and the model trained on the other variables, respectively. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig9">9</a> shows the resulting models coefficients. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig9">9</a>(A), we observe that clicks on images are more often related to topics such as transportation, visual arts, geography, and military. On the contrary, clicks on images are less likely in education, sports, and entertainment articles. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig9">9</a>(B), we observe that the most important negative predictor is the text offset, i.e. the relative position of the image with respect to the length of the article, meaning that images are more clicked if placed in the upper part of an article. Regarding the visual content, we observe a strong positive effect of outdoor settings, consistently with the positive coefficients of transportation and geography, topics in which a large portion of images display outdoor scenes. Regarding the image position on the page, we find that images in galleries show a high level of engagement, as well as images in the infobox, even though with a moderate effect. Noteworthy, the presence of faces has negative impact in predicting a high level of interactions with images, contrary to what we would expect from the literature [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Bakhshi S, Shamma DA, Gilbert E (2014) Faces engage us: photos with faces attract more likes and comments on Instagram. In: Proc. Conference on human factors in computing systems (SIGCHI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR8" id="ref-link-section-d26201386e2780">8</a>]. In the remainder of this section, we further investigate this inconsistency in depth, by performing a clustering experiment and an observational study on the images in our dataset. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9" data-title="Figure 9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig9_HTML.png?as=webp"><img aria-describedby="Fig9" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig9_HTML.png" alt="figure 9" loading="lazy" width="685" height="529"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Association of the features with the image iCTR expressed as coefficients of the logistic regressions. (<b>A</b>) Coefficients of the model trained with topics of the article as predictors. (<b>B</b>) Coefficients of the model trained with the other variables of the image. Error bars represent 95% confidence intervals</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/9" data-track-dest="link:Figure9 Full size image" aria-label="Full size image figure 9" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec23">Identifying prototypical image groups</h3><p>To dive deeper into the results emerging from the regression analysis, we provide in this section a non-linear multivariate analysis of our data.</p>
                  <h3 class="c-article__sub-heading" id="FPar18">Study design</h3>
                  <p>Our goal is to draw a complementary picture of the complex interplay between reader engagement and image features, identifying prototypical groups of Wikipedia images with homogeneous characteristics. To this extent, we perform a density-based clustering using HDBSCAN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Campello RJ, Moulavi D, Sander J (2013) Density-based clustering based on hierarchical density estimates. In: Proc. Pacific-Asia conference on knowledge discovery and data mining (PAKDD)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR54" id="ref-link-section-d26201386e2823">54</a>], which seeks partitions with high density areas of points separated by low density areas, possibly containing noise objects. The advantage of using HDBSCAN is threefold: first, its density-based structure allows to better identify areas of continuous, non-globular points compared to other clustering algorithms that rely on the assumptions of spherical shape clusters, e.g., k-means [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Lloyd S (1982) Least squares quantization in pcm. IEEE Trans Inf Theory 28(2):129–137&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR55" id="ref-link-section-d26201386e2826">55</a>]. Second, by labeling the sparse background points as noise, it aggregates data into coherent clusters rather than partitions. Finally, it extends DBSCAN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Ester M, Kriegel H-P, Sander J, Xu X et al. (1996) A density-based algorithm for discovering clusters in large spatial databases with noise. In: International conference on knowledge discovery and data mining (KDD)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR56" id="ref-link-section-d26201386e2829">56</a>] by implementing a hierarchical clustering approach that allows to extract the optimal flat grouping based on the stability of the clusters, allowing to find groups with non homogeneous density in contrast to a global density threshold adopted by DBSCAN.</p>
                  <p>We run HDBSCAN<sup><a href="#Fn17"><span class="u-visually-hidden">Footnote </span>17</a></sup> on the features set described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec22">6.2</a> including the binary iCTR variable and limiting the analysis to the eight most popular topics (geography, biography, entertainment, visual arts, transportation, sports, military, and biology) that account for 92% of the images in our corpus. HDBSCAN has two main hyper-parameters that have significant practical effect on the clustering: <i>min_cluster_size</i> which refers to the minimum number of grouped items to consider as a cluster, and <i>min_samples</i> which provides a measure of how conservative the clustering would be defining the level at which points are considered noise. The larger the value, the more conservative the clustering, that implies more points will be declared as noise, and clusters will be restricted to progressively more dense areas. We explore the hyper-parameter space with a grid search approach to find the best configuration that maximizes the Density-Based Clustering Validation (DBCV) index [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Moulavi D, Jaskowiak PA, Campello RJ, Zimek A, Sander J (2014) Density-based clustering validation. In: Proc. SIAM international conference on data mining (SDM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR58" id="ref-link-section-d26201386e2863">58</a>]. Due to computational constraints, we perform the clustering on a random sample of 50K images, we repeat the procedure 5 times to assess the stability of the tuning phase. We achieve the best configuration with <span class="mathjax-tex">\(\mathit{min}\_\mathit{cluster}\_\mathit{size}=600\)</span> and <span class="mathjax-tex">\(\mathit{min}\_\mathit{samples}=5\)</span> in the majority of the runs. With these settings, we identify 23 clusters, with a number of images ranging between 600 and 5000.</p>
                
                  <h3 class="c-article__sub-heading" id="FPar19">Results</h3>
                  <p>We summarize in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig10">10</a> the characteristics of the centroids of the 12 most populated clusters, where each facet represents the mean value of that feature across the examples in that cluster. For ease of visualization, we discretize continuous variables in three classes: <i>low</i>, <i>medium</i>, or <i>high</i>, according to whether the value falls, respectively, in the first, second, or third quantile of the feature distribution. To provide a more clear visual representation of the clusters, we labeled them with descriptive names. We also manually inspected the images in each cluster and chose two to four representative images among the most popular ones. A complete summary of the clustering results can be found in the Additional file <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#MOESM1">1</a> (Supplementary Figure). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10" data-title="Figure 10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig10_HTML.jpg?as=webp"><img aria-describedby="Fig10" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig10_HTML.jpg" alt="figure 10" loading="lazy" width="685" height="656"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Visual representation of the clustering. The radar plots show for a group centroid the intensity of each feature on a three classes scale. We summarize in green the topics that cover at least 85% of the images categories in a cluster</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/10" data-track-dest="link:Figure10 Full size image" aria-label="Full size image figure 10" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>In the rest of this section, we explore more in depth image quality and its interplay with images containing faces. Even though quality appears, on aggregate, to be moderately positively associated with the tendency to click on images, the underlying phenomenology is more nuanced. On one hand, high-quality images within the geography, transportation, visual arts, military, and biology categories (clusters 2, 3, 5, 6, 7, 8, and 9) show high iCTR across a wide range of contextual factors. A large portion of these images depicts outdoor sceneries that is coherent with the positive coefficient of the <i>outdoor</i> feature in the regression in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec22">6.2</a>. On the other hand, low quality images are often associated with the presence of faces, especially in topics such as biography, entertainment, and sports, wich overall tend to have a lower click-through rate. Focusing on the interplay between biographies and iCTR reveals significant differences across page popularity and topics worth studying. Images within unpopular biographies, predominantly inline and with a curated textual description, show high iCTR (cluster 10), as well as images placed in galleries in biographies of unpopular artists (cluster 1). On the contrary, popular biographies (cluster 11) or pages that present popular athletes (cluster 12), experience a low iCTR. A possible explanation for this behavior is that users may tend to click on an image in a biography if they do not recognize immediately the subject depicted, while for prominent celebrities, especially if the image is accessible in the infobox, the information need is fulfilled without the need of a click and the interaction with the Media Viewer.</p>
                <h3 class="c-article__sub-heading" id="Sec24">Are faces engaging on Wikipedia?</h3><p>As pointed out in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec13">4.3</a>, images with faces generally elicit high social engagement. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec22">6.2</a>, we found that the number of faces has negative weight with respect to the iCTR, while in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec23">6.3</a> we observed that Wikipedia readers are more likely to click on images with faces only when placed in less popular biographies. To further investigate this aspect, we design a matched observational study in which we compare the iCTR between images with and without faces. To reduce the effects of confounding factors, we perform a pairwise comparison of images with similar covariates using propensity score matching.</p>
                  <h3 class="c-article__sub-heading" id="FPar20">Propensity score matching</h3>
                  <p>Propensity score matching [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Abadie A, Imbens GW (2006) Large sample properties of matching estimators for average treatment effects. Econometrica 74(1):235–267&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR59" id="ref-link-section-d26201386e3006">59</a>] is a statistical technique to evaluate the efficacy of a treatment against a control group, while taking into account the effect of confounding factors. The propensity score is defined as the probability of a sample being treated as a function of the covariates, and it is obtained by training a logistic regression with the covariates as predictors, and the treatment/control variable as target. As a result, observations with the same propensity scores have the same distribution across the observed covariates.</p>
                  <p>In our experiment, we define images with at least one face as receiving the treatment, images without a face as the control group, and the variables used in the logistic regression (except for the topics and the page popularity) as the covariates.</p>
                
                  <h3 class="c-article__sub-heading" id="FPar21">Results</h3>
                  <p>We consider images in articles about biography, entertainment, and sports, accounting for 90% of all images with at least one face. We find pairs of images minimizing the propensity score within pairs of articles. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig11">11</a> shows the iCTR as a function of the page popularity, for images with (in orange) and without (in blue) faces. According to a Mann–Whitney <i>U</i> test, the difference between the two distributions is statistically significant, with <span class="mathjax-tex">\(p&lt;0.001\)</span>. The tendency to click on images with faces varies depending on page popularity. On pages with less that 1000 monthly pageviews, the presence of faces induces higher level of interactions, with a difference of 0.1%, whereas, after 1000 pageviews, we observe the opposite behavior, with a difference of 0.06%. This also confirms the findings of the clustering analysis. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11" data-title="Figure 11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig11_HTML.png?as=webp"><img aria-describedby="Fig11" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig11_HTML.png" alt="figure 11" loading="lazy" width="685" height="416"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Comparison of the iCTR for images with faces (orange) and without faces (blue) as function of the popularity (<i>pageviews</i>). Error bands represent bootstrapped 95% CIs</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/11" data-track-dest="link:Figure11 Full size image" aria-label="Full size image figure 11" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>To ascertain that our findings remain valid also for non-biographical articles, we replicate the same study by including all the topics in the matching procedure. In this case, we observe a different behavior. Images with faces are less likely to be clicked than others, across all the popularity range. This may explain the overall negative coefficient of the faces feature in the regression analysis, and highlight the role that faces play in increasing engagement on biographical articles.</p>
                </div></div></section><section data-title="RQ3: do images support reader’s need for additional information when navigating Wikipedia?"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">RQ3: do images support reader’s need for additional information when navigating Wikipedia?</h2><div class="c-article-section__content" id="Sec25-content"><p>We found that readers show a signal of interest in images when reading Wikipedia articles. But are images useful to fulfill part of the reader’s information need when navigating the website? To address this question, we design an additional study that attempts to estimate whether the presence of an image in an article preview can complement the textual information and support in-depth reading.</p>
                <h3 class="c-article__sub-heading" id="FPar22">Matching articles</h3>
                <p>To check the difference in terms of conversion rate between articles having and not having an image, we first need to reduce the impact of exogenous factors that may potentially drive reader attention on articles, other than the presence of an image. For example, events localized in time can have the effect of sporadically increasing the interest towards specific articles, and therefore on the number of edits [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Georgescu M, Kanhabua N, Krause D, Nejdl W, Siersdorfer S (2013) Extracting event-related information from article updates in Wikipedia. In: Proc. European conference on information retrieval (ECIR)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR60" id="ref-link-section-d26201386e3088">60</a>]. Similarly, the probability of clicking on an article may also depend on its centrality in the article network, i.e. on its <i>in-degree</i>, which is the number of page links pointing to that article. Ideally, we would like to find pairs of articles—one with, the other without image in the preview—that are similar in such factors. To control for these factors, we resort again to propensity score matching. In this experiment, articles with an image in the preview are the treatment group, articles without images are the control, and we use text length, number of edits, and in degree as variables for the matching procedure.</p>
              
                <h3 class="c-article__sub-heading" id="FPar23">Results</h3>
                <p>We find pairs of articles by minimizing the propensity score within pairs of articles. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig12">12</a> shows the conversion rate as a function of article popularity (total number of page views), for articles with (in blue) and without (in yellow) an image in the preview. We find that, according to a Mann–Whitney <i>U</i> test, the difference is statistically significant (<span class="mathjax-tex">\(p&lt;0.001\)</span>), across all the popularity spectrum, with a difference of 2% in the conversion rate. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12" data-title="Figure 12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Figure 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig12_HTML.jpg?as=webp"><img aria-describedby="Fig12" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_Fig12_HTML.jpg" alt="figure 12" loading="lazy" width="685" height="415"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Comparison of the conversion rate for preview tooltip with an image (purple) and without image (green) as function of the page popularity (<i>pageviews</i>). Error bands represent bootstrapped 95% CIs</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/10.1140/epjds/s13688-021-00312-8/figures/12" data-track-dest="link:Figure12 Full size image" aria-label="Full size image figure 12" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>We rank all pages by conversion rate, and manually inspect the top and bottom articles, with and without images. We find that most of the illustrated articles with higher conversion rate tend to be long lists of aggregated pieces of content related to the same topic, e,g., achievements/publications (movies, books, articles) from notable people or shows. Highly clicked illustrated page previews are often also historical events, or elections, namely information-dense articles where the lead image is only partially useful to grasp the entire article content and its complexity. Conversely, illustrated pages with low conversion rate are articles talking about a specific place (e.g., “Old Fortress, Corfu”), or a specific person, object or spieces (e.g., “Microvelia Macgregori”), namely articles where an illustration can satisfy most of the information need.</p>
                <p>Unillustrated page previews with high conversion rate are much more diverse, they go from individual objects or people, e.g. (“Fanny Sidney”), where more textual information is needed to understand the subject in absence an image, to lists and events. Unillustrated articles with lower conversion rate instead tend to be about subjects where a visual explanation is not necessarily needed in order to fully understand the information: for example, generic concept such as “Authority”. “Miniseries”, or “Bachelor of Science”, where images could actually be misleading or give a biased perception of the abstract piece of knowledge.</p>
              </div></div></section><section data-title="Discussion and conclusions"><div class="c-article-section" id="Sec26-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec26">Discussion and conclusions</h2><div class="c-article-section__content" id="Sec26-content"><p>We provided a comprehensive overview over Wikipedia’s visual world and how readers interact with it. We analyzed reader interactions with visual encyclopedic knowledge and found that images attract more attention than other interactive parts of the article: on average, click-through rate on images is 3.5%, while, for example, reference clicks happen only for 1 in 300 pageviews [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR35" id="ref-link-section-d26201386e3165">35</a>]. Our insights can be summarized as follows: </p><ul class="u-list-style-bullet">
                <li>
                  <p><i>Images serve a cognitive purpose.</i> We found a negative relation between article length and iCTR. This suggests that, similar to references [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR35" id="ref-link-section-d26201386e3176">35</a>], images might be used by readers to complement missing information in the article, fulfilling part of their <i>cognitive</i> function of providing knowledge complementary to the text [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Levie WH, Lentz R (1982) Effects of text illustrations: a review of research. ECTJ 30(4):195–232&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR2" id="ref-link-section-d26201386e3182">2</a>]. Through a matched observational study, we also found that readers tend to click more often on unillustrated Wikipedia page previews to expand their content. On the contrary, conversion rate on illustrated page previews is consistently much lower across popularity buckets, thus suggesting that readers’ need for contextual information is often fulfilled by the presence of an image on the preview popup. In this work, we also tested the relation betwen readers’ interactions with images and article readability: our hypothesis was that images provide a <i>compensatory</i> function for articles that are difficult to read. However, we found evidence of the opposite trend: more readable articles tend to elicit higher engagement with images. While this is a preliminary result, further investigation is needed to understand how images support learning in low readability contexts.</p>
                </li>
                <li>
                  <p><i>We engage more with images illustrating the world and complex objects.</i> Our different layers of analysis consistently expose that Wikipedia readers are attracted by images about geographic locations, especially monuments and maps, and illustrations about biological sciences. Moreover, while we did not explicitly encode the notion of image <i>complexity</i> into our models, we found that Wikipedia readers tend to interact more often with images of complex objects, such as the ones in articles about visual arts, transportation, and military topics. A similar relation between the complexity of the image and its visual interestingness, i.e., the extent to which an image catches the viewer attention, has been widely explored and verified in experimental psychology and computer vision literature [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Constantin MG, Redi M, Zen G, Ionescu B (2019) Computational understanding of visual interestingness beyond semantics: literature survey and analysis of covariates. ACM Comput Surv 52(2):1–37&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR10" id="ref-link-section-d26201386e3199">10</a>]. While this relation can be influenced by different visual factors, such as the image size and its content, our results seem to support similar hypothesis, and provide a starting point for further investigation on the relation between image complexity and reader engagement.</p>
                </li>
                <li>
                  <p><i>Faces engage us, but only if unfamiliar.</i> Consistently, research works from different fields suggest that people and web users engage more with faces [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Morton J, Johnson MH (1991) Conspec and conlern: a two-process theory of infant face recognition. Psychol Rev 98(2):164&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR61" id="ref-link-section-d26201386e3210">61</a>] and face pictures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Bakhshi S, Shamma DA, Gilbert E (2014) Faces engage us: photos with faces attract more likes and comments on Instagram. In: Proc. Conference on human factors in computing systems (SIGCHI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR8" id="ref-link-section-d26201386e3213">8</a>], especially celebrities [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Tsikrika T, Diou C (2014) Multi-evidence user group discovery in professional image search. In: Proc. European conference on information retrieval (ECIR)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR22" id="ref-link-section-d26201386e3216">22</a>], than with other objects or subject, both in online platforms and in the real world. In this work, we found an opposite trend: for Wikipedia readers, images with faces seem to be much less engaging than, for example, more “encyclopedic” images about monuments or transportation. However, we also found that readers do interact with face images when they are placed in unpopular articles, i.e. when those faces represent less well-known people or are <i>unfamiliar</i>. This positive relation between unfamiliarity and engagement again confirms findings from previous research linking the interestingness of a visual object with its familiarity to the observer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Constantin MG, Redi M, Zen G, Ionescu B (2019) Computational understanding of visual interestingness beyond semantics: literature survey and analysis of covariates. ACM Comput Surv 52(2):1–37&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR10" id="ref-link-section-d26201386e3222">10</a>].</p>
                </li>
              </ul>
                <h3 class="c-article__sub-heading" id="FPar24">Implications</h3>
                <p>This paper represents a first step towards understanding the importance of images for free knowledge ecosystems. Inspired by theories and ideas from experimental psychology and cognitive science, and by previous studies on Wikipedia readers and web users, our findings describe for the first time how web users interact with the largest source of visual encyclopedic knowledge on the Web. These insights have several implications for different audiences.</p>
                <p>For researchers, our results show the feasibility of large scale studies to understand the role of images in instructional settings using a multimodal, computational approach. To this end, experiments could be designed along the same lines of this research, analyzing data coming from, for example, online learning platforms or MOOCs. Researchers could expand the depth and breath of modalities to better understand how and where images should be placed to maximize engagement and learning on the platforms. Researchers could use this work as the basis to build predictive models for image engagement, on Wikipedia and beyond. While our work used basic visual features to understand how readers interact with images, more advanced vision techniques could be used to build end-to-end classifiers that predict the interestingness of an image for Wikipedia readers. This work represents a first step towards understanding the role of images in online instructional settings. While explaining the importance of images in learning is outside the scope of this work, our study shed light on how readers interact with images on Wikipedia, what attracts their attention and which types of visual content they engage with. We look at readers’ interest and usage of images using a fairly implicit, large-scale signal, namely image click-through rate. Future work looking at understanding how readers learn through Wikipedia will need to employ a different set of techniques and signals, i.e., large-scale user studies, focus groups and reading comprehension surveys. This work can be used as a starting point for learning studies. Our feature design is heavily inspired by theories from experimental psychology, computational aesthetics and educational technology research, as well as previous studies analyzing the behavior of Wikipedia readers. Researchers interested in working on learning aspects related to Wikipedia will be able to tap into the same corpus of literature, and look into similar feature design choices.</p>
                <p>For editors, given the large amount of unillustrated articles on Wikipedia, and the high level of interest in visual encyclopedic content, the analysis in this paper can help editors prioritize the inclusion of visual content in areas that are highly engaging for Wikipedia readers. Longer term, models and products incoporating signals of readers’ interest in visual content would be extremely helpful for editors. Tools designed to automatically predict reader engagement with images could be incorporated in services and models that help find and prioritize the right images for Wikipedia articles. Given the limited amount of information editors have about how readers interact and learn with Wikipedia content, having visibility over the potential usefuleness of an image in an article would be tremendously helpful to improve editor workflows.</p>
                <p>For the broader Wikimedia community, the fact that images help arise interest in free knowledge justifies investments and initiatives designed to improve the pictorial representations of Wikipedia. Our findings on readers interacting with images of monuments and science further encourage the flourishing of initiatives such as Wiki Loves Monuments and Wiki Loves Science which aim at increasing the pictorial representations of these topics. Similarly, the fact that readers are more interested in pictures of unfamiliar people further justifies the existence of organizations such as “Whose Knowledge?”,<sup><a href="#Fn18"><span class="u-visually-hidden">Footnote </span>18</a></sup> who pushes towards the inclusion of visual content in biographies of people from under-represented communities.</p>
                <p>For content creators interested in contributing to free knowledge communities and in making their content available in the open, our results provides an initial list of areas of content where closing the visual knowledge gap on Wikipedia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Redi M, Gerlach M, Johnson I, Morgan J, Zia L (2020) A taxonomy of knowledge gaps for wikimedia projects (second draft). arXiv preprint &#xA;                  2008.12314&#xA;                  &#xA;                " href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR62" id="ref-link-section-d26201386e3260">62</a>] is crucial. Knowing that readers tend to be attracted by specific subjects and topics can help the design of new content creation campaigns and donations. The Wikimedia communities, the Wikimedia Foundation, and any web user interested in free knowledge can use these findings to collaborate with GLAM institutions and content creators to make relevant visual content free to use.</p>
                <p>Finally, while the scope of this paper is limited to the encyclopedia, Wikipedia represents a central hub of the web ecosystem and the public domain. Its open visual content is re-used across multiple platforms and users, and its images are surfaced at the top of both text and image search results. With this paper, we hope to provide a novel set of results and insights that can build towards better, more open and accessible visual knowledge on Wikipedia, and in turn influence the global accessibility of open visual content in the broader web.</p>
              
                <h3 class="c-article__sub-heading" id="FPar25">Limitations</h3>
                <p>While the final goal of our research is to understand images on the broad free knowledge ecosystem, one main limitation of this work is that it mainly focuses on <i>English</i> Wikipedia. With this in mind, we hope in the future to extend this work to include a more representative set of Wikipedia language editions and compare how different language communities interact with visual content.</p>
                <p>Most of our analysis depends on the output of existing machine learning models, such as ORES [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Halfaker A, Geiger RS (2020) Ores: lowering barriers with participatory machine learning in wikipedia. Proc Human-Computer Interaction (HCI)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR5" id="ref-link-section-d26201386e3280">5</a>], MTCNN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Wen Y, Zhang K, Li Z, Qiao Y (2016) A discriminative feature learning approach for deep face recognition. In: Proc. European conference on computer vision (ECCV)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR50" id="ref-link-section-d26201386e3283">50</a>] or the novel Wikimeda Image Quality classifier. While pretty effective for this task, not all these models have been tested for fairness and inclusivity. As part of our improvements to this work, we would like to employ models that are as debiased as possible and that can be easily applied to images and articles from all around the world.</p>
                <p>Readers from different parts of the world come to Wikipedia with different information needs [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Lemmerich F, Sáez-Trumper D, West R, Zia L (2019) Why the world reads Wikipedia: beyond English speakers. In: Proc. International conference on web search and data mining (WSDM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR4" id="ref-link-section-d26201386e3289">4</a>]. Additionally, researchers in multimedia computing have shown that different language communities [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Pappas N, Redi M, Topkara M, Jou B, Liu H, Chen T, Chang S-F (2016) Multilingual visual sentiment concept matching. In: Proc. International conference on multimedia retrieval (ICMR)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR49" id="ref-link-section-d26201386e3292">49</a>] and geographies [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;You Q, García-García D, Paluri M, Luo J, Joo J (2017) Cultural diffusion and trends in Facebook photographs. In: Proc. International conference on web and social media (ICWSM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR63" id="ref-link-section-d26201386e3295">63</a>] perceive and produce visual content in different ways. While we focused here on the context and content of Wikipedia images, our analysis completely ignores the characteristics of <i>readers</i>, such as geographic location, internet connection availability for image download, and native language. Our early experiments on global reader behavior show that the way in which readers interact with images on Wikipedia tend to differ across geographic locations, mainly due to broadband availability, modality of access (mobile vs. desktop), and availability of content in their languages. Previous research has indeed shown that the scope and uniqueness of visual material, as well as the availability of content for specific topics largely varies across different language editions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;He S, Lin AY, Adar E, Hecht BJ (2018) The_tower_of_babel. jpg: diversity of visual encyclopedic knowledge across Wikipedia language editions. In: Proc. International conference on web and social media (ICWSM)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR6" id="ref-link-section-d26201386e3301">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;Piccardi T, West R (2021) Crosslingual topic modeling with WikiPDA. In: Proc. The web conference (WWW)&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR64" id="ref-link-section-d26201386e3305">64</a>]. Our next research will extend this analysis to understand, in a privacy-preserving manner, the behavior of different groups of readers with visual encyclopedic content and the impact of exogenous events on image viewership.</p>
                <p>Finally, this analysis merely <i>quantifies</i> reader interactions with images, without understanding the actual reason behind the action of clicking on visual content. Our choice of metrics for interest operationalization was driven by an extensive literature studying user interactions with content on web platforms, as reported in Related Work. Click-through rate and conversion rate are widely used to measure image relevance, search satisfaction, user interest in illustrated ads, and reader interactions with citations on Wikipedia. While providing a big picture of readers’ behavior with Wikipedia visual content, a more detailed representation of user interactions could provide complementary insights on this front. Future work will explore a larger set of metrics such as hovers, dwell-time, and eye tracking movements. These metrics are not currently collected by the Wikipedia instrumentation pipeline and we will need to research additional data collection tools. As part of our efforts to understand the importance of images in free knowledge ecosystems, in the future we will also use surveys and user studies to learn <i>why</i> readers look at images on Wikipedia, and further characterize how people use the largest visual encyclopedic knowledge repository.</p>
              </div></div></section>
                    <section data-title="Availability of data and materials"><div class="c-article-section" id="availability-of-data-and-materials-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="availability-of-data-and-materials">Availability of data and materials</h2><div class="c-article-section__content" id="availability-of-data-and-materials-content">
              
              <p>The user web logs collected during the current study and the quantities computed from them are not publicly available due to privacy restrictions. All the other image features mentioned in the Data Collection section are publicly availailable at the specified urls, or are available from the corresponding author on reasonable request.</p>
            </div></div></section><section data-title="Notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1" data-counter="1."><div class="c-article-footnote--listed__content"><p>List of Wikipedias. <a href="https://meta.wikimedia.org/wiki/List_of_Wikipedias">https://meta.wikimedia.org/wiki/List_of_Wikipedias</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2" data-counter="2."><div class="c-article-footnote--listed__content"><p>Wikimedia Commons. <a href="https://commons.wikimedia.org/wiki/Main_Page">https://commons.wikimedia.org/wiki/Main_Page</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3" data-counter="3."><div class="c-article-footnote--listed__content"><p>Galleries, Libraries, Archives, and Museums</p></div></li><li class="c-article-footnote--listed__item" id="Fn4" data-counter="4."><div class="c-article-footnote--listed__content"><p>Image use policy. <a href="https://en.wikipedia.org/wiki/Wikipedia:Image_use_policy#Image_content_and_selection">https://en.wikipedia.org/wiki/Wikipedia:Image_use_policy#Image_content_and_selection</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5" data-counter="5."><div class="c-article-footnote--listed__content"><p>Manual of Style. <a href="https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Images">https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Images</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6" data-counter="6."><div class="c-article-footnote--listed__content"><p>The Media Viewer. <a href="https://en.wikipedia.org/wiki/Wikipedia:Media_Viewer">https://en.wikipedia.org/wiki/Wikipedia:Media_Viewer</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7" data-counter="7."><div class="c-article-footnote--listed__content"><p>Wiki Loves Monument. <a href="https://www.wikilovesmonuments.org/">https://www.wikilovesmonuments.org/</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn8" data-counter="8."><div class="c-article-footnote--listed__content"><p>Wikipedia Pages Wanting Photos. <a href="https://meta.wikimedia.org/wiki/Wikipedia_Pages_Wanting_Photos">https://meta.wikimedia.org/wiki/Wikipedia_Pages_Wanting_Photos</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn9" data-counter="9."><div class="c-article-footnote--listed__content"><p>The Webrequest table. <a href="https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Traffic/Webrequest">https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Traffic/Webrequest</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn10" data-counter="10."><div class="c-article-footnote--listed__content"><p>Bot or Not? Identifying “fake” traffic on Wikipedia, Wikimedia Analytics team. <a href="https://techblog.wikimedia.org/2020/10/05/bot-or-not-identifying-fake-traffic-on-wikipedia/">https://techblog.wikimedia.org/2020/10/05/bot-or-not-identifying-fake-traffic-on-wikipedia/</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn11" data-counter="11."><div class="c-article-footnote--listed__content"><p>Wikidata topic model. <a href="https://github.com/geohci/wikidata-topic-model">https://github.com/geohci/wikidata-topic-model</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn12" data-counter="12."><div class="c-article-footnote--listed__content"><p>The WikiProject Directory. <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Directory">https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Directory</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn13" data-counter="13."><div class="c-article-footnote--listed__content"><p>Wikipedia:Manual of Style/Images Size. <a href="https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Images#Size">https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Images#Size</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn14" data-counter="14."><div class="c-article-footnote--listed__content"><p>Commons:Quality images. <a href="https://commons.wikimedia.org/wiki/Commons:Quality_images">https://commons.wikimedia.org/wiki/Commons:Quality_images</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn15" data-counter="15."><div class="c-article-footnote--listed__content"><p>Commons:Image guidelines. <a href="https://commons.wikimedia.org/wiki/Commons:Image_guidelines">https://commons.wikimedia.org/wiki/Commons:Image_guidelines</a>. Accessed March 2021.</p></div></li><li class="c-article-footnote--listed__item" id="Fn16" data-counter="16."><div class="c-article-footnote--listed__content"><p>We repeat the logistic regression analysis with different thresholds splitting the two classes, namely we focus on the highest vs. the lowest percentiles of the images according to their iCTR. We find no significant differences on the resulting regression coefficients. Therefore, we choose the median as the cutoff to maximize the presence of images in the analysis.</p></div></li><li class="c-article-footnote--listed__item" id="Fn17" data-counter="17."><div class="c-article-footnote--listed__content"><p>To run the algorithm, we use the <i>hdbscan</i> Python library [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;McInnes L, Healy J, Astels S (2017) hdbscan: hierarchical density based clustering. J Open Sour Softw 2(11):205&#xA;&#x9;&#x9;&#x9;&#x9;" href="/articles/10.1140/epjds/s13688-021-00312-8#ref-CR57" id="ref-link-section-d26201386e2842">57</a>]: <a href="https://hdbscan.readthedocs.io">https://hdbscan.readthedocs.io</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn18" data-counter="18."><div class="c-article-footnote--listed__content"><p>Whose Knowledge?. <a href="https://whoseknowledge.org/">https://whoseknowledge.org/</a>. Accessed March 2021.</p></div></li></ol></div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">
					Anthony D, Smith SW, Williamson T (2009) Reputation and reliability in collective goods: the case of the online encyclopedia Wikipedia. Ration Soc 21(3):283–306
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Reputation%20and%20reliability%20in%20collective%20goods%3A%20the%20case%20of%20the%20online%20encyclopedia%20Wikipedia&amp;journal=Ration%20Soc&amp;volume=21&amp;issue=3&amp;pages=283-306&amp;publication_year=2009&amp;author=Anthony%2CD&amp;author=Smith%2CSW&amp;author=Williamson%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">
					Levie WH, Lentz R (1982) Effects of text illustrations: a review of research. ECTJ 30(4):195–232
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20text%20illustrations%3A%20a%20review%20of%20research&amp;journal=ECTJ&amp;volume=30&amp;issue=4&amp;pages=195-232&amp;publication_year=1982&amp;author=Levie%2CWH&amp;author=Lentz%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">
					Yasseri T, Sumi R, Rung A, Kornai A, Kertész J (2012) Dynamics of conflicts in Wikipedia. PLoS ONE 7(6):38869
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamics%20of%20conflicts%20in%20Wikipedia&amp;journal=PLoS%20ONE&amp;volume=7&amp;issue=6&amp;publication_year=2012&amp;author=Yasseri%2CT&amp;author=Sumi%2CR&amp;author=Rung%2CA&amp;author=Kornai%2CA&amp;author=Kert%C3%A9sz%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">
					Lemmerich F, Sáez-Trumper D, West R, Zia L (2019) Why the world reads Wikipedia: beyond English speakers. In: Proc. International conference on web search and data mining (WSDM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Why%20the%20world%20reads%20Wikipedia%3A%20beyond%20English%20speakers&amp;publication_year=2019&amp;author=Lemmerich%2CF&amp;author=S%C3%A1ez-Trumper%2CD&amp;author=West%2CR&amp;author=Zia%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">
					Halfaker A, Geiger RS (2020) Ores: lowering barriers with participatory machine learning in wikipedia. Proc Human-Computer Interaction (HCI)
				</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">
					He S, Lin AY, Adar E, Hecht BJ (2018) The_tower_of_babel. jpg: diversity of visual encyclopedic knowledge across Wikipedia language editions. In: Proc. International conference on web and social media (ICWSM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=The_tower_of_babel.%20jpg%3A%20diversity%20of%20visual%20encyclopedic%20knowledge%20across%20Wikipedia%20language%20editions&amp;publication_year=2018&amp;author=He%2CS&amp;author=Lin%2CAY&amp;author=Adar%2CE&amp;author=Hecht%2CBJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">
					Viegas FB (2007) The visual side of Wikipedia. In: Proc. Hawaii international conference on system sciences (HICSS)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20visual%20side%20of%20Wikipedia&amp;publication_year=2007&amp;author=Viegas%2CFB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">
					Bakhshi S, Shamma DA, Gilbert E (2014) Faces engage us: photos with faces attract more likes and comments on Instagram. In: Proc. Conference on human factors in computing systems (SIGCHI)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Faces%20engage%20us%3A%20photos%20with%20faces%20attract%20more%20likes%20and%20comments%20on%20Instagram&amp;publication_year=2014&amp;author=Bakhshi%2CS&amp;author=Shamma%2CDA&amp;author=Gilbert%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">
					Park JY, O’Hare N, Schifanella R, Jaimes A, Chung C-W (2015) A large-scale study of user image search behavior on the web. In: Proc. Conference on human factors in computing systems (SIGCHI)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20large-scale%20study%20of%20user%20image%20search%20behavior%20on%20the%20web&amp;publication_year=2015&amp;author=Park%2CJY&amp;author=O%E2%80%99Hare%2CN&amp;author=Schifanella%2CR&amp;author=Jaimes%2CA&amp;author=Chung%2CC-W">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">
					Constantin MG, Redi M, Zen G, Ionescu B (2019) Computational understanding of visual interestingness beyond semantics: literature survey and analysis of covariates. ACM Comput Surv 52(2):1–37
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20understanding%20of%20visual%20interestingness%20beyond%20semantics%3A%20literature%20survey%20and%20analysis%20of%20covariates&amp;journal=ACM%20Comput%20Surv&amp;volume=52&amp;issue=2&amp;pages=1-37&amp;publication_year=2019&amp;author=Constantin%2CMG&amp;author=Redi%2CM&amp;author=Zen%2CG&amp;author=Ionescu%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">
					Guo D, Zhang S, Wright KL, McTigue EM (2020) Do you get the picture? A meta-analysis of the effect of graphics on reading comprehension. AERA Open 6(1):2332858420901696
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Do%20you%20get%20the%20picture%3F%20A%20meta-analysis%20of%20the%20effect%20of%20graphics%20on%20reading%20comprehension&amp;journal=AERA%20Open&amp;volume=6&amp;issue=1&amp;publication_year=2020&amp;author=Guo%2CD&amp;author=Zhang%2CS&amp;author=Wright%2CKL&amp;author=McTigue%2CEM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">
					Mayer RE (2002) Multimedia learning. In: Psychology of learning and motivation, vol 41, pp 85–139
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimedia%20learning&amp;pages=85-139&amp;publication_year=2002&amp;author=Mayer%2CRE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">
					Khamparia A, Pandey B (2018) Impact of interactive multimedia in e-learning technologies: role of multimedia in e-learning. In: Digital multimedia: concepts, methodologies, tools, and applications, pp 1087–1110
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Impact%20of%20interactive%20multimedia%20in%20e-learning%20technologies%3A%20role%20of%20multimedia%20in%20e-learning&amp;pages=1087-1110&amp;publication_year=2018&amp;author=Khamparia%2CA&amp;author=Pandey%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">
					Rudolph M (2017) Cognitive theory of multimedia learning. J Online Higher Educ 1(2):1–10
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20theory%20of%20multimedia%20learning&amp;journal=J%20Online%20Higher%20Educ&amp;volume=1&amp;issue=2&amp;pages=1-10&amp;publication_year=2017&amp;author=Rudolph%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">
					Tempelman-Kluit N (2006) Multimedia learning theories and online instruction. Coll Res Libr 67(4):364–369
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimedia%20learning%20theories%20and%20online%20instruction&amp;journal=Coll%20Res%20Libr&amp;volume=67&amp;issue=4&amp;pages=364-369&amp;publication_year=2006&amp;author=Tempelman-Kluit%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">
					Peeck J (1993) Increasing picture effects in learning from illustrated text. Learn Instr 3(3):227–238
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Increasing%20picture%20effects%20in%20learning%20from%20illustrated%20text&amp;journal=Learn%20Instr&amp;volume=3&amp;issue=3&amp;pages=227-238&amp;publication_year=1993&amp;author=Peeck%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">
					Bernard RM (1990) Using extended captions to improve learning from instructional illustrations. Br J Educ Technol 21(3):215–225
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20extended%20captions%20to%20improve%20learning%20from%20instructional%20illustrations&amp;journal=Br%20J%20Educ%20Technol&amp;volume=21&amp;issue=3&amp;pages=215-225&amp;publication_year=1990&amp;author=Bernard%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">
					Gygli M, Grabner H, Riemenschneider H, Nater F, Van Gool L (2013) The interestingness of images. In: Proc. International conference on computer vision (ICCV)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20interestingness%20of%20images&amp;publication_year=2013&amp;author=Gygli%2CM&amp;author=Grabner%2CH&amp;author=Riemenschneider%2CH&amp;author=Nater%2CF&amp;author=Gool%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">
					Khosla A, Das Sarma A, Hamid R (2014) What makes an image popular?. In: Proc. International world wide web conference (WWW)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20makes%20an%20image%20popular%3F&amp;publication_year=2014&amp;author=Khosla%2CA&amp;author=Das%20Sarma%2CA&amp;author=Hamid%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">
					Ding K, Ma K, Wang S (2019) Intrinsic image popularity assessment. In: Proc. International conference on multimedia (MM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Intrinsic%20image%20popularity%20assessment&amp;publication_year=2019&amp;author=Ding%2CK&amp;author=Ma%2CK&amp;author=Wang%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">
					Zhang W, Wang W, Wang J, Zha H (2018) User-guided hierarchical attention network for multi-modal social image popularity prediction. In: Proc. International world wide web conference (WWW)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=User-guided%20hierarchical%20attention%20network%20for%20multi-modal%20social%20image%20popularity%20prediction&amp;publication_year=2018&amp;author=Zhang%2CW&amp;author=Wang%2CW&amp;author=Wang%2CJ&amp;author=Zha%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">
					Tsikrika T, Diou C (2014) Multi-evidence user group discovery in professional image search. In: Proc. European conference on information retrieval (ECIR)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-evidence%20user%20group%20discovery%20in%20professional%20image%20search&amp;publication_year=2014&amp;author=Tsikrika%2CT&amp;author=Diou%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">
					Jansen BJ (2008) Searching for digital images on the web. J Doc
				</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">
					Huang J, Efthimiadis EN (2009) Analyzing and evaluating query reformulation strategies in web search logs. In: Proc. Conference on information and knowledge management (CIKM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Analyzing%20and%20evaluating%20query%20reformulation%20strategies%20in%20web%20search%20logs&amp;publication_year=2009&amp;author=Huang%2CJ&amp;author=Efthimiadis%2CEN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">
					Jansen BJ, Spink A, Pedersen JO (2004) The effect of specialized multimedia collections on web searching. J Web Eng 3(3–4):182–199
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20specialized%20multimedia%20collections%20on%20web%20searching&amp;journal=J%20Web%20Eng&amp;volume=3&amp;issue=3%E2%80%934&amp;pages=182-199&amp;publication_year=2004&amp;author=Jansen%2CBJ&amp;author=Spink%2CA&amp;author=Pedersen%2CJO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">
					Heald P, Erickson K, Kretschmer M (2015) The valuation of unprotected works: a case study of public domain images on Wikipedia. Harv JL &amp; Tech 29(1):1–31
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20valuation%20of%20unprotected%20works%3A%20a%20case%20study%20of%20public%20domain%20images%20on%20Wikipedia&amp;journal=Harv%20JL%20%26%20Tech&amp;volume=29&amp;issue=1&amp;pages=1-31&amp;publication_year=2015&amp;author=Heald%2CP&amp;author=Erickson%2CK&amp;author=Kretschmer%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">
					Erickson K, Perez FR, Perez JR (2018) What is the commons worth? Estimating the value of wikimedia imagery by observing downstream use. In: Proc. International symposium on open collaboration (OpenSym)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20is%20the%20commons%20worth%3F%20Estimating%20the%20value%20of%20wikimedia%20imagery%20by%20observing%20downstream%20use&amp;publication_year=2018&amp;author=Erickson%2CK&amp;author=Perez%2CFR&amp;author=Perez%2CJR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">
					Vaidya G, Kontokostas D, Knuth M, Lehmann J, Hellmann S (2015) Dbpedia commons: structured multimedia metadata from the wikimedia commons. In: Proc. International semantic web conference (ISWC)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Dbpedia%20commons%3A%20structured%20multimedia%20metadata%20from%20the%20wikimedia%20commons&amp;publication_year=2015&amp;author=Vaidya%2CG&amp;author=Kontokostas%2CD&amp;author=Knuth%2CM&amp;author=Lehmann%2CJ&amp;author=Hellmann%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">
					Ferrada S, Bustos B, Hogan A (2017) Imgpedia: a linked dataset with content-based analysis of wikimedia images. In: Proc. International semantic web conference (ISWC)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Imgpedia%3A%20a%20linked%20dataset%20with%20content-based%20analysis%20of%20wikimedia%20images&amp;publication_year=2017&amp;author=Ferrada%2CS&amp;author=Bustos%2CB&amp;author=Hogan%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">
					Navarrete T, Villaespesa E (2020) Image-based information: paintings in Wikipedia. J Doc
				</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">
					Lehmann J, Müller-Birn C, Laniado D, Lalmas M, Kaltenbrunner A (2014) Reader preferences and behavior on Wikipedia. In: Proc. Conference on hypertext and social media (HT)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Reader%20preferences%20and%20behavior%20on%20Wikipedia&amp;publication_year=2014&amp;author=Lehmann%2CJ&amp;author=M%C3%BCller-Birn%2CC&amp;author=Laniado%2CD&amp;author=Lalmas%2CM&amp;author=Kaltenbrunner%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">
					Spoerri A (2007) What is popular on Wikipedia and why? First Monday 12(4):1–6
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20is%20popular%20on%20Wikipedia%20and%20why%3F&amp;journal=First%20Monday&amp;volume=12&amp;issue=4&amp;pages=1-6&amp;publication_year=2007&amp;author=Spoerri%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">
					Salutari F, Da Hora D, Dubuc G, Rossi D (2019) A large-scale study of Wikipedia users’ quality of experience. In: Proc. International world wide web conference (WWW)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20large-scale%20study%20of%20Wikipedia%20users%E2%80%99%20quality%20of%20experience&amp;publication_year=2019&amp;author=Salutari%2CF&amp;author=Hora%2CD&amp;author=Dubuc%2CG&amp;author=Rossi%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">
					Singer P, Lemmerich F, West R, Zia L, Wulczyn E, Strohmaier M, Leskovec J (2017) Why we read Wikipedia. In: Proc. International world wide web conference (WWW)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Why%20we%20read%20Wikipedia&amp;publication_year=2017&amp;author=Singer%2CP&amp;author=Lemmerich%2CF&amp;author=West%2CR&amp;author=Zia%2CL&amp;author=Wulczyn%2CE&amp;author=Strohmaier%2CM&amp;author=Leskovec%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">
					Piccardi T, Redi M, Colavizza G, West R (2020) Quantifying engagement with citations on Wikipedia. In: Proc. The web conference (WWW)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantifying%20engagement%20with%20citations%20on%20Wikipedia&amp;publication_year=2020&amp;author=Piccardi%2CT&amp;author=Redi%2CM&amp;author=Colavizza%2CG&amp;author=West%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">
					Piccardi T, Redi M, Colavizza G, West R (2021) On the value of Wikipedia as a gateway to the web. In: Proc. The web conference (WWW)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20value%20of%20Wikipedia%20as%20a%20gateway%20to%20the%20web&amp;publication_year=2021&amp;author=Piccardi%2CT&amp;author=Redi%2CM&amp;author=Colavizza%2CG&amp;author=West%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">
					Chapelle O (2014) Modeling delayed feedback in display advertising. In: Proc. Conference on knowledge discovery and data mining (SIGKDD)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20delayed%20feedback%20in%20display%20advertising&amp;publication_year=2014&amp;author=Chapelle%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">
					Rosales R, Cheng H, Manavoglu E (2012) Post-click conversion modeling and analysis for non-guaranteed delivery display advertising. In: Proc. Conference on web search and data mining (WSDM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Post-click%20conversion%20modeling%20and%20analysis%20for%20non-guaranteed%20delivery%20display%20advertising&amp;publication_year=2012&amp;author=Rosales%2CR&amp;author=Cheng%2CH&amp;author=Manavoglu%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">
					Ta A-P (2015) Factorization machines with follow-the-regularized-leader for ctr prediction in display advertising. In: Proc. International conference on big data (big data)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Factorization%20machines%20with%20follow-the-regularized-leader%20for%20ctr%20prediction%20in%20display%20advertising&amp;publication_year=2015&amp;author=Ta%2CA-P">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">
					Richardson M, Dominowska E, Ragno R (2007) Predicting clicks: estimating the click-through rate for new ads. In: Proc. International world wide web conference (WWW)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicting%20clicks%3A%20estimating%20the%20click-through%20rate%20for%20new%20ads&amp;publication_year=2007&amp;author=Richardson%2CM&amp;author=Dominowska%2CE&amp;author=Ragno%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">
					Edizel B, Mantrach A, Bai X (2017) Deep character-level click-through rate prediction for sponsored search. In: Proc. Conference on research and development in information retrieval (SIGIR)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20character-level%20click-through%20rate%20prediction%20for%20sponsored%20search&amp;publication_year=2017&amp;author=Edizel%2CB&amp;author=Mantrach%2CA&amp;author=Bai%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">
					Lemmerich F, Sáez-Trumper D, West R, Zia L (2019) Why the world reads Wikipedia: beyond English speakers. In: Proc. International conference on web search and data mining (WSDM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Why%20the%20world%20reads%20Wikipedia%3A%20beyond%20English%20speakers&amp;publication_year=2019&amp;author=Lemmerich%2CF&amp;author=S%C3%A1ez-Trumper%2CD&amp;author=West%2CR&amp;author=Zia%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">
					Flesch R (1948) A new readability yardstick. J Appl Psychol 32(3):221
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20readability%20yardstick&amp;journal=J%20Appl%20Psychol&amp;volume=32&amp;issue=3&amp;publication_year=1948&amp;author=Flesch%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">
					Paranjape A, West R, Zia L, Leskovec J (2016) Improving website hyperlink structure using server logs. In: Proc. Conference on web search and data mining (WSDM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Improving%20website%20hyperlink%20structure%20using%20server%20logs&amp;publication_year=2016&amp;author=Paranjape%2CA&amp;author=West%2CR&amp;author=Zia%2CL&amp;author=Leskovec%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">
					Schifanella R, Redi M, Aiello LM (2015) An image is worth more than a thousand favorites: surfacing the hidden beauty of Flickr pictures. In: International conference on web and social media (ICWSM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20image%20is%20worth%20more%20than%20a%20thousand%20favorites%3A%20surfacing%20the%20hidden%20beauty%20of%20Flickr%20pictures&amp;publication_year=2015&amp;author=Schifanella%2CR&amp;author=Redi%2CM&amp;author=Aiello%2CLM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">
					Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016) Rethinking the inception architecture for computer vision. In: Proc. Conference on computer vision and pattern recognition (CVPR)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Rethinking%20the%20inception%20architecture%20for%20computer%20vision&amp;publication_year=2016&amp;author=Szegedy%2CC&amp;author=Vanhoucke%2CV&amp;author=Ioffe%2CS&amp;author=Shlens%2CJ&amp;author=Wojna%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">
					Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L (2009) Imagenet: a large-scale hierarchical image database. In: Proc. Conference on computer vision and pattern recognition (CVPR)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Imagenet%3A%20a%20large-scale%20hierarchical%20image%20database&amp;publication_year=2009&amp;author=Deng%2CJ&amp;author=Dong%2CW&amp;author=Socher%2CR&amp;author=Li%2CL-J&amp;author=Li%2CK&amp;author=Fei-Fei%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">
					Huh M, Agrawal P, Efros AA (2016) What makes imagenet good for transfer learning? arXiv preprint <a href="http://arxiv.org/abs/1608.08614">1608.08614</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">
					Pappas N, Redi M, Topkara M, Jou B, Liu H, Chen T, Chang S-F (2016) Multilingual visual sentiment concept matching. In: Proc. International conference on multimedia retrieval (ICMR)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Multilingual%20visual%20sentiment%20concept%20matching&amp;publication_year=2016&amp;author=Pappas%2CN&amp;author=Redi%2CM&amp;author=Topkara%2CM&amp;author=Jou%2CB&amp;author=Liu%2CH&amp;author=Chen%2CT&amp;author=Chang%2CS-F">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">
					Wen Y, Zhang K, Li Z, Qiao Y (2016) A discriminative feature learning approach for deep face recognition. In: Proc. European conference on computer vision (ECCV)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20discriminative%20feature%20learning%20approach%20for%20deep%20face%20recognition&amp;publication_year=2016&amp;author=Wen%2CY&amp;author=Zhang%2CK&amp;author=Li%2CZ&amp;author=Qiao%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">
					Zagoruyko S, Komodakis N (2016) Wide residual networks. arXiv preprint <a href="http://arxiv.org/abs/1605.07146">1605.07146</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">
					Zhou B, Lapedriza A, Khosla A, Oliva A, Torralba A (2017) Places: a 10 million image database for scene recognition. IEEE Trans Pattern Anal Mach Intell 40(6):1452–1464
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Places%3A%20a%2010%20million%20image%20database%20for%20scene%20recognition&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=40&amp;issue=6&amp;pages=1452-1464&amp;publication_year=2017&amp;author=Zhou%2CB&amp;author=Lapedriza%2CA&amp;author=Khosla%2CA&amp;author=Oliva%2CA&amp;author=Torralba%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">
					Kutner MH, Nachtsheim CJ, Neter J, Li W et al. (2005) Applied linear statistical models, vol 5
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Applied%20linear%20statistical%20models&amp;publication_year=2005&amp;author=Kutner%2CMH&amp;author=Nachtsheim%2CCJ&amp;author=Neter%2CJ&amp;author=Li%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">
					Campello RJ, Moulavi D, Sander J (2013) Density-based clustering based on hierarchical density estimates. In: Proc. Pacific-Asia conference on knowledge discovery and data mining (PAKDD)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=Density-based%20clustering%20based%20on%20hierarchical%20density%20estimates&amp;publication_year=2013&amp;author=Campello%2CRJ&amp;author=Moulavi%2CD&amp;author=Sander%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="55."><p class="c-article-references__text" id="ref-CR55">
					Lloyd S (1982) Least squares quantization in pcm. IEEE Trans Inf Theory 28(2):129–137
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=651807" aria-label="MathSciNet reference 55">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0504.94015" aria-label="MATH reference 55">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Least%20squares%20quantization%20in%20pcm&amp;journal=IEEE%20Trans%20Inf%20Theory&amp;volume=28&amp;issue=2&amp;pages=129-137&amp;publication_year=1982&amp;author=Lloyd%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="56."><p class="c-article-references__text" id="ref-CR56">
					Ester M, Kriegel H-P, Sander J, Xu X et al. (1996) A density-based algorithm for discovering clusters in large spatial databases with noise. In: International conference on knowledge discovery and data mining (KDD)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20density-based%20algorithm%20for%20discovering%20clusters%20in%20large%20spatial%20databases%20with%20noise&amp;publication_year=1996&amp;author=Ester%2CM&amp;author=Kriegel%2CH-P&amp;author=Sander%2CJ&amp;author=Xu%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="57."><p class="c-article-references__text" id="ref-CR57">
					McInnes L, Healy J, Astels S (2017) hdbscan: hierarchical density based clustering. J Open Sour Softw 2(11):205
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 57" href="http://scholar.google.com/scholar_lookup?&amp;title=hdbscan%3A%20hierarchical%20density%20based%20clustering&amp;journal=J%20Open%20Sour%20Softw&amp;volume=2&amp;issue=11&amp;publication_year=2017&amp;author=McInnes%2CL&amp;author=Healy%2CJ&amp;author=Astels%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="58."><p class="c-article-references__text" id="ref-CR58">
					Moulavi D, Jaskowiak PA, Campello RJ, Zimek A, Sander J (2014) Density-based clustering validation. In: Proc. SIAM international conference on data mining (SDM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Density-based%20clustering%20validation&amp;publication_year=2014&amp;author=Moulavi%2CD&amp;author=Jaskowiak%2CPA&amp;author=Campello%2CRJ&amp;author=Zimek%2CA&amp;author=Sander%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="59."><p class="c-article-references__text" id="ref-CR59">
					Abadie A, Imbens GW (2006) Large sample properties of matching estimators for average treatment effects. Econometrica 74(1):235–267
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2194325" aria-label="MathSciNet reference 59">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1112.62042" aria-label="MATH reference 59">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Large%20sample%20properties%20of%20matching%20estimators%20for%20average%20treatment%20effects&amp;journal=Econometrica&amp;volume=74&amp;issue=1&amp;pages=235-267&amp;publication_year=2006&amp;author=Abadie%2CA&amp;author=Imbens%2CGW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="60."><p class="c-article-references__text" id="ref-CR60">
					Georgescu M, Kanhabua N, Krause D, Nejdl W, Siersdorfer S (2013) Extracting event-related information from article updates in Wikipedia. In: Proc. European conference on information retrieval (ECIR)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Extracting%20event-related%20information%20from%20article%20updates%20in%20Wikipedia&amp;publication_year=2013&amp;author=Georgescu%2CM&amp;author=Kanhabua%2CN&amp;author=Krause%2CD&amp;author=Nejdl%2CW&amp;author=Siersdorfer%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="61."><p class="c-article-references__text" id="ref-CR61">
					Morton J, Johnson MH (1991) Conspec and conlern: a two-process theory of infant face recognition. Psychol Rev 98(2):164
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Conspec%20and%20conlern%3A%20a%20two-process%20theory%20of%20infant%20face%20recognition&amp;journal=Psychol%20Rev&amp;volume=98&amp;issue=2&amp;publication_year=1991&amp;author=Morton%2CJ&amp;author=Johnson%2CMH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="62."><p class="c-article-references__text" id="ref-CR62">
					Redi M, Gerlach M, Johnson I, Morgan J, Zia L (2020) A taxonomy of knowledge gaps for wikimedia projects (second draft). arXiv preprint <a href="http://arxiv.org/abs/2008.12314">2008.12314</a></p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="63."><p class="c-article-references__text" id="ref-CR63">
					You Q, García-García D, Paluri M, Luo J, Joo J (2017) Cultural diffusion and trends in Facebook photographs. In: Proc. International conference on web and social media (ICWSM)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=Cultural%20diffusion%20and%20trends%20in%20Facebook%20photographs&amp;publication_year=2017&amp;author=You%2CQ&amp;author=Garc%C3%ADa-Garc%C3%ADa%2CD&amp;author=Paluri%2CM&amp;author=Luo%2CJ&amp;author=Joo%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="64."><p class="c-article-references__text" id="ref-CR64">
					Piccardi T, West R (2021) Crosslingual topic modeling with WikiPDA. In: Proc. The web conference (WWW)
				</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=Crosslingual%20topic%20modeling%20with%20WikiPDA&amp;publication_year=2021&amp;author=Piccardi%2CT&amp;author=West%2CR">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1140/epjds/s13688-021-00312-8?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors thank the Wikimedia Research Team for their insightful discussions and the Analytics Team for their technical support.</p><p>The authors acknowledge the creators of the images included in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig8">8</a>, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig10">10</a>, and the Supplementary Figure. Attributions are provided below:</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig8">8</a>:</p><p><i>Detroit Photographic Company (0707) (cropped).jpg</i>, Unknown author, Public domain, via Wikimedia Commons</p><p><i>Ankor Wat temple.jpg</i>, Kheng Vungvuthy, CC BY-SA 4.0, via Wikimedia Commons</p><p><i>Shirdi Sai Baba 3.jpg</i>, Unknown author, Public domain, via Wikimedia Commons</p><p><i>Lana Turner—Marriage is a Private Affair portrait.jpg</i>, MGM, Public domain, via Wikimedia Commons</p><p><i>Denise Richards 2009.1.jpg</i>, Glenn Francis, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>RIAN archive 700096 Pacific fleet vessels’ sortie for combat training.jpg</i>, RIA Novosti archive, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Alhashimi.jpg</i>, Patrick Makhoul, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>Class-482-NSE-Bank3.jpg</i>, Spsmiler, Public domain, via Wikimedia Commons</p><p><i>2015 Brown SA Goalie Ian Hunter.jpg</i>, Orion 2012, CC0, via Wikimedia Commons</p><p><i>Metaleptus angulatus (38035311795).jpg</i>, Ben Sale, CC BY 2.0, via Wikimedia Commons</p><p><i>Kostomuksha Nature Reserve 2014.jpg</i>, Igor Georgievskiy, CC BY-SA 4.0, via Wikimedia Commons</p><p><i>CominoBlueLagoon.jpg</i>, JarekPT, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Kjersti Toppe.jpg</i>, Senter partiet, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>Kardinal Kaspar Karel.jpg</i>, Tomas Urban, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>RanellidaeShell.JPG</i>, Toby Hudson, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Nuestra Senora de Andalucia by Julio Romero de Torres.JPG</i>, Julio Romero de Torres, Public domain, via Wikimedia Commons</p><p><i>SS Eastern Chief (1917).jpg</i>, United States Navy History and Heritage Command photograph, Public domain, via Wikimedia Commons</p><p><i>Giuseppe gene.jpg</i>, Unknown author, Public domain, via Wikimedia Commons</p><p><i>Polscy karykaturzysci.JPG</i>, Unknown author, Public domain, via Wikimedia Commons</p><p><i>Ada English.jpg</i>, Unknown author, CC BY-SA 4.0, via Wikimedia Commons</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig10">10</a>:</p><p><i>01 mucha documentsdecoratifs 1901.jpg</i>, Alphonse Mucha, Public domain, via Wikimedia Commons</p><p><i>Kuniyoshi Utagawa, Mt fuji from Sumida.jpg</i>, Utagawa Kuniyoshi, Public domain, via Wikimedia Commons</p><p><i>La Route tournante en sous-bois, par Paul Cezanne.jpg</i>, Paul Cezanne, Public domain, via Wikimedia Commons</p><p><i>Mykonos City.jpg</i>, Cifo Buscemi, CC0, via Wikimedia Commons</p><p><i>Ladybower Reservoir From Above.jpg</i>, Joel Vardy, CC BY-SA 4.0, via Wikimedia Commons</p><p><i>Bierstadt Albert Old Faithful.jpg</i>, Albert Bierstadt, Public domain, via Wikimedia Commons</p><p><i>Palacio da Alvorada Exterior.JPG</i>, Palacio do Planalto, Attribution, via Wikimedia Commons</p><p><i>Victoria Clock Tower, Liverpool University—geograph.org.uk—374422.jpg</i>, Sue Adair, Liverpool University</p><p><i>View from 555 California Street in San Francisco—panoramio (4).jpg</i>, Eduardo Manchon, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Jaarbeurs.JPG</i>, Albert kok, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Apfel-Berlepsch.jpg</i>, Superbass, CC BY-SA 4.0, via Wikimedia Commons</p><p><i>Elements de decor dun immeuble art nouveau (Paris) (4810271270).jpg</i>, Jean-Pierre Dalbera, CC BY 2.0, via Wikimedia Commons</p><p><i>Snettisham HoardDSCF6580.jpg</i>, Johnbod, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Peasants in an Interior (1661) Adriaen van Ostade.jpg</i>, Adriaen van Ostade, Public domain, via Wikimedia Commons</p><p><i>740 Park Avenue.jpg</i>, Eden, Janine and Jim, CC BY 2.0, via Wikimedia Commons</p><p><i>Arch Titus, Forum Romanum, Rome, Italy.jpg</i>, Jebulon, CC0, via Wikimedia Commons</p><p><i>Elbphilharmonie, Hamburg.jpg</i>, Hackercatxxy, CC BY-SA 4.0, via Wikimedia Commons</p><p><i>Flowers (2425723494) cropped.jpg</i>, Michal Osmenda, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>Coccinella magnifica01.jpg</i>, Gilles San Martin, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>German Shepherd—DSC 0346 (10096362833).jpg</i>, gomagoti, CC BY-SA 2.5, via Wikimedia Commons</p><p><i>BMW 7er (E38) 20090314 front.jpg</i>, M 93, Public domain, via Wikimedia Commons</p><p><i>13-143 JF-17 LBG SIAE 2015 (18984327841).jpg</i>, Eric Salard, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>Delicate arch sunset.jpg</i>, Palacemusic, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Lake Natron (Tanzania)—2017-03-06 (very early in rainy season)—satellite image (cropped).jpg</i>, Joshua Stevens/NASA, Public domain, via Wikimedia Commons</p><p><i>Arabian Sea map.png</i>, NormanEinstein, Ras67, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Rob McElhenney and Kaitlin Olson (12063880473).jpg</i>, Sue Lukenbaugh,CC BY-SA 2.0, via Wikimedia Commons</p><p><i>(L-R) Larry Hagman, Ross Perot, Margot Perot and Suzanne Perot at the Rosewood Crescent Club (8392304697).jpg</i>, SMU Central University Libraries, No restrictions, via Wikimedia Commons</p><p><i>Obama family portrait in the Green Room.jpg</i>, Annie Leibovitz/Released by White House Photo Office, Public domain, via Wikimedia Commons</p><p><i>Brad Pitt June 2014 (cropped).jpg</i>, Foreign and Commonwealth Office, CC BY 2.0, via Wikimedia Commons</p><p><i>Elizabeth Olsen1 (cropped).jpg</i>, <a href="https://www.flickr.com/photos/tabercil/">https://www.flickr.com/photos/tabercil/</a>, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>Argentina celebrando copa (cropped).jpg</i>, Unknown author, Public domain, via Wikimedia Commons</p><p><i>Alonso 2016.jpg</i>, Box Repsol | Flickr, CC BY 2.0, via Wikimedia Commons</p><p><i>Jerami Grant free throw (cropped).jpg</i>, All-Pro Reels, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>Suresh Raina1.jpg</i>, vijay chennupati, CC BY 2.0, via Wikimedia Commons</p><p>Supplementary Figure:</p><p><i>B25-1 300.jpg, USAAF, Public domain, via Wikimedia Commons 1990 Nissan 300ZX.jpg</i>, Mike Reyher, CC BY 2.0, via Wikimedia Commons</p><p><i>BoraBora SEtienne.jpg</i>, Samuel Etienne, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>St Helens before 1980 eruption horizon fixed.jpg</i>, Jim Nieland, Public domain, via Wikimedia Commons</p><p><i>Jessica Pare 2014 at Paleyfest.jpg</i>, Dominick D, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>Mae Carol Jemison.jpg</i>, NASA, Public domain, via Wikimedia Commons</p><p><i>William Shu riseconf 2016 (27358004426).jpg</i>, RISE, CC BY 2.0, via Wikimedia Commons</p><p><i>Jordan by Lipofsky 16577.jpg</i>, Steve Lipofsky, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Hurricanes Quarterback.JPG</i>, BalticHurricanes, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>2019 Final da Copa America 2019—Alisson.jpg</i>, Palacio do Planalto, CC BY 2.0, via Wikimedia Commons</p><p><i>RupertGrint2018.jpg</i>, Sidewalks Entertainment, CC BY 3.0, via Wikimedia Commons</p><p><i>Salt Bae.png</i>, Terron F. Beckham, CC BY 3.0, via Wikimedia Commons</p><p><i>Rose Leslie (March 2013) (headshot).jpg</i>, Suzi Pratt, CC BY-SA 2.0, via Wikimedia Commons</p><p><i>Kit harrington by sachyn mital (cropped 2).jpg</i>, Sachyn, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Protein HIF1A PDB 1h2k.png</i>, Emw, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Dehydroepiandrosterone molecule ball.png</i>, Jynto, CC0, via Wikimedia Commons</p><p><i>Iceland-Hatari-ESC2019-002.jpg</i>, Martin Fjellanger, CC BY-SA 4.0, via Wikimedia Commons</p><p><i>Jack Sprat and his wife by Frederick Richardson.jpg</i>, Frederick Richardson, Public domain, via Wikimedia Commons</p><p><i>Incognito Bangkok.jpg</i>, Vairoj Arunyaangkul, CC BY 2.0, via Wikimedia Commons</p><p><i>WarrenGMagnuson (cropped).jpg</i>, Believed to be official senatorial portrait, Public domain, via Wikimedia Commons</p><p><i>1994 Washington senatorial election map.png</i>, JDPEG, CC BY-SA 4.0, via Wikimedia Commons</p><p><i>Ulm-Wiblingen-Fugger.png</i>, OwenBlacker, Public domain, via Wikimedia Commons</p><p><i>Blackwood’s Magazine—1899 cover.jpg</i>, William Blackwood, Public domain, via Wikimedia Commons</p><p><i>I Heard It Through the Grapevine by Marvin Gaye 1968 US single.png</i>, Tamla Records, Public domain, via Wikimedia Commons</p><p><i>AmphibiaLogoTransparent.png</i>, Disney, Public domain, via Wikimedia Commons</p><p><i>SXSW 2016—Rami Malek (25138464364) (cropped).jpg</i>, Daniel Benavides, CC BY 2.0, via Wikimedia Commons</p><p><i>Orlando Bloom Cannes 2013.jpg</i>, Georges Biard, CC BY-SA 3.0, via Wikimedia Commons</p><p><i>Charlize-theron-IMG 6045.jpg</i>, Fuzheado, CC BY-SA 4.0, via Wikimedia Commons</p><p><i>Lorne Michaels David Shankbone 2010.jpg</i>, David Shankbone, CC BY 3.0, via Wikimedia Commons</p><p><i>NYC—Washington Square Park—Arch.jpg</i>, Jean-Christophe Benoist, CC BY 3.0, via Wikimedia Commons</p><p><i>M-S Sarfaq Ittuk.jpg</i>, David Stanley, CC BY 2.0, via Wikimedia Commons</p></div></div></section><section data-title="Funding"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p>RS has been partially supported by the project “Countering Online hate speech through Effective on-line Monitoring” funded by the Compagnia di San Paolo. The funder had no role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">University of Turin, Turin, Italy</p><p class="c-article-author-affiliation__authors-list">Daniele Rama &amp; Rossano Schifanella</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">École polytechnique fédérale de Lausanne (EPFL), Lausanne, Switzerland</p><p class="c-article-author-affiliation__authors-list">Tiziano Piccardi</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Wikimedia Foundation, San Francisco, CA, USA</p><p class="c-article-author-affiliation__authors-list">Miriam Redi</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">ISI Foundation, Turin, Italy</p><p class="c-article-author-affiliation__authors-list">Rossano Schifanella</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Daniele-Rama"><span class="c-article-authors-search__title u-h3 js-search-name">Daniele Rama</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="https://www.biomedcentral.com/search?query=author%23%22Daniele%20Rama%22" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daniele%20Rama" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daniele%20Rama%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Tiziano-Piccardi"><span class="c-article-authors-search__title u-h3 js-search-name">Tiziano Piccardi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="https://www.biomedcentral.com/search?query=author%23%22Tiziano%20Piccardi%22" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tiziano%20Piccardi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tiziano%20Piccardi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Miriam-Redi"><span class="c-article-authors-search__title u-h3 js-search-name">Miriam Redi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="https://www.biomedcentral.com/search?query=author%23%22Miriam%20Redi%22" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Miriam%20Redi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Miriam%20Redi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Rossano-Schifanella"><span class="c-article-authors-search__title u-h3 js-search-name">Rossano Schifanella</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="https://www.biomedcentral.com/search?query=author%23%22Rossano%20Schifanella%22" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Rossano%20Schifanella" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rossano%20Schifanella%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>DR, TP, MR, and RS conceptualized the problem and designed research; DR, TP and MR collected data; DR ran the analysis and DR, TP, MR, and RS interpreted the results; DR, TP, MR, and RS wrote the paper. All authors read and approved the final manuscript.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:daniele.rama@unito.it">Daniele Rama</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
              
                <h3 class="c-article__sub-heading" id="FPar26">Competing interests</h3>
                <p>The authors declare that they have no competing interests.</p>
              
            </div></div></section><section data-title="Supplementary Information"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27">Supplementary Information</h2><div class="c-article-section__content" id="Sec27-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material. </p><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-track-label="link" data-test="supp-info-link" href="https://static-content.springer.com/esm/art%3A10.1140%2Fepjds%2Fs13688-021-00312-8/MediaObjects/13688_2021_312_MOESM1_ESM.pdf" data-supp-info-image="">13688_2021_312_MOESM1_ESM.pdf</a></h3><div class="c-article-supplementary__description" data-component="thumbnail-container"><p>The PDF file entitled “Supplementary Material” contains: a Supplementary Figure for the clustering analysis, depicting all the clusters not in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Fig10">10</a>; a Supplementary Table including the numerical values discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec13">4.3</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/articles/10.1140/epjds/s13688-021-00312-8#Sec21">6.1</a>. (PDF 6.8 MB)</p></div></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20large%20scale%20study%20of%20reader%20interactions%20with%20images%20on%20Wikipedia&amp;author=Daniele%20Rama%20et%20al&amp;contentID=10.1140%2Fepjds%2Fs13688-021-00312-8&amp;copyright=The%20Author%28s%29&amp;publication=2193-1127&amp;publicationDate=2022-01-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1140/epjds/s13688-021-00312-8" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1140/epjds/s13688-021-00312-8" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Rama, D., Piccardi, T., Redi, M. <i>et al.</i> A large scale study of reader interactions with images on Wikipedia.
                    <i>EPJ Data Sci.</i> <b>11, </b>1 (2022). https://doi.org/10.1140/epjds/s13688-021-00312-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" href="https://citation-needed.springer.com/v2/references/10.1140/epjds/s13688-021-00312-8?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-06-22">22 June 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-11-22">22 November 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2022-01-03">03 January 2022</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1140/epjds/s13688-021-00312-8</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span>Wikipedia</span></li><li class="c-article-subject-list__subject"><span>Images</span></li><li class="c-article-subject-list__subject"><span>Computer vision</span></li><li class="c-article-subject-list__subject"><span>User behavior</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>

                    


                </article>
            </main>

            <div class="c-article-extras u-text-sm u-hide-print" data-track-component="reading companion">
                <aside>
                    
    <div class="c-pdf-download u-clear-both">
        <a href="//epjdatascience.springeropen.com/track/pdf/10.1140/epjds/s13688-021-00312-8.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true"  data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="link" data-track-external>
            <span class="c-pdf-download__text">Download PDF</span>
            <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
        </a>
    </div>

                    

                    
    


                    <div class="c-reading-companion">
                        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                                <div class="js-ad">
    
    <aside class="adsbox c-ad c-ad--300x250 u-mt-16" data-component-mpu>
        <div class="c-ad__inner" >
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1"
                data-ad-type="MPU1"
                data-pa11y-ignore
                data-gpt
                data-gpt-unitpath="/270604982/springer_open/epjdatascience/articles"
                data-gpt-sizes="300x250"
                data-gpt-targeting="pos=MPU1;doi=10.1140/epjds/s13688-021-00312-8;type=article;kwrd=Wikipedia,Images,Computer vision,User behavior;pmc=I23028,P33030,T11022;"
            >
                <noscript>
                    <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/270604982/springer_open/epjdatascience/articles&amp;sz=300x250&amp;pos=MPU1&amp;doi=10.1140/epjds/s13688-021-00312-8&amp;type=article&amp;kwrd=Wikipedia,Images,Computer vision,User behavior&amp;pmc=I23028,P33030,T11022&amp;">
                        <img data-test="gpt-advert-fallback-img"
                            src="//pubads.g.doubleclick.net/gampad/ad?iu=/270604982/springer_open/epjdatascience/articles&amp;sz=300x250&amp;pos=MPU1&amp;doi=10.1140/epjds/s13688-021-00312-8&amp;type=article&amp;kwrd=Wikipedia,Images,Computer vision,User behavior&amp;pmc=I23028,P33030,T11022&amp;"
                            alt="Advertisement"
                            width="300"
                            height="250">
                    </a>
                </noscript>
            </div>
        </div>
    </aside>
</div>
                            </div>
                            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                        </div>
                    </div>
                </aside>
            </div>

        </div>


        
        
    <img rel="nofollow" class='tracker' style='display:none' src='/track/article/10.1140/epjds/s13688-021-00312-8' alt=""/>

         
    <footer>
        
            <div class="c-publisher-footer" data-test="publisher-footer">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between" data-test="publisher-footer-menu">
            <div class="u-display-flex">
                
                    
                        <ul class="c-list-group c-list-group--sm u-mr-24 u-mb-16">
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://support.biomedcentral.com/support/home">Support and Contact</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="//www.springeropen.com/about/jobs">Jobs</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="http://authorservices.springernature.com/language-editing/">Language editing for authors</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="http://authorservices.springernature.com/scientific-editing/">Scientific editing for authors</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="https://biomedcentral.typeform.com/to/VLXboo">Leave feedback</a>
                                </li>
                            
                        </ul>
                    
                        <ul class="c-list-group c-list-group--sm u-mr-24 u-mb-16">
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="//www.springeropen.com/terms-and-conditions">Terms and conditions</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="//www.springeropen.com/privacy-statement">Privacy statement</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="//www.springeropen.com/accessibility">Accessibility</a>
                                </li>
                            
                                <li class="c-list-group__item">
                                    <a class="u-gray-link" href="//www.springeropen.com/cookies">Cookies</a>
                                </li>
                            
                        </ul>
                    
                
            </div>
            <div class="u-mb-24">
                <h3 id="social-menu" class="u-text-sm u-reset-margin u-text-normal">Follow SpringerOpen</h3>
                <ul class="u-display-flex u-list-reset" data-test="footer-social-links">
                    
                        <li class="u-mt-8 u-mr-8">
                            <a href="https://twitter.com/springeropen"
                               data-track="click"
                               data-track-category="Social"
                               data-track-action="Clicked SpringerOpen Twitter"
                               class="u-gray-link">
                                <span class="u-visually-hidden">SpringerOpen Twitter page</span>
                                <svg class="u-icon u-text-lg" width="24" height="24" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-twitter-bordered"></use>
                                </svg>
                            </a>
                        </li>
                    
                        <li class="u-mt-8 u-mr-8">
                            <a href="https://www.facebook.com/SpringerOpn"
                               data-track="click"
                               data-track-category="Social"
                               data-track-action="Clicked SpringerOpen Facebook"
                               class="u-gray-link">
                                <span class="u-visually-hidden">SpringerOpen Facebook page</span>
                                <svg class="u-icon u-text-lg" width="24" height="24" aria-hidden="true" focusable="false">
                                    <use xlink:href="#icon-facebook-bordered"></use>
                                </svg>
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
        <p class="u-reset-margin">
            By using this website, you agree to our
            <a class="u-gray-link" href="//www.springeropen.com/terms-and-conditions">Terms and Conditions</a>,
            <a class="u-gray-link" href="https://www.springernature.com/ccpa">California Privacy Statement</a>,
            <a class="u-gray-link" href="//www.springeropen.com/privacy-statement">Privacy
                statement</a> and
            <a class="u-gray-link" href="//www.springeropen.com/cookies" data-test="cookie-link">Cookies</a> policy.
            
                <a class="optanon-toggle-display u-gray-link" data-cc-action="preferences" href="javascript:void(0);">Manage cookies/Do not sell my data</a> we use in the preference centre.
            
        </p>
    </div>
</div>

        
        <div class="c-corporate-footer">
    <div class="u-container">
        <img src=/static/images/logo-springernature-acb40b85fb.svg class="c-corporate-footer__logo" alt="Springer Nature" itemprop="logo" role="img">
        <p class="c-corporate-footer__legal" data-test="copyright"> &#169; 2022 BioMed Central Ltd unless otherwise stated. Part of
            <a class="c-corporate-footer__link" href="https://www.springernature.com" itemscope itemtype="http://schema.org/Organization" itemid="#parentOrganization">Springer Nature</a>.
        </p>
    </div>
</div>

    </footer>

    </div>



        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
        <symbol id="icon-info" viewBox="0 0 18 18">
            <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-success" viewBox="0 0 18 18">
            <path d="M9 0a9 9 0 110 18A9 9 0 019 0zm3.486 4.982l-4.718 5.506L5.14 8.465a.991.991 0 00-1.423.133 1.06 1.06 0 00.13 1.463l3.407 2.733a1 1 0 001.387-.133l5.385-6.334a1.06 1.06 0 00-.116-1.464.991.991 0 00-1.424.119z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-chevron-down" viewBox="0 0 16 16">
            <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/>
        </symbol>
        <symbol id="icon-warning" viewBox="0 0 18 18">
            <path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-plus" viewBox="0 0 16 16">
            <path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-minus" viewBox="0 0 16 16">
            <path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-error" viewBox="0 0 18 18">
            <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill="currentColor" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-springer-arrow-left">
            <path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/>
        </symbol>
        <symbol id="icon-springer-arrow-right">
            <path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/>
        </symbol>
        <symbol id="icon-arrow-up" viewBox="0 0 16 16">
            <path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-tick" viewBox="0 0 24 24">
            <path d="M12,24 C5.372583,24 0,18.627417 0,12 C0,5.372583 5.372583,0 12,0 C18.627417,0 24,5.372583 24,12 C24,18.627417 18.627417,24 12,24 Z M7.657,10.79 C7.45285634,10.6137568 7.18569967,10.5283283 6.91717333,10.5534259 C6.648647,10.5785236 6.40194824,10.7119794 6.234,10.923 C5.87705269,11.3666969 5.93445559,12.0131419 6.364,12.387 L10.261,15.754 C10.6765468,16.112859 11.3037113,16.0695601 11.666,15.657 L17.759,8.713 C18.120307,8.27302248 18.0695334,7.62621189 17.644,7.248 C17.4414817,7.06995024 17.1751516,6.9821166 16.9064461,7.00476032 C16.6377406,7.02740404 16.3898655,7.15856958 16.22,7.368 L10.768,13.489 L7.657,10.79 Z"/>
        </symbol>
    </svg>

    </body>
</html>

