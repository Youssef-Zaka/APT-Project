<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Research:Ethical and human-centered AI/Bias in recommender systems - Meta</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"b626641f-f70b-4a9a-b130-d4d06dd8db64","wgCSPNonce":false,"wgCanonicalNamespace":"Research","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":202,"wgPageName":"Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems","wgTitle":"Ethical and human-centered AI/Bias in recommender systems","wgCurRevisionId":22298932,"wgRevisionId":22298932,"wgArticleId":10676418,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: DOI","2018 projects","Completed research projects"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":
"Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems","wgRelevantArticleId":10676418,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRedirectedFrom":"Research:Preventing,_identifying,_and_addressing_bias_in_recommender_systems","wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":40000,"wgNoticeProject":"meta","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgULSCurrentAutonym":"English","wgInternalRedirectTargetUrl":"/wiki/Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems","wgEditSubmitButtonLabelPublish":true,"wgCentralAuthMobileDomain":false,"wgULSPosition":"personal","wgULSisCompactLinksEnabled":true};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":
"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","skins.vector.styles.legacy":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.wikimediaBadges":"ready","ext.uls.pt":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["mediawiki.action.view.redirect","ext.cite.ux-enhancements","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.CentralAuthInterlinkFixer","ext.gadget.wm-portal-preview","ext.gadget.AddTopic","ext.gadget.addMe","ext.gadget.formWizard","ext.gadget.susaresources","ext.gadget.aaq","ext.gadget.wrcEditor","ext.gadget.wrcCollapsible","ext.gadget.ReferenceTooltips","ext.gadget.wrcAddNewIndividual",
"ext.gadget.wrcAddNewGroup","ext.gadget.wadpGadgetsLoader","ext.centralauth.centralautologin","ext.uls.compactlinks","ext.uls.interface"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1i9g4",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.pt%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.39.0-wmf.10"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta name="format-detection" content="telephone=no"/>
<meta property="og:title" content="Research:Ethical and human-centered AI/Bias in recommender systems - Meta"/>
<meta property="og:type" content="website"/>
<link rel="preconnect" href="//upload.wikimedia.org"/>
<link rel="alternate" media="only screen and (max-width: 720px)" href="//meta.m.wikimedia.org/wiki/Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems"/>
<link rel="alternate" type="application/x-wiki" title="Edit" href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit"/>
<link rel="shortcut icon" href="/static/favicon/community.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Meta (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//meta.wikimedia.org/w/api.php?action=rsd"/>
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://meta.wikimedia.org/wiki/Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-202 ns-subject mw-editable page-Research_Ethical_and_human-centered_AI_Bias_in_recommender_systems rootpage-Research_Ethical_and_human-centered_AI skin-vector action-view skin-vector-legacy"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"><!-- CentralNotice --></div>
	<div class="mw-indicators">
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading">Research:Ethical and human-centered AI/Bias in recommender systems</h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Meta, a Wikimedia project coordination wiki</div>
		<div id="contentSub"><span class="subpages">&lt; <a href="/wiki/Research:Ethical_and_human-centered_AI" title="Research:Ethical and human-centered AI">Research:Ethical and human-centered AI</a></span><span class="mw-redirectedfrom">(Redirected from <a href="/w/index.php?title=Research:Preventing,_identifying,_and_addressing_bias_in_recommender_systems&amp;redirect=no" class="mw-redirect" title="Research:Preventing, identifying, and addressing bias in recommender systems">Research:Preventing, identifying, and addressing bias in recommender systems</a>)</span></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content mw-content-ltr" lang="en" dir="ltr"><div class="mw-parser-output"><div class="tracked tracked-floatright mw-trackedTemplate">
<style data-mw-deduplicate="TemplateStyles:r22504042">.mw-parser-output .tracked{font-size:85%;margin:2px;background-color:#eaecf0;border:1px solid #a2a9b1;border-radius:.5em;padding:2px .3em;width:12em;text-align:center}.mw-parser-output .tracked-floatright{float:right;clear:right;margin:2px 0 2px 2px}.mw-parser-output .tracked-floatleft{float:left;clear:left;margin:2px 2px 2px 0}</style>
Tracked in <a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Phabricator" class="extiw" title="mw:Special:MyLanguage/Phabricator">Phabricator</a>&#58; <br /> <a href="https://phabricator.wikimedia.org/T203049" class="extiw" title="phabricator:T203049"><strong class="trakfab-T203049">task T203049</strong></a></div>
<div style="float:right;clear:right;width:250px;margin-left:1em;margin-bottom:1em;background:#transparent;box-shadow: 0em 0em 0em rgba(0,0,0,.5);"><div style="padding: 0em 1em;"><div style="height: .5em;"></div><div style="background:rgba(0,0,0,.03); padding:.5ex .5em; margin-top:.5em;"><div style="border-bottom:1px solid #666"><b> Contact </b></div>
 <div>
    <div style="font-weight:bold;text-align:center;"><a href="/wiki/User:Jmorgan_(WMF)" title="User:Jmorgan (WMF)">Jonathan Morgan</a></div><div style="font-size:.8em;text-align:center;margin-bottom:.5em;"><i><a href="https://en.wikipedia.org/wiki/Wikimedia_Foundation" class="extiw" title="en:Wikimedia Foundation">Wikimedia Foundation</a></i></div>
</div>  </div><div style="background:rgba(0,0,0,.03); padding:.5ex .5em; margin-top:.5em;"><b>Duration:</b>&#160; 2018-August – 2018-December </div><div style="height: 1em;"><hr /><div class="center"><small><a href="/wiki/Research:Projects" title="Research:Projects">Research:Projects</a></small></div></div></div></div>
<div class="center"><div style="display: inline-block; width: 10em;"></div><div><div style="display: inline-block"><a href="/wiki/File:VisualEditor_-_Icon_-_Check.svg" class="image"><img alt="VisualEditor - Icon - Check.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/60/VisualEditor_-_Icon_-_Check.svg/24px-VisualEditor_-_Icon_-_Check.svg.png" decoding="async" width="24" height="24" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/60/VisualEditor_-_Icon_-_Check.svg/36px-VisualEditor_-_Icon_-_Check.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/60/VisualEditor_-_Icon_-_Check.svg/48px-VisualEditor_-_Icon_-_Check.svg.png 2x" data-file-width="12" data-file-height="12" /></a></div><b>This page documents a <a href="/wiki/Research:Completed_projects" class="mw-redirect" title="Research:Completed projects">completed research project</a>.</b></div>
</div>
<p><br />
</p>
<div class="messagebox-pnutshell" style="position:relative;margin:2px auto;width:80%;min-width:18em;border:1px solid #AAA;background:#F9F9F9;padding:2px">
<div style="position:absolute;top:2px;left:2px"><a href="/wiki/File:Walnut.svg" class="image"><img alt="Walnut.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Walnut.svg/30px-Walnut.svg.png" decoding="async" width="30" height="30" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Walnut.svg/45px-Walnut.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Walnut.svg/60px-Walnut.svg.png 2x" data-file-width="192" data-file-height="192" /></a></div><div style="position:relative;margin:0;margin-left:34px;min-height:30px"><div style="margin:0;padding:5px"><b>This page in a nutshell:&#32;</b>This document is an early draft of <a href="/wiki/Research:Ethical_and_human-centered_AI" title="Research:Ethical and human-centered AI">Research:Ethical_and_human-centered_AI</a></div></div></div>
<p>This document provides an overview of recent research, policy, process, methods, and critique related to addressing bias in machine learning applications, focusing on recommender systems.
</p><p>In the past few years, researchers, designers, developers and legal scholars have begun to develop new methods for preventing, identifying, and addressing bias in recommender systems and other machine learning applications, and to formulate best practices for organizations that engage in machine learning development.
</p><p>Below is an outline of considerations, methods, best practices, and external resources that inform the development of the Wikimedia Foundation's recommender systems.
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Overview"><span class="tocnumber">1</span> <span class="toctext">Overview</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Goals"><span class="tocnumber">1.1</span> <span class="toctext">Goals</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Concepts_and_key_terms"><span class="tocnumber">1.2</span> <span class="toctext">Concepts and key terms</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#How_to_use_this_document"><span class="tocnumber">1.3</span> <span class="toctext">How to use this document</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#Stage_1:_Design"><span class="tocnumber">2</span> <span class="toctext">Stage 1: Design</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Articulating_design_goals"><span class="tocnumber">2.1</span> <span class="toctext">Articulating design goals</span></a>
<ul>
<li class="toclevel-3 tocsection-7"><a href="#Identifying_audience,_purpose,_and_context"><span class="tocnumber">2.1.1</span> <span class="toctext">Identifying audience, purpose, and context</span></a></li>
<li class="toclevel-3 tocsection-8"><a href="#Developing_hypotheses_and_benchmarks"><span class="tocnumber">2.1.2</span> <span class="toctext">Developing hypotheses and benchmarks</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-9"><a href="#Selecting_models,_data,_and_features"><span class="tocnumber">2.2</span> <span class="toctext">Selecting models, data, and features</span></a>
<ul>
<li class="toclevel-3 tocsection-10"><a href="#Using_interpretable_models"><span class="tocnumber">2.2.1</span> <span class="toctext">Using interpretable models</span></a></li>
<li class="toclevel-3 tocsection-11"><a href="#Collecting_training_data"><span class="tocnumber">2.2.2</span> <span class="toctext">Collecting training data</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-12"><a href="#Engaging_relevant_stakeholders"><span class="tocnumber">2.3</span> <span class="toctext">Engaging relevant stakeholders</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Eliciting_stakeholder_input"><span class="tocnumber">2.3.1</span> <span class="toctext">Eliciting stakeholder input</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#Building_diverse_teams"><span class="tocnumber">2.3.2</span> <span class="toctext">Building diverse teams</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Defining_roles,_goals,_and_processes"><span class="tocnumber">2.3.3</span> <span class="toctext">Defining roles, goals, and processes</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-16"><a href="#Stage_2:_Pre-launch"><span class="tocnumber">3</span> <span class="toctext">Stage 2: Pre-launch</span></a>
<ul>
<li class="toclevel-2 tocsection-17"><a href="#Performing_comparative_and_iterative_testing"><span class="tocnumber">3.1</span> <span class="toctext">Performing comparative and iterative testing</span></a>
<ul>
<li class="toclevel-3 tocsection-18"><a href="#Offline_evaluation"><span class="tocnumber">3.1.1</span> <span class="toctext">Offline evaluation</span></a></li>
<li class="toclevel-3 tocsection-19"><a href="#User_studies"><span class="tocnumber">3.1.2</span> <span class="toctext">User studies</span></a></li>
<li class="toclevel-3 tocsection-20"><a href="#Online_evaluation"><span class="tocnumber">3.1.3</span> <span class="toctext">Online evaluation</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-21"><a href="#Interrogating_disparate_impacts_and_unintended_consequences"><span class="tocnumber">3.2</span> <span class="toctext">Interrogating disparate impacts and unintended consequences</span></a>
<ul>
<li class="toclevel-3 tocsection-22"><a href="#Offline_evaluation_2"><span class="tocnumber">3.2.1</span> <span class="toctext">Offline evaluation</span></a></li>
<li class="toclevel-3 tocsection-23"><a href="#User_studies_2"><span class="tocnumber">3.2.2</span> <span class="toctext">User studies</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-24"><a href="#Stage_3:_Post-launch"><span class="tocnumber">4</span> <span class="toctext">Stage 3: Post-launch</span></a>
<ul>
<li class="toclevel-2 tocsection-25"><a href="#Assessing_impacts"><span class="tocnumber">4.1</span> <span class="toctext">Assessing impacts</span></a>
<ul>
<li class="toclevel-3 tocsection-26"><a href="#Short_term"><span class="tocnumber">4.1.1</span> <span class="toctext">Short term</span></a></li>
<li class="toclevel-3 tocsection-27"><a href="#Long_term"><span class="tocnumber">4.1.2</span> <span class="toctext">Long term</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-28"><a href="#Enabling_ongoing_monitoring_and_re-evaluation"><span class="tocnumber">4.2</span> <span class="toctext">Enabling ongoing monitoring and re-evaluation</span></a>
<ul>
<li class="toclevel-3 tocsection-29"><a href="#Accountability"><span class="tocnumber">4.2.1</span> <span class="toctext">Accountability</span></a></li>
<li class="toclevel-3 tocsection-30"><a href="#Transparency"><span class="tocnumber">4.2.2</span> <span class="toctext">Transparency</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-31"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a>
<ul>
<li class="toclevel-2 tocsection-32"><a href="#Recommender_systems_on_Wikimedia_projects"><span class="tocnumber">5.1</span> <span class="toctext">Recommender systems on Wikimedia projects</span></a></li>
<li class="toclevel-2 tocsection-33"><a href="#Related_projects"><span class="tocnumber">5.2</span> <span class="toctext">Related projects</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-34"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Overview">Overview</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=1" title="Edit section: Overview">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="https://en.wikipedia.org/wiki/Recommender_systems" class="extiw" title="w:Recommender systems">Recommender systems</a> are among the most widespread and visible machine learning applications. Wikimedia has begun to develop recommender systems to power a variety of reader- and contributor-facing features across our platforms: such as articles to read next, articles to translate, and links and sections to add to existing articles.
</p><p>Recommender systems can be a powerful tool for increasing reader engagement, and encouraging contribution, and filling knowledge gaps. However, recommender systems, like all machine-learning applications, have the potential to reflect and reinforce harmful biases. Sources of bias in machine learning applications can stem from a wide variety of sources, such as:
</p>
<ul><li>limitations in training data</li>
<li>social and analytical assumptions of system developers</li>
<li>technical limitations of software infrastructure, and</li>
<li>evolving social norms</li></ul>
<p>Related issues around user expectations of privacy and anonymity, appropriate stewardship of personal data, and accountability and interpretability in algorithmic decision-making can compound the risks associated with bias in recommender systems. 
</p>
<h3><span class="mw-headline" id="Goals">Goals</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=2" title="Edit section: Goals">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>This document is intended to:
</p>
<ol><li>summarize the state of the art in research, scholarship, and organizational policy related to the causes and consequences of bias in recommender systems</li>
<li>describe general best practices and industry-standard methods for prevent, identifying, and addressing bias during software development</li>
<li>identify important considerations and external resources that are relevant at each stage of the recommender system development process</li></ol>
<h3><span class="mw-headline" id="Concepts_and_key_terms">Concepts and key terms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=3" title="Edit section: Concepts and key terms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<dl><dt>Recommender systems</dt>
<dd>Algorithmic tools for identifying items (e.g. products, services) of interest to users of a technology platform (Ekstrand et al. 2018)</dd></dl>
<dl><dt>Bias</dt>
<dd>Systematically and unfairly discriminating against certain individuals or groups of individuals in favor of others (Friedman and Nissenbaum, 1996)</dd></dl>
<dl><dt>Harm</dt>
<dd>Denying a group access to valuable resources and opportunities (<i>allocative harm</i>), or underscoring or reinforcing the subordination of some social or cultural group (<i>representative harm</i>) (Reisman et al. 2018)</dd></dl>
<dl><dt>Accountability</dt>
<dd>the process of assigning responsibility for harm when algorithmic decision-making results in discriminatory and inequitable outcomes (Donovan et al. 2018)</dd></dl>
<dl><dt>Transparency</dt>
<dd>the degree to which the factors that influence the decisions made by an algorithmic system are visible to the people who use, regulate or are impacted by those systems</dd></dl>
<dl><dt>Interpretability</dt>
<dd>the degree to which the people who use, regulate, or are impacted by algorithmic systems can understand how that system makes decisions and/or the reason why a particular decision was made</dd></dl>
<p><br />
</p>
<h3><span class="mw-headline" id="How_to_use_this_document">How to use this document</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=4" title="Edit section: How to use this document">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<dl><dt>This document is intended to be used as a menu, not a checklist</dt>
<dd>It is not assumed that every consideration presented here will be applicable to the development of any particular recommender systems in any particular context. This document is also not a comprehensive set of all possible considerations, methods, best practices, and resources related to bias in the development of recommender systems.</dd></dl>
<p>As such, this document is intended to provide teams engaged in the development and/or evaluation of recommender systems with guidance on approaches that they may want to consider applying, singly or in combination; common pitfalls to watch out for; and principles that others engaged in this work have identified as important for understanding causes and indicators of bias; assessing risk and preventing potential negative impacts of bias before they occur; as well as addressing those (in some cases, potentially inevitable) impacts of bias where they appear, and when they matter most.
</p>
<dl><dt>Document format</dt>
<dd>The recommendations, considerations and resources presented in this document are organized according to three stages of design and development identified by the Fairness, Accountability, and Transparency in Machine Learning organization: design, pre-launch, and post-launch.<sup id="cite_ref-:0_1-0" class="reference"><a href="#cite_note-:0-1">&#91;1&#93;</a></sup></dd></dl>
<h2><span class="mw-headline" id="Stage_1:_Design">Stage 1: Design</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=5" title="Edit section: Stage 1: Design">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Articulating_design_goals">Articulating design goals</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=6" title="Edit section: Articulating design goals">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span id="Identifying_audience.2C_purpose.2C_and_context"></span><span class="mw-headline" id="Identifying_audience,_purpose,_and_context">Identifying audience, purpose, and context</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=7" title="Edit section: Identifying audience, purpose, and context">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Direct and indirect stakeholders.</b> Identifying the direct (intended end users) and indirect stakeholders (non-users who may be impacted) of a recommender system is a key first step in human-centered algorithm design.<sup id="cite_ref-:2_2-0" class="reference"><a href="#cite_note-:2-2">&#91;2&#93;</a></sup> Considering who will use to system, what they may use it for, and the context of use helps ensure that the resulting product is usable and addresses a real human need.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">&#91;3&#93;</a></sup> Considering potential impacts on indirect stakeholders helps the developers understand the scope of their accountability: for example, whether the resulting system may benefit some people at the expense of others.</li>
<li><b>Risk assessment</b>. Several agencies have developed checklists, frameworks, and principles that development teams can use to assess the potential risks associated with building and deploying algorithmic systems.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup><sup id="cite_ref-ainow_5-0" class="reference"><a href="#cite_note-ainow-5">&#91;5&#93;</a></sup><sup id="cite_ref-:0_1-1" class="reference"><a href="#cite_note-:0-1">&#91;1&#93;</a></sup> In the case of recommender systems, risk assessments should focus on risks associated with potential sources of pre-existing, technical, and emergent bias in the planned system.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup></li></ul>
<h4><span class="mw-headline" id="Developing_hypotheses_and_benchmarks">Developing hypotheses and benchmarks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=8" title="Edit section: Developing hypotheses and benchmarks">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Current conditions.</b> An assessment of current conditions related to the problem the system is intended to address (e.g. gaps in Wikipedia content coverage mediated by low contributor diversity) and contributing factors should be conducted in order to develop benchmarks against which the impact of the system can be assessed.</li>
<li><b>Anticipated impacts.</b> Anticipated impacts should be clearly stated and justified based on relevant empirical research and social science theory. Specific, testable hypotheses should be developed in order to ensure that the system is evaluated according to criteria that are consistent with stated goals and against pre-specified benchmarks.</li></ul>
<h3><span id="Selecting_models.2C_data.2C_and_features"></span><span class="mw-headline" id="Selecting_models,_data,_and_features">Selecting models, data, and features</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=9" title="Edit section: Selecting models, data, and features">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Using_interpretable_models">Using interpretable models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=10" title="Edit section: Using interpretable models">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Model opacity.</b> Recommender models are opaque if an important stakeholder (and end-user or anyone else invested in or impacted by the model) are unable to understand how or why the algorithm arrived at a particular recommendation. Thus, opacity can be both a function of the computational literacy of the stakeholder and the characteristics of the model.<sup id="cite_ref-:1_7-0" class="reference"><a href="#cite_note-:1-7">&#91;7&#93;</a></sup> Opaque models present a greater risk of introducing or reinforcing bias because the ability to understand how the model works overall, and how it makes specific recommendations, is restricted to stakeholders with high levels of computational literacy. When selecting and designing recommender models, developers should consider not just accuracy and efficiency, but the computational literacy of their stakeholders and the degree to which the stakeholders need to be able to interpret model outputs.</li>
<li><b>Model complexity.</b> Some recommender models may be so operationally complex that it is very difficult, if not impossible, to articulate the model's features, operations, and underlying logic to an end user with sufficient completeness and correctness.<sup id="cite_ref-:1_7-1" class="reference"><a href="#cite_note-:1-7">&#91;7&#93;</a></sup> When selecting a model, developers should consider whether they can get sufficient accuracy and efficiency from a less complex (but more explainable) model, in order to reduce the risk that bias in the model will go undetected. They should also consider whether--given the model chosen, the anticipated context of use, the likelihood and anticipated consequences of bias, and the computational literacy of the stakeholders, it is more important to present stakeholders with a more descriptive vs. a more persuasive description of model's decision-making.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup></li></ul>
<h4><span class="mw-headline" id="Collecting_training_data">Collecting training data</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=11" title="Edit section: Collecting training data">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Labeling campaigns.</b> Human annotators may embed their own biases in training data based on their level of expertise and sociocultural background.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup> The way the annotation task is presented to annotators—including the task description, question prompts, choice architecture, and user interface—can also introduce bias in the training dataset. In the context of developing training data for recommender systems, developers should consider whether the interests and attitudes of the labelers match those of the intended audience.</li>
<li><b>Behavioral trace data.</b> When drawing on log data of user-system interactions (e.g. webrequest logs, edit histories) to train recommender algorithms, it is important to identify sources of noise and potential mis-matches between the population the sample is drawn from and the target population. For example, a dataset to train a recommender system for suggesting articles to good-faith new editors should exclude edits by likely vandals. A dataset of section headings in English Wikipedia used to train a section heading recommender system in Spanish Wikipedia may suggest sections that are discouraged by the Spanish community's policies or Manual of Style.</li>
<li><b>Existing datasets.</b> Datasets of behavioral traces, or annotated datasets, collected from other platforms or other contexts may introduce bias when used as training data for recommender systems. Recent proposals have called for the development of <i>data statements</i> for NLP datasets that include information such as speaker and annotator demographics, text characteristics, and speech situation in order to help researchers who re-use these corpora in different contexts assess the potential sources of and consequences of bias.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup></li></ul>
<h3><span class="mw-headline" id="Engaging_relevant_stakeholders">Engaging relevant stakeholders</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=12" title="Edit section: Engaging relevant stakeholders">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Eliciting_stakeholder_input">Eliciting stakeholder input</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=13" title="Edit section: Eliciting stakeholder input">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Generative research.</b> Performing qualitative research with a diverse set of stakeholders before development begins can help identify potential sources of bias, as well as helping to define the problem space and user requirements for the system. Successful approaches include surveys, interviews,<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup> focus groups,<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> and participatory design activities.<sup id="cite_ref-:2_2-1" class="reference"><a href="#cite_note-:2-2">&#91;2&#93;</a></sup></li>
<li><b>Community consultation.</b> Pitching your proposal to the impacted community and inviting feedback allows community members to raise concerns and point out un-interrogated assumptions, and can build trust. The AI Now Institute notes that holding multiple rounds of request for comment "provides a strong foundation for building public trust through appropriate levels of transparency... subsequent requests can solicit further information or the presentation of new evidence, research, or other inputs that the agency may not have adequately considered."<sup id="cite_ref-ainow_5-1" class="reference"><a href="#cite_note-ainow-5">&#91;5&#93;</a></sup></li></ul>
<h4><span class="mw-headline" id="Building_diverse_teams">Building diverse teams</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=14" title="Edit section: Building diverse teams">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Demographic diversity.</b> The team that develops recommender systems should include members with a diversity of backgrounds, including but not limited to age, sex, race, gender identity, culture, language, and geographic location. People are often blind to the biases that impact their own opinions and decisions. Including a diverse set of voices in discussions about the goals and process of system development, and actively encouraging team members to articulate their own perspectives in those discussions, can help surface hidden assumptions, edge cases, values conflicts, and disparate impacts.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup></li>
<li><b>Subject-matter expertise.</b> Social scientists, designers and researchers, legal and policy experts, and community leaders can provide invaluable insights on who important stakeholders and stakeholder communities are; their motivations, wants, needs, and identified problems; and the broader social, organizational, and societal impacts of developing and deploying a new system in an existing sociotechnical system. Incorporate people who have expertise beyond those of the core model and infrastructure development team—i.e. beyond data scientists, software engineers and project managers—into the team early on. Give them meaningful, ongoing roles in project scoping, goal setting, requirements development, evaluation criteria, and other important decision points and decision-making processes.</li></ul>
<h4><span id="Defining_roles.2C_goals.2C_and_processes"></span><span class="mw-headline" id="Defining_roles,_goals,_and_processes">Defining roles, goals, and processes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=15" title="Edit section: Defining roles, goals, and processes">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Internal accountability.</b> Decide who on the team is responsible for making final decisions about design, deployment, and evaluation. Define internal reporting processes, long-term maintenance and post-launch monitoring plans (and associated roles), and contingency plans that cover issues such as discovery of data breaches, harmful biases, and other unintended consequences.<sup id="cite_ref-:0_1-2" class="reference"><a href="#cite_note-:0-1">&#91;1&#93;</a></sup></li>
<li><b>External review.</b> People not directly involved in the development of the recommender system (inside and outside the organization) should be invited to review project plans and resources before launch. Develop mechanisms for external review of development, deployment, and evaluation plans; risk assessments; potential training datasets, models, and infrastructure components. External review can flag issues related to bias, risk, ethics, organizational policy and strategy, law, user privacy, and sustainability.<sup id="cite_ref-ainow_5-2" class="reference"><a href="#cite_note-ainow-5">&#91;5&#93;</a></sup></li></ul>
<h2><span class="mw-headline" id="Stage_2:_Pre-launch">Stage 2: Pre-launch</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=16" title="Edit section: Stage 2: Pre-launch">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Performing_comparative_and_iterative_testing">Performing comparative and iterative testing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=17" title="Edit section: Performing comparative and iterative testing">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Offline_evaluation">Offline evaluation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=18" title="Edit section: Offline evaluation">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Evaluation protocols.</b> Different offline evaluation metrics are more suitable to different recommender algorithms.<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup> Recent work has suggested that the design of the protocol selected for offline evaluation of can exaggerate particular kinds of bias in recommender systems, resulting in divergence between assessed accuracy and actual utility (as measured by online evaluation and user studies).<sup id="cite_ref-:4_15-0" class="reference"><a href="#cite_note-:4-15">&#91;15&#93;</a></sup></li>
<li><b>Comparative evaluation.</b> Research has shown that models that perform similarly according to accuracy-based metrics exhibit very different degrees of utility when compared using online evaluation or user studies.<sup id="cite_ref-:3_16-0" class="reference"><a href="#cite_note-:3-16">&#91;16&#93;</a></sup><sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup> Experimenting with multiple models and model parameterizations, rather than committing to a specific modeling approach at the start of development, enables comparative evaluation according to utility, bias, and fairness.</li></ul>
<h4><span class="mw-headline" id="User_studies">User studies</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=19" title="Edit section: User studies">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Expectations, explanations, and mental models.</b> Testing model prototypes with representative system users can help system developers understand the results people expect from the model, how they understand the way the model generates recommendations, and what kind of explanations of model processes and outcomes are useful for evaluating user trust and acceptance. Allowing users to explore input datasets, sample outputs, and interactive visualizations of model decision-making can help surface new features that can be included to improve the model performance. These studies can also be conducted before any model development work has been done, using "Wizard of Oz" protocols.<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup></li>
<li><b>Usability and utility.</b> The usability and utility of recommender systems can be quantitatively measured through user studies<sup id="cite_ref-:5_19-0" class="reference"><a href="#cite_note-:5-19">&#91;19&#93;</a></sup> consisting of questionnaires and/or task protocols based on established Human-Recommender Interaction frameworks<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup><sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup> psychometric evaluation protocols,<sup id="cite_ref-:3_16-1" class="reference"><a href="#cite_note-:3-16">&#91;16&#93;</a></sup> or designed to suit the particular goals and context of the recommender application. User study methods can be used to identify relationships between subjective factors such as user satisfaction, novelty, and item diversity when comparing different models or different iterations of a single model.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> Explicit user input can also be elicited interactively to improve personalization.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup></li></ul>
<h4><span class="mw-headline" id="Online_evaluation">Online evaluation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=20" title="Edit section: Online evaluation">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Pilot testing.</b> Pilot testing (usually A/B testing) should be performed in a way that is non-disruptive and that does not unfairly advantage or disadvantage particular groups. Evaluation of pilot tests should be compared against pre-defined and publicly disclosed benchmarks or pre-formulated hypotheses. Additionally, pilot tests provide the first opportunity to study the unintended impacts of the recommender system on non-participants (e.g. platform users who are not part of an experimental cohort), and on the dynamics of the platform or community as a whole.</li>
<li><b>UI design.</b> Recommendations are surfaced to users in many different ways depending on the platform, the content, the purpose of the system, and the anticipated desires of the users.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup> Design decisions (even small ones) in the user interface—such as the number of recommendations provided, the information about the recommendations, and the mechanisms used to gather implicit feedback—can have consequences for how, and how much, they use the system<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup>, and on what the algorithm learns.<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup> There are a variety of common <a href="https://en.wikipedia.org/wiki/Interaction_design_pattern" class="extiw" title="w:Interaction design pattern">design patterns</a> for the user interfaces of recommender system, and different patterns may be better suited to different audiences, purposes, and contexts.<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup></li>
<li><b>Explicit feedback.</b> Gathering explicit feedback from system users during short-term, experimental, or beta deployments. The recommender interface should clearly communicate to users that it is an experimental feature, and there should be clear calls to action for the users to provide substantial feedback (beyond binary or scalar relevance ratings or a "dismiss" icon) on the content of the recommendations and the way they are presented. Allowing meaningful feedback (e.g. via free-text comments or short multiple-choice surveys) not only provides additional insights into users expectations and impressions of the recommender system, it also allows them to flag potentially harmful unintended consequences, such as the presence of bias in recommendations, or negative impacts on user experience or community health.</li></ul>
<h3><span class="mw-headline" id="Interrogating_disparate_impacts_and_unintended_consequences">Interrogating disparate impacts and unintended consequences</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=21" title="Edit section: Interrogating disparate impacts and unintended consequences">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Offline_evaluation_2">Offline evaluation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=22" title="Edit section: Offline evaluation">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>False positives and false negatives.</b> When determining the proper thresholds for false-positives and false-negatives, consider the consequences of each type of error on user experience and user outcomes.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">&#91;28&#93;</a></sup></li>
<li><b>Subgroup fairness.</b> Recommender systems that perform well overall may nonetheless work better for some types of users than others. Therefore, it is important to evaluate the effectiveness of the recommender for particular subgroups of users when possible in order to identify and mitigate this source of bias.<sup id="cite_ref-:4_15-1" class="reference"><a href="#cite_note-:4-15">&#91;15&#93;</a></sup><sup id="cite_ref-29" class="reference"><a href="#cite_note-29">&#91;29&#93;</a></sup> However, evaluating accuracy for individual, identified subgroups (e.g. race, gender, geographic location, or site tenure) may not itself be sufficient to prevent harmful bias, as bias can also be reflected in intersectional/constructed subgroups, such as "newer female users" or "established users from southeast asia").<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">&#91;30&#93;</a></sup></li></ul>
<h4><span class="mw-headline" id="User_studies_2">User studies</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=23" title="Edit section: User studies">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Purposeful sampling.</b> User studies often attempt to recruit a relatively random sample of system users, in order to make generalizable claims about the usability or utility of a system. However, in cases where the purpose of the study is to identify issues--such as bias--where the system may have differential impacts on different people based on background, attitudes, or activities, it is important to purposefully sample from these groups when recruiting for user studies.<sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup> In the case of bias, it is important to test the system with users who are a) under-represented in the system, b) known to have needs that are poorly addressed by the current systems, c) likely to be more dependent on the functionality that the recommender system provides, and/or d) known to be subject to harmful biases in society at large.</li>
<li><b>Edge and corner cases.</b> User studies provide opportunities to investigate how well the recommender system performs (in terms of perceived utility and in terms of sources of bias in the recommended items) in cases where the recommender system is making recommendations outside of its normal operating parameters<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">&#91;32&#93;</a></sup> (<a href="https://en.wikipedia.org/wiki/Edge_case" class="extiw" title="w:Edge case">edge cases</a>) and in cases where multiple variables are near, but not exceeding, their extreme limits (<a href="https://en.wikipedia.org/wiki/Corner_case" class="extiw" title="w:Corner case">corner cases</a>). For example, a system that recommends articles to editors based on edit history should be tested with very new users, who have made very few edits (edge case); a system that recommends articles to readers based on a combination of article quality, user browsing history and user geographic location should be tested to see how well it performs for readers who live in regions in which there are relatively few geo-tagged articles, and most articles are of comparatively low assessed quality (corner case).</li></ul>
<h2><span class="mw-headline" id="Stage_3:_Post-launch">Stage 3: Post-launch</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=24" title="Edit section: Stage 3: Post-launch">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Assessing_impacts">Assessing impacts</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=25" title="Edit section: Assessing impacts">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Short_term">Short term</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=26" title="Edit section: Short term">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Research outcomes.</b> The most robust measure of the success of a recommender system is a large-scale evaluation with real users in the intended context of use. Pre-launch A/B test experiments can be effective for testing particular hypotheses about the impacts of the system. However, formulating more comprehensive metrics related to research outcomes, and evaluating the system against those outcomes after final deployment, is important to assure that the results from A/B tests (which are typically simplified or short-term, compared to real-world use cases) are generalizable and <a href="https://en.wikipedia.org/wiki/Ecologically_valid" class="extiw" title="w:Ecologically valid">ecologically valid</a>. Per Shani and Gunawardana: "When choosing an algorithm for a real application, we may want our conclusions to hold on the deployed system, and generalize beyond our experimental data set. Similarly, when developing new algorithms, we want our conclusions to hold beyond the scope of the specific application or data set that we experimented with."<sup id="cite_ref-:5_19-1" class="reference"><a href="#cite_note-:5-19">&#91;19&#93;</a></sup></li>
<li><b>Community response.</b> It is important to monitor the degree to which users adopt the system after deployment: how many people adopt it, who adopts it, and how quickly. The degree to which people who try out the system continue to use it over time can be a strong indicator of the systems long-term utility: user study participants and people involved in short-term experimental deployments may exhibit a high degree of interest in or engagement with a system because of its novelty or imagined utility, but decreased adoption rate (or decreased regular use among early adopters) can be a sign that regular use of the system decreases user trust, or that the system is poorly integrated into users accustomed workflows. Explicit feedback from early adopters, as well as non-users, in the form of bug reports, posts to technical support forums, or other public discussions can also provide insights into the impact that the system is having on the community as a whole.</li></ul>
<h4><span class="mw-headline" id="Long_term">Long term</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=27" title="Edit section: Long term">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Top-line metrics.</b> After a new recommender system is fully deployed, the impact of the system should be assessed in terms of the top-line metrics, key performance indicators, strategic goals of the organization.<sup id="cite_ref-33" class="reference"><a href="#cite_note-33">&#91;33&#93;</a></sup> Systems that show positive impacts according to their own specific success criteria may nevertheless have unintended consequences that are only apparent through retrospective analysis of the dynamics of the platform as a whole.<sup id="cite_ref-34" class="reference"><a href="#cite_note-34">&#91;34&#93;</a></sup></li>
<li><b>Iteration.</b> All software systems can benefit from iterative improvement. Feedback from system users and other community members can help developers identify sources of bias in recommender systems, as well as technical bugs and usability issues. Machine-learning driven applications may lose accuracy over time without explicit re-training and re-evaluation, as changes over time in user behavior impact the accuracy of existing model features.<sup id="cite_ref-35" class="reference"><a href="#cite_note-35">&#91;35&#93;</a></sup> Recommender systems are further susceptible to algorithmic confounding, in which user's interactions with the system increases the homogeneity of recommendations without increasing the utility of the recommender system.<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">&#91;36&#93;</a></sup></li></ul>
<h3><span class="mw-headline" id="Enabling_ongoing_monitoring_and_re-evaluation">Enabling ongoing monitoring and re-evaluation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=28" title="Edit section: Enabling ongoing monitoring and re-evaluation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Accountability">Accountability</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=29" title="Edit section: Accountability">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>External auditing.</b> The AI Now Institute recommends that public agencies "provide a meaningful and ongoing opportunity for external researchers to review, audit, and assess [algorithmic] systems using methods that allow them to identify and detect problems" in order to ensure greater accountability.<sup id="cite_ref-ainow_5-3" class="reference"><a href="#cite_note-ainow-5">&#91;5&#93;</a></sup> Journalists and social science researchers are in the process of adapting established methods for detecting fraud and bias in other domains performing audits of 'black box' algorithms.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup> External auditing can be made easier and more effective by publishing source code, providing public APIs,<sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup> and using interpretable algorithmic models.<sup id="cite_ref-39" class="reference"><a href="#cite_note-39">&#91;39&#93;</a></sup> In cases where full (public) transparency of models and data is not feasible, developers should work with researchers and community members to establish research access provisions, and the organization should maintain a public log of who is provided access to system code and/or data, and on what basis that access has been granted.<sup id="cite_ref-ainow_5-4" class="reference"><a href="#cite_note-ainow-5">&#91;5&#93;</a></sup></li>
<li><b>Reporting channels.</b> The development team should maintain rich reporting channels—such as mailing lists, public wiki pages, and bug tracking systems—to both disseminate and collect information about system performance.</li></ul>
<h4><span class="mw-headline" id="Transparency">Transparency</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=30" title="Edit section: Transparency">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>Logging.</b> Complete and comprehensive logs of model versions, UI changes, data schemas and inputs, performance evaluations, and deployments should be maintained and made publicly available (if possible) to facilitate auditing, error reporting, and retrospective analysis.</li>
<li><b>Documentation.</b> It is important to develop detailed, public, and readable documentation of system, including the system's limitations and assumptions embedded in its design.<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">&#91;40&#93;</a></sup> Documentation of input sources, features, and public APIs should be regularly updated to reflect changes in the system.</li></ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=31" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Recommender_systems_on_Wikimedia_projects">Recommender systems on Wikimedia projects</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=32" title="Edit section: Recommender systems on Wikimedia projects">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li><a href="https://www.mediawiki.org/wiki/GapFinder" class="extiw" title="mw:GapFinder">Gapfinder</a></li>
<li><a href="/wiki/Research:Expanding_Wikipedia_articles_across_languages#Section_Recommendations" title="Research:Expanding Wikipedia articles across languages">Section recommendations</a></li>
<li>Link recommendations</li>
<li>Related articles</li>
<li>Task recommendations</li>
<li>WikiProject recommendations</li>
<li>SuggestBot</li></ul>
<h3><span class="mw-headline" id="Related_projects">Related projects</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=33" title="Edit section: Related projects">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;section=34" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-:0-1"><span class="mw-cite-backlink">↑ <a href="#cite_ref-:0_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:0_1-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.fatml.org/resources/principles-for-accountable-algorithms">"Principles for Accountable Algorithms and a Social Impact Statement for Algorithms&#160;:: FAT ML"</a>. <i>www.fatml.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2018-09-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.btitle=Principles+for+Accountable+Algorithms+and+a+Social+Impact+Statement+for+Algorithms+%3A%3A+FAT+ML&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.fatml.org%2Fresources%2Fprinciples-for-accountable-algorithms&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-:2-2"><span class="mw-cite-backlink">↑ <a href="#cite_ref-:2_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:2_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Baumer, Eric PS (2017-07-25). <a rel="nofollow" class="external text" href="https://doi.org/10.1177/2053951717718854">"Toward human-centered algorithm design"</a>. <i>Big Data &amp; Society</i> <b>4</b> (2): 205395171771885. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2053-9517">2053-9517</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1177%2F2053951717718854">10.1177/2053951717718854</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Toward+human-centered+algorithm+design&amp;rft.aufirst=Eric+PS&amp;rft.aulast=Baumer&amp;rft.date=2017-07-25&amp;rft.genre=article&amp;rft.issn=2053-9517&amp;rft.issue=2&amp;rft.jtitle=Big+Data+%26+Society&amp;rft.pages=205395171771885&amp;rft.volume=4&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1177%2F2053951717718854&amp;rft_id=info%3Adoi%2F10.1177%2F2053951717718854&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><a href="#cite_ref-3">↑</a></span> <span class="reference-text">Kling, R. &amp; Star, L. (1997) <cite class="citation web"><a rel="nofollow" class="external text" href="https://scholarworks.iu.edu/dspace/bitstream/handle/2022/1798/wp97-04B.html">"Organizational and Social Informatics for Human Centered Systems"</a>. <i>scholarworks.iu.edu</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2018-09-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.btitle=Organizational+and+Social+Informatics+for+Human+Centered+Systems&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fscholarworks.iu.edu%2Fdspace%2Fbitstream%2Fhandle%2F2022%2F1798%2Fwp97-04B.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><a href="#cite_ref-4">↑</a></span> <span class="reference-text">Ethical OS (2018) Risk Mitigation Checklist. https://ethicalos.org/</span>
</li>
<li id="cite_note-ainow-5"><span class="mw-cite-backlink">↑ <a href="#cite_ref-ainow_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ainow_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ainow_5-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-ainow_5-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-ainow_5-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text">Reisman, D., Schultz, J., Crawford, K., &amp; Whittaker, M. (2018). <i>Algorithmic Impact Assessments: a Practical Framework for Public Agency Accountability</i>. Retrieved from https://ainowinstitute.org/aiareport2018.pdf</span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><a href="#cite_ref-6">↑</a></span> <span class="reference-text"><cite class="citation journal">Friedman, Batya; Nissenbaum, Helen (1996-07-01). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=230538.230561">"Bias in computer systems"</a>. <i>ACM Transactions on Information Systems (TOIS)</i> <b>14</b> (3): 330–347. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1046-8188">1046-8188</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F230538.230561">10.1145/230538.230561</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Bias+in+computer+systems&amp;rft.au=Nissenbaum%2C+Helen&amp;rft.aufirst=Batya&amp;rft.aulast=Friedman&amp;rft.date=1996-07-01&amp;rft.genre=article&amp;rft.issn=1046-8188&amp;rft.issue=3&amp;rft.jtitle=ACM+Transactions+on+Information+Systems+%28TOIS%29&amp;rft.pages=330-347&amp;rft.volume=14&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D230538.230561&amp;rft_id=info%3Adoi%2F10.1145%2F230538.230561&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-:1-7"><span class="mw-cite-backlink">↑ <a href="#cite_ref-:1_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Burrell, Jenna (2016-01-05). <a rel="nofollow" class="external text" href="https://doi.org/10.1177/2053951715622512">"How the machine ‘thinks’: Understanding opacity in machine learning algorithms"</a>. <i>Big Data &amp; Society</i> <b>3</b> (1): 205395171562251. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2053-9517">2053-9517</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1177%2F2053951715622512">10.1177/2053951715622512</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=How+the+machine+%E2%80%98thinks%E2%80%99%3A+Understanding+opacity+in+machine+learning+algorithms&amp;rft.aufirst=Jenna&amp;rft.aulast=Burrell&amp;rft.date=2016-01-05&amp;rft.genre=article&amp;rft.issn=2053-9517&amp;rft.issue=1&amp;rft.jtitle=Big+Data+%26+Society&amp;rft.pages=205395171562251&amp;rft.volume=3&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1177%2F2053951715622512&amp;rft_id=info%3Adoi%2F10.1177%2F2053951715622512&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><a href="#cite_ref-8">↑</a></span> <span class="reference-text"><cite class="citation journal">Herman, Bernease (2017-11-20). <a rel="nofollow" class="external text" href="http://arxiv.org/abs/1711.07414">"The Promise and Peril of Human Evaluation for Model Interpretability"</a>. <i>arXiv:1711.07414 [cs, stat]</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=The+Promise+and+Peril+of+Human+Evaluation+for+Model+Interpretability&amp;rft.aufirst=Bernease&amp;rft.aulast=Herman&amp;rft.date=2017-11-20&amp;rft.genre=article&amp;rft.jtitle=arXiv%3A1711.07414+%5Bcs%2C+stat%5D&amp;rft_id=http%3A%2F%2Farxiv.org%2Fabs%2F1711.07414&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><a href="#cite_ref-9">↑</a></span> <span class="reference-text"><cite class="citation journal">Sen, Shilad; Giesel, Margaret E.; Gold, Rebecca; Hillmann, Benjamin; Lesicko, Matt; Naden, Samuel; Russell, Jesse; Wang, Zixiao (Ken); Hecht, Brent (2015-02-28). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=2675133.2675285">"Turkers, Scholars, Arafat and Peace: Cultural Communities and Algorithmic Gold Standards"</a>. ACM. pp.&#160;826–838. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450329224" title="Special:BookSources/9781450329224">9781450329224</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F2675133.2675285">10.1145/2675133.2675285</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Turkers%2C+Scholars%2C+Arafat+and+Peace%3A+Cultural+Communities+and+Algorithmic+Gold+Standards&amp;rft.au=Giesel%2C+Margaret+E.&amp;rft.au=Gold%2C+Rebecca&amp;rft.au=Hecht%2C+Brent&amp;rft.au=Hillmann%2C+Benjamin&amp;rft.au=Lesicko%2C+Matt&amp;rft.au=Naden%2C+Samuel&amp;rft.au=Russell%2C+Jesse&amp;rft.au=Wang%2C+Zixiao+%28Ken%29&amp;rft.aufirst=Shilad&amp;rft.aulast=Sen&amp;rft.date=2015-02-28&amp;rft.genre=article&amp;rft.isbn=9781450329224&amp;rft.pages=826-838&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2675133.2675285&amp;rft_id=info%3Adoi%2F10.1145%2F2675133.2675285&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><a href="#cite_ref-10">↑</a></span> <span class="reference-text">Bender, E. M., &amp; Friedman, B. (2018). Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science. <i>Transactions of the ACL</i>. Retrieved from https://openreview.net/forum?id=By4oPeX9f</span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><a href="#cite_ref-11">↑</a></span> <span class="reference-text"><cite class="citation journal">Lee, Min Kyung; Kim, Ji Tae; Lizarondo, Leah (2017-05-02). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=3025453.3025884">"A Human-Centered Approach to Algorithmic Services: Considerations for Fair and Motivating Smart Community Service Management that Allocates Donations to Non-Profit Organizations"</a>. ACM. pp.&#160;3365–3376. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450346559" title="Special:BookSources/9781450346559">9781450346559</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F3025453.3025884">10.1145/3025453.3025884</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=A+Human-Centered+Approach+to+Algorithmic+Services%3A+Considerations+for+Fair+and+Motivating+Smart+Community+Service+Management+that+Allocates+Donations+to+Non-Profit+Organizations&amp;rft.au=Kim%2C+Ji+Tae&amp;rft.au=Lizarondo%2C+Leah&amp;rft.aufirst=Min+Kyung&amp;rft.aulast=Lee&amp;rft.date=2017-05-02&amp;rft.genre=article&amp;rft.isbn=9781450346559&amp;rft.pages=3365-3376&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D3025453.3025884&amp;rft_id=info%3Adoi%2F10.1145%2F3025453.3025884&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><a href="#cite_ref-12">↑</a></span> <span class="reference-text"><cite class="citation journal">Tintarev, Nava; Masthoff, Judith (2007-10-19). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1297231.1297259">"Effective explanations of recommendations: user-centered design"</a>. ACM. pp.&#160;153–156. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781595937308" title="Special:BookSources/9781595937308">9781595937308</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F1297231.1297259">10.1145/1297231.1297259</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Effective+explanations+of+recommendations%3A+user-centered+design&amp;rft.au=Masthoff%2C+Judith&amp;rft.aufirst=Nava&amp;rft.aulast=Tintarev&amp;rft.date=2007-10-19&amp;rft.genre=article&amp;rft.isbn=9781595937308&amp;rft.pages=153-156&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1297231.1297259&amp;rft_id=info%3Adoi%2F10.1145%2F1297231.1297259&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><a href="#cite_ref-13">↑</a></span> <span class="reference-text">Ethical OS (2018), A guide to Anticipating the Future Impact of Today's Technology, https://ethicalos.org/</span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><a href="#cite_ref-14">↑</a></span> <span class="reference-text">Gunawardana, A., &amp; Shani, G. (2009). A survey of accuracy evaluation metrics of recommendation tasks. <i>Journal of Machine Learning Research</i>, <i>10</i>(Dec), 2935-2962. Retrieved from <a rel="nofollow" class="external free" href="http://jmlr.csail.mit.edu/papers/volume10/gunawardana09a/gunawardana09a.pdf">http://jmlr.csail.mit.edu/papers/volume10/gunawardana09a/gunawardana09a.pdf</a></span>
</li>
<li id="cite_note-:4-15"><span class="mw-cite-backlink">↑ <a href="#cite_ref-:4_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:4_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web">Ekstrand, Michael; Tian, Mucun; Azpiazu, Ion Madrazo; Ekstrand, Jennifer D.; Anuyah, Oghenemaro; McNeill, David; Pera, Maria Soledad (2017). <a rel="nofollow" class="external text" href="https://scholarworks.boisestate.edu/cs_scripts/4/">"Scripts for All The Cool Kids, How Do They Fit In"</a>. <i>Boise State Data Sets</i>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.18122%2Fb2gm6f">10.18122/b2gm6f</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2018-09-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.au=Anuyah%2C+Oghenemaro&amp;rft.au=Azpiazu%2C+Ion+Madrazo&amp;rft.au=Ekstrand%2C+Jennifer+D.&amp;rft.au=McNeill%2C+David&amp;rft.au=Pera%2C+Maria+Soledad&amp;rft.au=Tian%2C+Mucun&amp;rft.aufirst=Michael&amp;rft.aulast=Ekstrand&amp;rft.btitle=Scripts+for+All+The+Cool+Kids%2C+How+Do+They+Fit+In&amp;rft.date=2017&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fscholarworks.boisestate.edu%2Fcs_scripts%2F4%2F&amp;rft_id=info%3Adoi%2F10.18122%2Fb2gm6f&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-:3-16"><span class="mw-cite-backlink">↑ <a href="#cite_ref-:3_16-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:3_16-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">McNee, Sean M.; Kapoor, Nishikant; Konstan, Joseph A. (2006-11-04). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1180875.1180903">"Don't look stupid: avoiding pitfalls when recommending research papers"</a>. ACM. pp.&#160;171–180. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1595932496" title="Special:BookSources/1595932496">1595932496</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F1180875.1180903">10.1145/1180875.1180903</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Don%27t+look+stupid%3A+avoiding+pitfalls+when+recommending+research+papers&amp;rft.au=Kapoor%2C+Nishikant&amp;rft.au=Konstan%2C+Joseph+A.&amp;rft.aufirst=Sean+M.&amp;rft.aulast=McNee&amp;rft.date=2006-11-04&amp;rft.genre=article&amp;rft.isbn=1595932496&amp;rft.pages=171-180&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1180875.1180903&amp;rft_id=info%3Adoi%2F10.1145%2F1180875.1180903&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><a href="#cite_ref-17">↑</a></span> <span class="reference-text"><cite class="citation journal">Beel, Joeran; Genzmehr, Marcel; Langer, Stefan; Nürnberger, Andreas; Gipp, Bela (2013-10-12). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=2532508.2532511">"A comparative analysis of offline and online evaluations and discussion of research paper recommender system evaluation"</a>. ACM. pp.&#160;7–14. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450324656" title="Special:BookSources/9781450324656">9781450324656</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F2532508.2532511">10.1145/2532508.2532511</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=A+comparative+analysis+of+offline+and+online+evaluations+and+discussion+of+research+paper+recommender+system+evaluation&amp;rft.au=Genzmehr%2C+Marcel&amp;rft.au=Gipp%2C+Bela&amp;rft.au=Langer%2C+Stefan&amp;rft.au=N%C3%BCrnberger%2C+Andreas&amp;rft.aufirst=Joeran&amp;rft.aulast=Beel&amp;rft.date=2013-10-12&amp;rft.genre=article&amp;rft.isbn=9781450324656&amp;rft.pages=7-14&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2532508.2532511&amp;rft_id=info%3Adoi%2F10.1145%2F2532508.2532511&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><a href="#cite_ref-18">↑</a></span> <span class="reference-text"><cite class="citation journal">Amershi, Saleema; Cakmak, Maya; Knox, William Bradley; Kulesza, Todd (2014-12-22). <a rel="nofollow" class="external text" href="https://aaai.org/ojs/index.php/aimagazine/article/view/2513">"Power to the People: The Role of Humans in Interactive Machine Learning"</a>. <i>AI Magazine</i> <b>35</b> (4): 105–120. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2371-9621">2371-9621</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1609%2Faimag.v35i4.2513">10.1609/aimag.v35i4.2513</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Power+to+the+People%3A+The+Role+of+Humans+in+Interactive+Machine+Learning&amp;rft.au=Cakmak%2C+Maya&amp;rft.au=Knox%2C+William+Bradley&amp;rft.au=Kulesza%2C+Todd&amp;rft.aufirst=Saleema&amp;rft.aulast=Amershi&amp;rft.date=2014-12-22&amp;rft.genre=article&amp;rft.issn=2371-9621&amp;rft.issue=4&amp;rft.jtitle=AI+Magazine&amp;rft.pages=105-120&amp;rft.volume=35&amp;rft_id=https%3A%2F%2Faaai.org%2Fojs%2Findex.php%2Faimagazine%2Farticle%2Fview%2F2513&amp;rft_id=info%3Adoi%2F10.1609%2Faimag.v35i4.2513&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-:5-19"><span class="mw-cite-backlink">↑ <a href="#cite_ref-:5_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:5_19-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Shani G., Gunawardana A. (2011) Evaluating Recommendation Systems. In: Ricci F., Rokach L., Shapira B., Kantor P. (eds) Recommender Systems Handbook. Springer, Boston, MA. Retrieved from: <a rel="nofollow" class="external free" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.600.7100&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.600.7100&amp;rep=rep1&amp;type=pdf</a></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><a href="#cite_ref-20">↑</a></span> <span class="reference-text"><cite class="citation journal">McNee, Sean M.; Riedl, John; Konstan, Joseph A. (2006-04-21). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1125451.1125660">"Making recommendations better: an analytic model for human-recommender interaction"</a>. ACM. pp.&#160;1103–1108. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1595932984" title="Special:BookSources/1595932984">1595932984</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F1125451.1125660">10.1145/1125451.1125660</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Making+recommendations+better%3A+an+analytic+model+for+human-recommender+interaction&amp;rft.au=Konstan%2C+Joseph+A.&amp;rft.au=Riedl%2C+John&amp;rft.aufirst=Sean+M.&amp;rft.aulast=McNee&amp;rft.date=2006-04-21&amp;rft.genre=article&amp;rft.isbn=1595932984&amp;rft.pages=1103-1108&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1125451.1125660&amp;rft_id=info%3Adoi%2F10.1145%2F1125451.1125660&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><a href="#cite_ref-21">↑</a></span> <span class="reference-text"><cite class="citation journal">Knijnenburg, Bart P.; Willemsen, Martijn C.; Gantner, Zeno; Soncu, Hakan; Newell, Chris (2012-03-10). <a rel="nofollow" class="external text" href="https://link.springer.com/article/10.1007%2Fs11257-011-9118-4">"Explaining the user experience of recommender systems"</a>. <i>User Modeling and User-Adapted Interaction</i> <b>22</b> (4-5): 441–504. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0924-1868">0924-1868</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2Fs11257-011-9118-4">10.1007/s11257-011-9118-4</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Explaining+the+user+experience+of+recommender+systems&amp;rft.au=Gantner%2C+Zeno&amp;rft.au=Newell%2C+Chris&amp;rft.au=Soncu%2C+Hakan&amp;rft.au=Willemsen%2C+Martijn+C.&amp;rft.aufirst=Bart+P.&amp;rft.aulast=Knijnenburg&amp;rft.date=2012-03-10&amp;rft.genre=article&amp;rft.issn=0924-1868&amp;rft.issue=4-5&amp;rft.jtitle=User+Modeling+and+User-Adapted+Interaction&amp;rft.pages=441-504&amp;rft.volume=22&amp;rft_id=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%252Fs11257-011-9118-4&amp;rft_id=info%3Adoi%2F10.1007%2Fs11257-011-9118-4&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><a href="#cite_ref-22">↑</a></span> <span class="reference-text"><cite class="citation journal">Ekstrand, Michael D.; Harper, F. Maxwell; Willemsen, Martijn C.; Konstan, Joseph A. (2014-10-06). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=2645710.2645737">"User perception of differences in recommender algorithms"</a>. ACM. pp.&#160;161–168. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450326681" title="Special:BookSources/9781450326681">9781450326681</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F2645710.2645737">10.1145/2645710.2645737</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=User+perception+of+differences+in+recommender+algorithms&amp;rft.au=Harper%2C+F.+Maxwell&amp;rft.au=Konstan%2C+Joseph+A.&amp;rft.au=Willemsen%2C+Martijn+C.&amp;rft.aufirst=Michael+D.&amp;rft.aulast=Ekstrand&amp;rft.date=2014-10-06&amp;rft.genre=article&amp;rft.isbn=9781450326681&amp;rft.pages=161-168&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2645710.2645737&amp;rft_id=info%3Adoi%2F10.1145%2F2645710.2645737&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><a href="#cite_ref-23">↑</a></span> <span class="reference-text"><cite class="citation journal">Loepp, Benedikt; Hussein, Tim; Ziegler, Jüergen; Loepp, Benedikt; Hussein, Tim; Ziegler, Jüergen (2014-04-26). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=2556288.2557069,%20http://dl.acm.org/citation.cfm?id=2611247.2557069">"Choice-based preference elicitation for collaborative filtering recommender systems, Choice-based preference elicitation for collaborative filtering recommender systems"</a>. ACM, ACM. pp.&#160;3085, 3085–3094, 3094. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450324731" title="Special:BookSources/9781450324731">9781450324731</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F2556288.2557069%2C+10.1145%2F2556288.2557069">10.1145/2556288.2557069, 10.1145/2556288.2557069</a> <span style="font-size:100%" class="error citation-comment">Check <code style="color:inherit; border:inherit; padding:inherit;">&#124;doi=</code> value (<a href="/wiki/Help:CS1_errors#bad_doi" title="Help:CS1 errors">help</a>)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Choice-based+preference+elicitation+for+collaborative+filtering+recommender+systems%2C+Choice-based+preference+elicitation+for+collaborative+filtering+recommender+systems&amp;rft.au=Hussein%2C+Tim&amp;rft.au=Hussein%2C+Tim&amp;rft.au=Loepp%2C+Benedikt&amp;rft.au=Ziegler%2C+J%C3%BCergen&amp;rft.au=Ziegler%2C+J%C3%BCergen&amp;rft.aufirst=Benedikt&amp;rft.aulast=Loepp&amp;rft.date=2014-04-26&amp;rft.genre=article&amp;rft.isbn=9781450324731&amp;rft.pages=3085%2C+3085-3094%2C+3094&amp;rft.pub=ACM%2C+ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2556288.2557069%2C%2520http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2611247.2557069&amp;rft_id=info%3Adoi%2F10.1145%2F2556288.2557069%2C+10.1145%2F2556288.2557069&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><a href="#cite_ref-24">↑</a></span> <span class="reference-text"><cite class="citation journal">Cremonesi, Paolo; Elahi, Mehdi; Garzotto, Franca (2016-09-24). <a rel="nofollow" class="external text" href="https://link.springer.com/article/10.1007%2Fs11042-016-3946-5">"User interface patterns in recommendation-empowered content intensive multimedia applications"</a>. <i>Multimedia Tools and Applications</i> <b>76</b> (4): 5275–5309. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1380-7501">1380-7501</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2Fs11042-016-3946-5">10.1007/s11042-016-3946-5</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=User+interface+patterns+in+recommendation-empowered+content+intensive+multimedia+applications&amp;rft.au=Elahi%2C+Mehdi&amp;rft.au=Garzotto%2C+Franca&amp;rft.aufirst=Paolo&amp;rft.aulast=Cremonesi&amp;rft.date=2016-09-24&amp;rft.genre=article&amp;rft.issn=1380-7501&amp;rft.issue=4&amp;rft.jtitle=Multimedia+Tools+and+Applications&amp;rft.pages=5275-5309&amp;rft.volume=76&amp;rft_id=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%252Fs11042-016-3946-5&amp;rft_id=info%3Adoi%2F10.1007%2Fs11042-016-3946-5&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><a href="#cite_ref-25">↑</a></span> <span class="reference-text"><cite class="citation journal">Cosley, Dan; Lam, Shyong K.; Albert, Istvan; Konstan, Joseph A.; Riedl, John (2003-04-05). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=642611.642713">"Is seeing believing?: how recommender system interfaces affect users' opinions"</a>. ACM. pp.&#160;585–592. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1581136307" title="Special:BookSources/1581136307">1581136307</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F642611.642713">10.1145/642611.642713</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Is+seeing+believing%3F%3A+how+recommender+system+interfaces+affect+users%27+opinions&amp;rft.au=Albert%2C+Istvan&amp;rft.au=Konstan%2C+Joseph+A.&amp;rft.au=Lam%2C+Shyong+K.&amp;rft.au=Riedl%2C+John&amp;rft.aufirst=Dan&amp;rft.aulast=Cosley&amp;rft.date=2003-04-05&amp;rft.genre=article&amp;rft.isbn=1581136307&amp;rft.pages=585-592&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D642611.642713&amp;rft_id=info%3Adoi%2F10.1145%2F642611.642713&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><a href="#cite_ref-26">↑</a></span> <span class="reference-text"><cite class="citation web">Sharma, Abhinav (2016-05-17). <a rel="nofollow" class="external text" href="https://medium.com/the-graph/designing-uis-for-recommender-systems-f7ffa2ca234f">"Designing Interfaces for Recommender Systems"</a>. <i>The Graph</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2018-09-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.aufirst=Abhinav&amp;rft.aulast=Sharma&amp;rft.btitle=Designing+Interfaces+for+Recommender+Systems&amp;rft.date=2016-05-17&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fmedium.com%2Fthe-graph%2Fdesigning-uis-for-recommender-systems-f7ffa2ca234f&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><a href="#cite_ref-27">↑</a></span> <span class="reference-text"><cite class="citation journal">Cremonesi, Paolo; Elahi, Mehdi; Garzotto, Franca (2016-09-24). <a rel="nofollow" class="external text" href="https://link.springer.com/article/10.1007%2Fs11042-016-3946-5">"User interface patterns in recommendation-empowered content intensive multimedia applications"</a>. <i>Multimedia Tools and Applications</i> <b>76</b> (4): 5275–5309. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1380-7501">1380-7501</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2Fs11042-016-3946-5">10.1007/s11042-016-3946-5</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=User+interface+patterns+in+recommendation-empowered+content+intensive+multimedia+applications&amp;rft.au=Elahi%2C+Mehdi&amp;rft.au=Garzotto%2C+Franca&amp;rft.aufirst=Paolo&amp;rft.aulast=Cremonesi&amp;rft.date=2016-09-24&amp;rft.genre=article&amp;rft.issn=1380-7501&amp;rft.issue=4&amp;rft.jtitle=Multimedia+Tools+and+Applications&amp;rft.pages=5275-5309&amp;rft.volume=76&amp;rft_id=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%252Fs11042-016-3946-5&amp;rft_id=info%3Adoi%2F10.1007%2Fs11042-016-3946-5&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><a href="#cite_ref-28">↑</a></span> <span class="reference-text"><cite class="citation journal">Diakopoulos, Nicholas (2016-01-25). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=2886013.2844110">"Accountability in algorithmic decision making"</a>. <i>Communications of the ACM</i> <b>59</b> (2): 56–62. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0001-0782">0001-0782</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F2844110">10.1145/2844110</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Accountability+in+algorithmic+decision+making&amp;rft.aufirst=Nicholas&amp;rft.aulast=Diakopoulos&amp;rft.date=2016-01-25&amp;rft.genre=article&amp;rft.issn=0001-0782&amp;rft.issue=2&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.pages=56-62&amp;rft.volume=59&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2886013.2844110&amp;rft_id=info%3Adoi%2F10.1145%2F2844110&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><a href="#cite_ref-29">↑</a></span> <span class="reference-text">World Economic Forum. (2016). How to Prevent Discriminatory Outcomes in Machine Learning. <i>Global Future Council on Human Rights</i>, (March). Retrieved from http://www3.weforum.org/docs/WEF_40065_White_Paper_How_to_Prevent_Discriminatory_Outcomes_in_Machine_Learning.pdf</span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><a href="#cite_ref-30">↑</a></span> <span class="reference-text"><cite class="citation journal">Kearns, Michael; Neel, Seth; Roth, Aaron; Wu, Zhiwei Steven (2017-11-14). <a rel="nofollow" class="external text" href="http://arxiv.org/abs/1711.05144">"Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness"</a>. <i>arXiv:1711.05144 [cs]</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Preventing+Fairness+Gerrymandering%3A+Auditing+and+Learning+for+Subgroup+Fairness&amp;rft.au=Neel%2C+Seth&amp;rft.au=Roth%2C+Aaron&amp;rft.au=Wu%2C+Zhiwei+Steven&amp;rft.aufirst=Michael&amp;rft.aulast=Kearns&amp;rft.date=2017-11-14&amp;rft.genre=article&amp;rft.jtitle=arXiv%3A1711.05144+%5Bcs%5D&amp;rft_id=http%3A%2F%2Farxiv.org%2Fabs%2F1711.05144&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><a href="#cite_ref-31">↑</a></span> <span class="reference-text"><cite class="citation journal">Palinkas, Lawrence A.; Horwitz, Sarah M.; Green, Carla A.; Wisdom, Jennifer P.; Duan, Naihua; Hoagwood, Kimberly (2013-11-06). <a rel="nofollow" class="external text" href="https://link.springer.com/article/10.1007%2Fs10488-013-0528-y">"Purposeful Sampling for Qualitative Data Collection and Analysis in Mixed Method Implementation Research"</a>. <i>Administration and Policy in Mental Health and Mental Health Services Research</i> <b>42</b> (5): 533–544. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0894-587X">0894-587X</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Central" class="extiw" title="w:PubMed Central">PMC</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC4012002">4012002</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" class="extiw" title="w:PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/24193818">24193818</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2Fs10488-013-0528-y">10.1007/s10488-013-0528-y</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Purposeful+Sampling+for+Qualitative+Data+Collection+and+Analysis+in+Mixed+Method+Implementation+Research&amp;rft.au=Duan%2C+Naihua&amp;rft.au=Green%2C+Carla+A.&amp;rft.au=Hoagwood%2C+Kimberly&amp;rft.au=Horwitz%2C+Sarah+M.&amp;rft.au=Wisdom%2C+Jennifer+P.&amp;rft.aufirst=Lawrence+A.&amp;rft.aulast=Palinkas&amp;rft.date=2013-11-06&amp;rft.genre=article&amp;rft.issn=0894-587X&amp;rft.issue=5&amp;rft.jtitle=Administration+and+Policy+in+Mental+Health+and+Mental+Health+Services+Research&amp;rft.pages=533-544&amp;rft.volume=42&amp;rft_id=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%252Fs10488-013-0528-y&amp;rft_id=info%3Adoi%2F10.1007%2Fs10488-013-0528-y&amp;rft_id=info%3Apmc%2F4012002&amp;rft_id=info%3Apmid%2F24193818&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><a href="#cite_ref-32">↑</a></span> <span class="reference-text"><cite class="citation journal">McNee, Sean M.; Riedl, John; Konstan, Joseph A. (2006-04-21). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1125451.1125660">"Making recommendations better: an analytic model for human-recommender interaction"</a>. ACM. pp.&#160;1103–1108. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1595932984" title="Special:BookSources/1595932984">1595932984</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F1125451.1125660">10.1145/1125451.1125660</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Making+recommendations+better%3A+an+analytic+model+for+human-recommender+interaction&amp;rft.au=Konstan%2C+Joseph+A.&amp;rft.au=Riedl%2C+John&amp;rft.aufirst=Sean+M.&amp;rft.aulast=McNee&amp;rft.date=2006-04-21&amp;rft.genre=article&amp;rft.isbn=1595932984&amp;rft.pages=1103-1108&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1125451.1125660&amp;rft_id=info%3Adoi%2F10.1145%2F1125451.1125660&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><a href="#cite_ref-33">↑</a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.ibm.com/support/knowledgecenter/en/SSPG9M/DigitalAnalytics/BestPractices/TopLineMetrics/report_topline_analysis.html">"IBM Knowledge Center"</a>. <i>www.ibm.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2018-09-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.btitle=IBM+Knowledge+Center&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fwww.ibm.com%2Fsupport%2Fknowledgecenter%2Fen%2FSSPG9M%2FDigitalAnalytics%2FBestPractices%2FTopLineMetrics%2Freport_topline_analysis.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><a href="#cite_ref-34">↑</a></span> <span class="reference-text"><cite class="citation journal">Halfaker, Aaron; Geiger, R. Stuart; Morgan, Jonathan T.; Riedl, John (2012-12-28). <a rel="nofollow" class="external text" href="https://doi.org/10.1177/0002764212469365">"The Rise and Decline of an Open Collaboration System"</a>. <i>American Behavioral Scientist</i> <b>57</b> (5): 664–688. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0002-7642">0002-7642</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1177%2F0002764212469365">10.1177/0002764212469365</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=The+Rise+and+Decline+of+an+Open+Collaboration+System&amp;rft.au=Geiger%2C+R.+Stuart&amp;rft.au=Morgan%2C+Jonathan+T.&amp;rft.au=Riedl%2C+John&amp;rft.aufirst=Aaron&amp;rft.aulast=Halfaker&amp;rft.date=2012-12-28&amp;rft.genre=article&amp;rft.issn=0002-7642&amp;rft.issue=5&amp;rft.jtitle=American+Behavioral+Scientist&amp;rft.pages=664-688&amp;rft.volume=57&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1177%2F0002764212469365&amp;rft_id=info%3Adoi%2F10.1177%2F0002764212469365&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><a href="#cite_ref-35">↑</a></span> <span class="reference-text"><cite class="citation journal">Harford, Tim (December 2014). <a rel="nofollow" class="external text" href="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2014.00778.x">"Big data: A big mistake?"</a>. <i>Significance</i> <b>11</b> (5): 14–19. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1740-9705">1740-9705</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1111%2Fj.1740-9713.2014.00778.x">10.1111/j.1740-9713.2014.00778.x</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Big+data%3A+A+big+mistake%3F&amp;rft.aufirst=Tim&amp;rft.aulast=Harford&amp;rft.date=December+2014&amp;rft.genre=article&amp;rft.issn=1740-9705&amp;rft.issue=5&amp;rft.jtitle=Significance&amp;rft.pages=14-19&amp;rft.volume=11&amp;rft_id=https%3A%2F%2Frss.onlinelibrary.wiley.com%2Fdoi%2Fabs%2F10.1111%2Fj.1740-9713.2014.00778.x&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1740-9713.2014.00778.x&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><a href="#cite_ref-36">↑</a></span> <span class="reference-text"><cite class="citation journal">Chaney, Allison J. B.; Stewart, Brandon M.; Engelhardt, Barbara E. (2017-10-30). <a rel="nofollow" class="external text" href="http://arxiv.org/abs/1710.11214">"How Algorithmic Confounding in Recommendation Systems Increases Homogeneity and Decreases Utility"</a>. <i>arXiv:1710.11214 [cs, stat]</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=How+Algorithmic+Confounding+in+Recommendation+Systems+Increases+Homogeneity+and+Decreases+Utility&amp;rft.au=Engelhardt%2C+Barbara+E.&amp;rft.au=Stewart%2C+Brandon+M.&amp;rft.aufirst=Allison+J.+B.&amp;rft.aulast=Chaney&amp;rft.date=2017-10-30&amp;rft.genre=article&amp;rft.jtitle=arXiv%3A1710.11214+%5Bcs%2C+stat%5D&amp;rft_id=http%3A%2F%2Farxiv.org%2Fabs%2F1710.11214&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><a href="#cite_ref-37">↑</a></span> <span class="reference-text">Sandvig, C., Hamilton, K., Karahalios, K., &amp; Langbort, C. (2014). <i>Auditing Algorithms : Research Methods for Detecting Discrimination on Internet Platforms</i>. <i>Data and Discrimination: Converting Critical Concerns into Productive Inquiry, a preconference at the 64th Annual Meeting of the International Communication Association</i>. Seattle, Washington, USA. Retrieved from <a rel="nofollow" class="external free" href="http://www-personal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf">http://www-personal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf</a></span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><a href="#cite_ref-38">↑</a></span> <span class="reference-text"><cite class="citation journal">Diakopoulos, Nicholas (2014). <a rel="nofollow" class="external text" href="https://doi.org/10.7916/D8ZK5TW2">"Algorithmic Accountability Reporting: On the Investigation of Black Boxes"</a>. <i>Academic Commons</i>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.7916%2FD8ZK5TW2">10.7916/D8ZK5TW2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=Algorithmic+Accountability+Reporting%3A+On+the+Investigation+of+Black+Boxes&amp;rft.aufirst=Nicholas&amp;rft.aulast=Diakopoulos&amp;rft.date=2014&amp;rft.genre=article&amp;rft.jtitle=Academic+Commons&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.7916%2FD8ZK5TW2&amp;rft_id=info%3Adoi%2F10.7916%2FD8ZK5TW2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><a href="#cite_ref-39">↑</a></span> <span class="reference-text">Bamman, D. (2016). Interpretability in human-centered data science. In <i>CSCW Workshop on Human-Centered Data Science</i>.</span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><a href="#cite_ref-40">↑</a></span> <span class="reference-text"><cite class="citation journal">Shahriari, Kyarash; Shahriari, Mana (July 2017). <a rel="nofollow" class="external text" href="https://ieeexplore.ieee.org/document/8058187/?reload=true">"IEEE standard review — Ethically aligned design: A vision for prioritizing human wellbeing with artificial intelligence and autonomous systems"</a>. <i>2017 IEEE Canada International Humanitarian Technology Conference (IHTC)</i> (IEEE). <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781509062645" title="Special:BookSources/9781509062645">9781509062645</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1109%2Fihtc.2017.8058187">10.1109/ihtc.2017.8058187</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems&amp;rft.atitle=IEEE+standard+review+%E2%80%94+Ethically+aligned+design%3A+A+vision+for+prioritizing+human+wellbeing+with+artificial+intelligence+and+autonomous+systems&amp;rft.au=Shahriari%2C+Mana&amp;rft.aufirst=Kyarash&amp;rft.aulast=Shahriari&amp;rft.date=July+2017&amp;rft.genre=article&amp;rft.isbn=9781509062645&amp;rft.jtitle=2017+IEEE+Canada+International+Humanitarian+Technology+Conference+%28IHTC%29&amp;rft.pub=IEEE&amp;rft_id=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F8058187%2F%3Freload%3Dtrue&amp;rft_id=info%3Adoi%2F10.1109%2Fihtc.2017.8058187&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
</ol></div>
<!-- 
NewPP limit report
Parsed by mw1354
Cached time: 20220419031041
Cache expiry: 1814400
Reduced expiry: false
Complications: []
CPU time usage: 0.395 seconds
Real time usage: 0.432 seconds
Preprocessor visited node count: 2208/1000000
Post‐expand include size: 75006/2097152 bytes
Template argument size: 1720/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 0/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 53341/5000000 bytes
Lua time usage: 0.099/10.000 seconds
Lua memory usage: 3129656/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  327.159      1 -total
 29.91%   97.847      1 Template:Tracked
 22.19%   72.598     26 Template:Cite_journal
 15.73%   51.465      5 Template:Cite_web
 14.61%   47.786      1 Template:Dir
 14.16%   46.329      1 Template:Research_project
 12.69%   41.523      1 Template:Pagelang
 11.63%   38.060      1 Template:Research_project/Infobox
 10.12%   33.117      1 Template:Basic_project/Infobox
  4.52%   14.773      1 Template:Basic_project/Block_item
-->

<!-- Saved in parser cache with key metawiki:pcache:idhash:10676418-0!canonical and timestamp 20220419031041 and revision id 22298932. Serialized with JSON.
 -->
</div><noscript><img src="//meta.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://meta.wikimedia.org/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;oldid=22298932">https://meta.wikimedia.org/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;oldid=22298932</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>: <ul><li><a href="/wiki/Category:2018_projects" title="Category:2018 projects">2018 projects</a></li><li><a href="/wiki/Category:Completed_research_projects" title="Category:Completed research projects">Completed research projects</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden category: <ul><li><a href="/wiki/Category:CS1_errors:_DOI" title="Category:CS1 errors: DOI">CS1 errors: DOI</a></li></ul></div></div>
	</div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label" role="navigation" 
	 >
	<label id="p-personal-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Personal tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="pt-uls" class="mw-list-item active"><a class="uls-trigger" href="#"><span>English</span></a></li><li id="pt-anonuserpage" class="mw-list-item"><span title="The user page for the IP address you are editing as">Not logged in</span></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li><li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-createaccount" class="mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Research%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems" title="You are encouraged to create an account and log in; however, it is not mandatory"><span>Create account</span></a></li><li id="pt-login" class="mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Research%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o"><span>Log in</span></a></li></ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" aria-labelledby="p-namespaces-label" role="navigation" 
	 >
	<label id="p-namespaces-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Namespaces</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-nstab-research" class="selected mw-list-item"><a href="/wiki/Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems"><span>Research</span></a></li><li id="ca-talk" class="new mw-list-item"><a href="/w/index.php?title=Research_talk:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit&amp;redlink=1" rel="discussion" title="Discussion about the content page (page does not exist) [t]" accesskey="t"><span>Discussion</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-variants-label" role="navigation" 
	 >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox" aria-labelledby="p-variants-label" />
	<label id="p-variants-label" aria-label="Change language variant" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">English</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" aria-labelledby="p-views-label" role="navigation" 
	 >
	<label id="p-views-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Views</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-view" class="selected mw-list-item"><a href="/wiki/Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems"><span>Read</span></a></li><li id="ca-edit" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-cactions-label" role="navigation"  title="More options"
	 >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox" aria-labelledby="p-cactions-label" />
	<label id="p-cactions-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">More</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<div>
			<h3 >
				<label for="searchInput">Search</label>
			</h3>
		<form action="/w/index.php" id="searchform"
			class="vector-search-box-form">
			<div id="simpleSearch"
				class="vector-search-box-inner"
				 data-search-loc="header-navigation">
				<input class="vector-search-box-input"
					 type="search" name="search" placeholder="Search Meta" aria-label="Search Meta" autocapitalize="sentences" title="Search Meta [f]" accesskey="f" id="searchInput"
				/>
				<input type="hidden" name="title" value="Special:Search"/>
				<input id="mw-searchButton"
					 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search the pages for this text" value="Search" />
				<input id="searchButton"
					 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go" />
			</div>
		</form>
	</div>
</div>

		</div>
	</div>
	

<div id="mw-panel">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Special:MyLanguage/Main_Page"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal" aria-labelledby="p-navigation-label" role="navigation" 
	 >
	<label id="p-navigation-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Navigation</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Main_Page" icon="home" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Wikimedia_News" title="Find background information on current events"><span>Wikimedia News</span></a></li><li id="n-translations" class="mw-list-item"><a href="/wiki/Special:LanguageStats" title="Help with translations"><span>Translations</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" icon="recentChanges" title="A list of recent changes in the wiki [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" icon="die" title="Load a random page [x]" accesskey="x"><span>Random page</span></a></li><li id="n-help" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Help:Help" icon="help" title="The place to find out"><span>Help</span></a></li><li id="n-Babel-text" class="mw-list-item"><a href="/wiki/Meta:Babel"><span>Babel</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-community" class="mw-portlet mw-portlet-community vector-menu vector-menu-portal portal" aria-labelledby="p-community-label" role="navigation" 
	 >
	<label id="p-community-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Community</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-WikimediaResourceCenter-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Wikimedia_Resource_Center"><span>Wikimedia Resource Center</span></a></li><li id="n-WikimediaForum-text" class="mw-list-item"><a href="/wiki/Wikimedia_Forum"><span>Wikimedia Forum</span></a></li><li id="n-mailinglists-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Mailing_lists/Overview"><span>Mailing lists</span></a></li><li id="n-Requests-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Meta:Requests_and_proposals"><span>Requests</span></a></li><li id="n-Babylon-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Meta:Babylon"><span>Babylon</span></a></li><li id="n-Reports-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Wikimedia_Affiliates_Data_Portal"><span>Reports</span></a></li><li id="n-Research-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Research:Index"><span>Research</span></a></li><li id="n-planet" class="mw-list-item"><a href="//en.planet.wikimedia.org/" title="Weblog aggregator of Wikimedia-related posts"><span>Planet Wikimedia</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-beyond_the_web" class="mw-portlet mw-portlet-beyond_the_web vector-menu vector-menu-portal portal" aria-labelledby="p-beyond_the_web-label" role="navigation" 
	 >
	<label id="p-beyond_the_web-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Beyond the Web</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-Meetup-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Meetup"><span>Meet Wikimedians</span></a></li><li id="n-Events-text" class="mw-list-item"><a href="/wiki/Events"><span>Events</span></a></li><li id="n-Affiliates-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Wikimedia_movement_affiliates"><span>Movement affiliates</span></a></li><li id="n-sitesupport" class="mw-list-item"><a href="//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_meta.wikimedia.org&amp;uselang=en" title="Support us"><span>Donate</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" aria-labelledby="p-tb-label" role="navigation" 
	 >
	<label id="p-tb-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems" title="A list of all wiki pages that link here [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;oldid=22298932" title="Permanent link to this revision of the page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;action=info" title="More information about this page"><span>Page information</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-coll-print_export" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" aria-labelledby="p-coll-print_export-label" role="navigation" 
	 >
	<label id="p-coll-print_export-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Print/export</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="coll-create_a_book" class="mw-list-item"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Research%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems"><span>Create a book</span></a></li><li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Research%3AEthical_and_human-centered_AI%2FBias_in_recommender_systems&amp;action=show-download-screen"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-lang" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" aria-labelledby="p-lang-label" role="navigation" 
	 >
	<label id="p-lang-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">In other languages</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-add wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:NewItem?site=metawiki&amp;page=Research%3AEthical+and+human-centered+AI%2FBias+in+recommender+systems" title="Add interlanguage links" class="wbc-editpage">Add links</a></span></div>
	</div>
</nav>

</div>

</div>

<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 4 November 2021, at 22:48.</li>
	<li id="footer-info-copyright">Text is available under the <a href="https://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike License</a>;
additional terms may apply.
See <a href="https://foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> for details.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Meta:About" title="Meta:About">About Meta</a></li>
	<li id="footer-places-disclaimer"><a href="https://foundation.wikimedia.org/wiki/Wikimedia:General_disclaimer" class="extiw" title="wmf:Wikimedia:General disclaimer">Disclaimers</a></li>
	<li id="footer-places-mobileview"><a href="//meta.m.wikimedia.org/w/index.php?title=Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/meta.wikimedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"/></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.395","walltime":"0.432","ppvisitednodes":{"value":2208,"limit":1000000},"postexpandincludesize":{"value":75006,"limit":2097152},"templateargumentsize":{"value":1720,"limit":2097152},"expansiondepth":{"value":12,"limit":100},"expensivefunctioncount":{"value":0,"limit":500},"unstrip-depth":{"value":0,"limit":20},"unstrip-size":{"value":53341,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  327.159      1 -total"," 29.91%   97.847      1 Template:Tracked"," 22.19%   72.598     26 Template:Cite_journal"," 15.73%   51.465      5 Template:Cite_web"," 14.61%   47.786      1 Template:Dir"," 14.16%   46.329      1 Template:Research_project"," 12.69%   41.523      1 Template:Pagelang"," 11.63%   38.060      1 Template:Research_project/Infobox"," 10.12%   33.117      1 Template:Basic_project/Infobox","  4.52%   14.773      1 Template:Basic_project/Block_item"]},"scribunto":{"limitreport-timeusage":{"value":"0.099","limit":"10.000"},"limitreport-memusage":{"value":3129656,"limit":52428800}},"cachereport":{"origin":"mw1354","timestamp":"20220419031041","ttl":1814400,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":193,"wgHostname":"mw1455"});});</script>
</body>
</html>