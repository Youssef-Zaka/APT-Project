<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 126 results for author: <span class="mathjax">Ré, C</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Ré, C">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=R%C3%A9%2C+C&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Ré, C">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08173">arXiv:2204.08173</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08173">pdf</a>, <a href="https://arxiv.org/format/2204.08173">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Leszczynski%2C+M">Megan Leszczynski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M+F">Mayee F. Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08173v1-abstract-short" style="display: inline;">
        Entity retrieval--retrieving information about entity mentions in a query--is a key step in open-domain tasks, such as question answering or fact checking. However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities. Incorporating knowledge graph types during training could help overcome popularity biases, but there a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08173v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08173v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08173v1-abstract-full" style="display: none;">
        Entity retrieval--retrieving information about entity mentions in a query--is a key step in open-domain tasks, such as question answering or fact checking. However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities. Incorporating knowledge graph types during training could help overcome popularity biases, but there are several challenges: (1) existing type-based retrieval methods require mention boundaries as input, but open-domain tasks run on unstructured text, (2) type-based methods should not compromise overall performance, and (3) type-based methods should be robust to noisy and missing types. In this work, we introduce TABi, a method to jointly train bi-encoders on knowledge graph types and unstructured text for entity retrieval for open-domain tasks. TABi leverages a type-enforced contrastive loss to encourage entities and queries of similar types to be close in the embedding space. TABi improves retrieval of rare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining strong overall retrieval performance on open-domain tasks in the KILT benchmark compared to state-of-the-art retrievers. TABi is also robust to incomplete type systems, improving rare entity retrieval over baselines with only 5% type coverage of the training dataset. We make our code publicly available at https://github.com/HazyResearch/tabi.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08173v1-abstract-full').style.display = 'none'; document.getElementById('2204.08173v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Findings of ACL 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07596">arXiv:2204.07596</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07596">pdf</a>, <a href="https://arxiv.org/format/2204.07596">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Perfectly Balanced: Improving Transfer and Robustness of Supervised Contrastive Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M+F">Mayee F. Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Narayan%2C+A">Avanika Narayan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Michael Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Z">Zhao Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fatahalian%2C+K">Kayvon Fatahalian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07596v1-abstract-short" style="display: inline;">
        An ideal learned representation should display transferability and robustness. Supervised contrastive learning (SupCon) is a promising method for training accurate models, but produces representations that do not capture these properties due to class collapse -- when all points in a class map to the same representation. Recent work suggests that &#34;spreading out&#34; these representations improves them,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07596v1-abstract-full').style.display = 'inline'; document.getElementById('2204.07596v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07596v1-abstract-full" style="display: none;">
        An ideal learned representation should display transferability and robustness. Supervised contrastive learning (SupCon) is a promising method for training accurate models, but produces representations that do not capture these properties due to class collapse -- when all points in a class map to the same representation. Recent work suggests that &#34;spreading out&#34; these representations improves them, but the precise mechanism is poorly understood. We argue that creating spread alone is insufficient for better representations, since spread is invariant to permutations within classes. Instead, both the correct degree of spread and a mechanism for breaking this invariance are necessary. We first prove that adding a weighted class-conditional InfoNCE loss to SupCon controls the degree of spread. Next, we study three mechanisms to break permutation invariance: using a constrained encoder, adding a class-conditional autoencoder, and using data augmentation. We show that the latter two encourage clustering of latent subclasses under more realistic conditions than the former. Using these insights, we show that adding a properly-weighted class-conditional InfoNCE loss and a class-conditional autoencoder to SupCon achieves 11.1 points of lift on coarse-to-fine transfer across 5 standard datasets and 4.7 points on worst-group robustness on 3 datasets, setting state-of-the-art on CelebA by 11.5 points.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07596v1-abstract-full').style.display = 'none'; document.getElementById('2204.07596v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.00595">arXiv:2204.00595</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.00595">pdf</a>, <a href="https://arxiv.org/format/2204.00595">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Monarch: Expressive Structured Matrices for Efficient and Accurate Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Beidi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N">Nimit Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Desai%2C+A">Arjun Desai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Poli%2C+M">Michael Poli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grogan%2C+J">Jessica Grogan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+A">Alexander Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rao%2C+A">Aniruddh Rao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.00595v1-abstract-short" style="display: inline;">
        Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dens&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00595v1-abstract-full').style.display = 'inline'; document.getElementById('2204.00595v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.00595v1-abstract-full" style="display: none;">
        Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called &#34;reverse sparsification,&#34; Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00595v1-abstract-full').style.display = 'none'; document.getElementById('2204.00595v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14960">arXiv:2203.14960</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14960">pdf</a>, <a href="https://arxiv.org/format/2203.14960">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Domino: Discovering Systematic Errors with Cross-Modal Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Eyuboglu%2C+S">Sabri Eyuboglu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Varma%2C+M">Maya Varma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saab%2C+K">Khaled Saab</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Delbrouck%2C+J">Jean-Benoit Delbrouck</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee-Messer%2C+C">Christopher Lee-Messer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dunnmon%2C+J">Jared Dunnmon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+J">James Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14960v2-abstract-short" style="display: inline;">
        Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDM&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14960v2-abstract-full').style.display = 'inline'; document.getElementById('2203.14960v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14960v2-abstract-full" style="display: none;">
        Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data). Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework - a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14960v2-abstract-full').style.display = 'none'; document.getElementById('2203.14960v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2022 (Oral)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13270">arXiv:2203.13270</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13270">pdf</a>, <a href="https://arxiv.org/format/2203.13270">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Shoring Up the Foundations: Fusing Model Embeddings and Weak Supervision
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M+F">Mayee F. Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Adila%2C+D">Dyah Adila</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Michael Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sala%2C+F">Frederic Sala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fatahalian%2C+K">Kayvon Fatahalian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13270v1-abstract-short" style="display: inline;">
        Foundation models offer an exciting new paradigm for constructing models with out-of-the-box embeddings and a few labeled examples. However, it is not clear how to best apply foundation models without labeled data. A potential approach is to fuse foundation models with weak supervision frameworks, which use weak label sources -- pre-trained models, heuristics, crowd-workers -- to construct pseudol&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13270v1-abstract-full').style.display = 'inline'; document.getElementById('2203.13270v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13270v1-abstract-full" style="display: none;">
        Foundation models offer an exciting new paradigm for constructing models with out-of-the-box embeddings and a few labeled examples. However, it is not clear how to best apply foundation models without labeled data. A potential approach is to fuse foundation models with weak supervision frameworks, which use weak label sources -- pre-trained models, heuristics, crowd-workers -- to construct pseudolabels. The challenge is building a combination that best exploits the signal available in both foundation models and weak sources. We propose Liger, a combination that uses foundation model embeddings to improve two crucial elements of existing weak supervision techniques. First, we produce finer estimates of weak source quality by partitioning the embedding space and learning per-part source accuracies. Second, we improve source coverage by extending source votes in embedding space. Despite the black-box nature of foundation models, we prove results characterizing how our approach improves performance and show that lift scales with the smoothness of label distributions in embedding space. On six benchmark NLP and video tasks, Liger outperforms vanilla weak supervision by 14.1 points, weakly-supervised kNN and adapters by 11.8 points, and kNN and adapters supervised by traditional hand labels by 7.2 points.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13270v1-abstract-full').style.display = 'none'; document.getElementById('2203.13270v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11027">arXiv:2203.11027</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11027">pdf</a>, <a href="https://arxiv.org/format/2203.11027">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reasoning over Public and Private Data in Retrieval-Based Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+S">Simran Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lewis%2C+P">Patrick Lewis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+A">Angela Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kahn%2C+J">Jacob Kahn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11027v1-abstract-short" style="display: inline;">
        Users and organizations are generating ever-increasing amounts of private data from a wide range of sources. Incorporating private data is important to personalize open-domain applications such as question-answering, fact-checking, and personal assistants. State-of-the-art systems for these tasks explicitly retrieve relevant information to a user question from a background corpus before producing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11027v1-abstract-full').style.display = 'inline'; document.getElementById('2203.11027v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11027v1-abstract-full" style="display: none;">
        Users and organizations are generating ever-increasing amounts of private data from a wide range of sources. Incorporating private data is important to personalize open-domain applications such as question-answering, fact-checking, and personal assistants. State-of-the-art systems for these tasks explicitly retrieve relevant information to a user question from a background corpus before producing an answer. While today&#39;s retrieval systems assume the corpus is fully accessible, users are often unable or unwilling to expose their private data to entities hosting public data. We first define the PUBLIC-PRIVATE AUTOREGRESSIVE INFORMATION RETRIEVAL (PAIR) privacy framework for the novel retrieval setting over multiple privacy scopes. We then argue that an adequate benchmark is missing to study PAIR since existing textual benchmarks require retrieving from a single data distribution. However, public and private data intuitively reflect different distributions, motivating us to create ConcurrentQA, the first textual QA benchmark to require concurrent retrieval over multiple data-distributions. Finally, we show that existing systems face large privacy vs. performance tradeoffs when applied to our proposed retrieval setting and investigate how to mitigate these tradeoffs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11027v1-abstract-full').style.display = 'none'; document.getElementById('2203.11027v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.06823">arXiv:2203.06823</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.06823">pdf</a>, <a href="https://arxiv.org/format/2203.06823">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SKM-TEA: A Dataset for Accelerated MRI Reconstruction with Dense Image Labels for Quantitative Clinical Evaluation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Desai%2C+A+D">Arjun D Desai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schmidt%2C+A+M">Andrew M Schmidt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rubin%2C+E+B">Elka B Rubin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sandino%2C+C+M">Christopher M Sandino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Black%2C+M+S">Marianne S Black</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mazzoli%2C+V">Valentina Mazzoli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stevens%2C+K+J">Kathryn J Stevens</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boutin%2C+R">Robert Boutin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gold%2C+G+E">Garry E Gold</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hargreaves%2C+B+A">Brian A Hargreaves</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chaudhari%2C+A+S">Akshay S Chaudhari</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.06823v1-abstract-short" style="display: inline;">
        Magnetic resonance imaging (MRI) is a cornerstone of modern medical imaging. However, long image acquisition times, the need for qualitative expert analysis, and the lack of (and difficulty extracting) quantitative indicators that are sensitive to tissue health have curtailed widespread clinical and research studies. While recent machine learning methods for MRI reconstruction and analysis have sh&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.06823v1-abstract-full').style.display = 'inline'; document.getElementById('2203.06823v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.06823v1-abstract-full" style="display: none;">
        Magnetic resonance imaging (MRI) is a cornerstone of modern medical imaging. However, long image acquisition times, the need for qualitative expert analysis, and the lack of (and difficulty extracting) quantitative indicators that are sensitive to tissue health have curtailed widespread clinical and research studies. While recent machine learning methods for MRI reconstruction and analysis have shown promise for reducing this burden, these techniques are primarily validated with imperfect image quality metrics, which are discordant with clinically-relevant measures that ultimately hamper clinical deployment and clinician trust. To mitigate this challenge, we present the Stanford Knee MRI with Multi-Task Evaluation (SKM-TEA) dataset, a collection of quantitative knee MRI (qMRI) scans that enables end-to-end, clinically-relevant evaluation of MRI reconstruction and analysis tools. This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient MRI scans, the corresponding scanner-generated DICOM images, manual segmentations of four tissues, and bounding box annotations for sixteen clinically relevant pathologies. We provide a framework for using qMRI parameter maps, along with image reconstructions and dense image labels, for measuring the quality of qMRI biomarker estimates extracted from MRI reconstruction, segmentation, and detection techniques. Finally, we use this framework to benchmark state-of-the-art baselines on this dataset. We hope our SKM-TEA dataset and code can enable a broad spectrum of research for modular image reconstruction and image analysis in a clinically informed manner. Dataset access, code, and benchmarks are available at https://github.com/StanfordMIMI/skm-tea.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.06823v1-abstract-full').style.display = 'none'; document.getElementById('2203.06823v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NeurIPS Datasets &amp; Benchmarks (2021)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.01517">arXiv:2203.01517</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.01517">pdf</a>, <a href="https://arxiv.org/format/2203.01517">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Michael Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N+S">Nimit S. Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H+R">Hongyang R. Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Finn%2C+C">Chelsea Finn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.01517v1-abstract-short" style="display: inline;">
        Spurious correlations pose a major challenge for robust machine learning. Models trained with empirical risk minimization (ERM) may learn to rely on correlations between class labels and spurious attributes, leading to poor performance on data groups without these correlations. This is particularly challenging to address when spurious attribute labels are unavailable. To improve worst-group perfor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01517v1-abstract-full').style.display = 'inline'; document.getElementById('2203.01517v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.01517v1-abstract-full" style="display: none;">
        Spurious correlations pose a major challenge for robust machine learning. Models trained with empirical risk minimization (ERM) may learn to rely on correlations between class labels and spurious attributes, leading to poor performance on data groups without these correlations. This is particularly challenging to address when spurious attribute labels are unavailable. To improve worst-group performance on spuriously correlated data without training attribute labels, we propose Correct-N-Contrast (CNC), a contrastive approach to directly learn representations robust to spurious correlations. As ERM models can be good spurious attribute predictors, CNC works by (1) using a trained ERM model&#39;s outputs to identify samples with the same class but dissimilar spurious features, and (2) training a robust model with contrastive learning to learn similar representations for same-class samples. To support CNC, we introduce new connections between worst-group error and a representation alignment loss that CNC aims to minimize. We empirically observe that worst-group error closely tracks with alignment loss, and prove that the alignment loss over a class helps upper-bound the class&#39;s worst-group vs. average error gap. On popular benchmarks, CNC reduces alignment loss drastically, and achieves state-of-the-art worst-group accuracy by 3.6% average absolute lift. CNC is also competitive with oracle methods that require group labels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01517v1-abstract-full').style.display = 'none'; document.getElementById('2203.01517v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">38 pages, 14 figures. Preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.09729">arXiv:2202.09729</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.09729">pdf</a>, <a href="https://arxiv.org/format/2202.09729">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        It&#39;s Raw! Audio Generation with State-Space Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+K">Karan Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Donahue%2C+C">Chris Donahue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.09729v1-abstract-short" style="display: inline;">
        Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.09729v1-abstract-full').style.display = 'inline'; document.getElementById('2202.09729v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.09729v1-abstract-full" style="display: none;">
        Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling. We identify that S4 can be unstable during autoregressive generation, and provide a simple improvement to its parameterization by drawing connections to Hurwitz matrices. SaShiMi yields state-of-the-art performance for unconditional waveform generation in the autoregressive setting. Additionally, SaShiMi improves non-autoregressive generation performance when used as the backbone architecture for a diffusion model. Compared to prior architectures in the autoregressive generation setting, SaShiMi generates piano and speech waveforms which humans find more musical and coherent respectively, e.g. 2x better mean opinion scores than WaveNet on an unconditional speech generation task. On a music generation task, SaShiMi outperforms WaveNet on density estimation and speed at both training and inference even when using 3x fewer parameters. Code can be found at https://github.com/HazyResearch/state-spaces and samples at https://hazyresearch.stanford.edu/sashimi-examples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.09729v1-abstract-full').style.display = 'none'; document.getElementById('2202.09729v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, 7 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.00072">arXiv:2201.00072</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.00072">pdf</a>, <a href="https://arxiv.org/format/2201.00072">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BARACK: Partially Supervised Group Robustness With Guarantees
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N+S">Nimit S. Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sanjabi%2C+M">Maziar Sanjabi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ballas%2C+N">Nicolas Ballas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grover%2C+A">Aditya Grover</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nie%2C+S">Shaoliang Nie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Firooz%2C+H">Hamed Firooz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.00072v2-abstract-short" style="display: inline;">
        While neural networks have shown remarkable success on classification tasks in terms of average-case performance, they often fail to perform well on certain groups of the data. Such group information may be expensive to obtain; thus, recent works in robustness and fairness have proposed ways to improve worst-group performance even when group labels are unavailable for the training data. However, t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.00072v2-abstract-full').style.display = 'inline'; document.getElementById('2201.00072v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.00072v2-abstract-full" style="display: none;">
        While neural networks have shown remarkable success on classification tasks in terms of average-case performance, they often fail to perform well on certain groups of the data. Such group information may be expensive to obtain; thus, recent works in robustness and fairness have proposed ways to improve worst-group performance even when group labels are unavailable for the training data. However, these methods generally underperform methods that utilize group information at training time. In this work, we assume access to a small number of group labels alongside a larger dataset without group labels. We propose BARACK, a simple two-step framework to utilize this partial group information to improve worst-group performance: train a model to predict the missing group labels for the training data, and then use these predicted group labels in a robust optimization objective. Theoretically, we provide generalization bounds for our approach in terms of the worst-group performance, which scale with respect to both the total number of training points and the number of training points with group labels. Empirically, our method outperforms the baselines that do not use group information, even when only 1-33% of points have group labels. We provide ablation studies to support the robustness and extensibility of our framework.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.00072v2-abstract-full').style.display = 'none'; document.getElementById('2201.00072v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">26 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.00029">arXiv:2112.00029</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.00029">pdf</a>, <a href="https://arxiv.org/format/2112.00029">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Beidi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+K">Kaizhao Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jiaming Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Z">Zhao Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Christopher Re</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.00029v1-abstract-short" style="display: inline;">
        Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model com&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.00029v1-abstract-full').style.display = 'inline'; document.getElementById('2112.00029v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.00029v1-abstract-full" style="display: none;">
        Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.00029v1-abstract-full').style.display = 'none'; document.getElementById('2112.00029v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.04260">arXiv:2111.04260</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.04260">pdf</a>, <a href="https://arxiv.org/format/2111.04260">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalized Benchmarking with the Ludwig Benchmarking Toolkit
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Narayan%2C+A">Avanika Narayan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Molino%2C+P">Piero Molino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+K">Karan Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Neiswanger%2C+W">Willie Neiswanger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.04260v1-abstract-short" style="display: inline;">
        The rapid proliferation of machine learning models across domains and deployment settings has given rise to various communities (e.g. industry practitioners) which seek to benchmark models across tasks and objectives of personal value. Unfortunately, these users cannot use standard benchmark results to perform such value-driven comparisons as traditional benchmarks evaluate models on a single obje&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.04260v1-abstract-full').style.display = 'inline'; document.getElementById('2111.04260v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.04260v1-abstract-full" style="display: none;">
        The rapid proliferation of machine learning models across domains and deployment settings has given rise to various communities (e.g. industry practitioners) which seek to benchmark models across tasks and objectives of personal value. Unfortunately, these users cannot use standard benchmark results to perform such value-driven comparisons as traditional benchmarks evaluate models on a single objective (e.g. average accuracy) and fail to facilitate a standardized training framework that controls for confounding variables (e.g. computational budget), making fair comparisons difficult. To address these challenges, we introduce the open-source Ludwig Benchmarking Toolkit (LBT), a personalized benchmarking toolkit for running end-to-end benchmark studies (from hyperparameter optimization to evaluation) across an easily extensible set of tasks, deep learning models, datasets and evaluation metrics. LBT provides a configurable interface for controlling training and customizing evaluation, a standardized training framework for eliminating confounding variables, and support for multi-objective evaluation. We demonstrate how LBT can be used to create personalized benchmark studies with a large-scale comparative analysis for text classification across 7 models and 9 datasets. We explore the trade-offs between inference latency and performance, relationships between dataset attributes and performance, and the effects of pretraining on convergence and robustness, showing how LBT can be used to satisfy various benchmarking objectives.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.04260v1-abstract-full').style.display = 'none'; document.getElementById('2111.04260v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 14 figures, 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.00396">arXiv:2111.00396</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.00396">pdf</a>, <a href="https://arxiv.org/format/2111.00396">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficiently Modeling Long Sequences with Structured State Spaces
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+K">Karan Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.00396v2-abstract-short" style="display: inline;">
        A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promis&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.00396v2-abstract-full').style.display = 'inline'; document.getElementById('2111.00396v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.00396v2-abstract-full" style="display: none;">
        A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \( x&#39;(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state matrix \( A \), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \( A \) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.00396v2-abstract-full').style.display = 'none'; document.getElementById('2111.00396v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2022 (Oral)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.15343">arXiv:2110.15343</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.15343">pdf</a>, <a href="https://arxiv.org/format/2110.15343">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scatterbrain: Unifying Sparse and Low-rank Attention Approximation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Beidi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Winsor%2C+E">Eric Winsor</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Z">Zhao Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.15343v1-abstract-short" style="display: inline;">
        Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15343v1-abstract-full').style.display = 'inline'; document.getElementById('2110.15343v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.15343v1-abstract-full" style="display: none;">
        Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15343v1-abstract-full').style.display = 'none'; document.getElementById('2110.15343v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.13985">arXiv:2110.13985</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.13985">pdf</a>, <a href="https://arxiv.org/format/2110.13985">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Johnson%2C+I">Isys Johnson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+K">Karan Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saab%2C+K">Khaled Saab</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.13985v1-abstract-short" style="display: inline;">
        Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.13985v1-abstract-full').style.display = 'inline'; document.getElementById('2110.13985v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.13985v1-abstract-full" style="display: none;">
        Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \mapsto y$ by simply simulating a linear continuous-time state-space representation $\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.13985v1-abstract-full').style.display = 'none'; document.getElementById('2110.13985v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.08430">arXiv:2110.08430</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.08430">pdf</a>, <a href="https://arxiv.org/format/2110.08430">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Metadata Shaping: Natural Language Annotations for the Tail
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+S">Simran Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+E">Enci Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Christopher Re</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.08430v1-abstract-short" style="display: inline;">
        Language models (LMs) have made remarkable progress, but still struggle to generalize beyond the training data to rare linguistic patterns. Since rare entities and facts are prevalent in the queries users submit to popular applications such as search and personal assistant systems, improving the ability of LMs to reliably capture knowledge over rare entities is a pressing challenge studied in sign&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.08430v1-abstract-full').style.display = 'inline'; document.getElementById('2110.08430v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.08430v1-abstract-full" style="display: none;">
        Language models (LMs) have made remarkable progress, but still struggle to generalize beyond the training data to rare linguistic patterns. Since rare entities and facts are prevalent in the queries users submit to popular applications such as search and personal assistant systems, improving the ability of LMs to reliably capture knowledge over rare entities is a pressing challenge studied in significant prior work. Noticing that existing approaches primarily modify the LM architecture or introduce auxiliary objectives to inject useful entity knowledge, we ask to what extent we could match the quality of these architectures using a base LM architecture, and only changing the data? We propose metadata shaping, a method in which readily available metadata, such as entity descriptions and categorical tags, are appended to examples based on information theoretic metrics. Intuitively, if metadata corresponding to popular entities overlap with metadata for rare entities, the LM may be able to better reason about the rare entities using patterns learned from similar popular entities. On standard entity-rich tasks (TACRED, FewRel, OpenEntity), with no changes to the LM whatsoever, metadata shaping exceeds the BERT-baseline by up to 5.3 F1 points, and achieves or competes with state-of-the-art results. We further show the improvements are up to 10x larger on examples containing tail versus popular entities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.08430v1-abstract-full').style.display = 'none'; document.getElementById('2110.08430v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.08228">arXiv:2110.08228</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.08228">pdf</a>, <a href="https://arxiv.org/format/2110.08228">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-Domain Data Integration for Named Entity Disambiguation in Biomedical Text
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Varma%2C+M">Maya Varma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Orr%2C+L">Laurel Orr</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Leszczynski%2C+M">Megan Leszczynski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ling%2C+X">Xiao Ling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.08228v1-abstract-short" style="display: inline;">
        Named entity disambiguation (NED), which involves mapping textual mentions to structured entities, is particularly challenging in the medical domain due to the presence of rare entities. Existing approaches are limited by the presence of coarse-grained structural resources in biomedical knowledge bases as well as the use of training datasets that provide low coverage over uncommon resources. In th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.08228v1-abstract-full').style.display = 'inline'; document.getElementById('2110.08228v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.08228v1-abstract-full" style="display: none;">
        Named entity disambiguation (NED), which involves mapping textual mentions to structured entities, is particularly challenging in the medical domain due to the presence of rare entities. Existing approaches are limited by the presence of coarse-grained structural resources in biomedical knowledge bases as well as the use of training datasets that provide low coverage over uncommon resources. In this work, we address these issues by proposing a cross-domain data integration method that transfers structural knowledge from a general text knowledge base to the medical domain. We utilize our integration scheme to augment structural resources and generate a large biomedical NED dataset for pretraining. Our pretrained model with injected structural knowledge achieves state-of-the-art performance on two benchmark medical NED datasets: MedMentions and BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57 accuracy points.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.08228v1-abstract-full').style.display = 'none'; document.getElementById('2110.08228v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Findings of EMNLP 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.00075">arXiv:2110.00075</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.00075">pdf</a>, <a href="https://arxiv.org/format/2110.00075">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Noise2Recon: A Semi-Supervised Framework for Joint MRI Reconstruction and Denoising
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Desai%2C+A+D">Arjun D Desai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ozturkler%2C+B+M">Batu M Ozturkler</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sandino%2C+C+M">Christopher M Sandino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vasanawala%2C+S">Shreyas Vasanawala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hargreaves%2C+B+A">Brian A Hargreaves</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C+M">Christopher M Re</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pauly%2C+J+M">John M Pauly</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chaudhari%2C+A+S">Akshay S Chaudhari</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.00075v1-abstract-short" style="display: inline;">
        Deep learning (DL) has shown promise for faster, high quality accelerated MRI reconstruction. However, standard supervised DL methods depend on extensive amounts of fully-sampled ground-truth data and are sensitive to out-of-distribution (OOD) shifts, in particular for low signal-to-noise ratio (SNR) acquisitions. To alleviate this challenge, we propose a semi-supervised, consistency-based framewo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.00075v1-abstract-full').style.display = 'inline'; document.getElementById('2110.00075v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.00075v1-abstract-full" style="display: none;">
        Deep learning (DL) has shown promise for faster, high quality accelerated MRI reconstruction. However, standard supervised DL methods depend on extensive amounts of fully-sampled ground-truth data and are sensitive to out-of-distribution (OOD) shifts, in particular for low signal-to-noise ratio (SNR) acquisitions. To alleviate this challenge, we propose a semi-supervised, consistency-based framework (termed Noise2Recon) for joint MR reconstruction and denoising. Our method enables the usage of a limited number of fully-sampled and a large number of undersampled-only scans. We compare our method to augmentation-based supervised techniques and fine-tuned denoisers. Results demonstrate that even with minimal ground-truth data, Noise2Recon (1) achieves high performance on in-distribution (low-noise) scans and (2) improves generalizability to OOD, noisy scans.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.00075v1-abstract-full').style.display = 'none'; document.getElementById('2110.00075v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.07258">arXiv:2108.07258</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.07258">pdf</a>, <a href="https://arxiv.org/format/2108.07258">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Opportunities and Risks of Foundation Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bommasani%2C+R">Rishi Bommasani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hudson%2C+D+A">Drew A. Hudson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Adeli%2C+E">Ehsan Adeli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Altman%2C+R">Russ Altman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+S">Simran Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=von+Arx%2C+S">Sydney von Arx</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bernstein%2C+M+S">Michael S. Bernstein</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bohg%2C+J">Jeannette Bohg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bosselut%2C+A">Antoine Bosselut</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brunskill%2C+E">Emma Brunskill</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brynjolfsson%2C+E">Erik Brynjolfsson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Buch%2C+S">Shyamal Buch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Card%2C+D">Dallas Card</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Castellon%2C+R">Rodrigo Castellon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chatterji%2C+N">Niladri Chatterji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+A">Annie Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Creel%2C+K">Kathleen Creel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Davis%2C+J+Q">Jared Quincy Davis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Demszky%2C+D">Dora Demszky</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Donahue%2C+C">Chris Donahue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Doumbouya%2C+M">Moussa Doumbouya</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Durmus%2C+E">Esin Durmus</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ermon%2C+S">Stefano Ermon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Etchemendy%2C+J">John Etchemendy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ethayarajh%2C+K">Kawin Ethayarajh</a>
      , et al. (89 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.07258v2-abstract-short" style="display: inline;">
        AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their cap&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.07258v2-abstract-full').style.display = 'inline'; document.getElementById('2108.07258v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.07258v2-abstract-full" style="display: none;">
        AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.07258v2-abstract-full').style.display = 'none'; document.getElementById('2108.07258v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.06896">arXiv:2108.06896</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.06896">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Challenges for cognitive decoding using deep learning methods
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Thomas%2C+A+W">Armin W. Thomas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Poldrack%2C+R+A">Russell A. Poldrack</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.06896v1-abstract-short" style="display: inline;">
        In cognitive decoding, researchers aim to characterize a brain region&#39;s representations by identifying the cognitive states (e.g., accepting/rejecting a gamble) that can be identified from the region&#39;s activity. Deep learning (DL) methods are highly promising for cognitive decoding, with their unmatched ability to learn versatile representations of complex data. Yet, their widespread application i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.06896v1-abstract-full').style.display = 'inline'; document.getElementById('2108.06896v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.06896v1-abstract-full" style="display: none;">
        In cognitive decoding, researchers aim to characterize a brain region&#39;s representations by identifying the cognitive states (e.g., accepting/rejecting a gamble) that can be identified from the region&#39;s activity. Deep learning (DL) methods are highly promising for cognitive decoding, with their unmatched ability to learn versatile representations of complex data. Yet, their widespread application in cognitive decoding is hindered by their general lack of interpretability as well as difficulties in applying them to small datasets and in ensuring their reproducibility and robustness. We propose to approach these challenges by leveraging recent advances in explainable artificial intelligence and transfer learning, while also providing specific recommendations on how to improve the reproducibility and robustness of DL modeling results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.06896v1-abstract-full').style.display = 'none'; document.getElementById('2108.06896v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.08148">arXiv:2107.08148</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.08148">pdf</a>, <a href="https://arxiv.org/format/2107.08148">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Declarative Machine Learning Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Molino%2C+P">Piero Molino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.08148v1-abstract-short" style="display: inline;">
        In the last years machine learning (ML) has moved from a academic endeavor to a pervasive technology adopted in almost every aspect of computing. ML-powered products are now embedded in our digital lives: from recommendations of what to watch, to divining our search intent, to powering virtual assistants in consumer and enterprise settings. Recent successes in applying ML in natural sciences revea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.08148v1-abstract-full').style.display = 'inline'; document.getElementById('2107.08148v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.08148v1-abstract-full" style="display: none;">
        In the last years machine learning (ML) has moved from a academic endeavor to a pervasive technology adopted in almost every aspect of computing. ML-powered products are now embedded in our digital lives: from recommendations of what to watch, to divining our search intent, to powering virtual assistants in consumer and enterprise settings. Recent successes in applying ML in natural sciences revealed that ML can be used to tackle some of the hardest real-world problems humanity faces today. For these reasons ML has become central in the strategy of tech companies and has gathered even more attention from academia than ever before. Despite these successes, what we have witnessed so far is just the beginning. Right now the people training and using ML models are expert developers working within large organizations, but we believe the next wave of ML systems will allow a larger amount of people, potentially without coding skills, to perform the same tasks. These new ML systems will not require users to fully understand all the details of how models are trained and utilized for obtaining predictions. Declarative interfaces are well suited for this goal, by hiding complexity and favouring separation of interests, and can lead to increased productivity. We worked on such abstract interfaces by developing two declarative ML systems, Overton and Ludwig, that require users to declare only their data schema (names and types of inputs) and tasks rather then writing low level ML code. In this article we will describe how ML systems are currently structured, highlight important factors for their success and adoption, what are the issues current ML systems are facing and how the systems we developed addressed them. Finally we will talk about learnings from the development of ML systems throughout the years and how we believe the next generation of ML systems will look like.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.08148v1-abstract-full').style.display = 'none'; document.getElementById('2107.08148v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.00643">arXiv:2107.00643</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.00643">pdf</a>, <a href="https://arxiv.org/format/2107.00643">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mandoline: Model Evaluation under Distribution Shift
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M">Mayee Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+K">Karan Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N+S">Nimit S. Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Poms%2C+F">Fait Poms</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fatahalian%2C+K">Kayvon Fatahalian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.00643v2-abstract-short" style="display: inline;">
        Machine learning models are often deployed in different settings than they were trained and validated on, posing a challenge to practitioners who wish to predict how well the deployed model will perform on a target distribution. If an unlabeled sample from the target distribution is available, along with a labeled sample from a possibly different source distribution, standard approaches such as im&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00643v2-abstract-full').style.display = 'inline'; document.getElementById('2107.00643v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.00643v2-abstract-full" style="display: none;">
        Machine learning models are often deployed in different settings than they were trained and validated on, posing a challenge to practitioners who wish to predict how well the deployed model will perform on a target distribution. If an unlabeled sample from the target distribution is available, along with a labeled sample from a possibly different source distribution, standard approaches such as importance weighting can be applied to estimate performance on the target. However, importance weighting struggles when the source and target distributions have non-overlapping support or are high-dimensional. Taking inspiration from fields such as epidemiology and polling, we develop Mandoline, a new evaluation framework that mitigates these issues. Our key insight is that practitioners may have prior knowledge about the ways in which the distribution shifts, which we can use to better guide the importance weighting procedure. Specifically, users write simple &#34;slicing functions&#34; - noisy, potentially correlated binary functions intended to capture possible axes of distribution shift - to compute reweighted performance estimates. We further describe a density ratio estimation framework for the slices and show how its estimation error scales with slice quality and dataset size. Empirical validation on NLP and vision tasks shows that Mandoline can estimate performance on the target distribution up to 3x more accurately compared to standard baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00643v2-abstract-full').style.display = 'none'; document.getElementById('2107.00643v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">33 pages. Published as a conference paper at ICML 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.03306">arXiv:2106.03306</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.03306">pdf</a>, <a href="https://arxiv.org/format/2106.03306">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HoroPCA: Hyperbolic Dimensionality Reduction via Horospherical Projections
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chami%2C+I">Ines Chami</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+D">Dat Nguyen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.03306v1-abstract-short" style="display: inline;">
        This paper studies Principal Component Analysis (PCA) for data lying in hyperbolic spaces. Given directions, PCA relies on: (1) a parameterization of subspaces spanned by these directions, (2) a method of projection onto subspaces that preserves information in these directions, and (3) an objective to optimize, namely the variance explained by projections. We generalize each of these concepts to t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03306v1-abstract-full').style.display = 'inline'; document.getElementById('2106.03306v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.03306v1-abstract-full" style="display: none;">
        This paper studies Principal Component Analysis (PCA) for data lying in hyperbolic spaces. Given directions, PCA relies on: (1) a parameterization of subspaces spanned by these directions, (2) a method of projection onto subspaces that preserves information in these directions, and (3) an objective to optimize, namely the variance explained by projections. We generalize each of these concepts to the hyperbolic space and propose HoroPCA, a method for hyperbolic dimensionality reduction. By focusing on the core problem of extracting principal directions, HoroPCA theoretically better preserves information in the original data such as distances, compared to previous generalizations of PCA. Empirically, we validate that HoroPCA outperforms existing dimensionality reduction methods, significantly reducing error in distance preservation. As a data whitening method, it improves downstream classification by up to 3.9% compared to methods that don&#39;t use whitening. Finally, we show that HoroPCA can be used to visualize hyperbolic data in two dimensions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03306v1-abstract-full').style.display = 'none'; document.getElementById('2106.03306v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICML 2021</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        PMLR 139:1419-1429, 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.01501">arXiv:2106.01501</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.01501">pdf</a>, <a href="https://arxiv.org/format/2106.01501">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Suri%2C+S">Sahaana Suri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ilyas%2C+I+F">Ihab F. Ilyas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rekatsinas%2C+T">Theodoros Rekatsinas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.01501v1-abstract-short" style="display: inline;">
        Structured data, or data that adheres to a pre-defined schema, can suffer from fragmented context: information describing a single entity can be scattered across multiple datasets or tables tailored for specific business needs, with no explicit linking keys (e.g., primary key-foreign key relationships or heuristic functions). Context enrichment, or rebuilding fragmented context, using keyless join&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.01501v1-abstract-full').style.display = 'inline'; document.getElementById('2106.01501v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.01501v1-abstract-full" style="display: none;">
        Structured data, or data that adheres to a pre-defined schema, can suffer from fragmented context: information describing a single entity can be scattered across multiple datasets or tables tailored for specific business needs, with no explicit linking keys (e.g., primary key-foreign key relationships or heuristic functions). Context enrichment, or rebuilding fragmented context, using keyless joins is an implicit or explicit step in machine learning (ML) pipelines over structured data sources. This process is tedious, domain-specific, and lacks support in now-prevalent no-code ML systems that let users create ML pipelines using just input data and high-level configuration files. In response, we propose Ember, a system that abstracts and automates keyless joins to generalize context enrichment. Our key insight is that Ember can enable a general keyless join operator by constructing an index populated with task-specific embeddings. Ember learns these embeddings by leveraging Transformer-based representation learning techniques. We describe our core architectural principles and operators when developing Ember, and empirically demonstrate that Ember allows users to develop no-code pipelines for five domains, including search, recommendation and question answering, and can exceed alternatives by up to 39% recall, with as little as a single line configuration change.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.01501v1-abstract-full').style.display = 'none'; document.getElementById('2106.01501v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.15798">arXiv:2103.15798</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.15798">pdf</a>, <a href="https://arxiv.org/format/2103.15798">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rethinking Neural Operations for Diverse Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Roberts%2C+N">Nicholas Roberts</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khodak%2C+M">Mikhail Khodak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L">Liam Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Talwalkar%2C+A">Ameet Talwalkar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.15798v2-abstract-short" style="display: inline;">
        An important goal of AutoML is to automate-away the design of neural networks on new tasks in under-explored domains. Motivated by this goal, we study the problem of enabling users to discover the right neural operations given data from their specific domain. We introduce a search space of operations called XD-Operations that mimic the inductive bias of standard multi-channel convolutions while be&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15798v2-abstract-full').style.display = 'inline'; document.getElementById('2103.15798v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.15798v2-abstract-full" style="display: none;">
        An important goal of AutoML is to automate-away the design of neural networks on new tasks in under-explored domains. Motivated by this goal, we study the problem of enabling users to discover the right neural operations given data from their specific domain. We introduce a search space of operations called XD-Operations that mimic the inductive bias of standard multi-channel convolutions while being much more expressive: we prove that it includes many named operations across multiple application areas. Starting with any standard backbone such as ResNet, we show how to transform it into a search space over XD-operations and how to traverse the space using a simple weight-sharing scheme. On a diverse set of tasks -- solving PDEs, distance prediction for protein folding, and music modeling -- our approach consistently yields models with lower error than baseline networks and often even lower error than expert-designed domain-specific approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15798v2-abstract-full').style.display = 'none'; document.getElementById('2103.15798v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.02761">arXiv:2103.02761</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.02761">pdf</a>, <a href="https://arxiv.org/format/2103.02761">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comparing the Value of Labeled and Unlabeled Data in Method-of-Moments Latent Variable Estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M+F">Mayee F. Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cohen-Wang%2C+B">Benjamin Cohen-Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mussmann%2C+S">Stephen Mussmann</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sala%2C+F">Frederic Sala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.02761v1-abstract-short" style="display: inline;">
        Labeling data for modern machine learning is expensive and time-consuming. Latent variable models can be used to infer labels from weaker, easier-to-acquire sources operating on unlabeled data. Such models can also be trained using labeled data, presenting a key question: should a user invest in few labeled or many unlabeled points? We answer this via a framework centered on model misspecification&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.02761v1-abstract-full').style.display = 'inline'; document.getElementById('2103.02761v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.02761v1-abstract-full" style="display: none;">
        Labeling data for modern machine learning is expensive and time-consuming. Latent variable models can be used to infer labels from weaker, easier-to-acquire sources operating on unlabeled data. Such models can also be trained using labeled data, presenting a key question: should a user invest in few labeled or many unlabeled points? We answer this via a framework centered on model misspecification in method-of-moments latent variable estimation. Our core result is a bias-variance decomposition of the generalization error, which shows that the unlabeled-only approach incurs additional bias under misspecification. We then introduce a correction that provably removes this bias in certain cases. We apply our decomposition framework to three scenarios -- well-specified, misspecified, and corrected models -- to 1) choose between labeled and unlabeled data and 2) learn from their combination. We observe theoretically and with synthetic experiments that for well-specified models, labeled points are worth a constant factor more than unlabeled points. With misspecification, however, their relative value is higher due to the additional bias but can be reduced with correction. We also apply our approach to study real-world weak supervision techniques for dataset construction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.02761v1-abstract-full').style.display = 'none'; document.getElementById('2103.02761v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in AISTATS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.04840">arXiv:2101.04840</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.04840">pdf</a>, <a href="https://arxiv.org/format/2101.04840">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robustness Gym: Unifying the NLP Evaluation Landscape
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+K">Karan Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rajani%2C+N">Nazneen Rajani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vig%2C+J">Jesse Vig</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+S">Samson Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jason Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+S">Stephan Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiong%2C+C">Caiming Xiong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bansal%2C+M">Mohit Bansal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.04840v1-abstract-short" style="display: inline;">
        Despite impressive performance on standard benchmarks, deep neural networks are often brittle when deployed in real-world systems. Consequently, recent research has focused on testing the robustness of such models, resulting in a diverse set of evaluation methodologies ranging from adversarial attacks to rule-based data transformations. In this work, we identify challenges with evaluating NLP syst&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.04840v1-abstract-full').style.display = 'inline'; document.getElementById('2101.04840v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.04840v1-abstract-full" style="display: none;">
        Despite impressive performance on standard benchmarks, deep neural networks are often brittle when deployed in real-world systems. Consequently, recent research has focused on testing the robustness of such models, resulting in a diverse set of evaluation methodologies ranging from adversarial attacks to rule-based data transformations. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, Robustness Gym enables practitioners to compare results from all 4 evaluation paradigms with just a few clicks, and to easily develop and share novel evaluation methods using a built-in set of abstractions. To validate Robustness Gym&#39;s utility to practitioners, we conducted a real-world case study with a sentiment-modeling team, revealing performance degradations of 18%+. To verify that Robustness Gym can aid novel research analyses, we perform the first study of state-of-the-art commercial and academic named entity linking (NEL) systems, as well as a fine-grained analysis of state-of-the-art summarization models. For NEL, commercial systems struggle to link rare entities and lag their academic counterparts by 10%+, while state-of-the-art summarization models struggle on examples that require abstraction and distillation, degrading by 9%+. Robustness Gym can be found at https://robustnessgym.com/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.04840v1-abstract-full').style.display = 'none'; document.getElementById('2101.04840v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">34 pages, 8 figures, 6 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.14966">arXiv:2012.14966</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.14966">pdf</a>, <a href="https://arxiv.org/format/2012.14966">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N+S">Nimit S. Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eichhorn%2C+M">Matthew Eichhorn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Blonder%2C+A">Amit Blonder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Leszczynski%2C+M">Megan Leszczynski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.14966v2-abstract-short" style="display: inline;">
        Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.14966v2-abstract-full').style.display = 'inline'; document.getElementById('2012.14966v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.14966v2-abstract-full" style="display: none;">
        Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. K-matrices can also simplify hand-engineered pipelines -- we replace filter bank feature computation in speech data preprocessing with a learnable kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. In addition, K-matrices can capture latent structure in models: for a challenging permuted image classification task, a K-matrix based representation of permutations is able to learn the right latent structure and improves accuracy of a downstream convolutional model by over 9%. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.14966v2-abstract-full').style.display = 'none'; document.getElementById('2012.14966v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Conference on Learning Representations (ICLR) 2020 spotlight</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.12945">arXiv:2011.12945</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.12945">pdf</a>, <a href="https://arxiv.org/format/2011.12945">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N+S">Nimit S. Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dunnmon%2C+J+A">Jared A. Dunnmon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Angus%2C+G">Geoffrey Angus</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.12945v2-abstract-short" style="display: inline;">
        In real-world classification tasks, each class often comprises multiple finer-grained &#34;subclasses.&#34; As the subclass labels are frequently unavailable, models trained using only the coarser-grained class labels often exhibit highly variable performance across different subclasses. This phenomenon, known as hidden stratification, has important consequences for models deployed in safety-critical appl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.12945v2-abstract-full').style.display = 'inline'; document.getElementById('2011.12945v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.12945v2-abstract-full" style="display: none;">
        In real-world classification tasks, each class often comprises multiple finer-grained &#34;subclasses.&#34; As the subclass labels are frequently unavailable, models trained using only the coarser-grained class labels often exhibit highly variable performance across different subclasses. This phenomenon, known as hidden stratification, has important consequences for models deployed in safety-critical applications such as medicine. We propose GEORGE, a method to both measure and mitigate hidden stratification even when subclass labels are unknown. We first observe that unlabeled subclasses are often separable in the feature space of deep neural networks, and exploit this fact to estimate subclass labels for the training data via clustering techniques. We then use these approximate subclass labels as a form of noisy supervision in a distributionally robust optimization objective. We theoretically characterize the performance of GEORGE in terms of the worst-case generalization error across any subclass. We empirically validate GEORGE on a mix of real-world and benchmark image classification datasets, and show that our approach boosts worst-case subclass accuracy by up to 22 percentage points compared to standard training techniques, without requiring any prior information about the subclasses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.12945v2-abstract-full').style.display = 'none'; document.getElementById('2011.12945v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">40 pages. Published as a conference paper at NeurIPS 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.11750">arXiv:2010.11750</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.11750">pdf</a>, <a href="https://arxiv.org/format/2010.11750">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analysis of Information Transfer from Heterogeneous Sources via Precise High-dimensional Asymptotics
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+F">Fan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H+R">Hongyang R. Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+W+J">Weijie J. Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.11750v2-abstract-short" style="display: inline;">
        We consider the problem of transfer learning -- gaining knowledge from one source task and applying it to a different but related target task. A fundamental question in transfer learning is whether combining the data of both tasks works better than using only the target task&#39;s data (equivalently, whether a &#34;positive information transfer&#34; happens). We study this question formally in a linear regres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.11750v2-abstract-full').style.display = 'inline'; document.getElementById('2010.11750v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.11750v2-abstract-full" style="display: none;">
        We consider the problem of transfer learning -- gaining knowledge from one source task and applying it to a different but related target task. A fundamental question in transfer learning is whether combining the data of both tasks works better than using only the target task&#39;s data (equivalently, whether a &#34;positive information transfer&#34; happens). We study this question formally in a linear regression setting where a two-layer linear neural network estimator combines both tasks&#39; data. The estimator uses a shared parameter vector for both tasks and exhibits positive or negative information transfer by varying dataset characteristics.
  We characterize the precise asymptotic limit of the prediction risk of the above estimator when the sample sizes increase with the feature dimension proportionally at fixed ratios. We also show that the asymptotic limit is sufficiently accurate for finite dimensions. Then, we provide the exact condition to determine positive (and negative) information transfer in a random-effect model, leading to several theoretical insights. For example, the risk curve is non-monotone under model shift, thus motivating a transfer learning procedure that progressively adds data from the source task. We validate this procedure&#39;s efficiency on text classification tasks with a neural network that applies a shared feature space for both tasks, similar to the above estimator. The main ingredient of the analysis is finding the high-dimensional asymptotic limits of various functions involving the sum of two independent sample covariance matrices with different population covariance matrices, which may be of independent interest.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.11750v2-abstract-full').style.display = 'none'; document.getElementById('2010.11750v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">66 pages, 8 figures; Rewritten the paper with new materials and changed the title</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.10363">arXiv:2010.10363</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.10363">pdf</a>, <a href="https://arxiv.org/format/2010.10363">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Orr%2C+L">Laurel Orr</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Leszczynski%2C+M">Megan Leszczynski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+S">Simran Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guha%2C+N">Neel Guha</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ling%2C+X">Xiao Ling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Christopher Re</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.10363v3-abstract-short" style="display: inline;">
        A challenge for named entity disambiguation (NED), the task of mapping textual mentions to entities in a knowledge base, is how to disambiguate entities that appear rarely in the training data, termed tail entities. Humans use subtle reasoning patterns based on knowledge of entity facts, relations, and types to disambiguate unfamiliar entities. Inspired by these patterns, we introduce Bootleg, a s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.10363v3-abstract-full').style.display = 'inline'; document.getElementById('2010.10363v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.10363v3-abstract-full" style="display: none;">
        A challenge for named entity disambiguation (NED), the task of mapping textual mentions to entities in a knowledge base, is how to disambiguate entities that appear rarely in the training data, termed tail entities. Humans use subtle reasoning patterns based on knowledge of entity facts, relations, and types to disambiguate unfamiliar entities. Inspired by these patterns, we introduce Bootleg, a self-supervised NED system that is explicitly grounded in reasoning patterns for disambiguation. We define core reasoning patterns for disambiguation, create a learning procedure to encourage the self-supervised model to learn the patterns, and show how to use weak supervision to enhance the signals in the training data. Encoding the reasoning patterns in a simple Transformer architecture, Bootleg meets or exceeds state-of-the-art on three NED benchmarks. We further show that the learned representations from Bootleg successfully transfer to other non-disambiguation tasks that require entity-based knowledge: we set a new state-of-the-art in the popular TACRED relation extraction task by 1.0 F1 points and demonstrate up to 8% performance lift in highly optimized production search and assistant tasks at a major technology company
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.10363v3-abstract-full').style.display = 'none'; document.getElementById('2010.10363v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.00402">arXiv:2010.00402</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.00402">pdf</a>, <a href="https://arxiv.org/format/2010.00402">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical Clustering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chami%2C+I">Ines Chami</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chatziafratis%2C+V">Vaggos Chatziafratis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.00402v1-abstract-short" style="display: inline;">
        Similarity-based Hierarchical Clustering (HC) is a classical unsupervised machine learning algorithm that has traditionally been solved with heuristic algorithms like Average-Linkage. Recently, Dasgupta reframed HC as a discrete optimization problem by introducing a global cost function measuring the quality of a given tree. In this work, we provide the first continuous relaxation of Dasgupta&#39;s di&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.00402v1-abstract-full').style.display = 'inline'; document.getElementById('2010.00402v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.00402v1-abstract-full" style="display: none;">
        Similarity-based Hierarchical Clustering (HC) is a classical unsupervised machine learning algorithm that has traditionally been solved with heuristic algorithms like Average-Linkage. Recently, Dasgupta reframed HC as a discrete optimization problem by introducing a global cost function measuring the quality of a given tree. In this work, we provide the first continuous relaxation of Dasgupta&#39;s discrete optimization problem with provable quality guarantees. The key idea of our method, HypHC, is showing a direct correspondence from discrete trees to continuous representations (via the hyperbolic embeddings of their leaf nodes) and back (via a decoding algorithm that maps leaf embeddings to a dendrogram), allowing us to search the space of discrete binary trees with continuous optimization. Building on analogies between trees and hyperbolic space, we derive a continuous analogue for the notion of lowest common ancestor, which leads to a continuous relaxation of Dasgupta&#39;s discrete objective. We can show that after decoding, the global minimizer of our continuous relaxation yields a discrete tree with a (1 + epsilon)-factor approximation for Dasgupta&#39;s optimal tree, where epsilon can be made arbitrarily small and controls optimization challenges. We experimentally evaluate HypHC on a variety of HC benchmarks and find that even approximate solutions found with gradient descent have superior clustering quality than agglomerative heuristics or other gradient based algorithms. Finally, we highlight the flexibility of HypHC using end-to-end training in a downstream classification task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.00402v1-abstract-full').style.display = 'none'; document.getElementById('2010.00402v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.09983">arXiv:2008.09983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.09983">pdf</a>, <a href="https://arxiv.org/format/2008.09983">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.14778/3415478.3415559">10.14778/3415478.3415559 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Leveraging Organizational Resources to Adapt Models to New Data Modalities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Suri%2C+S">Sahaana Suri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chanda%2C+R">Raghuveer Chanda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bulut%2C+N">Neslihan Bulut</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Narayana%2C+P">Pradyumna Narayana</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+Y">Yemao Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bailis%2C+P">Peter Bailis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Basu%2C+S">Sugato Basu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Narlikar%2C+G">Girija Narlikar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Christopher Re</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sethi%2C+A">Abishek Sethi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.09983v1-abstract-short" style="display: inline;">
        As applications in large organizations evolve, the machine learning (ML) models that power them must adapt the same predictive tasks to newly arising data modalities (e.g., a new video content launch in a social media application requires existing text or image models to extend to video). To solve this problem, organizations typically create ML pipelines from scratch. However, this fails to utiliz&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09983v1-abstract-full').style.display = 'inline'; document.getElementById('2008.09983v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.09983v1-abstract-full" style="display: none;">
        As applications in large organizations evolve, the machine learning (ML) models that power them must adapt the same predictive tasks to newly arising data modalities (e.g., a new video content launch in a social media application requires existing text or image models to extend to video). To solve this problem, organizations typically create ML pipelines from scratch. However, this fails to utilize the domain expertise and data they have cultivated from developing tasks for existing modalities. We demonstrate how organizational resources, in the form of aggregate statistics, knowledge bases, and existing services that operate over related tasks, enable teams to construct a common feature space that connects new and existing data modalities. This allows teams to apply methods for training data curation (e.g., weak supervision and label propagation) and model training (e.g., forms of multi-modal learning) across these different data modalities. We study how this use of organizational resources composes at production scale in over 5 classification tasks at Google, and demonstrate how it reduces the time needed to develop models for new modalities from months to weeks to days.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.09983v1-abstract-full').style.display = 'none'; document.getElementById('2008.09983v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        PVLDB,13(12): 3396-3410, 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.07669">arXiv:2008.07669</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.07669">pdf</a>, <a href="https://arxiv.org/format/2008.07669">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HiPPO: Recurrent Memory with Optimal Polynomial Projections
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ermon%2C+S">Stefano Ermon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Christopher Re</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.07669v2-abstract-short" style="display: inline;">
        A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal soluti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07669v2-abstract-full').style.display = 'inline'; document.getElementById('2008.07669v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.07669v2-abstract-full" style="display: none;">
        A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07669v2-abstract-full').style.display = 'none'; document.getElementById('2008.07669v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.06775">arXiv:2008.06775</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.06775">pdf</a>, <a href="https://arxiv.org/format/2008.06775">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Model Patching: Closing the Subgroup Performance Gap with Data Augmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+K">Karan Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yixuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.06775v1-abstract-short" style="display: inline;">
        Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that enc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.06775v1-abstract-full').style.display = 'inline'; document.getElementById('2008.06775v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.06775v1-abstract-full" style="display: none;">
        Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. Model patching first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate subgroup features. We instantiate model patching with CAMEL, which (1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and (2) balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. We demonstrate CAMEL&#39;s effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real-world skin cancer dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.06775v1-abstract-full').style.display = 'none'; document.getElementById('2008.06775v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.13828">arXiv:2007.13828</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.13828">pdf</a>, <a href="https://arxiv.org/format/2007.13828">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GRIP: A Graph Neural Network Accelerator Architecture
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kiningham%2C+K">Kevin Kiningham</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Christopher Re</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Levis%2C+P">Philip Levis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.13828v2-abstract-short" style="display: inline;">
        We present GRIP, a graph neural network accelerator architecture designed for low-latency inference. AcceleratingGNNs is challenging because they combine two distinct types of computation: arithmetic-intensive vertex-centric operations and memory-intensive edge-centric operations. GRIP splits GNN inference into a fixed set of edge- and vertex-centric execution phases that can be implemented in har&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.13828v2-abstract-full').style.display = 'inline'; document.getElementById('2007.13828v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.13828v2-abstract-full" style="display: none;">
        We present GRIP, a graph neural network accelerator architecture designed for low-latency inference. AcceleratingGNNs is challenging because they combine two distinct types of computation: arithmetic-intensive vertex-centric operations and memory-intensive edge-centric operations. GRIP splits GNN inference into a fixed set of edge- and vertex-centric execution phases that can be implemented in hardware. We then specialize each unit for the unique computational structure found in each phase.For vertex-centric phases, GRIP uses a high performance matrix multiply engine coupled with a dedicated memory subsystem for weights to improve reuse. For edge-centric phases, GRIP use multiple parallel prefetch and reduction engines to alleviate the irregularity in memory accesses. Finally, GRIP supports severalGNN optimizations, including a novel optimization called vertex-tiling which increases the reuse of weight data.We evaluate GRIP by performing synthesis and place and route for a 28nm implementation capable of executing inference for several widely-used GNN models (GCN, GraphSAGE, G-GCN, and GIN). Across several benchmark graphs, it reduces 99th percentile latency by a geometric mean of 17x and 23x compared to a CPU and GPU baseline, respectively, while drawing only 5W.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.13828v2-abstract-full').style.display = 'none'; document.getElementById('2007.13828v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.15168">arXiv:2006.15168</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.15168">pdf</a>, <a href="https://arxiv.org/format/2006.15168">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Train and You&#39;ll Miss It: Interactive Model Iteration with Weak Supervision and Pre-Trained Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M+F">Mayee F. Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sala%2C+F">Frederic Sala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mullapudi%2C+R+T">Ravi Teja Mullapudi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Poms%2C+F">Fait Poms</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fatahalian%2C+K">Kayvon Fatahalian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.15168v1-abstract-short" style="display: inline;">
        Our goal is to enable machine learning systems to be trained interactively. This requires models that perform well and train quickly, without large amounts of hand-labeled data. We take a step forward in this direction by borrowing from weak supervision (WS), wherein models can be trained with noisy sources of signal instead of hand-labeled data. But WS relies on training downstream deep networks&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.15168v1-abstract-full').style.display = 'inline'; document.getElementById('2006.15168v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.15168v1-abstract-full" style="display: none;">
        Our goal is to enable machine learning systems to be trained interactively. This requires models that perform well and train quickly, without large amounts of hand-labeled data. We take a step forward in this direction by borrowing from weak supervision (WS), wherein models can be trained with noisy sources of signal instead of hand-labeled data. But WS relies on training downstream deep networks to extrapolate to unseen data points, which can take hours or days. Pre-trained embeddings can remove this requirement. We do not use the embeddings as features as in transfer learning (TL), which requires fine-tuning for high performance, but instead use them to define a distance function on the data and extend WS source votes to nearby points. Theoretically, we provide a series of results studying how performance scales with changes in source coverage, source accuracy, and the Lipschitzness of label distributions in the embedding space, and compare this rate to standard WS without extension and TL without fine-tuning. On six benchmark NLP and video tasks, our method outperforms WS without extension by 4.1 points, TL without fine-tuning by 12.8 points, and traditionally-supervised deep networks by 13.1 points, and comes within 0.7 points of state-of-the-art weakly-supervised deep networks-all while training in less than half a second.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.15168v1-abstract-full').style.display = 'none'; document.getElementById('2006.15168v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.09117">arXiv:2005.09117</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.09117">pdf</a>, <a href="https://arxiv.org/format/2005.09117">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contextual Embeddings: When Are They Worth It?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Arora%2C+S">Simran Arora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=May%2C+A">Avner May</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jian Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.09117v1-abstract-short" style="display: inline;">
        We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline---random word embeddings---focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.09117v1-abstract-full').style.display = 'inline'; document.getElementById('2005.09117v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.09117v1-abstract-full" style="display: none;">
        We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline---random word embeddings---focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks. Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.09117v1-abstract-full').style.display = 'none'; document.getElementById('2005.09117v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.03675">arXiv:2005.03675</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.03675">pdf</a>, <a href="https://arxiv.org/format/2005.03675">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Machine Learning on Graphs: A Model and Comprehensive Taxonomy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chami%2C+I">Ines Chami</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abu-El-Haija%2C+S">Sami Abu-El-Haija</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Perozzi%2C+B">Bryan Perozzi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Murphy%2C+K">Kevin Murphy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.03675v3-abstract-short" style="display: inline;">
        There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.03675v3-abstract-full').style.display = 'inline'; document.getElementById('2005.03675v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.03675v3-abstract-full" style="display: none;">
        There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.03675v3-abstract-full').style.display = 'none'; document.getElementById('2005.03675v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.00944">arXiv:2005.00944</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.00944">pdf</a>, <a href="https://arxiv.org/format/2005.00944">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding and Improving Information Transfer in Multi-Task Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H+R">Hongyang R. Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.00944v1-abstract-short" style="display: inline;">
        We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks&#39; data are well-al&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.00944v1-abstract-full').style.display = 'inline'; document.getElementById('2005.00944v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.00944v1-abstract-full" style="display: none;">
        We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks&#39; data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks&#39; embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtain a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT-LARGE using our alignment method. We also design an SVD-based task reweighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.00944v1-abstract-full').style.display = 'none'; document.getElementById('2005.00944v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Appeared in ICLR 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.00695">arXiv:2005.00695</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.00695">pdf</a>, <a href="https://arxiv.org/format/2005.00695">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Generalization Effects of Linear Transformations in Data Augmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H+R">Hongyang R. Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Valiant%2C+G">Gregory Valiant</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.00695v2-abstract-short" style="display: inline;">
        Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transfor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.00695v2-abstract-full').style.display = 'inline'; document.getElementById('2005.00695v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.00695v2-abstract-full" style="display: none;">
        Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations which preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations which mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms RandAugment by 1.24% on CIFAR-100 using Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA Adversarial AutoAugment on CIFAR datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.00695v2-abstract-full').style.display = 'none'; document.getElementById('2005.00695v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Conference on Machine learning (ICML) 2020. Added experimental results on ImageNet</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.00545">arXiv:2005.00545</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.00545">pdf</a>, <a href="https://arxiv.org/format/2005.00545">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low-Dimensional Hyperbolic Knowledge Graph Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chami%2C+I">Ines Chami</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wolf%2C+A">Adva Wolf</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Juan%2C+D">Da-Cheng Juan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sala%2C+F">Frederic Sala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ravi%2C+S">Sujith Ravi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.00545v1-abstract-short" style="display: inline;">
        Knowledge graph (KG) embeddings learn low-dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.00545v1-abstract-full').style.display = 'inline'; document.getElementById('2005.00545v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.00545v1-abstract-full" style="display: none;">
        Knowledge graph (KG) embeddings learn low-dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention-based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.00545v1-abstract-full').style.display = 'none'; document.getElementById('2005.00545v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.05316">arXiv:2004.05316</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.05316">pdf</a>, <a href="https://arxiv.org/format/2004.05316">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ivy: Instrumental Variable Synthesis for Causal Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kuang%2C+Z">Zhaobin Kuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sala%2C+F">Frederic Sala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N">Nimit Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=C%C3%B3rdova-Palomera%2C+A">Aldo Córdova-Palomera</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dunnmon%2C+J">Jared Dunnmon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Priest%2C+J">James Priest</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.05316v1-abstract-short" style="display: inline;">
        A popular way to estimate the causal effect of a variable x on y from observational data is to use an instrumental variable (IV): a third variable z that affects y only through x. The more strongly z is associated with x, the more reliable the estimate is, but such strong IVs are difficult to find. Instead, practitioners combine more commonly available IV candidates---which are not necessarily str&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.05316v1-abstract-full').style.display = 'inline'; document.getElementById('2004.05316v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.05316v1-abstract-full" style="display: none;">
        A popular way to estimate the causal effect of a variable x on y from observational data is to use an instrumental variable (IV): a third variable z that affects y only through x. The more strongly z is associated with x, the more reliable the estimate is, but such strong IVs are difficult to find. Instead, practitioners combine more commonly available IV candidates---which are not necessarily strong, or even valid, IVs---into a single &#34;summary&#34; that is plugged into causal effect estimators in place of an IV. In genetic epidemiology, such approaches are known as allele scores. Allele scores require strong assumptions---independence and validity of all IV candidates---for the resulting estimate to be reliable. To relax these assumptions, we propose Ivy, a new method to combine IV candidates that can handle correlated and invalid IV candidates in a robust manner. Theoretically, we characterize this robustness, its limits, and its impact on the resulting causal estimates. Empirically, Ivy can correctly identify the directionality of known relationships and is robust against false discovery (median effect size &lt;= 0.025) on three real-world datasets with no causal effects, while allele scores return more biased estimates (median effect size &gt;= 0.118).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.05316v1-abstract-full').style.display = 'none'; document.getElementById('2004.05316v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.07977">arXiv:2003.07977</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.07977">pdf</a>, <a href="https://arxiv.org/format/2003.07977">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Assessing Robustness to Noise: Low-Cost Head CT Triage
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hooper%2C+S+M">Sarah M. Hooper</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dunnmon%2C+J+A">Jared A. Dunnmon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lungren%2C+M+P">Matthew P. Lungren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gambhir%2C+S+S">Sanjiv Sam Gambhir</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+A+S">Adam S. Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Patel%2C+B+N">Bhavik N. Patel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.07977v2-abstract-short" style="display: inline;">
        Automated medical image classification with convolutional neural networks (CNNs) has great potential to impact healthcare, particularly in resource-constrained healthcare systems where fewer trained radiologists are available. However, little is known about how well a trained CNN can perform on images with the increased noise levels, different acquisition protocols, or additional artifacts that ma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.07977v2-abstract-full').style.display = 'inline'; document.getElementById('2003.07977v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.07977v2-abstract-full" style="display: none;">
        Automated medical image classification with convolutional neural networks (CNNs) has great potential to impact healthcare, particularly in resource-constrained healthcare systems where fewer trained radiologists are available. However, little is known about how well a trained CNN can perform on images with the increased noise levels, different acquisition protocols, or additional artifacts that may arise when using low-cost scanners, which can be underrepresented in datasets collected from well-funded hospitals. In this work, we investigate how a model trained to triage head computed tomography (CT) scans performs on images acquired with reduced x-ray tube current, fewer projections per gantry rotation, and limited angle scans. These changes can reduce the cost of the scanner and demands on electrical power but come at the expense of increased image noise and artifacts. We first develop a model to triage head CTs and report an area under the receiver operating characteristic curve (AUROC) of 0.77. We then show that the trained model is robust to reduced tube current and fewer projections, with the AUROC dropping only 0.65% for images acquired with a 16x reduction in tube current and 0.22% for images acquired with 8x fewer projections. Finally, for significantly degraded images acquired by a limited angle scan, we show that a model trained specifically to classify such images can overcome the technological limitations to reconstruction and maintain an AUROC within 0.09% of the original model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.07977v2-abstract-full').style.display = 'none'; document.getElementById('2003.07977v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">AI for Affordable Healthcare Workshop at ICLR 2020. First two authors have equal contribution; last two authors have equal contribution. Revision made to manuscript header according to workshop guidelines on 3/28/20</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.06248">arXiv:2003.06248</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.06248">pdf</a>, <a href="https://arxiv.org/format/2003.06248">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Instrumentation and Methods for Astrophysics">astro-ph.IM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.13140/RG.2.2.10451.73762">10.13140/RG.2.2.10451.73762 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        High Quality Software for Planetary Science from Space
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lazzarotto%2C+F">Francesco Lazzarotto</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cremonese%2C+G">Gabriele Cremonese</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lucchetti%2C+A">Alice Lucchetti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Cristina Re</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simioni%2C+E">Emanuele Simioni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pajola%2C+M">Maurizio Pajola</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cambianica%2C+P">Pamela Cambianica</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Munaretto%2C+G">Giovanni Munaretto</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.06248v2-abstract-short" style="display: inline;">
        Planetary science space missions need high quality software ed efficient algorithms in order to extract innovative scientific results from flight data. Reliable and efficient software technologies are increasingly vital to improve and prolong the exploiting of the results of a mission, to allow the application of established algorithms and technologies also to future space missions and for the sci&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.06248v2-abstract-full').style.display = 'inline'; document.getElementById('2003.06248v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.06248v2-abstract-full" style="display: none;">
        Planetary science space missions need high quality software ed efficient algorithms in order to extract innovative scientific results from flight data. Reliable and efficient software technologies are increasingly vital to improve and prolong the exploiting of the results of a mission, to allow the application of established algorithms and technologies also to future space missions and for the scientific analysis of archived data. Here after will be given an in-depth analysis study accompanied by implementation examples on ESA and ASI missions and some remarkable results fruit of decades of important experience reached by space agencies and research institutes in the field. Space applications software quality analysis is not different from other application contexts, among the hi-tech and hi-reliability fields. We describe here a Software Quality study in general, then we will focus on the quality of space mission software (s/w) with details on some notable cases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.06248v2-abstract-full').style.display = 'none'; document.getElementById('2003.06248v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">presentation at XVI Congresso Nazionale di Scienze Planetarie (National Conference on Planetary Sciences) held at Centro Culturale San Gaetano, via Altinate, 71, Padova, Italy on 3-7 February, 2020 Affiliation: University of Padova, 6 pages, 4 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          85-04 (Primary); 85-05; 85-06 (Secondary)
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          D.2.2; B.8.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.04983">arXiv:2003.04983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.04983">pdf</a>, <a href="https://arxiv.org/format/2003.04983">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding the Downstream Instability of Word Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Leszczynski%2C+M">Megan Leszczynski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=May%2C+A">Avner May</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jian Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Sen Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aberger%2C+C+R">Christopher R. Aberger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.04983v1-abstract-short" style="display: inline;">
        Many industrial machine learning (ML) systems require frequent retraining to keep up-to-date with constantly changing data. This retraining exacerbates a large challenge facing ML systems today: model training is unstable, i.e., small changes in training data can cause significant changes in the model&#39;s predictions. In this paper, we work on developing a deeper understanding of this instability, w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.04983v1-abstract-full').style.display = 'inline'; document.getElementById('2003.04983v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.04983v1-abstract-full" style="display: none;">
        Many industrial machine learning (ML) systems require frequent retraining to keep up-to-date with constantly changing data. This retraining exacerbates a large challenge facing ML systems today: model training is unstable, i.e., small changes in training data can cause significant changes in the model&#39;s predictions. In this paper, we work on developing a deeper understanding of this instability, with a focus on how a core building block of modern natural language processing (NLP) pipelines---pre-trained word embeddings---affects the instability of downstream NLP models. We first empirically reveal a tradeoff between stability and memory: increasing the embedding memory 2x can reduce the disagreement in predictions due to small changes in training data by 5% to 37% (relative). To theoretically explain this tradeoff, we introduce a new measure of embedding instability---the eigenspace instability measure---which we prove bounds the disagreement in downstream predictions introduced by the change in word embeddings. Practically, we show that the eigenspace instability measure can be a cost-effective way to choose embedding parameters to minimize instability without training downstream models, outperforming other embedding distance measures and performing competitively with a nearest neighbor-based measure. Finally, we demonstrate that the observed stability-memory tradeoffs extend to other types of embeddings as well, including knowledge graph and contextual word embeddings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.04983v1-abstract-full').style.display = 'none'; document.getElementById('2003.04983v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 February, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In Proceedings of the 3rd MLSys Conference, 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.11955">arXiv:2002.11955</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.11955">pdf</a>, <a href="https://arxiv.org/format/2002.11955">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M+F">Mayee F. Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sala%2C+F">Frederic Sala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hooper%2C+S+M">Sarah M. Hooper</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fatahalian%2C+K">Kayvon Fatahalian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.11955v2-abstract-short" style="display: inline;">
        Weak supervision is a popular method for building machine learning models without relying on ground truth annotations. Instead, it generates probabilistic training labels by estimating the accuracies of multiple noisy labeling sources (e.g., heuristics, crowd workers). Existing approaches use latent variable estimation to model the noisy sources, but these methods can be computationally expensive,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.11955v2-abstract-full').style.display = 'inline'; document.getElementById('2002.11955v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.11955v2-abstract-full" style="display: none;">
        Weak supervision is a popular method for building machine learning models without relying on ground truth annotations. Instead, it generates probabilistic training labels by estimating the accuracies of multiple noisy labeling sources (e.g., heuristics, crowd workers). Existing approaches use latent variable estimation to model the noisy sources, but these methods can be computationally expensive, scaling superlinearly in the data. In this work, we show that, for a class of latent variable models highly applicable to weak supervision, we can find a closed-form solution to model parameters, obviating the need for iterative solutions like stochastic gradient descent (SGD). We use this insight to build FlyingSquid, a weak supervision framework that runs orders of magnitude faster than previous weak supervision approaches and requires fewer assumptions. In particular, we prove bounds on generalization error without assuming that the latent variable model can exactly parameterize the underlying data distribution. Empirically, we validate FlyingSquid on benchmark weak supervision datasets and find that it achieves the same or higher quality compared to previous approaches without the need to tune an SGD procedure, recovers model parameters 170 times faster on average, and enables new video analysis and online learning applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.11955v2-abstract-full').style.display = 'none'; document.getElementById('2002.11955v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 July, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 February, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.12933">arXiv:1910.12933</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.12933">pdf</a>, <a href="https://arxiv.org/format/1910.12933">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hyperbolic Graph Convolutional Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chami%2C+I">Ines Chami</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ying%2C+R">Rex Ying</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Leskovec%2C+J">Jure Leskovec</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.12933v1-abstract-short" style="display: inline;">
        Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space, which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative, as it enables embeddings with much smaller distortion. However, extending GCNs to hyperbolic geometry presents several unique challenge&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.12933v1-abstract-full').style.display = 'inline'; document.getElementById('1910.12933v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.12933v1-abstract-full" style="display: none;">
        Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space, which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative, as it enables embeddings with much smaller distortion. However, extending GCNs to hyperbolic geometry presents several unique challenges because it is not clear how to define neural network operations, such as feature transformation and aggregation, in hyperbolic space. Furthermore, since input features are often Euclidean, it is unclear how to transform the features into hyperbolic embeddings with the right amount of curvature. Here we propose Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs. We derive GCN operations in the hyperboloid model of hyperbolic space and map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer. Experiments demonstrate that HGCN learns embeddings that preserve hierarchical structure, and leads to improved performance when compared to Euclidean analogs, even with very low dimensional embeddings: compared to state-of-the-art GCNs, HGCN achieves an error reduction of up to 63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node classification, also improving state-of-the art on the Pubmed dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.12933v1-abstract-full').style.display = 'none'; document.getElementById('1910.12933v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 October, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at Conference NeurIPS 2019. First 2 authors have equal contribution</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.09505">arXiv:1910.09505</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.09505">pdf</a>, <a href="https://arxiv.org/format/1910.09505">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Resolution Weak Supervision for Sequential Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sala%2C+F">Frederic Sala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Varma%2C+P">Paroma Varma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fries%2C+J">Jason Fries</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sagawa%2C+S">Shiori Sagawa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khattar%2C+S">Saelig Khattar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ramamoorthy%2C+A">Ashwini Ramamoorthy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+K">Ke Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fatahalian%2C+K">Kayvon Fatahalian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Priest%2C+J">James Priest</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.09505v1-abstract-short" style="display: inline;">
        Since manually labeling training data is slow and expensive, recent industrial and scientific research efforts have turned to weaker or noisier forms of supervision sources. However, existing weak supervision approaches fail to model multi-resolution sources for sequential data, like video, that can assign labels to individual elements or collections of elements in a sequence. A key challenge in w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.09505v1-abstract-full').style.display = 'inline'; document.getElementById('1910.09505v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.09505v1-abstract-full" style="display: none;">
        Since manually labeling training data is slow and expensive, recent industrial and scientific research efforts have turned to weaker or noisier forms of supervision sources. However, existing weak supervision approaches fail to model multi-resolution sources for sequential data, like video, that can assign labels to individual elements or collections of elements in a sequence. A key challenge in weak supervision is estimating the unknown accuracies and correlations of these sources without using labeled data. Multi-resolution sources exacerbate this challenge due to complex correlations and sample complexity that scales in the length of the sequence. We propose Dugong, the first framework to model multi-resolution weak supervision sources with complex correlations to assign probabilistic labels to training data. Theoretically, we prove that Dugong, under mild conditions, can uniquely recover the unobserved accuracy and correlation parameters and use parameter sharing to improve sample complexity. Our method assigns clinician-validated labels to population-scale biomedical video repositories, helping outperform traditional supervision by 36.8 F1 points and addressing a key use case where machine learning has been severely limited by the lack of expert labeled data. On average, Dugong improves over traditional supervision by 16.0 F1 points and existing weak supervision approaches by 24.2 F1 points across several video and sensor classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.09505v1-abstract-full').style.display = 'none'; document.getElementById('1910.09505v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 October, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2019 (Conference on Neural Information Processing Systems)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.05124">arXiv:1910.05124</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.05124">pdf</a>, <a href="https://arxiv.org/format/1910.05124">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PipeMare: Asynchronous Pipeline Parallel DNN Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+B">Bowen Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jian Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jonathan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aberger%2C+C+R">Christopher R. Aberger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=De+Sa%2C+C">Christopher De Sa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.05124v2-abstract-short" style="display: inline;">
        Pipeline parallelism (PP) when training neural networks enables larger models to be partitioned spatially, leading to both lower network communication and overall higher hardware utilization. Unfortunately, to preserve the statistical efficiency of sequential training, existing PP techniques sacrifice hardware efficiency by decreasing pipeline utilization or incurring extra memory costs. In this p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.05124v2-abstract-full').style.display = 'inline'; document.getElementById('1910.05124v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.05124v2-abstract-full" style="display: none;">
        Pipeline parallelism (PP) when training neural networks enables larger models to be partitioned spatially, leading to both lower network communication and overall higher hardware utilization. Unfortunately, to preserve the statistical efficiency of sequential training, existing PP techniques sacrifice hardware efficiency by decreasing pipeline utilization or incurring extra memory costs. In this paper, we investigate to what extent these sacrifices are necessary. We devise PipeMare, a simple yet robust training method that tolerates asynchronous updates during PP execution without sacrificing utilization or memory, which allows efficient use of fine-grained pipeline parallelism. Concretely, when tested on ResNet and Transformer networks, asynchrony enables PipeMare to use up to $2.7\times$ less memory or get $4.3\times$ higher pipeline utilization, with similar model quality, when compared to state-of-the-art synchronous PP training techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.05124v2-abstract-full').style.display = 'none'; document.getElementById('1910.05124v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 February, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 October, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=R%C3%A9%2C+C&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>