<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 71 results for author: <span class="mathjax">Hsu, Y</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Hsu%2C+Y">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Hsu, Y">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Hsu%2C+Y&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Hsu, Y">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+Y&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Hsu%2C+Y&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Hsu%2C+Y&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06382">arXiv:2204.06382</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06382">pdf</a>, <a href="https://arxiv.org/ps/2204.06382">ps</a>, <a href="https://arxiv.org/format/2204.06382">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Empathy-Centric Design At Scale
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mauri%2C+A">Andrea Mauri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brambilla%2C+M">Marco Brambilla</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Kane%2C+A+A">Aisling Ann O&#39;Kane</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T+%27">Ting-Hao &#39;Kenneth&#39; Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Verma%2C+H">Himanshu Verma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06382v1-abstract-short" style="display: inline;">
        EmpathiCH aims at bringing together and blend different expertise to develop new research agenda in the context of &#34;Empathy-Centric Design at Scale&#34;. The main research question is to investigate how new technologies can contribute to the elicitation of empathy across and within multiple stakeholders at scale; and how empathy can be used to design solutions to societal problems that are not only ef&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06382v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06382v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06382v1-abstract-full" style="display: none;">
        EmpathiCH aims at bringing together and blend different expertise to develop new research agenda in the context of &#34;Empathy-Centric Design at Scale&#34;. The main research question is to investigate how new technologies can contribute to the elicitation of empathy across and within multiple stakeholders at scale; and how empathy can be used to design solutions to societal problems that are not only effective but also balanced, inclusive, and aware of their effect on society. Through presentations, participatory sessions, and a living experiment -- where data about the peoples&#39; interactions is collected throughout the event -- we aim to make this workshop the ideal venue to foster collaboration, build networks, and shape the future direction of &#34;Empathy-Centric Design at Scale&#34;.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06382v1-abstract-full').style.display = 'none'; document.getElementById('2204.06382v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted at Workshops at the 2022 CHI Conference on Human Factors in Computing Systems (CHI 2022)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06289">arXiv:2204.06289</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06289">pdf</a>, <a href="https://arxiv.org/format/2204.06289">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        COCTEAU: an Empathy-Based Tool for Decision-Making
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mauri%2C+A">Andrea Mauri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tocchetti%2C+A">Andrea Tocchetti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Corti%2C+L">Lorenzo Corti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Verma%2C+H">Himanshu Verma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brambilla%2C+M">Marco Brambilla</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06289v1-abstract-short" style="display: inline;">
        Traditional approaches to data-informed policymaking are often tailored to specific contexts and lack strong citizen involvement and collaboration, which are required to design sustainable policies. We argue the importance of empathy-based methods in the policymaking domain given the successes in diverse settings, such as healthcare and education. In this paper, we introduce COCTEAU (Co-Creating T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06289v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06289v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06289v1-abstract-full" style="display: none;">
        Traditional approaches to data-informed policymaking are often tailored to specific contexts and lack strong citizen involvement and collaboration, which are required to design sustainable policies. We argue the importance of empathy-based methods in the policymaking domain given the successes in diverse settings, such as healthcare and education. In this paper, we introduce COCTEAU (Co-Creating The European Union), a novel framework built on the combination of empathy and gamification to create a tool aimed at strengthening interactions between citizens and policy-makers. We describe our design process and our concrete implementation, which has already undergone preliminary assessments with different stakeholders. Moreover, we briefly report pilot results from the assessment. Finally, we describe the structure and goals of our demonstration regarding the newfound formats and organizational aspects of academic conferences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06289v1-abstract-full').style.display = 'none'; document.getElementById('2204.06289v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted at Posters and Demos Track at The Web Conference 2022 (WWW 2022)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.17269">arXiv:2203.17269</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.17269">pdf</a>, <a href="https://arxiv.org/format/2203.17269">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Closer Look at Rehearsal-Free Continual Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Smith%2C+J+S">James Seale Smith</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+J">Junjiao Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.17269v1-abstract-short" style="display: inline;">
        Continual learning describes a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes (a phenomenon known as the catastrophic forgetting problem) which may disappear from the training data for extended periods of time. Current approaches for continual learning of a single&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.17269v1-abstract-full').style.display = 'inline'; document.getElementById('2203.17269v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.17269v1-abstract-full" style="display: none;">
        Continual learning describes a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes (a phenomenon known as the catastrophic forgetting problem) which may disappear from the training data for extended periods of time. Current approaches for continual learning of a single expanding task (aka class-incremental continual learning) require extensive rehearsal of previously seen data to avoid this degradation of knowledge. Unfortunately, rehearsal comes at a sharp cost to memory and computation, and it may also violate data-privacy. Instead, we explore combining knowledge distillation and parameter regularization in new ways to achieve strong continual learning performance without rehearsal. Specifically, we take a deep dive into common continual learning techniques: prediction distillation, feature distillation, L2 parameter regularization, and EWC parameter regularization. We first disprove the common assumption that parameter regularization techniques fail for rehearsal-free continual learning of a single, expanding task. Next, we explore how to leverage knowledge from a pre-trained model in rehearsal-free continual learning and find that vanilla L2 parameter regularization outperforms EWC parameter regularization and feature distillation. We then highlight the impact of the rehearsal-free continual learning settings with a classifier expansion benchmark, showing that a strategy based on our findings combined with a positive/negative label balancing heuristic can close the performance gap between the upper bound and the existing strategies by up to roughly 50%. Finally, we show that a simple method consisting of pre-training, L2 regularization, and prediction distillation can even outperform rehearsal-based methods on the common CIFAR-100 benchmark.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.17269v1-abstract-full').style.display = 'none'; document.getElementById('2203.17269v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10163">arXiv:2203.10163</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10163">pdf</a>, <a href="https://arxiv.org/format/2203.10163">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Closer Look at Knowledge Distillation with Features, Logits, and Gradients
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Smith%2C+J">James Smith</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yilin Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hongxia Jin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10163v1-abstract-short" style="display: inline;">
        Knowledge distillation (KD) is a substantial strategy for transferring learned knowledge from one neural network model to another. A vast number of methods have been developed for this strategy. While most method designs a more efficient way to facilitate knowledge transfer, less attention has been put on comparing the effect of knowledge sources such as features, logits, and gradients. This work&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10163v1-abstract-full').style.display = 'inline'; document.getElementById('2203.10163v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10163v1-abstract-full" style="display: none;">
        Knowledge distillation (KD) is a substantial strategy for transferring learned knowledge from one neural network model to another. A vast number of methods have been developed for this strategy. While most method designs a more efficient way to facilitate knowledge transfer, less attention has been put on comparing the effect of knowledge sources such as features, logits, and gradients. This work provides a new perspective to motivate a set of knowledge distillation strategies by approximating the classical KL-divergence criteria with different knowledge sources, making a systematic comparison possible in model compression and incremental learning. Our analysis indicates that logits are generally a more efficient knowledge source and suggests that having sufficient feature dimensions is crucial for the model design, providing a practical guideline for effective KD-based transfer learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10163v1-abstract-full').style.display = 'none'; document.getElementById('2203.10163v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.01420">arXiv:2201.01420</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.01420">pdf</a>, <a href="https://arxiv.org/ps/2201.01420">ps</a>, <a href="https://arxiv.org/format/2201.01420">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/2021.naacl-main.212">10.18653/v1/2021.naacl-main.212 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hua%2C+T">Ting Hua</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yilin Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+C">Changsheng Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hongxia Jin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.01420v1-abstract-short" style="display: inline;">
        Domain classification is the fundamental task in natural language understanding (NLU), which often requires fast accommodation to new emerging domains. This constraint makes it impossible to retrain all previous domains, even if they are accessible to the new model. Most existing continual learning approaches suffer from low accuracy and performance fluctuation, especially when the distributions o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.01420v1-abstract-full').style.display = 'inline'; document.getElementById('2201.01420v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.01420v1-abstract-full" style="display: none;">
        Domain classification is the fundamental task in natural language understanding (NLU), which often requires fast accommodation to new emerging domains. This constraint makes it impossible to retrain all previous domains, even if they are accessible to the new model. Most existing continual learning approaches suffer from low accuracy and performance fluctuation, especially when the distributions of old and new data are significantly different. In fact, the key real-world problem is not the absence of old data, but the inefficiency to retrain the model with the whole old dataset. Is it potential to utilize some old data to yield high accuracy and maintain stable performance, while at the same time, without introducing extra hyperparameters? In this paper, we proposed a hyperparameter-free continual learning model for text data that can stably produce high performance under various environments. Specifically, we utilize Fisher information to select exemplars that can &#34;record&#34; key information of the original model. Also, a novel scheme called dynamical weight consolidation is proposed to enable hyperparameter-free learning during the retrain process. Extensive experiments demonstrate that baselines suffer from fluctuated performance and therefore useless in practice. On the contrary, our proposed model CCFI significantly and consistently outperforms the best state-of-the-art method by up to 20% in average accuracy, and each component of CCFI contributes effectively to overall performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.01420v1-abstract-full').style.display = 'none'; document.getElementById('2201.01420v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,pages 2669--2678
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.05686">arXiv:2112.05686</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.05686">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICASSP43922.2022.974685910.1109/ICASSP43922.2022.9746859">10.1109/ICASSP43922.2022.974685910.1109/ICASSP43922.2022.9746859 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning-based personal speech enhancement for teleconferencing by exploiting spatial-spectral features
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yicheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+Y">Yonghan Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bai%2C+M+R">Mingsian R. Bai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.05686v3-abstract-short" style="display: inline;">
        Teleconferencing is becoming essential during the COVID-19 pandemic. However, in real-world applications, speech quality can deteriorate due to, for example, background interference, noise, or reverberation. To solve this problem, target speech extraction from the mixture signals can be performed with the aid of the user&#39;s vocal features. Various features are accounted for in this study&#39;s proposed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.05686v3-abstract-full').style.display = 'inline'; document.getElementById('2112.05686v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.05686v3-abstract-full" style="display: none;">
        Teleconferencing is becoming essential during the COVID-19 pandemic. However, in real-world applications, speech quality can deteriorate due to, for example, background interference, noise, or reverberation. To solve this problem, target speech extraction from the mixture signals can be performed with the aid of the user&#39;s vocal features. Various features are accounted for in this study&#39;s proposed system, including speaker embeddings derived from user enrollment and a novel long-short-term spatial coherence feature pertaining to the target speaker activity. As a learning-based approach, a target speech sifting network was employed to extract the relevant features. The network trained with LSTSC in the proposed approach is robust to microphone array geometries and the number of microphones. Furthermore, the proposed enhancement system was compared with a baseline system with speaker embeddings and interchannel phase difference. The results demonstrated the superior performance of the proposed system over the baseline in enhancement performance and robustness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.05686v3-abstract-full').style.display = 'none'; document.getElementById('2112.05686v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted by ICASSP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.00894">arXiv:2112.00894</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.00894">pdf</a>, <a href="https://arxiv.org/format/2112.00894">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Context-Dependent Semantic Parsing for Temporal Relation Extraction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+B">Bo-Ying Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+S">Shang-Ling Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lai%2C+K">Kuan-Yin Lai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+J+Y">Jane Yung-jen Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.00894v1-abstract-short" style="display: inline;">
        Extracting temporal relations among events from unstructured text has extensive applications, such as temporal reasoning and question answering. While it is difficult, recent development of Neural-symbolic methods has shown promising results on solving similar tasks. Current temporal relation extraction methods usually suffer from limited expressivity and inconsistent relation inference. For examp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.00894v1-abstract-full').style.display = 'inline'; document.getElementById('2112.00894v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.00894v1-abstract-full" style="display: none;">
        Extracting temporal relations among events from unstructured text has extensive applications, such as temporal reasoning and question answering. While it is difficult, recent development of Neural-symbolic methods has shown promising results on solving similar tasks. Current temporal relation extraction methods usually suffer from limited expressivity and inconsistent relation inference. For example, in TimeML annotations, the concept of intersection is absent. Additionally, current methods do not guarantee the consistency among the predicted annotations. In this work, we propose SMARTER, a neural semantic parser, to extract temporal information in text effectively. SMARTER parses natural language to an executable logical form representation, based on a custom typed lambda calculus. In the training phase, dynamic programming on denotations (DPD) technique is used to provide weak supervision on logical forms. In the inference phase, SMARTER generates a temporal relation graph by executing the logical form. As a result, our neural semantic parser produces logical forms capturing the temporal information of text precisely. The accurate logical form representations of an event given the context ensure the correctness of the extracted relations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.00894v1-abstract-full').style.display = 'none'; document.getElementById('2112.00894v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.08543">arXiv:2111.08543</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.08543">pdf</a>, <a href="https://arxiv.org/format/2111.08543">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WikiContradiction: Detecting Self-Contradiction Articles on Wikipedia
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cheng-Te Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saez-Trumper%2C+D">Diego Saez-Trumper</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yi-Zhan Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.08543v1-abstract-short" style="display: inline;">
        While Wikipedia has been utilized for fact-checking and claim verification to debunk misinformation and disinformation, it is essential to either improve article quality and rule out noisy articles. Self-contradiction is one of the low-quality article types in Wikipedia. In this work, we propose a task of detecting self-contradiction articles in Wikipedia. Based on the &#34;self-contradictory&#34; templat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08543v1-abstract-full').style.display = 'inline'; document.getElementById('2111.08543v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.08543v1-abstract-full" style="display: none;">
        While Wikipedia has been utilized for fact-checking and claim verification to debunk misinformation and disinformation, it is essential to either improve article quality and rule out noisy articles. Self-contradiction is one of the low-quality article types in Wikipedia. In this work, we propose a task of detecting self-contradiction articles in Wikipedia. Based on the &#34;self-contradictory&#34; template, we create a novel dataset for the self-contradiction detection task. Conventional contradiction detection focuses on comparing pairs of sentences or claims, but self-contradiction detection needs to further reason the semantics of an article and simultaneously learn the contradiction-aware comparison from all pairs of sentences. Therefore, we present the first model, Pairwise Contradiction Neural Network (PCNN), to not only effectively identify self-contradiction articles, but also highlight the most contradiction pairs of contradiction sentences. The main idea of PCNN is two-fold. First, to mitigate the effect of data scarcity on self-contradiction articles, we pre-train the module of pairwise contradiction learning using SNLI and MNLI benchmarks. Second, we select top-K sentence pairs with the highest contradiction probability values and model their correlation to determine whether the corresponding article belongs to self-contradiction. Experiments conducted on the proposed WikiContradiction dataset exhibit that PCNN can generate promising performance and comprehensively highlight the sentence pairs the contradiction locates.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08543v1-abstract-full').style.display = 'none'; document.getElementById('2111.08543v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at IEEE BigData 2021 (regular paper). Data and code can be access via: https://github.com/Wiki-Contradictory/Wiki-Self-Contradictory/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.04436">arXiv:2111.04436</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.04436">pdf</a>, <a href="https://arxiv.org/format/2111.04436">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SEOFP-NET: Compression and Acceleration of Deep Neural Networks for Speech Enhancement Using Sign-Exponent-Only Floating-Points
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yu-Chen Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+C">Cheng Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yi-Te Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+S">Szu-Wei Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsao%2C+Y">Yu Tsao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kuo%2C+T">Tei-Wei Kuo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.04436v1-abstract-short" style="display: inline;">
        Numerous compression and acceleration strategies have achieved outstanding results on classification tasks in various fields, such as computer vision and speech signal processing. Nevertheless, the same strategies have yielded ungratified performance on regression tasks because the nature between these and classification tasks differs. In this paper, a novel sign-exponent-only floating-point netwo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.04436v1-abstract-full').style.display = 'inline'; document.getElementById('2111.04436v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.04436v1-abstract-full" style="display: none;">
        Numerous compression and acceleration strategies have achieved outstanding results on classification tasks in various fields, such as computer vision and speech signal processing. Nevertheless, the same strategies have yielded ungratified performance on regression tasks because the nature between these and classification tasks differs. In this paper, a novel sign-exponent-only floating-point network (SEOFP-NET) technique is proposed to compress the model size and accelerate the inference time for speech enhancement, a regression task of speech signal processing. The proposed method compressed the sizes of deep neural network (DNN)-based speech enhancement models by quantizing the fraction bits of single-precision floating-point parameters during training. Before inference implementation, all parameters in the trained SEOFP-NET model are slightly adjusted to accelerate the inference time by replacing the floating-point multiplier with an integer-adder. For generalization, the SEOFP-NET technique is introduced to different speech enhancement tasks in speech signal processing with different model architectures under various corpora. The experimental results indicate that the size of SEOFP-NET models can be significantly compressed by up to 81.249% without noticeably downgrading their speech enhancement performance, and the inference time can be accelerated to 1.212x compared with the baseline models. The results also verify that the proposed SEOFP-NET can cooperate with other efficiency strategies to achieve a synergy effect for model compression. In addition, the just noticeable difference (JND) was applied to the user study experiment to statistically analyze the effect of speech enhancement on listening. The results indicate that the listeners cannot facilely differentiate between the enhanced speech signals processed by the baseline model and the proposed SEOFP-NET.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.04436v1-abstract-full').style.display = 'none'; document.getElementById('2111.04436v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.00844">arXiv:2111.00844</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.00844">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Non-Autoregressive End-To-End Neural Modeling For English Mispronunciation Detection And Diagnosis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hsin-Wei Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+B">Bi-Cheng Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chiu%2C+H">Hsuan-Sheng Chiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yung-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Berlin Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.00844v2-abstract-short" style="display: inline;">
        End-to-end (E2E) neural modeling has emerged as one predominant school of thought to develop computer-assisted language training (CAPT) systems, showing competitive performance to conventional pronunciation-scoring based methods. However, current E2E neural methods for CAPT are faced with at least two pivotal challenges. On one hand, most of the E2E methods operate in an autoregressive manner with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.00844v2-abstract-full').style.display = 'inline'; document.getElementById('2111.00844v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.00844v2-abstract-full" style="display: none;">
        End-to-end (E2E) neural modeling has emerged as one predominant school of thought to develop computer-assisted language training (CAPT) systems, showing competitive performance to conventional pronunciation-scoring based methods. However, current E2E neural methods for CAPT are faced with at least two pivotal challenges. On one hand, most of the E2E methods operate in an autoregressive manner with left-to-right beam search to dictate the pronunciations of an L2 learners. This however leads to very slow inference speed, which inevitably hinders their practical use. On the other hand, E2E neural methods are normally data greedy and meanwhile an insufficient amount of nonnative training data would often reduce their efficacy on mispronunciation detection and diagnosis (MD&amp;D). In response, we put forward a novel MD&amp;D method that leverages non-autoregressive (NAR) E2E neural modeling to dramatically speed up the inference time while maintaining performance in line with the conventional E2E neural methods. In addition, we design and develop a pronunciation modeling network stacked on top of the NAR E2E models of our method to further boost the effectiveness of MD&amp;D. Empirical experiments conducted on the L2-ARCTIC English dataset seems to validate the feasibility of our method, in comparison to some top-of-the-line E2E models and an iconic pronunciation-scoring based method built on a DNN-HMM acoustic model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.00844v2-abstract-full').style.display = 'none'; document.getElementById('2111.00844v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for ICASSP2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.15231">arXiv:2110.15231</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.15231">pdf</a>, <a href="https://arxiv.org/format/2110.15231">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Covariate and Concept Shift for Detection and Calibration of Out-of-Distribution Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+J">Junjiao Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Change Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yilin Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hongxia Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.15231v2-abstract-short" style="display: inline;">
        Moving beyond testing on in-distribution data works on Out-of-Distribution (OOD) detection have recently increased in popularity. A recent attempt to categorize OOD data introduces the concept of near and far OOD detection. Specifically, prior works define characteristics of OOD data in terms of detection difficulty. We propose to characterize the spectrum of OOD data using two types of distributi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15231v2-abstract-full').style.display = 'inline'; document.getElementById('2110.15231v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.15231v2-abstract-full" style="display: none;">
        Moving beyond testing on in-distribution data works on Out-of-Distribution (OOD) detection have recently increased in popularity. A recent attempt to categorize OOD data introduces the concept of near and far OOD detection. Specifically, prior works define characteristics of OOD data in terms of detection difficulty. We propose to characterize the spectrum of OOD data using two types of distribution shifts: covariate shift and concept shift, where covariate shift corresponds to change in style, e.g., noise, and concept shift indicates a change in semantics. This characterization reveals that sensitivity to each type of shift is important to the detection and confidence calibration of OOD data. Consequently, we investigate score functions that capture sensitivity to each type of dataset shift and methods that improve them. To this end, we theoretically derive two score functions for OOD detection, the covariate shift score and concept shift score, based on the decomposition of KL-divergence for both scores, and propose a geometrically-inspired method (Geometric ODIN) to improve OOD detection under both shifts with only in-distribution data. Additionally, the proposed method naturally leads to an expressive post-hoc calibration function which yields state-of-the-art calibration performance on both in-distribution and out-of-distribution data. We are the first to propose a method that works well across both OOD detection and calibration and under different types of shifts. View project page at https://sites.google.com/view/geometric-decomposition.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15231v2-abstract-full').style.display = 'none'; document.getElementById('2110.15231v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">A short version of the paper is accepted to NeurIPS DistShift Workshop 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.14577">arXiv:2110.14577</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.14577">pdf</a>, <a href="https://arxiv.org/format/2110.14577">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Geometric Perspective towards Neural Calibration via Sensitivity Decomposition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+J">Junjiao Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yung%2C+D">Dylan Yung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.14577v3-abstract-short" style="display: inline;">
        It is well known that vision classification models suffer from poor calibration in the face of data distribution shifts. In this paper, we take a geometric approach to this problem. We propose Geometric Sensitivity Decomposition (GSD) which decomposes the norm of a sample feature embedding and the angular similarity to a target classifier into an instance-dependent and an instance-independent comp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.14577v3-abstract-full').style.display = 'inline'; document.getElementById('2110.14577v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.14577v3-abstract-full" style="display: none;">
        It is well known that vision classification models suffer from poor calibration in the face of data distribution shifts. In this paper, we take a geometric approach to this problem. We propose Geometric Sensitivity Decomposition (GSD) which decomposes the norm of a sample feature embedding and the angular similarity to a target classifier into an instance-dependent and an instance-independent component. The instance-dependent component captures the sensitive information about changes in the input while the instance-independent component represents the insensitive information serving solely to minimize the loss on the training dataset. Inspired by the decomposition, we analytically derive a simple extension to current softmax-linear models, which learns to disentangle the two components during training. On several common vision models, the disentangled model outperforms other calibration methods on standard calibration metrics in the face of out-of-distribution (OOD) data and corruption with significantly less complexity. Specifically, we surpass the current state of the art by 30.8% relative improvement on corrupted CIFAR100 in Expected Calibration Error. Code available at https://github.com/GT-RIPL/Geometric-Sensitivity-Decomposition.git.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.14577v3-abstract-full').style.display = 'none'; document.getElementById('2110.14577v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at NeurIPS 2021 as a spotlight paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.06848">arXiv:2110.06848</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.06848">pdf</a>, <a href="https://arxiv.org/format/2110.06848">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decoupled Contrastive Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yeh%2C+C">Chun-Hsiao Yeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+C">Cheng-Yao Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chi Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+T">Tyng-Luh Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yubei Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=LeCun%2C+Y">Yann LeCun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.06848v2-abstract-short" style="display: inline;">
        Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented &#34;views&#34; of the same image as positive to be pulled closer, and all other images negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, includin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.06848v2-abstract-full').style.display = 'inline'; document.getElementById('2110.06848v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.06848v2-abstract-full" style="display: none;">
        Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented &#34;views&#34; of the same image as positive to be pulled closer, and all other images negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and aim at establishing a simple, efficient, and yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used cross-entropy (InfoNCE) loss, leading to unsuitable learning efficiency with respect to the batch size. Indeed the phenomenon tends to be neglected in that optimizing infoNCE loss with a small-size batch is effective in solving easier SSL tasks. By properly addressing the NPC effect, we reach a decoupled contrastive learning (DCL) objective function, significantly improving SSL efficiency. DCL can achieve competitive performance, requiring neither large batches in SimCLR, momentum encoding in MoCo, or large epochs. We demonstrate the usefulness of DCL in various benchmarks, while manifesting its robustness being much less sensitive to suboptimal hyperparameters. Notably, our approach achieves $66.9\%$ ImageNet top-1 accuracy using batch size 256 within 200 epochs pre-training, outperforming its baseline SimCLR by $5.1\%$. With further optimized hyperparameters, DCL can improve the accuracy to $68.2\%$. We believe DCL provides a valuable baseline for future contrastive learning-based SSL studies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.06848v2-abstract-full').style.display = 'none'; document.getElementById('2110.06848v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 October, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.02007">arXiv:2110.02007</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.02007">pdf</a>, <a href="https://arxiv.org/format/2110.02007">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.patter.2022.100449">10.1016/j.patter.2022.100449 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Empowering Local Communities Using Artificial Intelligence
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T+%27">Ting-Hao &#39;Kenneth&#39; Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Verma%2C+H">Himanshu Verma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mauri%2C+A">Andrea Mauri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nourbakhsh%2C+I">Illah Nourbakhsh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bozzon%2C+A">Alessandro Bozzon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.02007v3-abstract-short" style="display: inline;">
        Artificial Intelligence (AI) is increasingly used to analyze large amounts of data in various practices, such as object recognition. We are specifically interested in using AI-powered systems to engage local communities in developing plans or solutions for pressing societal and environmental concerns. Such local contexts often involve multiple stakeholders with different and even contradictory age&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.02007v3-abstract-full').style.display = 'inline'; document.getElementById('2110.02007v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.02007v3-abstract-full" style="display: none;">
        Artificial Intelligence (AI) is increasingly used to analyze large amounts of data in various practices, such as object recognition. We are specifically interested in using AI-powered systems to engage local communities in developing plans or solutions for pressing societal and environmental concerns. Such local contexts often involve multiple stakeholders with different and even contradictory agendas, resulting in mismatched expectations of these systems&#39; behaviors and desired outcomes. There is a need to investigate if AI models and pipelines can work as expected in different contexts through co-creation and field deployment. Based on case studies in co-creating AI-powered systems with local people, we explain challenges that require more attention and provide viable paths to bridge AI research with citizen needs. We advocate for developing new collaboration approaches and mindsets that are needed to co-create AI-powered systems in multi-stakeholder contexts to address local concerns.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.02007v3-abstract-full').style.display = 'none'; document.getElementById('2110.02007v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This manuscript is peer-reviewed and accepted by the Patterns journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.09448">arXiv:2108.09448</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.09448">pdf</a>, <a href="https://arxiv.org/format/2108.09448">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Thing Constellation Visualizer: Exploring Emergent Relationships of Everyday Objects
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y+%27">Yi-Ching &#39;Janet&#39; Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+Y">Yu-Ting Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+R">Rung-Huei Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+J+Y">Jane Yung-jen Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+L">Lin-Lin Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.09448v3-abstract-short" style="display: inline;">
        Designing future IoT ecosystems requires new approaches and perspectives to understand everyday practices. While researchers recognize the importance of understanding social aspects of everyday objects, limited studies have explored the possibilities of combining data-driven patterns with human interpretations to investigate emergent relationships among objects. This work presents Thing Constellat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.09448v3-abstract-full').style.display = 'inline'; document.getElementById('2108.09448v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.09448v3-abstract-full" style="display: none;">
        Designing future IoT ecosystems requires new approaches and perspectives to understand everyday practices. While researchers recognize the importance of understanding social aspects of everyday objects, limited studies have explored the possibilities of combining data-driven patterns with human interpretations to investigate emergent relationships among objects. This work presents Thing Constellation Visualizer (thingCV), a novel interactive tool for visualizing the social network of objects based on their co-occurrence as computed from a large collection of photos. ThingCV enables perspective-changing design explorations over the network of objects with scalable links. Two exploratory workshops were conducted to investigate how designers navigate and make sense of a network of objects through thingCV. The results of eight participants showed that designers were actively engaged in identifying interesting objects and their associated clusters of related objects. The designers projected social qualities onto the identified objects and their communities. Furthermore, the designers changed their perspectives to revisit familiar contexts and to generate new insights through the exploration process. This work contributes a novel approach to combining data-driven models with designerly interpretations of thing constellation towards More-Than Human-Centred Design of IoT ecosystems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.09448v3-abstract-full').style.display = 'none'; document.getElementById('2108.09448v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CSCW 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.13760">arXiv:2107.13760</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.13760">pdf</a>, <a href="https://arxiv.org/format/2107.13760">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Viewpoint-Invariant Exercise Repetition Counting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y+C">Yu Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Q">Qingpeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsougenis%2C+E">Efstratios Tsougenis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsui%2C+K">Kwok-Leung Tsui</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.13760v1-abstract-short" style="display: inline;">
        Counting the repetition of human exercise and physical rehabilitation is a common task in rehabilitation and exercise training. The existing vision-based repetition counting methods less emphasize the concurrent motions in the same video. This work presents a vision-based human motion repetition counting applicable to counting concurrent motions through the skeleton location extracted from various&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.13760v1-abstract-full').style.display = 'inline'; document.getElementById('2107.13760v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.13760v1-abstract-full" style="display: none;">
        Counting the repetition of human exercise and physical rehabilitation is a common task in rehabilitation and exercise training. The existing vision-based repetition counting methods less emphasize the concurrent motions in the same video. This work presents a vision-based human motion repetition counting applicable to counting concurrent motions through the skeleton location extracted from various pose estimation methods. The presented method was validated on the University of Idaho Physical Rehabilitation Movements Data Set (UI-PRMD), and MM-fit dataset. The overall mean absolute error (MAE) for mm-fit was 0.06 with off-by-one Accuracy (OBOA) 0.94. Overall MAE for UI-PRMD dataset was 0.06 with OBOA 0.95. We have also tested the performance in a variety of camera locations and concurrent motions with conveniently collected video with overall MAE 0.06 and OBOA 0.88. The proposed method provides a view-angle and motion agnostic concurrent motion counting. This method can potentially use in large-scale remote rehabilitation and exercise training with only one camera.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.13760v1-abstract-full').style.display = 'none'; document.getElementById('2107.13760v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.14464">arXiv:2106.14464</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.14464">pdf</a>, <a href="https://arxiv.org/format/2106.14464">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing the Generalization for Intent Classification and Out-of-Domain Detection in SLU
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yilin Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ray%2C+A">Avik Ray</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hongxia Jin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.14464v1-abstract-short" style="display: inline;">
        Intent classification is a major task in spoken language understanding (SLU). Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use. Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to colle&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14464v1-abstract-full').style.display = 'inline'; document.getElementById('2106.14464v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.14464v1-abstract-full" style="display: none;">
        Intent classification is a major task in spoken language understanding (SLU). Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use. Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to collect such data. This paper proposes to train a model with only IND data while supporting both IND intent classification and OOD detection. Our method designs a novel domain-regularized module (DRM) to reduce the overconfident phenomenon of a vanilla classifier, achieving a better generalization in both cases. Besides, DRM can be used as a drop-in replacement for the last layer in any neural network-based intent classifier, providing a low-cost strategy for a significant improvement. The evaluation on four datasets shows that our method built on BERT and RoBERTa models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.14464v1-abstract-full').style.display = 'none'; document.getElementById('2106.14464v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.10853">arXiv:2106.10853</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.10853">pdf</a>, <a href="https://arxiv.org/format/2106.10853">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Importance of Environments in Human-Robot Coordination
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fontaine%2C+M+C">Matthew C. Fontaine</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Ya-Chuan Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yulun Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tjanaka%2C+B">Bryon Tjanaka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nikolaidis%2C+S">Stefanos Nikolaidis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.10853v2-abstract-short" style="display: inline;">
        When studying robots collaborating with humans, much of the focus has been on robot policies that coordinate fluently with human teammates in collaborative tasks. However, less emphasis has been placed on the effect of the environment on coordination behaviors. To thoroughly explore environments that result in diverse behaviors, we propose a framework for procedural generation of environments that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10853v2-abstract-full').style.display = 'inline'; document.getElementById('2106.10853v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.10853v2-abstract-full" style="display: none;">
        When studying robots collaborating with humans, much of the focus has been on robot policies that coordinate fluently with human teammates in collaborative tasks. However, less emphasis has been placed on the effect of the environment on coordination behaviors. To thoroughly explore environments that result in diverse behaviors, we propose a framework for procedural generation of environments that are (1) stylistically similar to human-authored environments, (2) guaranteed to be solvable by the human-robot team, and (3) diverse with respect to coordination measures. We analyze the procedurally generated environments in the Overcooked benchmark domain via simulation and an online user study. Results show that the environments result in qualitatively different emerging behaviors and statistically significant differences in collaborative fluency metrics, even when the robot runs the same planning algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10853v2-abstract-full').style.display = 'none'; document.getElementById('2106.10853v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Robotics: Science and Systems (RSS) 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.10159">arXiv:2106.10159</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.10159">pdf</a>, <a href="https://arxiv.org/format/2106.10159">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yi-Ling Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsai%2C+Y">Yu-Che Tsai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cheng-Te Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.10159v1-abstract-short" style="display: inline;">
        Financial technology (FinTech) has drawn much attention among investors and companies. While conventional stock analysis in FinTech targets at predicting stock prices, less effort is made for profitable stock recommendation. Besides, in existing approaches on modeling time series of stock prices, the relationships among stocks and sectors (i.e., categories of stocks) are either neglected or pre-de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10159v1-abstract-full').style.display = 'inline'; document.getElementById('2106.10159v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.10159v1-abstract-full" style="display: none;">
        Financial technology (FinTech) has drawn much attention among investors and companies. While conventional stock analysis in FinTech targets at predicting stock prices, less effort is made for profitable stock recommendation. Besides, in existing approaches on modeling time series of stock prices, the relationships among stocks and sectors (i.e., categories of stocks) are either neglected or pre-defined. Ignoring stock relationships will miss the information shared between stocks while using pre-defined relationships cannot depict the latent interactions or influence of stock prices between stocks. In this work, we aim at recommending the top-K profitable stocks in terms of return ratio using time series of stock prices and sector information. We propose a novel deep learning-based model, Financial Graph Attention Networks (FinGAT), to tackle the task under the setting that no pre-defined relationships between stocks are given. The idea of FinGAT is three-fold. First, we devise a hierarchical learning component to learn short-term and long-term sequential patterns from stock time series. Second, a fully-connected graph between stocks and a fully-connected graph between sectors are constructed, along with graph attention networks, to learn the latent interactions among stocks and sectors. Third, a multi-task objective is devised to jointly recommend the profitable stocks and predict the stock movement. Experiments conducted on Taiwan Stock, S&amp;P 500, and NASDAQ datasets exhibit remarkable recommendation performance of our FinGAT, comparing to state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.10159v1-abstract-full').style.display = 'none'; document.getElementById('2106.10159v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE TKDE 2021. The first two authors equally contribute to this work. Code is available at https://github.com/Roytsai27/Financial-GraphAttention</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.09701">arXiv:2106.09701</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.09701">pdf</a>, <a href="https://arxiv.org/format/2106.09701">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Smith%2C+J">James Smith</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Balloch%2C+J">Jonathan Balloch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yilin Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hongxia Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.09701v2-abstract-short" style="display: inline;">
        Modern computer vision applications suffer from catastrophic forgetting when incrementally learning new concepts over time. The most successful approaches to alleviate this forgetting require extensive replay of previously seen data, which is problematic when memory constraints or data legality concerns exist. In this work, we consider the high-impact problem of Data-Free Class-Incremental Learnin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.09701v2-abstract-full').style.display = 'inline'; document.getElementById('2106.09701v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.09701v2-abstract-full" style="display: none;">
        Modern computer vision applications suffer from catastrophic forgetting when incrementally learning new concepts over time. The most successful approaches to alleviate this forgetting require extensive replay of previously seen data, which is problematic when memory constraints or data legality concerns exist. In this work, we consider the high-impact problem of Data-Free Class-Incremental Learning (DFCIL), where an incremental learning agent must learn new concepts over time without storing generators or training data from past tasks. One approach for DFCIL is to replay synthetic images produced by inverting a frozen copy of the learner&#39;s classification model, but we show this approach fails for common class-incremental benchmarks when using standard distillation strategies. We diagnose the cause of this failure and propose a novel incremental distillation strategy for DFCIL, contributing a modified cross-entropy training and importance-weighted feature distillation, and show that our method results in up to a 25.1% increase in final task accuracy (absolute difference) compared to SOTA DFCIL methods for common class-incremental benchmarks. Our method even outperforms several standard replay based methods which store a coreset of images.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.09701v2-abstract-full').style.display = 'none'; document.getElementById('2106.09701v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by the 2021 International Conference on Computer Vision (ICCV 2021)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.13355">arXiv:2102.13355</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.13355">pdf</a>, <a href="https://arxiv.org/format/2102.13355">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Predicting gender and age categories in English conversations using lexical, non-lexical, and turn-taking features
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liesenfeld%2C+A">Andreas Liesenfeld</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Parti%2C+G">Gbor Parti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Yin Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+C">Chu-Ren Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.13355v1-abstract-short" style="display: inline;">
        This paper examines gender and age salience and (stereo)typicality in British English talk with the aim to predict gender and age categories based on lexical, phrasal and turn-taking features. We examine the SpokenBNC, a corpus of around 11.4 million words of British English conversations and identify behavioural differences between speakers that are labelled for gender and age categories. We expl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.13355v1-abstract-full').style.display = 'inline'; document.getElementById('2102.13355v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.13355v1-abstract-full" style="display: none;">
        This paper examines gender and age salience and (stereo)typicality in British English talk with the aim to predict gender and age categories based on lexical, phrasal and turn-taking features. We examine the SpokenBNC, a corpus of around 11.4 million words of British English conversations and identify behavioural differences between speakers that are labelled for gender and age categories. We explore differences in language use and turn-taking dynamics and identify a range of characteristics that set the categories apart. We find that female speakers tend to produce more and slightly longer turns, while turns by male speakers feature a higher type-token ratio and a distinct range of minimal particles such as &#34;eh&#34;, &#34;uh&#34; and &#34;em&#34;. Across age groups, we observe, for instance, that swear words and laughter characterize young speakers&#39; talk, while old speakers tend to produce more truncated words. We then use the observed characteristics to predict gender and age labels of speakers per conversation and per turn as a classification task, showing that non-lexical utterances such as minimal particles that are usually left out of dialog data can contribute to setting the categories apart.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.13355v1-abstract-full').style.display = 'none'; document.getElementById('2102.13355v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.09536">arXiv:2101.09536</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.09536">pdf</a>, <a href="https://arxiv.org/format/2101.09536">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Memory-Efficient Semi-Supervised Continual Learning: The World is its Own Replay Buffer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Smith%2C+J">James Smith</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Balloch%2C+J">Jonathan Balloch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.09536v2-abstract-short" style="display: inline;">
        Rehearsal is a critical component for class-incremental continual learning, yet it requires a substantial memory budget. Our work investigates whether we can significantly reduce this memory budget by leveraging unlabeled data from an agent&#39;s environment in a realistic and challenging continual learning paradigm. Specifically, we explore and formalize a novel semi-supervised continual learning (SS&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09536v2-abstract-full').style.display = 'inline'; document.getElementById('2101.09536v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.09536v2-abstract-full" style="display: none;">
        Rehearsal is a critical component for class-incremental continual learning, yet it requires a substantial memory budget. Our work investigates whether we can significantly reduce this memory budget by leveraging unlabeled data from an agent&#39;s environment in a realistic and challenging continual learning paradigm. Specifically, we explore and formalize a novel semi-supervised continual learning (SSCL) setting, where labeled data is scarce yet non-i.i.d. unlabeled data from the agent&#39;s environment is plentiful. Importantly, data distributions in the SSCL setting are realistic and therefore reflect object class correlations between, and among, the labeled and unlabeled data distributions. We show that a strategy built on pseudo-labeling, consistency regularization, Out-of-Distribution (OoD) detection, and knowledge distillation reduces forgetting in this setting. Our approach, DistillMatch, increases performance over the state-of-the-art by no less than 8.7% average task accuracy and up to 54.5% average task accuracy in SSCL CIFAR-100 experiments. Moreover, we demonstrate that DistillMatch can save up to 0.23 stored images per processed unlabeled image compared to the next best method which only saves 0.08. Our results suggest that focusing on realistic correlated distributions is a significantly new perspective, which accentuates the importance of leveraging the world&#39;s structure as a continual learning strategy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.09536v2-abstract-full').style.display = 'none'; document.getElementById('2101.09536v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by the 2021 International Joint Conference on Neural Networks (IJCNN 2021)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.15498">arXiv:2012.15498</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.15498">pdf</a>, <a href="https://arxiv.org/ps/2012.15498">ps</a>, <a href="https://arxiv.org/format/2012.15498">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Online Algorithm for Maximum-Likelihood Quantum State Tomography
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+C">Chien-Ming Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Min Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yen-Huan Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.15498v2-abstract-short" style="display: inline;">
        We propose, to the best of our knowledge, the first online algorithm to compute the maximum-likelihood estimate in quantum state tomography. Suppose the quantum state to be estimated corresponds to a $D$-by-$D$ density matrix. The per-iteration computational complexity of the algorithm is $O ( D ^ 3 )$, independent of the data size. The expected optimization error of the algorithm is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.15498v2-abstract-full').style.display = 'inline'; document.getElementById('2012.15498v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.15498v2-abstract-full" style="display: none;">
        We propose, to the best of our knowledge, the first online algorithm to compute the maximum-likelihood estimate in quantum state tomography. Suppose the quantum state to be estimated corresponds to a $D$-by-$D$ density matrix. The per-iteration computational complexity of the algorithm is $O ( D ^ 3 )$, independent of the data size. The expected optimization error of the algorithm is $O(\sqrt{ ( 1 / T ) D \log D })$, where $T$ denotes the number of iterations. The algorithm can be viewed as a quantum extension of Soft-Bayes, a recent algorithm for online portfolio selection (Orseau et al. Soft-Bayes: Prod for mixtures of experts with log-loss. Int. Conf. Algorithmic Learning Theory. 2017).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.15498v2-abstract-full').style.display = 'none'; document.getElementById('2012.15498v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, name of the second author corrected, slight revision to address possible confusion with least sqaures trace regression and shadow tomography</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.06860">arXiv:2012.06860</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.06860">pdf</a>, <a href="https://arxiv.org/ps/2012.06860">ps</a>, <a href="https://arxiv.org/format/2012.06860">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Age-Optimal Power Allocation in Industrial IoT: A Risk-Sensitive Federated Learning Approach
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yung-Lin Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Chen-Feng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Samarakoon%2C+S">Sumudu Samarakoon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+H">Hung-Yu Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bennis%2C+M">Mehdi Bennis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.06860v2-abstract-short" style="display: inline;">
        This work studies a real-time environment monitoring scenario in the industrial Internet of things, where wireless sensors proactively collect environmental data and transmit it to the controller. We adopt the notion of risk-sensitivity in financial mathematics as the objective to jointly minimize the mean, variance, and other higher-order statistics of the network energy consumption subject to th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.06860v2-abstract-full').style.display = 'inline'; document.getElementById('2012.06860v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.06860v2-abstract-full" style="display: none;">
        This work studies a real-time environment monitoring scenario in the industrial Internet of things, where wireless sensors proactively collect environmental data and transmit it to the controller. We adopt the notion of risk-sensitivity in financial mathematics as the objective to jointly minimize the mean, variance, and other higher-order statistics of the network energy consumption subject to the constraints on the age of information (AoI) threshold violation probability and the AoI exceedances over a pre-defined threshold. We characterize the extreme AoI staleness using results in extreme value theory and propose a distributed power allocation approach by weaving in together principles of Lyapunov optimization and federated learning (FL). Simulation results demonstrate that the proposed FL-based distributed solution is on par with the centralized baseline while consuming 28.50% less system energy and outperforms the other baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.06860v2-abstract-full').style.display = 'none'; document.getElementById('2012.06860v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in IEEE PIMRC 2021 with 6 pages and 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.05159">arXiv:2012.05159</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.05159">pdf</a>, <a href="https://arxiv.org/ps/2012.05159">ps</a>, <a href="https://arxiv.org/format/2012.05159">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Iterative Collision Resolution for Slotted ALOHA with NOMA for Heterogeneous Devices
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yu-Chih Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shieh%2C+S">Shin-Lin Shieh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Pin Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+H">Hao-Ping Cheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.05159v2-abstract-short" style="display: inline;">
        In this paper, the problem of using uncoordinated multiple access (UMA) to serve a massive amount of heterogeneous users is investigated. Leveraging the heterogeneity, we propose a novel UMA protocol, called iterative collision resolution for slotted ALOHA (IRSA) with non-orthogonal multiple access (NOMA), to improve the conventional IRSA. In addition to the inter-slot successive interference canc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.05159v2-abstract-full').style.display = 'inline'; document.getElementById('2012.05159v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.05159v2-abstract-full" style="display: none;">
        In this paper, the problem of using uncoordinated multiple access (UMA) to serve a massive amount of heterogeneous users is investigated. Leveraging the heterogeneity, we propose a novel UMA protocol, called iterative collision resolution for slotted ALOHA (IRSA) with non-orthogonal multiple access (NOMA), to improve the conventional IRSA. In addition to the inter-slot successive interference cancellation (SIC) technique used in existing IRSA-based schemes, the proposed protocol further employs the intra-slot SIC technique that enables collision resolution for certain configurations of collided packets. A novel multi-dimensional density evolution is then proposed to analyze and to optimize the proposed protocol. Simulation results show that the proposed IRSA with NOMA protocol can efficiently exploit the heterogeneity among users and the multi-dimensional density evolution can accurately predict the throughput performance. Last, an extension of the proposed IRSA with NOMA protocol to the frame-asynchronous setting is investigated, where a boundary effect similar to that in spatially-coupled low-density parity check codes can be observed to bootstrap the decoding process.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.05159v2-abstract-full').style.display = 'none'; document.getElementById('2012.05159v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 December, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE Transactions on Communications, submitted, under revision</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.11463">arXiv:2011.11463</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.11463">pdf</a>, <a href="https://arxiv.org/ps/2011.11463">ps</a>, <a href="https://arxiv.org/format/2011.11463">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Broadcast for Timely Updates in Mobile Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Pin Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.11463v4-abstract-short" style="display: inline;">
        This study considers a wireless network where an access point (AP) broadcasts timely updates to numerous mobile users. The timeliness of information owned by a user is characterized by the age of information. Frequently broadcasting the timely updates at constant maximum power can minimize the age of information for all users, but wastes valuable communication resources (ie., time and energy). To&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.11463v4-abstract-full').style.display = 'inline'; document.getElementById('2011.11463v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.11463v4-abstract-full" style="display: none;">
        This study considers a wireless network where an access point (AP) broadcasts timely updates to numerous mobile users. The timeliness of information owned by a user is characterized by the age of information. Frequently broadcasting the timely updates at constant maximum power can minimize the age of information for all users, but wastes valuable communication resources (ie., time and energy). To address the age-energy trade-off, it is critical to develop an efficient scheduling algorithm that identifies broadcast times and allocates power. Moreover, unpredictable user movement would cause rapidly varying communication channels; in particular, those channels can be non-stationary. Our main contribution is to develop an online scheduling algorithm and a channel-agnostic scheduling algorithm for such a mobile network with a provable performance guarantee.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.11463v4-abstract-full').style.display = 'none'; document.getElementById('2011.11463v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, technical report for the IEEE CL paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.11378">arXiv:2011.11378</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.11378">pdf</a>, <a href="https://arxiv.org/format/2011.11378">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning for Automatic Quality Grading of Mangoes: Methods and Insights
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Shih-Lun Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tung%2C+H">Hsiao-Yen Tung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Lun Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.11378v1-abstract-short" style="display: inline;">
        The quality grading of mangoes is a crucial task for mango growers as it vastly affects their profit. However, until today, this process still relies on laborious efforts of humans, who are prone to fatigue and errors. To remedy this, the paper approaches the grading task with various convolutional neural networks (CNN), a tried-and-tested deep learning technology in computer vision. The models in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.11378v1-abstract-full').style.display = 'inline'; document.getElementById('2011.11378v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.11378v1-abstract-full" style="display: none;">
        The quality grading of mangoes is a crucial task for mango growers as it vastly affects their profit. However, until today, this process still relies on laborious efforts of humans, who are prone to fatigue and errors. To remedy this, the paper approaches the grading task with various convolutional neural networks (CNN), a tried-and-tested deep learning technology in computer vision. The models involved include Mask R-CNN (for background removal), the numerous past winners of the ImageNet challenge, namely AlexNet, VGGs, and ResNets; and, a family of self-defined convolutional autoencoder-classifiers (ConvAE-Clfs) inspired by the claimed benefit of multi-task learning in classification tasks. Transfer learning is also adopted in this work via utilizing the ImageNet pretrained weights. Besides elaborating on the preprocessing techniques, training details, and the resulting performance, we go one step further to provide explainable insights into the model&#39;s working with the help of saliency maps and principal component analysis (PCA). These insights provide a succinct, meaningful glimpse into the intricate deep learning black box, fostering trust, and can also be presented to humans in real-world use cases for reviewing the grading results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.11378v1-abstract-full').style.display = 'none'; document.getElementById('2011.11378v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICMLA 2020; 8 pages, 8 figures, 3 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.14049">arXiv:2010.14049</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.14049">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Effective FAQ Retrieval and Question Matching With Unsupervised Knowledge Injection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tseng%2C+W">Wen-Ting Tseng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lo%2C+T">Tien-Hong Lo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yung-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Berlin Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.14049v1-abstract-short" style="display: inline;">
        Frequently asked question (FAQ) retrieval, with the purpose of providing information on frequent questions or concerns, has far-reaching applications in many areas, where a collection of question-answer (Q-A) pairs compiled a priori can be employed to retrieve an appropriate answer in response to a user\u2019s query that is likely to reoccur frequently. To this end, predominant approaches to FAQ r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.14049v1-abstract-full').style.display = 'inline'; document.getElementById('2010.14049v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.14049v1-abstract-full" style="display: none;">
        Frequently asked question (FAQ) retrieval, with the purpose of providing information on frequent questions or concerns, has far-reaching applications in many areas, where a collection of question-answer (Q-A) pairs compiled a priori can be employed to retrieve an appropriate answer in response to a user\u2019s query that is likely to reoccur frequently. To this end, predominant approaches to FAQ retrieval typically rank question-answer pairs by considering either the similarity between the query and a question (q-Q), the relevance between the query and the associated answer of a question (q-A), or combining the clues gathered from the q-Q similarity measure and the q-A relevance measure. In this paper, we extend this line of research by combining the clues gathered from the q-Q similarity measure and the q-A relevance measure and meanwhile injecting extra word interaction information, distilled from a generic (open domain) knowledge base, into a contextual language model for inferring the q-A relevance. Furthermore, we also explore to capitalize on domain-specific topically-relevant relations between words in an unsupervised manner, acting as a surrogate to the supervised domain-specific knowledge base information. As such, it enables the model to equip sentence representations with the knowledge about domain-specific and topically-relevant relations among words, thereby providing a better q-A relevance measure. We evaluate variants of our approach on a publicly-available Chinese FAQ dataset, and further apply and contextualize it to a large-scale question-matching task, which aims to search questions from a QA dataset that have a similar intent as an input query. Extensive experimental results on these two datasets confirm the promising performance of the proposed approach in relation to some state-of-the-art ones.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.14049v1-abstract-full').style.display = 'none'; document.getElementById('2010.14049v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.11820">arXiv:2010.11820</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.11820">pdf</a>, <a href="https://arxiv.org/format/2010.11820">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Posterior Re-calibration for Imbalanced Datasets
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+J">Junjiao Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yen-Cheng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Glaser%2C+N">Nathan Glaser</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.11820v1-abstract-short" style="display: inline;">
        Neural Networks can perform poorly when the training label distribution is heavily imbalanced, as well as when the testing data differs from the training distribution. In order to deal with shift in the testing label distribution, which imbalance causes, we motivate the problem from the perspective of an optimal Bayes classifier and derive a post-training prior rebalancing technique that can be so&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.11820v1-abstract-full').style.display = 'inline'; document.getElementById('2010.11820v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.11820v1-abstract-full" style="display: none;">
        Neural Networks can perform poorly when the training label distribution is heavily imbalanced, as well as when the testing data differs from the training distribution. In order to deal with shift in the testing label distribution, which imbalance causes, we motivate the problem from the perspective of an optimal Bayes classifier and derive a post-training prior rebalancing technique that can be solved through a KL-divergence based optimization. This method allows a flexible post-training hyper-parameter to be efficiently tuned on a validation set and effectively modify the classifier margin to deal with this imbalance. We further combine this method with existing likelihood shift methods, re-interpreting them from the same Bayesian perspective, and demonstrating that our method can deal with both problems in a unified way. The resulting algorithm can be conveniently used on probabilistic classification problems agnostic to underlying architectures. Our results on six different datasets and five different architectures show state of art accuracy, including on large-scale imbalanced datasets such as iNaturalist for classification and Synthia for semantic segmentation. Please see https://github.com/GT-RIPL/UNO-IC.git for implementation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.11820v1-abstract-full').style.display = 'none'; document.getElementById('2010.11820v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NeurIPS 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.02416">arXiv:2010.02416</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.02416">pdf</a>, <a href="https://arxiv.org/format/2010.02416">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Inference For Neural Machine Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yi-Te Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garg%2C+S">Sarthak Garg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+Y">Yi-Hsiu Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chatsviorkin%2C+I">Ilya Chatsviorkin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.02416v2-abstract-short" style="display: inline;">
        Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attenti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02416v2-abstract-full').style.display = 'inline'; document.getElementById('2010.02416v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.02416v2-abstract-full" style="display: none;">
        Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02416v2-abstract-full').style.display = 'none'; document.getElementById('2010.02416v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted SustaiNLP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.12500">arXiv:2005.12500</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.12500">pdf</a>, <a href="https://arxiv.org/format/2005.12500">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CalliGAN: Style and Structure-aware Chinese Calligraphy Character Generator
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+S">Shan-Jean Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+C">Chih-Yuan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+J+Y">Jane Yung-jen Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.12500v1-abstract-short" style="display: inline;">
        Chinese calligraphy is the writing of Chinese characters as an art form performed with brushes so Chinese characters are rich of shapes and details. Recent studies show that Chinese characters can be generated through image-to-image translation for multiple styles using a single model. We propose a novel method of this approach by incorporating Chinese characters&#39; component information into its mo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.12500v1-abstract-full').style.display = 'inline'; document.getElementById('2005.12500v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.12500v1-abstract-full" style="display: none;">
        Chinese calligraphy is the writing of Chinese characters as an art form performed with brushes so Chinese characters are rich of shapes and details. Recent studies show that Chinese characters can be generated through image-to-image translation for multiple styles using a single model. We propose a novel method of this approach by incorporating Chinese characters&#39; component information into its model. We also propose an improved network to convert characters to their embedding space. Experiments show that the proposed method generates high-quality Chinese calligraphy characters over state-of-the-art methods measured through numerical evaluations and human subject studies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.12500v1-abstract-full').style.display = 'none'; document.getElementById('2005.12500v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">the work has been accepted to the AI for content creation workshop at CVPR 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.08545">arXiv:2005.08545</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.08545">pdf</a>, <a href="https://arxiv.org/ps/2005.08545">ps</a>, <a href="https://arxiv.org/format/2005.08545">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Index Coding and Incentive Design for Selfish Clients
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Pin Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hou%2C+I">I-Hong Hou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sprintson%2C+A">Alex Sprintson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.08545v4-abstract-short" style="display: inline;">
        The index coding problem includes a server, a group of clients, and a set of data chunks. While each client wants a subset of the data chunks and already has another subset as its side information, the server transmits some uncoded data chunks or coded data chunks to the clients over a noiseless broadcast channel. The objective of the problem is to satisfy the demands of all clients with the minim&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.08545v4-abstract-full').style.display = 'inline'; document.getElementById('2005.08545v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.08545v4-abstract-full" style="display: none;">
        The index coding problem includes a server, a group of clients, and a set of data chunks. While each client wants a subset of the data chunks and already has another subset as its side information, the server transmits some uncoded data chunks or coded data chunks to the clients over a noiseless broadcast channel. The objective of the problem is to satisfy the demands of all clients with the minimum number of transmissions. In this paper, we investigate the index coding setting from a game-theoretical perspective. We consider selfish clients, where each selfish client has private side information and a private valuation of each data chunk it wants. In this context, our objectives are following: 1) to motivate each selfish client to reveal the correct side information and true valuation of each data chunk it wants; 2) to maximize the social welfare, i.e., the total valuation of the data chunks recovered by the clients minus the total cost incurred by the transmissions from the server. Our main contribution is to jointly develop coding schemes and incentive schemes for achieving the first objective perfectly and achieving the second objective optimally or approximately with guaranteed approximation ratios (potentially within some restricted sets of coding matrices).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.08545v4-abstract-full').style.display = 'none'; document.getElementById('2005.08545v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 December, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">38 pages (single column), technical report for the IEEE T-COM paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.06111">arXiv:2005.06111</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.06111">pdf</a>, <a href="https://arxiv.org/format/2005.06111">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Project RISE: Recognizing Industrial Smoke Emissions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T+%27">Ting-Hao &#39;Kenneth&#39; Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+T">Ting-Yao Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dille%2C+P">Paul Dille</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Prendi%2C+S">Sean Prendi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hoffman%2C+R">Ryan Hoffman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsuhlares%2C+A">Anastasia Tsuhlares</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pachuta%2C+J">Jessica Pachuta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sargent%2C+R">Randy Sargent</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nourbakhsh%2C+I">Illah Nourbakhsh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.06111v8-abstract-short" style="display: inline;">
        Industrial smoke emissions pose a significant concern to human health. Prior works have shown that using Computer Vision (CV) techniques to identify smoke as visual evidence can influence the attitude of regulators and empower citizens to pursue environmental justice. However, existing datasets are not of sufficient quality nor quantity to train the robust CV models needed to support air quality a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.06111v8-abstract-full').style.display = 'inline'; document.getElementById('2005.06111v8-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.06111v8-abstract-full" style="display: none;">
        Industrial smoke emissions pose a significant concern to human health. Prior works have shown that using Computer Vision (CV) techniques to identify smoke as visual evidence can influence the attitude of regulators and empower citizens to pursue environmental justice. However, existing datasets are not of sufficient quality nor quantity to train the robust CV models needed to support air quality advocacy. We introduce RISE, the first large-scale video dataset for Recognizing Industrial Smoke Emissions. We adopted a citizen science approach to collaborate with local community members to annotate whether a video clip has smoke emissions. Our dataset contains 12,567 clips from 19 distinct views from cameras that monitored three industrial facilities. These daytime clips span 30 days over two years, including all four seasons. We ran experiments using deep neural networks to establish a strong performance baseline and reveal smoke recognition challenges. Our survey study discussed community feedback, and our data analysis displayed opportunities for integrating citizen scientists and crowd workers into the application of Artificial Intelligence for Social Impact.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.06111v8-abstract-full').style.display = 'none'; document.getElementById('2005.06111v8-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by AAAI 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.02367">arXiv:2005.02367</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.02367">pdf</a>, <a href="https://arxiv.org/format/2005.02367">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the COVID-19 Open Research Dataset
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T+%27">Ting-Hao &#39;Kenneth&#39; Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+C">Chieh-Yang Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+C+C">Chien-Kuang Cornelia Ding</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Giles%2C+C+L">C. Lee Giles</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.02367v5-abstract-short" style="display: inline;">
        This paper introduces CODA-19, a human-annotated dataset that codes the Background, Purpose, Method, Finding/Contribution, and Other sections of 10,966 English abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved labeling quality comparable to that of experts. Each abstract was annotated by nine different&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.02367v5-abstract-full').style.display = 'inline'; document.getElementById('2005.02367v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.02367v5-abstract-full" style="display: none;">
        This paper introduces CODA-19, a human-annotated dataset that codes the Background, Purpose, Method, Finding/Contribution, and Other sections of 10,966 English abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved labeling quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were acquired by majority vote. The inter-annotator agreement (Cohen&#39;s kappa) between the crowd and the biomedical expert (0.741) is comparable to inter-expert agreement (0.788). CODA-19&#39;s labels have an accuracy of 82.2% when compared to the biomedical expert&#39;s labels, while the accuracy between experts was 85.0%. Reliable human annotations help scientists access and integrate the rapidly accelerating coronavirus literature, and also serve as the battery of AI/NLP research, but obtaining expert annotations can be slow. We demonstrated that a non-expert crowd can be rapidly employed at scale to join the fight against COVID-19.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.02367v5-abstract-full').style.display = 'none'; document.getElementById('2005.02367v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by the NLP COVID-19 Workshop at ACL 2020. (The data, code, and model are available at: https://github.com/windx0303/CODA-19)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.11297">arXiv:2002.11297</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.11297">pdf</a>, <a href="https://arxiv.org/format/2002.11297">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yilin Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hongxia Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.11297v2-abstract-short" style="display: inline;">
        Deep neural networks have attained remarkable performance when applied to data that comes from the same distribution as that of the training set, but can significantly degrade otherwise. Therefore, detecting whether an example is out-of-distribution (OoD) is crucial to enable a system that can reject such samples or alert users. Recent works have made significant progress on OoD benchmarks consist&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.11297v2-abstract-full').style.display = 'inline'; document.getElementById('2002.11297v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.11297v2-abstract-full" style="display: none;">
        Deep neural networks have attained remarkable performance when applied to data that comes from the same distribution as that of the training set, but can significantly degrade otherwise. Therefore, detecting whether an example is out-of-distribution (OoD) is crucial to enable a system that can reject such samples or alert users. Recent works have made significant progress on OoD benchmarks consisting of small image datasets. However, many recent methods based on neural networks rely on training or tuning with both in-distribution and out-of-distribution data. The latter is generally hard to define a-priori, and its selection can easily bias the learning. We base our work on a popular method ODIN, proposing two strategies for freeing it from the needs of tuning with OoD data, while improving its OoD detection performance. We specifically propose to decompose confidence scoring as well as a modified input pre-processing method. We show that both of these significantly help in detection performance. Our further analysis on a larger scale image dataset shows that the two types of distribution shifts, specifically semantic shift and non-semantic shift, present a significant difference in the difficulty of the problem, providing an analysis of when ODIN-like strategies do or do not work.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.11297v2-abstract-full').style.display = 'none'; document.getElementById('2002.11297v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 February, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.11936">arXiv:1912.11936</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.11936">pdf</a>, <a href="https://arxiv.org/format/1912.11936">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Smell Pittsburgh: Engaging Community Citizen Science for Air Quality
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cross%2C+J">Jennifer Cross</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dille%2C+P">Paul Dille</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tasota%2C+M">Michael Tasota</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dias%2C+B">Beatrice Dias</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sargent%2C+R">Randy Sargent</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T+%27">Ting-Hao &#39;Kenneth&#39; Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nourbakhsh%2C+I">Illah Nourbakhsh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.11936v4-abstract-short" style="display: inline;">
        Urban air pollution has been linked to various human health concerns, including cardiopulmonary diseases. Communities who suffer from poor air quality often rely on experts to identify pollution sources due to the lack of accessible tools. Taking this into account, we developed Smell Pittsburgh, a system that enables community members to report odors and track where these odors are frequently conc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.11936v4-abstract-full').style.display = 'inline'; document.getElementById('1912.11936v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.11936v4-abstract-full" style="display: none;">
        Urban air pollution has been linked to various human health concerns, including cardiopulmonary diseases. Communities who suffer from poor air quality often rely on experts to identify pollution sources due to the lack of accessible tools. Taking this into account, we developed Smell Pittsburgh, a system that enables community members to report odors and track where these odors are frequently concentrated. All smell report data are publicly accessible online. These reports are also sent to the local health department and visualized on a map along with air quality data from monitoring stations. This visualization provides a comprehensive overview of the local pollution landscape. Additionally, with these reports and air quality data, we developed a model to predict upcoming smell events and send push notifications to inform communities. We also applied regression analysis to identify statistically significant effects of push notifications on user engagement. Our evaluation of this system demonstrates that engaging residents in documenting their experiences with pollution odors can help identify local air pollution patterns, and can empower communities to advocate for better air quality. All citizen-contributed smell data are publicly accessible and can be downloaded from https://smellpgh.org.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.11936v4-abstract-full').style.display = 'none'; document.getElementById('1912.11936v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 December, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACM Transactions on Interactive Intelligent Systems on 2020. This is an extended version of the arXiv:1810.11143, which was accepted by the ACM IUI 2019 conference. arXiv admin note: substantial text overlap with arXiv:1810.11143</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.12926">arXiv:1911.12926</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.12926">pdf</a>, <a href="https://arxiv.org/format/1911.12926">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        J-Net: Randomly weighted U-Net for audio source separation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Bo-Wen Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Min Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+H">Hung-Yi Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.12926v1-abstract-short" style="display: inline;">
        Several results in the computer vision literature have shown the potential of randomly weighted neural networks. While they perform fairly well as feature extractors for discriminative tasks, a positive correlation exists between their performance and their fully trained counterparts. According to these discoveries, we pose two questions: what is the value of randomly weighted networks in difficul&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.12926v1-abstract-full').style.display = 'inline'; document.getElementById('1911.12926v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.12926v1-abstract-full" style="display: none;">
        Several results in the computer vision literature have shown the potential of randomly weighted neural networks. While they perform fairly well as feature extractors for discriminative tasks, a positive correlation exists between their performance and their fully trained counterparts. According to these discoveries, we pose two questions: what is the value of randomly weighted networks in difficult generative audio tasks such as audio source separation and does such positive correlation still exist when it comes to large random networks and their trained counterparts? In this paper, we demonstrate that the positive correlation still exists. Based on this discovery, we can try out different architecture designs or tricks without training the whole model. Meanwhile, we find a surprising result that in comparison to the non-trained encoder (down-sample path) in Wave-U-Net, fixing the decoder (up-sample path) to random weights results in better performance, almost comparable to the fully trained model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.12926v1-abstract-full').style.display = 'none'; document.getElementById('1911.12926v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.11991">arXiv:1911.11991</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.11991">pdf</a>, <a href="https://arxiv.org/format/1911.11991">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A selected review on reinforcement learning based control for autonomous underwater vehicles
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yachu Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Hui Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=You%2C+K">Keyou You</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+S">Shiji Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.11991v1-abstract-short" style="display: inline;">
        Recently, reinforcement learning (RL) has been extensively studied and achieved promising results in a wide range of control tasks. Meanwhile, autonomous underwater vehicle (AUV) is an important tool for executing complex and challenging underwater tasks. The advances in RL offers ample opportunities for developing intelligent AUVs. This paper provides a selected review on RL based control for AUV&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11991v1-abstract-full').style.display = 'inline'; document.getElementById('1911.11991v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.11991v1-abstract-full" style="display: none;">
        Recently, reinforcement learning (RL) has been extensively studied and achieved promising results in a wide range of control tasks. Meanwhile, autonomous underwater vehicle (AUV) is an important tool for executing complex and challenging underwater tasks. The advances in RL offers ample opportunities for developing intelligent AUVs. This paper provides a selected review on RL based control for AUVs with the focus on applications of RL to low-level control tasks for underwater regulation and tracking. To this end, we first present a concise introduction to the RL based control framework. Then, we provide an overview of RL methods for AUVs control problems, where the main challenges and recent progresses are discussed. Finally, two representative cases of RL-based controllers are given in detail for the model-free RL methods on AUVs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11991v1-abstract-full').style.display = 'none'; document.getElementById('1911.11991v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.08078">arXiv:1911.08078</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.08078">pdf</a>, <a href="https://arxiv.org/ps/1911.08078">ps</a>, <a href="https://arxiv.org/format/1911.08078">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Delay-Aware Wireless Network Coding in Adversarial Traffic
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Pin Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.08078v4-abstract-short" style="display: inline;">
        We analyze a wireless line network employing wireless network coding. The two end nodes exchange their packets through relays. While a packet at a relay might not find its coding pair upon arrival, a transmission cost can be reduced by waiting for coding with a packet from the other side. To strike a balance between the reduced transmission cost and the cost incurred by the delay, a scheduling alg&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.08078v4-abstract-full').style.display = 'inline'; document.getElementById('1911.08078v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.08078v4-abstract-full" style="display: none;">
        We analyze a wireless line network employing wireless network coding. The two end nodes exchange their packets through relays. While a packet at a relay might not find its coding pair upon arrival, a transmission cost can be reduced by waiting for coding with a packet from the other side. To strike a balance between the reduced transmission cost and the cost incurred by the delay, a scheduling algorithm determining either to transmit an uncoded packet or to wait for coding is needed. Because of highly uncertain traffic injections, scheduling with no assumption of the traffic is critical. This paper proposes a randomized online scheduling algorithm for a relay in arbitrary traffic, which can be non-stationary or adversarial. The expected total cost (including a transmission cost and a delay cost) incurred by the proposed algorithm is at most e/(e-1) ~ 1.58 times the minimum achievable total cost. In particular, the proposed algorithm is universal in the sense that the ratio is independent of the traffic. With the universality, the proposed algorithm can be implemented at each relay distributedly (in a multi-relay network) with the same ratio. Moreover, the proposed algorithm turns out to generalize the classic ski-rental online algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.08078v4-abstract-full').style.display = 'none'; document.getElementById('1911.08078v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 November, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">39 pages (single column), technical report for the IEEE T-COM paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.12544">arXiv:1910.12544</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.12544">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Human-AI Co-Learning for Data-Driven AI
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yi-Ching Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+Y">Yu-Ting Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+L">Lin-Lin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+J+Y">Jane Yung-jen Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.12544v1-abstract-short" style="display: inline;">
        Human and AI are increasingly interacting and collaborating to accomplish various complex tasks in the context of diverse application domains (e.g., healthcare, transportation, and creative design). Two dynamic, learning entities (AI and human) have distinct mental model, expertise, and ability; such fundamental difference/mismatch offers opportunities for bringing new perspectives to achieve bett&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.12544v1-abstract-full').style.display = 'inline'; document.getElementById('1910.12544v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.12544v1-abstract-full" style="display: none;">
        Human and AI are increasingly interacting and collaborating to accomplish various complex tasks in the context of diverse application domains (e.g., healthcare, transportation, and creative design). Two dynamic, learning entities (AI and human) have distinct mental model, expertise, and ability; such fundamental difference/mismatch offers opportunities for bringing new perspectives to achieve better results. However, this mismatch can cause unexpected failure and result in serious consequences. While recent research has paid much attention to enhancing interpretability or explainability to allow machine to explain how it makes a decision for supporting humans, this research argues that there is urging the need for both human and AI should develop specific, corresponding ability to interact and collaborate with each other to form a human-AI team to accomplish superior results. This research introduces a conceptual framework called &#34;Co-Learning,&#34; in which people can learn with/from and grow with AI partners over time. We characterize three key concepts of co-learning: &#34;mutual understanding,&#34; &#34;mutual benefits,&#34; and &#34;mutual growth&#34; for facilitating human-AI collaboration on complex problem solving. We will present proof-of-concepts to investigate whether and how our approach can help human-AI team to understand and benefit each other, and ultimately improve productivity and creativity on creative problem domains. The insights will contribute to the design of Human-AI collaboration.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.12544v1-abstract-full').style.display = 'none'; document.getElementById('1910.12544v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 October, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.12423">arXiv:1910.12423</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.12423">pdf</a>, <a href="https://arxiv.org/format/1910.12423">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ACE: Adaptive Confusion Energy for Natural World Data Distribution
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chi Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+C">Cheng-Yao Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fan%2C+W">Wan-Cyuan Fan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+M">Ming-Sui Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Geiger%2C+D">Davi Geiger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+T">Tyng-Luh Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.12423v3-abstract-short" style="display: inline;">
        With the development of deep learning, standard classification problems have achieved good results. However, conventional classification problems are often too idealistic. Most data in the natural world usually have imbalanced distribution and fine-grained characteristics. Recently, many state-of-the-art approaches tend to focus on one or another separately, but rarely on both. In this paper, we i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.12423v3-abstract-full').style.display = 'inline'; document.getElementById('1910.12423v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.12423v3-abstract-full" style="display: none;">
        With the development of deep learning, standard classification problems have achieved good results. However, conventional classification problems are often too idealistic. Most data in the natural world usually have imbalanced distribution and fine-grained characteristics. Recently, many state-of-the-art approaches tend to focus on one or another separately, but rarely on both. In this paper, we introduce a novel and adaptive batch-wise regularization based on the proposed Adaptive Confusion Energy (ACE) to flexibly address the nature world distribution, which usually involves fine-grained and long-tailed properties at the same time. ACE increases the difficulty of the training process and further alleviates the overfitting problem. Through the datasets with the technical issue in fine-grained (CUB, CAR, AIR) and long-tailed (ImageNet-LT), or comprehensive issues (CUB-LT, iNaturalist), the result shows that the ACE is not only competitive to some state-of-the-art on performance but also demonstrates the effectiveness of training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.12423v3-abstract-full').style.display = 'none'; document.getElementById('1910.12423v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 October, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.00916">arXiv:1910.00916</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.00916">pdf</a>, <a href="https://arxiv.org/ps/1910.00916">ps</a>, <a href="https://arxiv.org/format/1910.00916">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scheduling Stochastic Real-Time Jobs in Unreliable Workers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Pin Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yu-Chih Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shieh%2C+S">Shin-Lin Shieh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.00916v5-abstract-short" style="display: inline;">
        We consider a distributed computing network consisting of a master and multiple workers processing tasks of different types. The master is running multiple applications. Each application stochastically generates real-time jobs with a strict job deadline, where each job is a collection of tasks of some types specified by the application. A real-time job is completed only when all its tasks are comp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.00916v5-abstract-full').style.display = 'inline'; document.getElementById('1910.00916v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.00916v5-abstract-full" style="display: none;">
        We consider a distributed computing network consisting of a master and multiple workers processing tasks of different types. The master is running multiple applications. Each application stochastically generates real-time jobs with a strict job deadline, where each job is a collection of tasks of some types specified by the application. A real-time job is completed only when all its tasks are completed by the corresponding workers within the deadline. Moreover, we consider unreliable workers, whose processing speeds are uncertain. Because of the limited processing abilities of the workers, an algorithm for scheduling the jobs in the workers is needed to maximize the average number of completed jobs for each application. The scheduling problem is not only critical but also practical in distributed computing networks. In this paper, we develop two scheduling algorithms, namely, a feasibility-optimal scheduling algorithm and an approximate scheduling algorithm. The feasibility-optimal scheduling algorithm can fulfill the largest region of applications&#39; requirements for the average number of completed jobs. However, the feasibility-optimal scheduling algorithm suffers from high computational complexity when the number of applications is large. To address the issue, the approximate scheduling algorithm is proposed with a guaranteed approximation ratio in the worst-case scenario. The approximate scheduling algorithm is also validated in the average-case scenario via computer simulations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.00916v5-abstract-full').style.display = 'none'; document.getElementById('1910.00916v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 January, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 October, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, technical report for the WCNC paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.03245">arXiv:1909.03245</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.03245">pdf</a>, <a href="https://arxiv.org/format/1909.03245">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+W">Wenjie Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+S">Shiji Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Hui Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Ya-Chu Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Cheng Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+G">Gao Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.03245v3-abstract-short" style="display: inline;">
        Model-free deep reinforcement learning (RL) algorithms have been widely used for a range of complex control tasks. However, slow convergence and sample inefficiency remain challenging problems in RL, especially when handling continuous and high-dimensional state spaces. To tackle this problem, we propose a general acceleration method for model-free, off-policy deep RL algorithms by drawing the ide&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.03245v3-abstract-full').style.display = 'inline'; document.getElementById('1909.03245v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.03245v3-abstract-full" style="display: none;">
        Model-free deep reinforcement learning (RL) algorithms have been widely used for a range of complex control tasks. However, slow convergence and sample inefficiency remain challenging problems in RL, especially when handling continuous and high-dimensional state spaces. To tackle this problem, we propose a general acceleration method for model-free, off-policy deep RL algorithms by drawing the idea underlying regularized Anderson acceleration (RAA), which is an effective approach to accelerating the solving of fixed point problems with perturbations. Specifically, we first explain how policy iteration can be applied directly with Anderson acceleration. Then we extend RAA to the case of deep RL by introducing a regularization term to control the impact of perturbation induced by function approximation errors. We further propose two strategies, i.e., progressive update and adaptive restart, to enhance the performance. The effectiveness of our method is evaluated on a variety of benchmark tasks, including Atari 2600 and MuJoCo. Experimental results show that our approach substantially improves both the learning speed and final performance of state-of-the-art deep RL algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.03245v3-abstract-full').style.display = 'none'; document.getElementById('1909.03245v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 September, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">33rd Conference on Neural Information Processing Systems (NeurIPS 2019)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.11260">arXiv:1907.11260</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.11260">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When Human-Computer Interaction Meets Community Citizen Science
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nourbakhsh%2C+I">Illah Nourbakhsh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.11260v1-abstract-short" style="display: inline;">
        Human-computer interaction (HCI) studies the design and use of interfaces and interactive systems. HCI has been adopted successfully in modern commercial products. Recently, its use for promoting social good and pursuing sustainability, known as sustainable HCI, has begun to receive wide attention. Conventionally, scientists and decision-makers apply top-down approaches to lead research activities&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.11260v1-abstract-full').style.display = 'inline'; document.getElementById('1907.11260v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.11260v1-abstract-full" style="display: none;">
        Human-computer interaction (HCI) studies the design and use of interfaces and interactive systems. HCI has been adopted successfully in modern commercial products. Recently, its use for promoting social good and pursuing sustainability, known as sustainable HCI, has begun to receive wide attention. Conventionally, scientists and decision-makers apply top-down approaches to lead research activities that engage lay people in facilitating sustainability, such as saving energy. We introduce an alternative framework, Community Citizen Science (CCS), to closely connect research and social issues by empowering communities to produce scientific knowledge, represent their needs, address their concerns, and advocate for impact. CCS advances the current science-oriented concept to a deeper level that aims to sustain community engagement when researchers are no longer involved after the intervention of interactive systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.11260v1-abstract-full').style.display = 'none'; document.getElementById('1907.11260v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Viewpoints accepted by the Communications of the ACM</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.01764">arXiv:1906.01764</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.01764">pdf</a>, <a href="https://arxiv.org/format/1906.01764">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual Story Post-Editing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+T">Ting-Yao Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+C">Chieh-Yang Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T+%27">Ting-Hao &#39;Kenneth&#39; Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.01764v1-abstract-short" style="display: inline;">
        We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit, includes 14,905 human edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.01764v1-abstract-full').style.display = 'inline'; document.getElementById('1906.01764v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.01764v1-abstract-full" style="display: none;">
        We introduce the first dataset for human edits of machine-generated visual stories and explore how these collected edits may be used for the visual story post-editing task. The dataset, VIST-Edit, includes 14,905 human edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions. We establish baselines for the task, showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models. We also discuss the weak correlation between automatic evaluation scores and human ratings, motivating the need for new automatic metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.01764v1-abstract-full').style.display = 'none'; document.getElementById('1906.01764v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 June, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACL 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.00933">arXiv:1903.00933</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.00933">pdf</a>, <a href="https://arxiv.org/format/1903.00933">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Bai Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yi-Te Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudzicz%2C+F">Frank Rudzicz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.00933v2-abstract-short" style="display: inline;">
        Machine learning has shown promise for automatic detection of Alzheimer&#39;s disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of out-of-domain movie dialogue data. We apply i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.00933v2-abstract-full').style.display = 'inline'; document.getElementById('1903.00933v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.00933v2-abstract-full" style="display: none;">
        Machine learning has shown promise for automatic detection of Alzheimer&#39;s disease (AD) through speech; however, efforts are hampered by a scarcity of data, especially in languages other than English. We propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages, using a large parallel corpus of out-of-domain movie dialogue data. We apply it to dementia detection in Mandarin Chinese, and demonstrate that our method outperforms both unilingual and machine translation-based baselines. This appears to be the first study that transfers feature domains in detecting cognitive decline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.00933v2-abstract-full').style.display = 'none'; document.getElementById('1903.00933v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 March, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NAACL 2019 (Short paper)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.08327">arXiv:1902.08327</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.08327">pdf</a>, <a href="https://arxiv.org/format/1902.08327">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On How Users Edit Computer-Generated Visual Stories
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+T">Ting-Yao Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chia Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+T+%27">Ting-Hao &#39;Kenneth&#39; Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.08327v2-abstract-short" style="display: inline;">
        A significant body of research in Artificial Intelligence (AI) has focused on generating stories automatically, either based on prior story plots or input images. However, literature has little to say about how users would receive and use these stories. Given the quality of stories generated by modern AI algorithms, users will nearly inevitably have to edit these stories before putting them to rea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.08327v2-abstract-full').style.display = 'inline'; document.getElementById('1902.08327v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.08327v2-abstract-full" style="display: none;">
        A significant body of research in Artificial Intelligence (AI) has focused on generating stories automatically, either based on prior story plots or input images. However, literature has little to say about how users would receive and use these stories. Given the quality of stories generated by modern AI algorithms, users will nearly inevitably have to edit these stories before putting them to real use. In this paper, we present the first analysis of how human users edit machine-generated stories. We obtained 962 short stories generated by one of the state-of-the-art visual storytelling models. For each story, we recruited five crowd workers from Amazon Mechanical Turk to edit it. Our analysis of these edits shows that, on average, users (i) slightly shortened machine-generated stories, (ii) increased lexical diversity in these stories, and (iii) often replaced nouns and their determiners/articles with pronouns. Our study provides a better understanding on how users receive and edit machine-generated stories,informing future researchers to create more usable and helpful story generation systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.08327v2-abstract-full').style.display = 'none'; document.getElementById('1902.08327v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 March, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 February, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in CHI&#39;19 Late-Breaking Work on Human Factors in Computing Systems (CHI LBW 2019), 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.10713">arXiv:1901.10713</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.10713">pdf</a>, <a href="https://arxiv.org/format/1901.10713">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Mobile Robot Generating Video Summaries of Seniors&#39; Indoor Activities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+C">Chih-Yuan Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yun%2C+H">Heeseung Yun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Varadaraj%2C+S">Srenavis Varadaraj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+J+Y">Jane Yung-jen Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.10713v2-abstract-short" style="display: inline;">
        We develop a system which generates summaries from seniors&#39; indoor-activity videos captured by a social robot to help remote family members know their seniors&#39; daily activities at home. Unlike the traditional video summarization datasets, indoor videos captured from a moving robot poses additional challenges, namely, (i) the video sequences are very long (ii) a significant number of video-frames c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.10713v2-abstract-full').style.display = 'inline'; document.getElementById('1901.10713v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.10713v2-abstract-full" style="display: none;">
        We develop a system which generates summaries from seniors&#39; indoor-activity videos captured by a social robot to help remote family members know their seniors&#39; daily activities at home. Unlike the traditional video summarization datasets, indoor videos captured from a moving robot poses additional challenges, namely, (i) the video sequences are very long (ii) a significant number of video-frames contain no-subject or with subjects at ill-posed locations and scales (iii) most of the well-posed frames contain highly redundant information. To address this problem, we propose to \hl{exploit} pose estimation \hl{for detecting} people in frames\hl{. This guides the robot} to follow the user and capture effective videos. We use person identification to distinguish a target senior from other people. We \hl{also make use of} action recognition to analyze seniors&#39; major activities at different moments, and develop a video summarization method to select diverse and representative keyframes as summaries.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.10713v2-abstract-full').style.display = 'none'; document.getElementById('1901.10713v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 January, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted by MobileHCI&#39;19</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.03137">arXiv:1901.03137</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.03137">pdf</a>, <a href="https://arxiv.org/ps/1901.03137">ps</a>, <a href="https://arxiv.org/format/1901.03137">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Online Energy-Efficient Scheduling for Timely Information Downloads in Mobile Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tseng%2C+Y">Yi-Hsuan Tseng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yu-Pin Hsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.03137v5-abstract-short" style="display: inline;">
        We consider a mobile network where a mobile device is running an application that requires timely information. The information at the device can be updated by downloading the latest information through neighboring access points. The freshness of the information at the device is characterized by the recently proposed age of information. However, minimizing the age of information by frequent downloa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.03137v5-abstract-full').style.display = 'inline'; document.getElementById('1901.03137v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.03137v5-abstract-full" style="display: none;">
        We consider a mobile network where a mobile device is running an application that requires timely information. The information at the device can be updated by downloading the latest information through neighboring access points. The freshness of the information at the device is characterized by the recently proposed age of information. However, minimizing the age of information by frequent downloading increases power consumption of the device. In this context, an energy-efficient scheduling algorithm for timely information downloads is critical, especially for power-limited mobile devices. Moreover, unpredictable movement of the mobile device causes uncertainty of the channel dynamics, which is even non-stationary within a finite amount of time for running the application. Thus, in this paper we devise a randomized online scheduling algorithm for mobile devices, which can move arbitrarily and run the application for any amount of time. We show that the expected total cost incurred by the proposed algorithm, including an age cost and a downloading cost, is (asymptotically) at most e/(e-1) ~ 1.58 times the minimum total cost achieved by an optimal offline scheduling algorithm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.03137v5-abstract-full').style.display = 'none'; document.getElementById('1901.03137v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 January, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, technical report for the ISIT 2019 paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.00544">arXiv:1901.00544</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.00544">pdf</a>, <a href="https://arxiv.org/format/1901.00544">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-class Classification without Multi-class Labels
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+Y">Yen-Chang Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Z">Zhaoyang Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schlosser%2C+J">Joel Schlosser</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Odom%2C+P">Phillip Odom</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kira%2C+Z">Zsolt Kira</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.00544v1-abstract-short" style="display: inline;">
        This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.00544v1-abstract-full').style.display = 'inline'; document.getElementById('1901.00544v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.00544v1-abstract-full" style="display: none;">
        This work presents a new strategy for multi-class classification that requires no class-specific labels, but instead leverages pairwise similarity between examples, which is a weaker form of annotation. The proposed method, meta classification learning, optimizes a binary classifier for pairwise similarity prediction and through this process learns a multi-class classifier as a submodule. We formulate this approach, present a probabilistic graphical model for it, and derive a surprisingly simple loss function that can be used to learn neural network-based models. We then demonstrate that this same framework generalizes to the supervised, unsupervised cross-task, and semi-supervised settings. Our method is evaluated against state of the art in all three learning paradigms and shows a superior or comparable accuracy, providing evidence that learning multi-class classification without multi-class labels is a viable learning option.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.00544v1-abstract-full').style.display = 'none'; document.getElementById('1901.00544v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 January, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Conference on Learning Representations (ICLR 2019)</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+Y&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Hsu%2C+Y&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Hsu%2C+Y&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>