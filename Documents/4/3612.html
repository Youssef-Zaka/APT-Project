<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;7 of 7 results for author: <span class="mathjax">Chowdhery, A</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Chowdhery%2C+A">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Chowdhery, A">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Chowdhery%2C+A&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Chowdhery, A">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.02311">arXiv:2204.02311</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.02311">pdf</a>, <a href="https://arxiv.org/format/2204.02311">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PaLM: Scaling Language Modeling with Pathways
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Narang%2C+S">Sharan Narang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Devlin%2C+J">Jacob Devlin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bosma%2C+M">Maarten Bosma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mishra%2C+G">Gaurav Mishra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roberts%2C+A">Adam Roberts</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barham%2C+P">Paul Barham</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chung%2C+H+W">Hyung Won Chung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sutton%2C+C">Charles Sutton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gehrmann%2C+S">Sebastian Gehrmann</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schuh%2C+P">Parker Schuh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+K">Kensen Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsvyashchenko%2C+S">Sasha Tsvyashchenko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Maynez%2C+J">Joshua Maynez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rao%2C+A">Abhishek Rao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barnes%2C+P">Parker Barnes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tay%2C+Y">Yi Tay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shazeer%2C+N">Noam Shazeer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Prabhakaran%2C+V">Vinodkumar Prabhakaran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reif%2C+E">Emily Reif</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+N">Nan Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hutchinson%2C+B">Ben Hutchinson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pope%2C+R">Reiner Pope</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bradbury%2C+J">James Bradbury</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Austin%2C+J">Jacob Austin</a>
      , et al. (42 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.02311v3-abstract-short" style="display: inline;">
        Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Tran&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.02311v3-abstract-full').style.display = 'inline'; document.getElementById('2204.02311v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.02311v3-abstract-full" style="display: none;">
        Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.02311v3-abstract-full').style.display = 'none'; document.getElementById('2204.02311v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.17189">arXiv:2203.17189</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.17189">pdf</a>, <a href="https://arxiv.org/format/2203.17189">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Roberts%2C+A">Adam Roberts</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chung%2C+H+W">Hyung Won Chung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Levskaya%2C+A">Anselm Levskaya</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mishra%2C+G">Gaurav Mishra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bradbury%2C+J">James Bradbury</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Andor%2C+D">Daniel Andor</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Narang%2C+S">Sharan Narang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lester%2C+B">Brian Lester</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gaffney%2C+C">Colin Gaffney</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mohiuddin%2C+A">Afroz Mohiuddin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hawthorne%2C+C">Curtis Hawthorne</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lewkowycz%2C+A">Aitor Lewkowycz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Salcianu%2C+A">Alex Salcianu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=van+Zee%2C+M">Marc van Zee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Austin%2C+J">Jacob Austin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goodman%2C+S">Sebastian Goodman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Soares%2C+L+B">Livio Baldini Soares</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+H">Haitang Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsvyashchenko%2C+S">Sasha Tsvyashchenko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bastings%2C+J">Jasmijn Bastings</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bulian%2C+J">Jannis Bulian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garcia%2C+X">Xavier Garcia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ni%2C+J">Jianmo Ni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+A">Andrew Chen</a>
      , et al. (18 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.17189v1-abstract-short" style="display: inline;">
        Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we presen&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.17189v1-abstract-full').style.display = 'inline'; document.getElementById('2203.17189v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.17189v1-abstract-full" style="display: none;">
        Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we present two software libraries that ease these issues: $\texttt{t5x}$ simplifies the process of building and training large language models at scale while maintaining ease of use, and $\texttt{seqio}$ provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on datasets with multiple terabytes of training data.
  Along with the libraries, we release configurations and instructions for T5-like encoder-decoder models as well as GPT-like decoder-only architectures.
  $\texttt{t5x}$ and $\texttt{seqio}$ are open source and available at https://github.com/google-research/t5x and https://github.com/google/seqio, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.17189v1-abstract-full').style.display = 'none'; document.getElementById('2203.17189v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.12533">arXiv:2203.12533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.12533">pdf</a>, <a href="https://arxiv.org/format/2203.12533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pathways: Asynchronous Distributed Dataflow for ML
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Barham%2C+P">Paul Barham</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dean%2C+J">Jeff Dean</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ghemawat%2C+S">Sanjay Ghemawat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hand%2C+S">Steven Hand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hurt%2C+D">Dan Hurt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Isard%2C+M">Michael Isard</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lim%2C+H">Hyeontaek Lim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pang%2C+R">Ruoming Pang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roy%2C+S">Sudip Roy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saeta%2C+B">Brennan Saeta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schuh%2C+P">Parker Schuh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sepassi%2C+R">Ryan Sepassi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shafey%2C+L+E">Laurent El Shafey</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Thekkath%2C+C+A">Chandramohan A. Thekkath</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yonghui Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.12533v1-abstract-short" style="display: inline;">
        We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous paral&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12533v1-abstract-full').style.display = 'inline'; document.getElementById('2203.12533v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.12533v1-abstract-full" style="display: none;">
        We present the design of a new large scale orchestration layer for accelerators. Our system, Pathways, is explicitly designed to enable exploration of new systems and ML research ideas, while retaining state of the art performance for current models. Pathways uses a sharded dataflow graph of asynchronous operators that consume and produce futures, and efficiently gang-schedules heterogeneous parallel computations on thousands of accelerators while coordinating data transfers over their dedicated interconnects. Pathways makes use of a novel asynchronous distributed dataflow design that lets the control plane execute in parallel despite dependencies in the data plane. This design, with careful engineering, allows Pathways to adopt a single-controller model that makes it easier to express complex new parallelism patterns. We demonstrate that Pathways can achieve performance parity (~100% accelerator utilization) with state-of-the-art systems when running SPMD computations over 2048 TPUs, while also delivering throughput comparable to the SPMD case for Transformer models that are pipelined across 16 stages, or sharded across two islands of accelerators connected over a data center network.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12533v1-abstract-full').style.display = 'none'; document.getElementById('2203.12533v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">MLSys 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11171">arXiv:2203.11171</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11171">pdf</a>, <a href="https://arxiv.org/format/2203.11171">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Consistency Improves Chain of Thought Reasoning in Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xuezhi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+J">Jason Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schuurmans%2C+D">Dale Schuurmans</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Le%2C+Q">Quoc Le</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chi%2C+E">Ed Chi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Narang%2C+S">Sharan Narang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+D">Denny Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11171v2-abstract-short" style="display: inline;">
        We explore a simple ensemble strategy, self-consistency, that significantly improves the reasoning accuracy of large language models. The idea is to sample a diverse set of reasoning paths from a language model via chain of thought prompting then return the most consistent final answer in the set. We evaluate self-consistency on a range of arithmetic and commonsense reasoning benchmarks, and find&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11171v2-abstract-full').style.display = 'inline'; document.getElementById('2203.11171v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11171v2-abstract-full" style="display: none;">
        We explore a simple ensemble strategy, self-consistency, that significantly improves the reasoning accuracy of large language models. The idea is to sample a diverse set of reasoning paths from a language model via chain of thought prompting then return the most consistent final answer in the set. We evaluate self-consistency on a range of arithmetic and commonsense reasoning benchmarks, and find that it robustly improves accuracy across a variety of language models and model scales without the need for additional training or auxiliary models. When combined with a recent large language model, PaLM-540B, self-consistency increases performance to state-of-the-art levels across several benchmark reasoning tasks, including GSM8K (56.5% -&gt; 74.4%), SVAMP (79.0% -&gt; 86.6%), AQuA (35.8% -&gt; 48.3%), StrategyQA (75.3% -&gt; 81.6%) and ARC-challenge (85.2% -&gt; 88.7%).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11171v2-abstract-full').style.display = 'none'; document.getElementById('2203.11171v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">V2: added PaLM based results</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.12763">arXiv:2111.12763</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.12763">pdf</a>, <a href="https://arxiv.org/format/2111.12763">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sparse is Enough in Scaling Transformers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jaszczur%2C+S">Sebastian Jaszczur</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mohiuddin%2C+A">Afroz Mohiuddin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaiser%2C+%C5%81">Łukasz Kaiser</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gajewski%2C+W">Wojciech Gajewski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Michalewski%2C+H">Henryk Michalewski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kanerva%2C+J">Jonni Kanerva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.12763v1-abstract-short" style="display: inline;">
        Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to sca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12763v1-abstract-full').style.display = 'inline'; document.getElementById('2111.12763v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.12763v1-abstract-full" style="display: none;">
        Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12763v1-abstract-full').style.display = 'none'; document.getElementById('2111.12763v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.03760">arXiv:2106.03760</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.03760">pdf</a>, <a href="https://arxiv.org/format/2106.03760">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hazimeh%2C+H">Hussein Hazimeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Z">Zhe Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sathiamoorthy%2C+M">Maheswaran Sathiamoorthy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yihua Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mazumder%2C+R">Rahul Mazumder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+L">Lichan Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chi%2C+E+H">Ed H. Chi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.03760v3-abstract-short" style="display: inline;">
        The Mixture-of-Experts (MoE) architecture is showing promising results in improving parameter sharing in multi-task learning (MTL) and in scaling high-capacity neural networks. State-of-the-art MoE models use a trainable sparse gate to select a subset of the experts for each input example. While conceptually appealing, existing sparse gates, such as Top-k, are not smooth. The lack of smoothness ca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03760v3-abstract-full').style.display = 'inline'; document.getElementById('2106.03760v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.03760v3-abstract-full" style="display: none;">
        The Mixture-of-Experts (MoE) architecture is showing promising results in improving parameter sharing in multi-task learning (MTL) and in scaling high-capacity neural networks. State-of-the-art MoE models use a trainable sparse gate to select a subset of the experts for each input example. While conceptually appealing, existing sparse gates, such as Top-k, are not smooth. The lack of smoothness can lead to convergence and statistical performance issues when training with gradient-based methods. In this paper, we develop DSelect-k: a continuously differentiable and sparse gate for MoE, based on a novel binary encoding formulation. The gate can be trained using first-order methods, such as stochastic gradient descent, and offers explicit control over the number of experts to select. We demonstrate the effectiveness of DSelect-k on both synthetic and real MTL datasets with up to $128$ tasks. Our experiments indicate that DSelect-k can achieve statistically significant improvements in prediction and expert selection over popular MoE gates. Notably, on a real-world, large-scale recommender system, DSelect-k achieves over $22\%$ improvement in predictive performance compared to Top-k. We provide an open-source implementation of DSelect-k.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03760v3-abstract-full').style.display = 'none'; document.getElementById('2106.03760v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Appeared in NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.05721">arXiv:1906.05721</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.05721">pdf</a>, <a href="https://arxiv.org/format/1906.05721">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual Wake Words Dataset
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chowdhery%2C+A">Aakanksha Chowdhery</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Warden%2C+P">Pete Warden</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Howard%2C+A">Andrew Howard</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rhodes%2C+R">Rocky Rhodes</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.05721v1-abstract-short" style="display: inline;">
        The emergence of Internet of Things (IoT) applications requires intelligence on the edge. Microcontrollers provide a low-cost compute platform to deploy intelligent IoT applications using machine learning at scale, but have extremely limited on-chip memory and compute capability. To deploy computer vision on such devices, we need tiny vision models that fit within a few hundred kilobytes of memory&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.05721v1-abstract-full').style.display = 'inline'; document.getElementById('1906.05721v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.05721v1-abstract-full" style="display: none;">
        The emergence of Internet of Things (IoT) applications requires intelligence on the edge. Microcontrollers provide a low-cost compute platform to deploy intelligent IoT applications using machine learning at scale, but have extremely limited on-chip memory and compute capability. To deploy computer vision on such devices, we need tiny vision models that fit within a few hundred kilobytes of memory footprint in terms of peak usage and model size on device storage. To facilitate the development of microcontroller friendly models, we present a new dataset, Visual Wake Words, that represents a common microcontroller vision use-case of identifying whether a person is present in the image or not, and provides a realistic benchmark for tiny vision models. Within a limited memory footprint of 250 KB, several state-of-the-art mobile models achieve accuracy of 85-90% on the Visual Wake Words dataset. We anticipate the proposed dataset will advance the research on tiny vision models that can push the pareto-optimal boundary in terms of accuracy versus memory usage for microcontroller applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.05721v1-abstract-full').style.display = 'none'; document.getElementById('1906.05721v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 4 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.10; B.7.1; I.5.2
        
      </p>
    

    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>