<!DOCTYPE html>
<html lang="en"     class="pb-page"  data-request-id="0beff509-d1f6-4896-9e71-6e68041e2b45"  
><head data-pb-dropzone="head"><meta name="pbContext" content=";taxonomy:taxonomy:conference-collections;wgroup:string:ACM Publication Websites;groupTopic:topic:acm-pubtype&gt;proceeding;article:article:doi\:10.1145/3404835.3463257;issue:issue:doi\:10.1145/3404835;ctype:string:Book Content;page:string:Article/Chapter View;subPage:string:Abstract;csubtype:string:Conference Proceedings;website:website:dl-site;topic:topic:conference-collections&gt;ir;journal:journal:acmconferences;pageGroup:string:Publication Pages"/>
<link rel="schema.DC" href="http://purl.org/DC/elements/1.0/"></link><meta name="dc.Format" content="text/HTML"></meta><meta name="dc.Language" content="EN"></meta><meta name="dc.Coverage" content="world"></meta><!-- ${citation_fulltext_world_readable:10.1145/3404835.3463257} -->






<title>WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning | Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>






<meta charset="UTF-8">
<meta name="robots" content="noarchive"/>















        <meta property="og:title" content="WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning | Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval"/>
    


        <meta property="og:type" content="Chapter"/>
    


        <meta property="og:url" content="https://dl.acm.org/doi/abs/10.1145/3404835.3463257"/>
    

        <meta property="og:image" content="https://dl.acm.org/cms/asset/d3008eac-eecf-4e32-8e62-f3f1fcfa3517/3404835.3463257.key.jpg" />
        
            <meta property="og:image:width" content="960" />
            <meta property="og:image:height" content="540" />
        
    


        <meta property="og:site_name" content="ACM Conferences"/>
    


<meta name="viewport" content="width=device-width,initial-scale=1"/>
        <meta name="citation_fulltext_world_readable" content=""/>
    



    <!--link(rel='stylesheet', href='https://fonts.googleapis.com/css?family=Merriweather+Sans:300,400,700,800|Merriweather:300,400,700,900')--><link rel="stylesheet" href="/products/acm/releasedAssets/css/build-6a887fc5a1adefe2abba.css"/><link rel="stylesheet" href="/products/acm/releasedAssets/css/print-6a887fc5a1adefe2abba.css" media="print"/>



<meta http-equiv="X-UA-Compatible" content="IE=edge">
<link rel="stylesheet" href="/pb-assets/styles/acm-custom-1649942624337.css">
<script defer src="https://scholar.google.com/scholar_js/casa.js" async></script>
<script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=t.forceSSL||"https:"===document.location.protocol,a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=(r?"https:":"http:")+"//cdn.heapanalytics.com/js/heap-"+e+".js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n);for(var o=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","resetIdentity","removeEventProperty","setEventProperties","track","unsetEventProperty"],c=0;c<p.length;c++)heap[p[c]]=o(p[c])};
      heap.load("1083010732");
</script>
<link rel="apple-touch-icon" sizes="180x180" href="/pb-assets/head-metadata/apple-touch-icon-1574252172393.png">
<link rel="icon" type="image/png" sizes="32x32" href="/pb-assets/head-metadata/favicon-32x32-1574252172003.png">
<link rel="icon" type="image/png" sizes="16x16" href="/pb-assets/head-metadata/favicon-16x16-1574252172937.png">
<link rel="manifest" href="/pb-assets/head-metadata/site-1574252171130.webmanifest" crossorigin="use-credentials">
<link rel="mask-icon" href="/pb-assets/head-metadata/safari-pinned-tab-1574252171193.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#00a300">
<meta name="theme-color" content="#ffffff">
<script defer src="https://static.cloudflareinsights.com/beacon.min.js/v652eace1692a40cfa3763df669d7439c1639079717194" integrity="sha512-Gi7xpJR8tSkrpF7aordPZQlW2DLtzUlZcumS8dMQjwDHEnw9I7ZLyiOj/6tZStRBGtGgN6ceN6cMH8z7etPGlw==" data-cf-beacon='{"rayId":"6e539318ac0a38db","token":"b7f168b3cd354a55a4dd51b513830799","version":"2021.12.0","si":100}' crossorigin="anonymous"></script>

    <!-- Google Tag Manager -->
    
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-NFGCMX');</script>
    <!-- End Google Tag Manager -->





</head>
<body class="pb-ui">


    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NFGCMX" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->




















<script type="text/javascript">

    if(false) {
        document.getElementById("skipNavigationLink").onclick =function skipElement () {
            var element = document.getElementById('');
            if(element == null || element == undefined) {
                element = document.getElementsByClassName('').item(0);
            }
            element.setAttribute('tabindex','0');
            element.focus();

        }

    }

</script>





    <div id="pb-page-content" data-ng-non-bindable>
        <div data-pb-dropzone="main" data-pb-dropzone-name="Main">




        
        









        <header data-widget-def="ux3-layout-widget" data-widget-id="88f434fd-b4d9-4f0e-b7b6-cde0fadcc9ef" class="header base ir">
        



        
        <div class="header__fixed-items fixed fixed-element">









        <div data-widget-def="ux3-layout-widget" data-widget-id="c844bab8-b1d5-4df4-8bea-71f6156459e4" class="container header--first-row">
        



        
        <div class="pull-left">









        <div data-widget-def="ux3-general-image" data-widget-id="83eada61-9003-4cb3-99a8-a38fa7692761" class="header__logo1">
        



        
        <a href="/" title="ACM Digital Library home"><img id="" alt="ACM Digital Library home" src="/specs/products/acm/releasedAssets/images/acm-dl-logo-white.png"/></a>

        </div>
    










        <div data-widget-def="ux3-general-image" data-widget-id="5517ee17-e847-46ff-b42e-c5a6754e032d" class="header__logo2">
        



        
        <a href="https://www.acm.org" title="ACM home"><img id="" alt="ACM home" src="specs/products/acm/releasedAssets/images/acm-logo-1.png"/></a>

        </div>
    
</div><div class="pull-right">









        <div data-widget-def="ux3-layout-widget" data-widget-id="ab4ba9a1-6e57-4b90-b9c2-fc7914d9a3a1" class="header__quick-menu text-onDark">
        



        
        <ul class="rlist--inline"><li class="advanced-search-link hidden-lg hidden-md hidden-sm">



        
        <a href="/search/advanced" >Advanced Search</a>
</li><li class="show-on-mobile">



        
        <a href="/browse/" title="browse by title or publisher">Browse</a>
</li><li class="show-on-mobile">



        
        <a href="/about" title="About the ACM Digital Library">About</a>
</li><li class="login-list">



        
        




    








        





    











<div class="dropBlock loginBar login-open">
    <ul class="rlist--inline">
        <li class="login-link">
            <a href="/action/showLogin?redirectUri=%2Fdoi%2Fabs%2F10.1145%2F3404835.3463257">
                Sign in
            </a>
        </li>
        <li class="register-link">
            
                    <a href="https://accounts.acm.org?redirectUri=%2Fdoi%2Fabs%2F10.1145%2F3404835.3463257" class="btn" title="Register">
                        Register
                    </a>
                
        </li>
    </ul>
</div>



    

</li></ul>

        </div>
    
</div>

        </div>
    










        <div data-widget-def="menuWidget" data-widget-id="13da1316-80a7-4370-a291-8968cd9cb35e" class="container header--second-row">
        



        
        <div class="responsive-menu-container clearfix"><div class="left-section pull-left"></div><div class="menu-section"><div class="main-nav responsive-menu-nav"><a href="#global-menu" data-target="global-menu" data-toggle="nav" title="menu drawer"><span class="drawer__icons-box"><span class="drawer__icons-controls"></span></span></a><nav role="navigation" id="global-menu" data-ctrl-res="screen-sm" class="drawer__nav container gutterless hidden-sm hidden-xs"><ul id="menubar" role="menubar" class="menubar rlist--inline"><li class="hidden-lg hidden-md"><div data-toggle="transplant" data-target-class="hidden-xs hidden-sm" data-target=".header--second-row .quick-search--form" data-direction="from" data-transplant="self" class="quick-search-mobile"></div></li><li role="menuitem" aria-label="Advanced Search" id="menu-item-global-menu-0" class="menu-item mobile-menu-item"><a href="/search/advanced"><span>Advanced Search</span></a></li><li role="menuitem" aria-label="Journals" id="menu-item-global-menu-1" class="menu-item"><a href="/journals"><span>Journals</span></a></li><li role="menuitem" aria-label="Magazines" id="menu-item-global-menu-2" class="menu-item"><a href="/magazines"><span>Magazines</span></a></li><li role="menuitem" aria-label="Proceedings" id="menu-item-global-menu-3" class="menu-item"><a href="/proceedings"><span>Proceedings</span></a></li><li role="menuitem" aria-label="Books" id="menu-item-global-menu-4" class="menu-item"><a href="/acmbooks"><span>Books</span></a></li><li role="menuitem" aria-label="SIGs" id="menu-item-global-menu-5" class="menu-item"><a href="/sigs"><span>SIGs</span></a></li><li role="menuitem" aria-label="Conferences" id="menu-item-global-menu-6" class="menu-item"><a href="/conferences"><span>Conferences</span></a></li><li role="menuitem" aria-label="People" id="menu-item-global-menu-7" class="menu-item"><a href="/people"><span>People</span></a></li><li class="hidden-lg hidden-md"><div data-toggle="transplant" data-target-class="hidden-xs hidden-sm" data-target=".header__quick-menu .show-on-mobile" data-direction="from" data-transplant="self"></div></li><li role="menuitem" aria-haspopup="true" aria-label="More" class="dropdown-more hidden dropdown menu-parent"><a href="#" title="More" data-toggle="dropdown" class="dropdown__toggle"><span>More</span><i aria-hidden="true" class="icon-section_arrow_d pull-right hidden-sm hidden-xs"></i></a><ul aria-labelledby="main0-1" aria-hidden="true" role="menu" class="rlist dropdown__menu"></ul></li><li class="menubar__extra-mob-links"></li></ul></nav></div></div><div class="right-section pull-right">



        
        <div class="quickSearchFormContainer"><div class="quick-search"><div class="clearfix quick-search__clearfix"><form action="/action/doSearch" name="defaultQuickSearch" method="get" title="Quick Search" class="quick-search--form"><div class="quick-search--input"><div class="input-group option-0"><label for="AllFieldf73847c0-d936-48ec-9bf9-139b3828507f" class="sr-only">Search ACM Digital Library</label><input id="AllFieldf73847c0-d936-48ec-9bf9-139b3828507f" type="search" name="AllField" placeholder="Search ACM Digital Library" data-auto-complete-max-words="7" data-auto-complete-max-chars="32" data-contributors-conf="3" data-topics-conf="3" data-publication-titles-conf="3" data-history-items-conf="3" data-group-titles-conf="3" value="" class="autocomplete quick-search__input"/></div></div><div class="quick-search--button"><button type="submit" title="Search" aria-label="Search" class="btn quick-search__button icon-Icon_Search"><span class="sr-only">Search</span><span>Search</span></button></div></form></div>



        
        <a class="quick-search__advancedHeader" title="link to Advanced Search form" href="/search/advanced">Advanced Search</a>
</div></div>
</div></div><div class="header__dropzone"></div>

        </div>
    




        
        <span style="display: none;">10.1145/3404835.3463257</span><span style="display: none;">acmconferences</span><span style="display: none;">Article/Chapter View</span><span style="display: none;">Abstract</span><span style="display: none;">Publication Pages</span><span style="display: none;">ir</span><span style="display: none;">Conference Proceedings</span><span style="display: none;">conference-collections</span><div class="sub-nav fixed-element"><a href="#" data-db-target-for="sub-nav__items" data-more="false" data-lines="1" style="text-transform: uppercase;" class="sub-nav__toggle hidden-lg truncate-text-css">ir</a><div class="sub-nav__container"><div class="container"><div data-ctrl-res="screen-md" class="responsive-menu-container"><div class="left-section pull-left"></div><div class="menu-section"><div style="display: block" class="responsive-menu-nav"><nav id="sub-nav__items" data-db-target-of="sub-nav__items" class="sub-nav__holder drawer__nav"><ul role="menu" class="rlist--inline menubar"><li role="menuitem"><a href="/conference/ir" class="sub-nav__item">Conference</a></li><li role="menuitem"><a href="/conference/ir/proceedings" class="sub-nav__item">Proceedings</a></li><li role="menuitem"><a href="/conference/ir/upcoming" class="sub-nav__item">Upcoming Events</a></li><li role="menuitem"><a href="/conference/ir/authors" class="sub-nav__item">Authors</a></li><li role="menuitem"><a href="/conference/ir/affiliations" class="sub-nav__item">Affiliations</a></li><li role="menuitem"><a href="/conference/ir/award-winners" class="sub-nav__item">Award Winners</a></li><li role="menuitem" aria-haspopup="true" aria-label="More" class="dropdown-more hidden dropdown menu-parent"><a href="#" title="More" data-toggle="dropdown" class="dropdown__toggle hidden-md hidden-sm hidden-xs"><span>More</span><i aria-hidden="true" class="icon-section_arrow_d"></i></a><ul aria-labelledby="main0-1" aria-hidden="true" role="menu" class="rlist dropdown__menu visible-md visible-sm visible-xs"></ul></li></ul></nav></div></div><div class="right-section hidden-md hidden-sm hidden-xs"></div></div></div></div><div class="hidden-xs hidden-sm"><div class="progress-bar--wrapper"><div style="width: 0.116818%;" class="progress-bar"></div></div></div></div>
</div><div></div><div class="banner conference-landing"></div><div class="header-breadcrumb">









        <div data-widget-def="ux3-layout-widget" data-widget-id="f5f275c5-a862-48bf-8119-8528d04615da" class="container">
        



        
        <div>



        
        



        
        <nav class="article__breadcrumbs separator"><a href="https://dl.acm.org/" class="article__tocHeading">Home</a><a href="/conferences" class="article__tocHeading">Conferences</a><a href="/conference/ir" class="article__tocHeading">IR</a><a href="/conference/ir/proceedings" class="article__tocHeading">Proceedings</a><a href="/doi/proceedings/10.1145/3404835" class="article__tocHeading">SIGIR '21</a><a href="/doi/abs/10.1145/3404835.3463257" class="article__tocHeading">WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning</a></nav>

</div>

        </div>
    
</div>

        </header>
    






        
        <main class="content ir loi-page no-margin">









        <div data-widget-def="UX3combinedRecommendationsWidget" data-widget-id="bf2b14e9-d9dc-4518-8bb0-d9b157f47c7e" class="recommendations" data-show-id="sec-ref">
        



        
        <div data-ajaxurl="/pb/widgets/lazyLoadRecommended?widgetId=bf2b14e9-d9dc-4518-8bb0-d9b157f47c7e&amp;doi=10.1145%2F3404835.3463257&amp;pbContext=%3Btaxonomy%3Ataxonomy%3Aconference-collections%3Bwgroup%3Astring%3AACM+Publication+Websites%3BgroupTopic%3Atopic%3Aacm-pubtype%26gt%3Bproceeding%3Barticle%3Aarticle%3Adoi%5C%3A10.1145%2F3404835.3463257%3Bissue%3Aissue%3Adoi%5C%3A10.1145%2F3404835%3Bctype%3Astring%3ABook+Content%3Bpage%3Astring%3AArticle%2FChapter+View%3BsubPage%3Astring%3AAbstract%3Bcsubtype%3Astring%3AConference+Proceedings%3Bwebsite%3Awebsite%3Adl-site%3Btopic%3Atopic%3Aconference-collections%26gt%3Bir%3Bjournal%3Ajournal%3Aacmconferences%3BpageGroup%3Astring%3APublication+Pages" class="recommended--lazyLoad"></div>

        </div>
    










        <div data-widget-def="ux3-publicationContent-widget" data-widget-id="f69d88a8-b404-4aae-83a9-9acea4426d78" class="container">
        



        
        <article data-enable-mathjax="true" data-citationInfo="/action/acmCitationInfo?widgetId=f69d88a8-b404-4aae-83a9-9acea4426d78&ajax=true&doi=10.1145%2F3404835.3463257&pbContext=%3Btaxonomy%3Ataxonomy%3Aconference-collections%3Bwgroup%3Astring%3AACM+Publication+Websites%3BgroupTopic%3Atopic%3Aacm-pubtype%26gt%3Bproceeding%3Barticle%3Aarticle%3Adoi%5C%3A10.1145%2F3404835.3463257%3Bissue%3Aissue%3Adoi%5C%3A10.1145%2F3404835%3Bctype%3Astring%3ABook+Content%3Bpage%3Astring%3AArticle%2FChapter+View%3BsubPage%3Astring%3AAbstract%3Bcsubtype%3Astring%3AConference+Proceedings%3Bwebsite%3Awebsite%3Adl-site%3Btopic%3Atopic%3Aconference-collections%26gt%3Bir%3Bjournal%3Ajournal%3Aacmconferences%3BpageGroup%3Astring%3APublication+Pages" data-proceedingInfo="/action/proceedingArticleMeta?widgetId=f69d88a8-b404-4aae-83a9-9acea4426d78&ajax=true&doi=10.1145%2F3404835.3463257&pbContext=%3Btaxonomy%3Ataxonomy%3Aconference-collections%3Bwgroup%3Astring%3AACM+Publication+Websites%3BgroupTopic%3Atopic%3Aacm-pubtype%26gt%3Bproceeding%3Barticle%3Aarticle%3Adoi%5C%3A10.1145%2F3404835.3463257%3Bissue%3Aissue%3Adoi%5C%3A10.1145%2F3404835%3Bctype%3Astring%3ABook+Content%3Bpage%3Astring%3AArticle%2FChapter+View%3BsubPage%3Astring%3AAbstract%3Bcsubtype%3Astring%3AConference+Proceedings%3Bwebsite%3Awebsite%3Adl-site%3Btopic%3Atopic%3Aconference-collections%26gt%3Bir%3Bjournal%3Ajournal%3Aacmconferences%3BpageGroup%3Astring%3APublication+Pages"><div class="row"><div class="col-md-2 col-sm-3"></div><div class="col-md-8 col-sm-7"><div class="article-citations"><div class="citation"><div class="issue-item__citation clearfix"><span class="issue-heading">short-paper </span><span class="article__access"><span><div title="This content is available through an open access license" class="access-icon open-access">Open Access</div></span></span>



        
        <!-- Go to www.addthis.com/dashboard to customize your tools --><script type="text/javascript" async="async" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=xa-4faab26f2cff13a7"></script><div class="share__block share__inline-links"><div class="pb-dropzone" data-pb-dropzone="shareBlock" title="shareBlock"></div><span class="sr-only">Share on</span><div class="rlist--inline addthis addthis_toolbox addthis_default_style addthis_32x32_style"><a role="link" class="addthis_button_twitter"><i aria-hidden="true" class="at-icon-wrapper icon-Icon_Twitter"></i></a><a role="link" class="addthis_button_linkedin"><i aria-hidden="true" class="at-icon-wrapper icon-linkedin"></i></a><a role="link" class="addthis_button_reddit"><i aria-hidden="true" class="at-icon-wrapper icon-riddit-filled"></i></a><a role="link" class="addthis_button_facebook"><i aria-hidden="true" class="at-icon-wrapper icon-Icon_Facebook"></i></a><a role="link" class="addthis_button_email"><i aria-hidden="true" class="at-icon-wrapper icon-Icon_mail"></i></a><div class="pb-dropzone" data-pb-dropzone="share-additional-links" title="share-additional-links"></div></div></div>
</div><div class="border-bottom clearfix"><h1 class="citation__title">WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning</h1></div><div class="border-bottom clearfix"> <div id="sb-1"> <ul data-lines="2" ariaa-label="authors" class="rlist--inline loa truncate-list"> <li class="label"><b>Authors: </b></li> <li class="loa__item"><a id="arnd_20833013291022749_Ctrl" href="javascript:void(0);" aria-controls="arnd_20833013291022749" aria-haspopup="true" class="author-name" title="Krishna Srinivasan"><span class="loa__author-info"> <div class="author-data"><span class="loa__author-name"><span><img class="author-picture" src="/pb-assets/icons/DOs/default-profile-1543932446943.svg" width="24" height="24" alt="" aria-hidden="true"/>Krishna Srinivasan</span></span></div><span class="loa_author_inst"> <p data-pill-inst="arnd_20833013291022749" data-doi="10.1145/contrib-99659817967">Google, Mountain View, CA, USA</p></span></span></a><div id="arnd_20833013291022749" aria-labelledby="arnd_20833013291022749_Ctrl" class="author-info"> <div class="author-info__header" data-pill="d5537430e136"></div> <div class="author-info__body"> <p data-pill-inst="d5537430e136" data-doi="10.1145/contrib-99659817967">Google, Mountain View, CA, USA</p><a href="/profile/99659817967" class="btn blue stretched">View Profile</a></div> </div><span>,</span></li> <li class="loa__item"><a id="arnd_20833013291416690_Ctrl" href="javascript:void(0);" aria-controls="arnd_20833013291416690" aria-haspopup="true" class="author-name" title="Karthik Raman"><span class="loa__author-info"> <div class="author-data"><span class="loa__author-name"><span><img class="author-picture" src="/pb-assets/icons/DOs/default-profile-1543932446943.svg" width="24" height="24" alt="" aria-hidden="true"/>Karthik Raman</span></span></div><span class="loa_author_inst"> <p data-pill-inst="arnd_20833013291416690" data-doi="10.1145/contrib-99659122518">Google, Mountain View, CA, USA</p></span></span></a><div id="arnd_20833013291416690" aria-labelledby="arnd_20833013291416690_Ctrl" class="author-info"> <div class="author-info__header" data-pill="d5537430e150"></div> <div class="author-info__body"> <p data-pill-inst="d5537430e150" data-doi="10.1145/contrib-99659122518">Google, Mountain View, CA, USA</p><a href="/profile/99659122518" class="btn blue stretched">View Profile</a></div> </div><span>,</span></li> <li class="loa__item"><a id="arnd_20833013291887544_Ctrl" href="javascript:void(0);" aria-controls="arnd_20833013291887544" aria-haspopup="true" class="author-name" title="Jiecao Chen"><span class="loa__author-info"> <div class="author-data"><span class="loa__author-name"><span><img class="author-picture" src="/pb-assets/icons/DOs/default-profile-1543932446943.svg" width="24" height="24" alt="" aria-hidden="true"/>Jiecao Chen</span></span></div><span class="loa_author_inst"> <p data-pill-inst="arnd_20833013291887544" data-doi="10.1145/contrib-99659733722">Google, Mountain View, CA, USA</p></span></span></a><div id="arnd_20833013291887544" aria-labelledby="arnd_20833013291887544_Ctrl" class="author-info"> <div class="author-info__header" data-pill="d5537430e164"></div> <div class="author-info__body"> <p data-pill-inst="d5537430e164" data-doi="10.1145/contrib-99659733722">Google, Mountain View, CA, USA</p><a href="/profile/99659733722" class="btn blue stretched">View Profile</a></div> </div><span>,</span></li> <li class="loa__item"><a id="arnd_20833013292259580_Ctrl" href="javascript:void(0);" aria-controls="arnd_20833013292259580" aria-haspopup="true" class="author-name" title="Michael Bendersky"><span class="loa__author-info"> <div class="author-data"><span class="loa__author-name"><span><img class="author-picture" src="/pb-assets/icons/DOs/default-profile-1543932446943.svg" width="24" height="24" alt="" aria-hidden="true"/>Michael Bendersky</span></span></div><span class="loa_author_inst"> <p data-pill-inst="arnd_20833013292259580" data-doi="10.1145/contrib-81363606444">Google, Mountain View, CA, USA</p></span></span></a><div id="arnd_20833013292259580" aria-labelledby="arnd_20833013292259580_Ctrl" class="author-info"> <div class="author-info__header" data-pill="d5537430e178"></div> <div class="author-info__body"> <p data-pill-inst="d5537430e178" data-doi="10.1145/contrib-81363606444">Google, Mountain View, CA, USA</p><a href="/profile/81363606444" class="btn blue stretched">View Profile</a></div> </div><span>,</span></li> <li class="loa__item"><a id="arnd_20833013292697277_Ctrl" href="javascript:void(0);" aria-controls="arnd_20833013292697277" aria-haspopup="true" class="author-name" title="Marc Najork"><span class="loa__author-info"> <div class="author-data"><span class="loa__author-name"><span><img class="author-picture" src="/do/10.1145/contrib-81100303448/rel-imgonly/81100303448.jpg" width="24" height="24" alt="" aria-hidden="true"/>Marc Najork</span></span></div><span class="loa_author_inst"> <p data-pill-inst="arnd_20833013292697277" data-doi="10.1145/contrib-81100303448">Google, Mountain View, CA, USA</p></span></span></a><div id="arnd_20833013292697277" aria-labelledby="arnd_20833013292697277_Ctrl" class="author-info"> <div class="author-info__header" data-pill="d5537430e192"></div> <div class="author-info__body"> <p data-pill-inst="d5537430e192" data-doi="10.1145/contrib-81100303448">Google, Mountain View, CA, USA</p><a href="/profile/81100303448" class="btn blue stretched">View Profile</a></div> </div> </li> </ul><a href="#pill-authors__contentcon" data-slide-target="#pill-information" data-full-screen="false" data-ctrl-res="screen-xlg" data-tab="pill-authors__content" data-label="<i class='icon-Icon_Information'&gt;</i&gt; Information &amp; Authors" class="loa__link w-slide__btn tab-link slide-active">Authors Info &amp; Claims </a></div> </div> <div class="border-bottom clearfix"><div class="issue-item__detail"><a href="/doi/proceedings/10.1145/3404835" title="SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval"><span class="epub-section__title">SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</span></a><span class="dot-separator"><span class="epub-section__date">July 2021  </span></span><span class="dot-separator"><span class="epub-section__pagerange">Pages   2443–2449</span></span><span class="dot-separator"><a href="https://doi.org/10.1145/3404835.3463257" class="issue-item__doi">https://doi.org/10.1145/3404835.3463257</a></span></div></div><div class="border-bottom clearfix"><span class="bold label">Online:</span><span class="CitationCoverDate">11 July 2021</span><a href="#" data-slide-target="#pill-information" data-ctrl-res="screen-xlg" data-full-screen="false" data-tab="pill-information__content" class="publication-history-link w-slide__btn tab-link">Publication History</a></div>



        
        
<div class="border-bottom clearfix"><div class="issue-item__footer"><div class="issue-item__footer-info pull-left"><div tabindex="0" class="tooltip"><ul title="Click to stay open" class="rlist--inline"><li><span class="citation"><i class="icon-quote"></i><span>1</span><span class="sr-only">citation</span></span></li><li><span class="metric"><i class="icon-metric"></i><span>861</span></span></li><span class="sr-only">Downloads</span></ul><div class="tooltip__body"><span class="arrow"></span><div class="left-bordered-title">Metrics</div><a href="javascript:void(0);" data-slide-target="#pill-metric" data-ctrl-res="screen-xlg" data-full-screen="false" data-tab="pill-citations__content" title="See Bibliometrics &amp; Citations section" data-label="&lt;i class=&quot;icon-metric &quot;&gt;&lt;/i&gt;Bibliometrics &amp; Citations" class="w-slide__btn tab-link"><div class="citation">Total Citations<span class="bold">1</span></div></a><a href="javascript:void(0);" data-slide-target="#pill-metric" data-ctrl-res="screen-xlg" data-full-screen="false" data-tab="pill-bibliometrics__content" title="See Bibliometrics &amp; Citations section" data-label="&lt;i class=&quot;icon-metric &quot;&gt;&lt;/i&gt;Bibliometrics &amp; Citations" class="w-slide__btn tab-link"><div class="metric">Total Downloads<span class="bold">861</span></div></a><div class="info">Last 12 Months<span class="bold">861</span></div><div class="info">Last 6 weeks<span class="bold">161</span></div></div></div></div><div class="issue-item__footer-links pull-right"><ul class="rlist--inline">



        
        <li role="menu" class="article-tool"><a id="manageAlert__crtl" href="/action/addCitationAlert?doi=10.1145/3404835.3463257" role="menuitem" data-title="Get Citation Alerts" data-target="#manageAlert" aria-label="Get Citation Alerts" class="btn--icon simple-tooltip__block--b"><i aria-hidden="true" class="icon-Icon_Alerts"></i><span class="visibility-hidden">Get Citation Alerts</span></a><div class="ux-modal-container"><div id="manageAlert" class="modal"><div class="modal__dialog"><div class="modal__dialog--success"><div class="modal__header"><button type="button" data-dismiss="modal" class="close"><i aria-hidden="true" class="icon-close_thin"></i></button><a id="id-hatemile-navigation-6073290063892647-7" name="id-hatemile-navigation-6073290063892647-7" data-headinganchorfor="id-hatemile-navigation-6073290063892647-6" class="heading-anchor"></a><h2 id="id-hatemile-navigation-6073290063892647-6">New Citation Alert added!</h2></div><div class="modal__body"><p>This alert has been successfully added and will be sent to:<span class="email"></span></p><p>You will be notified whenever a record that you have chosen has been cited.</p></div><div class="modal__footer"><p>To manage your alert preferences, click on the button below.</p><a href="/action/showPreferences?menuTab=Alerts" title="Manage my Alerts" id="id-hatemile-display-1019331959102511-5" aria-label="Manage my Alerts" class="btn blue big stretched manageAlert__btn"><span data-attributetitleof="id-hatemile-display-1019331959102511-5" class="force-read-before">Manage my Alerts</span></a></div></div><div class="modal__dialog--error"><div class="modal__header"><button type="button" data-dismiss="modal" class="close"><i aria-hidden="true" class="icon-close_thin"></i></button><a id="id-hatemile-navigation-6073290063892647-9" name="id-hatemile-navigation-6073290063892647-9" data-headinganchorfor="id-hatemile-navigation-6073290063892647-8" class="heading-anchor"></a><h2 id="id-hatemile-navigation-6073290063892647-8">New Citation Alert!</h2></div><div class="modal__body"><p>Please <a href="/action/showLogin?redirectUri=/doi/abs/10.1145/3404835.3463257" title="Sign In" aria-label="Sign In" class="link">log in to your account</a></p></div></div></div></div></div></li>
<li role="menu" class="article-tool"><a href="javascript:void(0)" role="menuitem" data-title="Save to Binder" data-db-target-for="save-to-binder" data-link="/action/binderList?doi=10.1145/3404835.3463257" aria-label="save to binder" class="btn--icon simple-tooltip__block--b saveToBinders"><i aria-hidden="true" class="icon-add-folder"></i><span class="visibility-hidden">Save to Binder</span></a><div data-db-target-of="save-to-binder"><div class="saveToBinders-list"><input type="hidden" name="doiVal" value="10.1145/3404835.3463257"/><h5>Save to Binder</h5><div class="binder-list-alerts"></div><div class="saveToBinders-list__container"><ul class="rlist saveToBinders-list__items"><div class="loader"><img src="/specs/products/acm/releasedAssets/images/loader.gif"/></div></ul></div><div class="binder__create"><a data-action="show" href="/action/showMyBinders" title="Create a New Binder" class="show-binder-form toggle-binder-form"><i class="icon-plus-light"></i>Create a New Binder</a><div class="binder-create-form binder__create-mini-form"><div class="input-group input-group--gray-bg"><label>Name</label><div class="input-group__field-container--right-icon"><input type="text" placeholder="My new Binder" name="binderName" data-maxChar="256" class="binder-create-form__name charCount__text form-control"/><span class="input-group-addon input-group-addon--right"><span class="charLimit"></span></span></div></div><div><ul class="rlist--inline binder__create-btns-container"><li><button type="reset" value="Cancel" data-action="hide" class="toggle-binder-form btn btn--border-less btn transparent binder__create-cancel">Cancel</button></li><li class="pull-right"><button type="submit" value="Create Binder" disabled="disabled" data-append-newbinder-link="/action/createBinder" class="btn btn--no-margin blue big binder__create-submit binder-create-form__create"><i aria-hidden="true" class="icon-plus-light"></i>Create</button></li></ul></div></div></div></div></div></li><li role="menu" class="article-tool"><a href="/" role="menuitem" data-title="Export Citation" data-toggle="modal" data-target="#exportCitation" aria-label="Export Citations" class="btn--icon simple-tooltip__block--b"><i aria-hidden="true" class="icon-quote"></i><span class="visibility-hidden">Export Citation</span><input type="hidden" name="doiVal" value="10.1145/3404835.3463257"/></a></li><li class="publisher-site article-tool"><a href="#" class="btn blue"><span>Publisher Site</span><i aria-hidden="true" class="icon-export"></i></a></li></ul><ul class="rlist--inline right"><input type="hidden" id="formatsCount" value="2"/><li><a href="/doi/epdf/10.1145/3404835.3463257" title="View online with eReader" target="_blank" class="btn blue"><i class="icon-eReader"></i><span>eReader</span></a></li><li class="pdf-file"><a href="/doi/pdf/10.1145/3404835.3463257" title="PDF" target="_blank" class="btn red"><i class="icon-pdf-file"></i><span>PDF</span></a></li></ul></div></div></div></div></div></div><div class="col-sm-2"></div></div><div class="row"><div class="col-md-2 col-sm-3 sidebar-section article-sidebar-section"><div class="article__sections"><div class="sections-box"><div><div class="sections-box__static-items"><div class="parent-item"><h5 class="parent-item__title"><a href="/doi/proceedings/10.1145/3404835" data-lines="2" data-title="SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval" class="simple-tooltip__inline--r truncate-text-css">SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</a></h5><span href="/doi/proceedings/10.1145/3404835" data-lines="2" data-title="WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning" class="parent-item__subtitle simple-tooltip__inline--r truncate-text-css">WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning</span><div class="pageRange">Pages   2443–2449</div></div>



        
        <div class="content-navigation clearfix"><a href="/doi/10.1145/3404835.3463253" title="Previous item in TOC" class="content-navigation__btn--pre "><span><i class="icon-Arrow-Stroke"></i></span><span class="control">Previous</span><span class="type">Chapter</span></a><a href="/doi/10.1145/3404835.3463261" title="Next item in TOC" class="content-navigation__btn--next "><span class="control">Next</span><span class="type">Chapter</span><span><i class="icon-Arrow-Stroke"></i></span></a></div>
</div><div class="sections-block scroll-to-target"></div></div><div class="logo"><img src="/specs/products/acm/releasedAssets/images/footer-logo1.png" alt="ACM Digital Library" title="ACM Digital Library"/></div></div></div></div><div class="col-md-8 col-sm-7 sticko__side-content"><div class="stickybar__original__position"></div><nav class="stickybar coolBar trans visible-xs visible-sm"><div class="stickybar__wrapper coolBar__wrapper clearfix"><div class="coolBar__section"><a id="article_Ctrl" href="#" data-db-target-for="article-sec_Pop" role="button" aria-expanded="false" class="coolBar__ctrl"><i class="icon-burger"></i></a><div id="article_Pop" data-db-target-of="article-sec_Pop" aria-hidden="true" class="coolBar__drop fixed rlist"><div data-target=".article__sections" data-remove="false" data-target-class="hidden-xs hidden-sm" data-toggle="transplant" data-direction="from" data-transplant="self" class="transplant showit"></div></div></div><div data-target=".pill-section .pill-list" data-remove="false" data-target-class="hidden-xs hidden-sm" data-toggle="transplant" data-direction="from" data-transplant="self" class="transplant showit"></div></div></nav><div class="article__body article__abstractView"><!-- abstract content --><div class="hlFld-Abstract"><div class="article__section article__abstract hlFld-Abstract"><div class="colored-block__title"><h2 id="d5539997e1" class="section__title left-bordered-title">ABSTRACT</h2></div><div class="left-side-image"><img src="/cms/asset/d3008eac-eecf-4e32-8e62-f3f1fcfa3517/3404835.3463257.key.jpg" class="key-image" alt-text=""></div><div class="abstractSection abstractInFull">
		<p>The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information across image and text modalities. In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.5 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example. WIT Dataset is available for download and use via a Creative Commons license here: https://github.com/google-research-datasets/wit.</p>
	</div></div></div><!-- /abstract content --><div class="pb-dropzone" data-pb-dropzone="pubContentAccessDenialDropzone"><!-- Empty dropzone --></div><div data-rel="videos" class="article__section article__supplemental-material"><div class="colored-block__title"><h2 id="sec-supp" class="section__title left-bordered-title">Supplemental Material</h2></div><div class="article-media"><div class="row"><div class="component-container"><div class="article-media__item separated-block--dashed--bottom clearfix"><div class="video__image col-sm-5"><div class="cloudflare-stream-container"><input type="hidden" class="cloudFlareScript" name="cloudFlareScript" value="https://embed.videodelivery.net/embed/sdk.latest.js">
 <script>
window.addEventListener('load', function () {
            var script = document.querySelector('script[src="https://embed.videodelivery.net/embed/sdk.latest.js"]');
            if(script === null) {
            var s = document.createElement('script');
            s.type = 'text/javascript';
            s.defer = 'defer';
            s.src = "https://embed.videodelivery.net/embed/sdk.latest.js";
            document.getElementsByTagName('body')[0].appendChild(s);
            }
        });
</script>
<iframe src="https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiIyOWQzMmU3YTI3NThhZTk3YWI3YjUwNzlhZmY5MGYxYiIsImtpZCI6IjdiODM1ODc0NmU1YmY0MzQyNjljMTBlNjAwODRmNWJiIiwiZXhwIjoxNjUxNzg1NjA1fQ.ptZT1XQ4FeSYzU4c3PC0YJL4WhlyJdtUA55cyXVOk_M26HI_LVFEcLBlIWRmI5U8qN_x6tFvJHG7THR8CTFvWi9402xKlLsQAD_bRNB4ZX-ajShvkTZpdMW1D6fsHoyiwQ0RT2fCoNtXeKtF6tI-2Gye03nHQcPIKvv3ntu6A5jc07Txp6tcbZZoQMAn4imnUSVWYcc8g42OTJmcFQeKjQZo9pKRg0mZzdhCUrwkCbl4P9LEWvzWyz3RfoO3k3QD2O60djQi14cGSywaLrIy38la50p6V-MQXLb_Z2DAfciavtb6y3P6-Q1jhNM7LVNa55CtoM_TiQx7rpXA_1uOFQ?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiIyOWQzMmU3YTI3NThhZTk3YWI3YjUwNzlhZmY5MGYxYiIsImtpZCI6IjdiODM1ODc0NmU1YmY0MzQyNjljMTBlNjAwODRmNWJiIiwiZXhwIjoxNjUxNzg1NjA1fQ.ptZT1XQ4FeSYzU4c3PC0YJL4WhlyJdtUA55cyXVOk_M26HI_LVFEcLBlIWRmI5U8qN_x6tFvJHG7THR8CTFvWi9402xKlLsQAD_bRNB4ZX-ajShvkTZpdMW1D6fsHoyiwQ0RT2fCoNtXeKtF6tI-2Gye03nHQcPIKvv3ntu6A5jc07Txp6tcbZZoQMAn4imnUSVWYcc8g42OTJmcFQeKjQZo9pKRg0mZzdhCUrwkCbl4P9LEWvzWyz3RfoO3k3QD2O60djQi14cGSywaLrIy38la50p6V-MQXLb_Z2DAfciavtb6y3P6-Q1jhNM7LVNa55CtoM_TiQx7rpXA_1uOFQ%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D10.0s"  poster="https://videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiIyOWQzMmU3YTI3NThhZTk3YWI3YjUwNzlhZmY5MGYxYiIsImtpZCI6IjdiODM1ODc0NmU1YmY0MzQyNjljMTBlNjAwODRmNWJiIiwiZXhwIjoxNjUxNzg1NjA1fQ.ptZT1XQ4FeSYzU4c3PC0YJL4WhlyJdtUA55cyXVOk_M26HI_LVFEcLBlIWRmI5U8qN_x6tFvJHG7THR8CTFvWi9402xKlLsQAD_bRNB4ZX-ajShvkTZpdMW1D6fsHoyiwQ0RT2fCoNtXeKtF6tI-2Gye03nHQcPIKvv3ntu6A5jc07Txp6tcbZZoQMAn4imnUSVWYcc8g42OTJmcFQeKjQZo9pKRg0mZzdhCUrwkCbl4P9LEWvzWyz3RfoO3k3QD2O60djQi14cGSywaLrIy38la50p6V-MQXLb_Z2DAfciavtb6y3P6-Q1jhNM7LVNa55CtoM_TiQx7rpXA_1uOFQ/thumbnails/thumbnail.jpg?time=10.0s"  class="cloudflare-stream-player" id="video_stream_uuid%3Ab8fbe60c-654f-436d-82e0-59f68c26d532" height="187px" width="100%" allow="accelerometer; gyroscope; autoplay; encrypted-media; picture-in-picture;" loading="lazy" allowfullscreen="true" ></iframe></div></div><div class="video__meta col-sm-7"><div data-id="1710_sigir2021_WIT_dataset_talk.mp4" class="issue-downloads__item__details"><h5 class="article-media__title file-name">1710_sigir2021_WIT_dataset_talk.mp4</h5><p class="info--text">Presentation video.</p><div class="table-view"><div class="video__details table__cell-view"><p class="bold text-uppercase">mp4</p><p>35.6 MB</p></div><div class="video__links table__cell-view"><a href="#" title="Play stream" data-show-browse-icon="false" class="btn open-media-viewer"><i class="icon-play_circle_filled"></i><span>Play stream</span></a><a href="/action/downloadSupplement?doi=10.1145%2F3404835.3463257&amp;file=1710_sigir2021_WIT_dataset_talk.mp4" title="Download" download="1710_sigir2021_WIT_dataset_talk.mp4" class="btn"><i class="icon-Icon_Download"></i><span>Download</span></a></div></div></div></div></div></div></div></div></div><div class="article__section article__references show-more-items" data-sectionname="References"><p class="explanation__text"></p><div class="colored-block__title"><h2 id="sec-ref" class="section__title left-bordered-title">
                    References
                </h2></div><ol class="rlist references__list references__numeric"><li id="ref-00001" class="&#xA;                references__item&#xA;            "><span class="references__note">Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. 2019. Fusion of Detected Objects in Text for Visual Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2131--2140.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Chris+Alberti%2C+Jeffrey+Ling%2C+Michael+Collins%2C+and+David+Reitter.+2019.+Fusion+of+Detected+Objects+in+Text+for+Visual+Question+Answering.+In+Proceedings+of+the+2019+Conference+on+Empirical+Methods+in+Natural+Language+Processing+and+the+9th+International+Joint+Conference+on+Natural+Language+Processing+%28EMNLP-IJCNLP%29.+2131%2D%2D2140."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_1_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FD19-1219" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00002" class="&#xA;                references__item&#xA;            "><span class="references__note">Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE International Conference on Computer Vision (ICCV). 2425--2433.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Stanislaw+Antol%2C+Aishwarya+Agrawal%2C+Jiasen+Lu%2C+Margaret+Mitchell%2C+Dhruv+Batra%2C+C+Lawrence+Zitnick%2C+and+Devi+Parikh.+2015.+Vqa%3A+Visual+question+answering.+In+Proceedings+of+the+IEEE+International+Conference+on+Computer+Vision+%28ICCV%29.+2425%2D%2D2433."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/doi/10.1109/ICCV.2015.279" target="_blank"><span class="visibility-hidden">Digital Library</span><img class="simple-tooltip__block--b" data-title="Digital Library" alt="Digital Library" src="/templates/jsp/_ux3/_acm/images/DL_icon.svg"></img></a></span></span></li><li id="ref-00003" class="&#xA;                references__item&#xA;            "><span class="references__note">Jason Armitage, Endri Kacupaj, Golsa Tahmasebzadeh, Maria Maleshkova, Ralph Ewerth, and Jens Lehmann. 2020. MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities. In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management (CIKM). 2967--2974.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Jason+Armitage%2C+Endri+Kacupaj%2C+Golsa+Tahmasebzadeh%2C+Maria+Maleshkova%2C+Ralph+Ewerth%2C+and+Jens+Lehmann.+2020.+MLM%3A+A+Benchmark+Dataset+for+Multitask+Learning+with+Multiple+Languages+and+Modalities.+In+Proceedings+of+the+29th+ACM+International+Conference+on+Information+%26+Knowledge+Management+%28CIKM%29.+2967%2D%2D2974."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/doi/10.1145/3340531.3412783" target="_blank"><span class="visibility-hidden">Digital Library</span><img class="simple-tooltip__block--b" data-title="Digital Library" alt="Digital Library" src="/templates/jsp/_ux3/_acm/images/DL_icon.svg"></img></a></span></span></li><li id="ref-00004" class="&#xA;                references__item&#xA;            "><span class="references__note">Lo"ic Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018. Findings of the Third Shared Task on Multimodal Machine Translation. In Proceedings of the Third Conference on Machine Translation (Volume 2: Shared Task Papers (WMT). 304--323.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Lo%22ic+Barrault%2C+Fethi+Bougares%2C+Lucia+Specia%2C+Chiraag+Lala%2C+Desmond+Elliott%2C+and+Stella+Frank.+2018.+Findings+of+the+Third+Shared+Task+on+Multimodal+Machine+Translation.+In+Proceedings+of+the+Third+Conference+on+Machine+Translation+%28Volume+2%3A+Shared+Task+Papers+%28WMT%29.+304%2D%2D323."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00005" class="&#xA;                references__item&#xA;            "><span class="references__note">Craig Chambers, Ashish Raniwala, Frances Perry, Stephen Adams, Robert R Henry, Robert Bradshaw, and Nathan Weizenbaum. 2010. FlumeJava: Easy, Efficient Data-Parallel Pipelines. ACM Sigplan Notices, Vol. 45, 6 (2010), 363--375.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Craig+Chambers%2C+Ashish+Raniwala%2C+Frances+Perry%2C+Stephen+Adams%2C+Robert+R+Henry%2C+Robert+Bradshaw%2C+and+Nathan+Weizenbaum.+2010.+FlumeJava%3A+Easy%2C+Efficient+Data-Parallel+Pipelines.+ACM+Sigplan+Notices%2C+Vol.+45%2C+6+%282010%29%2C+363%2D%2D375."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/doi/10.1145/1809028.1806638" target="_blank"><span class="visibility-hidden">Digital Library</span><img class="simple-tooltip__block--b" data-title="Digital Library" alt="Digital Library" src="/templates/jsp/_ux3/_acm/images/DL_icon.svg"></img></a></span></span></li><li id="ref-00006" class="&#xA;                references__item&#xA;            "><span class="references__note">Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts. arXiv preprint arXiv:2102.08981 (2021).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Soravit+Changpinyo%2C+Piyush+Sharma%2C+Nan+Ding%2C+and+Radu+Soricut.+2021.+Conceptual+12M%3A+Pushing+Web-Scale+Image-Text+Pre-Training+To+Recognize+Long-Tail+Visual+Concepts.+arXiv+preprint+arXiv%3A2102.08981+%282021%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00007" class="&#xA;                references__item&#xA;            "><span class="references__note">Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019. Uniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740 (2019).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Yen-Chun+Chen%2C+Linjie+Li%2C+Licheng+Yu%2C+Ahmed+El+Kholy%2C+Faisal+Ahmed%2C+Zhe+Gan%2C+Yu+Cheng%2C+and+Jingjing+Liu.+2019.+Uniter%3A+Learning+universal+image-text+representations.+arXiv+preprint+arXiv%3A1909.11740+%282019%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00008" class="&#xA;                references__item&#xA;            "><span class="references__note">Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 248--255.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Jia+Deng%2C+Wei+Dong%2C+Richard+Socher%2C+Li-Jia+Li%2C+Kai+Li%2C+and+Li+Fei-Fei.+2009.+Imagenet%3A+A+large-scale+hierarchical+image+database.+In+Proceedings+of+the+2009+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+%28CVPR%29.+248%2D%2D255."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_8_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1109%2FCVPR.2009.5206848" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00009" class="&#xA;                references__item&#xA;            "><span class="references__note">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (NAACL-HLT). 4171--4186.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Jacob+Devlin%2C+Ming-Wei+Chang%2C+Kenton+Lee%2C+and+Kristina+Toutanova.+2019.+BERT%3A+Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding.+In+Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics%3A+Human+Language+Technologies%2C+Volume+1+%28Long+and+Short+Papers%29+%28NAACL-HLT%29.+4171%2D%2D4186."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00010" class="&#xA;                references__item&#xA;            "><span class="references__note">Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proceedings of the 9th International Conference on Learning Representations (ICLR).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Alexey+Dosovitskiy%2C+Lucas+Beyer%2C+Alexander+Kolesnikov%2C+Dirk+Weissenborn%2C+Xiaohua+Zhai%2C+Thomas+Unterthiner%2C+Mostafa+Dehghani%2C+Matthias+Minderer%2C+Georg+Heigold%2C+Sylvain+Gelly%2C+Jakob+Uszkoreit%2C+and+Neil+Houlsby.+2021.+An+Image+is+Worth+16x16+Words%3A+Transformers+for+Image+Recognition+at+Scale.+In+Proceedings+of+the+9th+International+Conference+on+Learning+Representations+%28ICLR%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00011" class="&#xA;                references__item&#xA;            "><span class="references__note">Desmond Elliott, Stella Frank, Loic Barrault, Fethi Bougares, and Lucia Specia. 2017. Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description. In Proceedings of the 2nd Conference on Machine Translation (WMT). 215--233.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Desmond+Elliott%2C+Stella+Frank%2C+Loic+Barrault%2C+Fethi+Bougares%2C+and+Lucia+Specia.+2017.+Findings+of+the+Second+Shared+Task+on+Multimodal+Machine+Translation+and+Multilingual+Image+Description.+In+Proceedings+of+the+2nd+Conference+on+Machine+Translation+%28WMT%29.+215%2D%2D233."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_11_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FW17-4718" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00012" class="&#xA;                references__item&#xA;            "><span class="references__note">Desmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia. 2016. Multi30K: Multilingual English-German Image Descriptions. In Proceedings of the 5th Workshop on Vision and Language (VL). 70--74.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Desmond+Elliott%2C+Stella+Frank%2C+Khalil+Sima%27an%2C+and+Lucia+Specia.+2016.+Multi30K%3A+Multilingual+English-German+Image+Descriptions.+In+Proceedings+of+the+5th+Workshop+on+Vision+and+Language+%28VL%29.+70%2D%2D74."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_12_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FW16-3210" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00013" class="&#xA;                references__item&#xA;            "><span class="references__note">John Hewitt, Daphne Ippolito, Brendan Callahan, Reno Kriz, Derry Tanti Wijaya, and Chris Callison-Burch. 2018. Learning Translations via Images with a Massively Multilingual Image Dataset. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (ACL). 2566--2576.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=John+Hewitt%2C+Daphne+Ippolito%2C+Brendan+Callahan%2C+Reno+Kriz%2C+Derry+Tanti+Wijaya%2C+and+Chris+Callison-Burch.+2018.+Learning+Translations+via+Images+with+a+Massively+Multilingual+Image+Dataset.+In+Proceedings+of+the+56th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+%28Volume+1%3A+Long+Papers%29+%28ACL%29.+2566%2D%2D2576."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_13_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FP18-1239" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00014" class="&#xA;                references__item&#xA;            "><span class="references__note">Julian Hitschler, Shigehiko Schamoni, and Stefan Riezler. 2016. Multimodal Pivots for Image Caption Translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (ACL). 2399--2409.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Julian+Hitschler%2C+Shigehiko+Schamoni%2C+and+Stefan+Riezler.+2016.+Multimodal+Pivots+for+Image+Caption+Translation.+In+Proceedings+of+the+54th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+%28Volume+1%3A+Long+Papers%29+%28ACL%29.+2399%2D%2D2409."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_14_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FP16-1227" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00015" class="&#xA;                references__item&#xA;            "><span class="references__note">Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. Journal of Artificial Intelligence Research, Vol. 47 (2013), 853--899.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Micah+Hodosh%2C+Peter+Young%2C+and+Julia+Hockenmaier.+2013.+Framing+Image+Description+as+a+Ranking+Task%3A+Data%2C+Models+and+Evaluation+Metrics.+Journal+of+Artificial+Intelligence+Research%2C+Vol.+47+%282013%29%2C+853%2D%2D899."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_15_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.5555%2F2566972.2566993" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00016" class="&#xA;                references__item&#xA;            "><span class="references__note">Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. arXiv preprint arXiv:2102.05918 (2021).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Chao+Jia%2C+Yinfei+Yang%2C+Ye+Xia%2C+Yi-Ting+Chen%2C+Zarana+Parekh%2C+Hieu+Pham%2C+Quoc+V+Le%2C+Yunhsuan+Sung%2C+Zhen+Li%2C+and+Tom+Duerig.+2021.+Scaling+Up+Visual+and+Vision-Language+Representation+Learning+With+Noisy+Text+Supervision.+arXiv+preprint+arXiv%3A2102.05918+%282021%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00017" class="&#xA;                references__item&#xA;            "><span class="references__note">Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Aleksei Timofeev, Yi-Ting Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, and Sujith Ravi. 2019. Graph-RISE: Graph-Regularized Image Semantic Embedding. arXiv preprint arXiv:1902.10814 (2019).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Da-Cheng+Juan%2C+Chun-Ta+Lu%2C+Zhen+Li%2C+Futang+Peng%2C+Aleksei+Timofeev%2C+Yi-Ting+Chen%2C+Yaxi+Gao%2C+Tom+Duerig%2C+Andrew+Tomkins%2C+and+Sujith+Ravi.+2019.+Graph-RISE%3A+Graph-Regularized+Image+Semantic+Embedding.+arXiv+preprint+arXiv%3A1902.10814+%282019%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00018" class="&#xA;                references__item&#xA;            "><span class="references__note">Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. 2019 a. Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training. arXiv e-prints, page. arXiv preprint arXiv:1908.06066 (2019).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Gen+Li%2C+Nan+Duan%2C+Yuejian+Fang%2C+Ming+Gong%2C+Daxin+Jiang%2C+and+Ming+Zhou.+2019+a.+Unicoder-VL%3A+A+Universal+Encoder+for+Vision+and+Language+by+Cross-modal+Pre-training.+arXiv+e-prints%2C+page.+arXiv+preprint+arXiv%3A1908.06066+%282019%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00019" class="&#xA;                references__item&#xA;            "><span class="references__note">Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. 2020. Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). 11336--11344.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Gen+Li%2C+Nan+Duan%2C+Yuejian+Fang%2C+Ming+Gong%2C+Daxin+Jiang%2C+and+Ming+Zhou.+2020.+Unicoder-VL%3A+A+Universal+Encoder+for+Vision+and+Language+by+Cross-Modal+Pre-Training.+In+Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence+%28AAAI%29.+11336%2D%2D11344."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_19_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1609%2Faaai.v34i07.6795" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00020" class="&#xA;                references__item&#xA;            "><span class="references__note">Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019 c. VisualBERT: A Simple and Performant Baseline for Vision and Language. arXiv preprint arXiv:1908.03557 (2019).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Liunian+Harold+Li%2C+Mark+Yatskar%2C+Da+Yin%2C+Cho-Jui+Hsieh%2C+and+Kai-Wei+Chang.+2019+c.+VisualBERT%3A+A+Simple+and+Performant+Baseline+for+Vision+and+Language.+arXiv+preprint+arXiv%3A1908.03557+%282019%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00021" class="&#xA;                references__item&#xA;            "><span class="references__note">Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu. 2019 b. COCO-CN for Cross-Lingual Image Tagging, Captioning, and Retrieval. IEEE Transactions on Multimedia, Vol. 21, 9 (2019), 2347--2360.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Xirong+Li%2C+Chaoxi+Xu%2C+Xiaoxu+Wang%2C+Weiyu+Lan%2C+Zhengxiong+Jia%2C+Gang+Yang%2C+and+Jieping+Xu.+2019+b.+COCO-CN+for+Cross-Lingual+Image+Tagging%2C+Captioning%2C+and+Retrieval.+IEEE+Transactions+on+Multimedia%2C+Vol.+21%2C+9+%282019%29%2C+2347%2D%2D2360."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_21_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1109%2FTMM.2019.2896494" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00022" class="&#xA;                references__item&#xA;            "><span class="references__note">Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In Proceedings of the 13th European Conference on Computer Vision (Part V) (ECCV). 740--755.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Tsung-Yi+Lin%2C+Michael+Maire%2C+Serge+Belongie%2C+James+Hays%2C+Pietro+Perona%2C+Deva+Ramanan%2C+Piotr+Doll%C3%A1r%2C+and+C+Lawrence+Zitnick.+2014.+Microsoft+COCO%3A+Common+Objects+in+Context.+In+Proceedings+of+the+13th+European+Conference+on+Computer+Vision+%28Part+V%29+%28ECCV%29.+740%2D%2D755."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_22_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1007%2F978-3-319-10602-1_48" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00023" class="&#xA;                references__item&#xA;            "><span class="references__note">Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS). 13--23.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Jiasen+Lu%2C+Dhruv+Batra%2C+Devi+Parikh%2C+and+Stefan+Lee.+2019.+ViLBERT%3A+Pretraining+Task-Agnostic+Visiolinguistic+Representations+for+Vision-and-Language+Tasks.+In+Proceedings+of+the+33rd+Conference+on+Neural+Information+Processing+Systems+%28NeurIPS%29.+13%2D%2D23."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00024" class="&#xA;                references__item&#xA;            "><span class="references__note">Takashi Miyazaki and Nobuyuki Shimizu. 2016. Cross-Lingual Image Caption Generation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (ACL). 1780--1790. https://doi.org/10.18653/v1/P16--1168<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Takashi+Miyazaki+and+Nobuyuki+Shimizu.+2016.+Cross-Lingual+Image+Caption+Generation.+In+Proceedings+of+the+54th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+%28Volume+1%3A+Long+Papers%29+%28ACL%29.+1780%2D%2D1790.+https%3A%2F%2Fdoi.org%2F10.18653%2Fv1%2FP16%2D%2D1168"target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_24_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FP16-1168" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00025" class="&#xA;                references__item&#xA;            "><span class="references__note">Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011. Im2Text: Describing Images Using 1 Million Captioned Photographs. In Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NeurIPS). 1143--1151.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Vicente+Ordonez%2C+Girish+Kulkarni%2C+and+Tamara+Berg.+2011.+Im2Text%3A+Describing+Images+Using+1+Million+Captioned+Photographs.+In+Proceedings+of+the+25th+Annual+Conference+on+Neural+Information+Processing+Systems+%28NeurIPS%29.+1143%2D%2D1151."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00026" class="&#xA;                references__item&#xA;            "><span class="references__note">Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. 2020. ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data. arXiv preprint arXiv:2001.07966 (2020).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Di+Qi%2C+Lin+Su%2C+Jia+Song%2C+Edward+Cui%2C+Taroon+Bharti%2C+and+Arun+Sacheti.+2020.+ImageBERT%3A+Cross-modal+Pre-training+with+Large-scale+Weak-supervised+Image-Text+Data.+arXiv+preprint+arXiv%3A2001.07966+%282020%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00027" class="&#xA;                references__item&#xA;            "><span class="references__note">Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. Image, Vol. 2 (2021), T2.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Alec+Radford%2C+Jong+Wook+Kim%2C+Chris+Hallacy%2C+Aditya+Ramesh%2C+Gabriel+Goh%2C+Sandhini+Agarwal%2C+Girish+Sastry%2C+Amanda+Askell%2C+Pamela+Mishkin%2C+Jack+Clark%2C+Gretchen+Krueger%2C+and+Ilya+Sutskever.+2021.+Learning+transferable+visual+models+from+natural+language+supervision.+Image%2C+Vol.+2+%282021%29%2C+T2."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00028" class="&#xA;                references__item&#xA;            "><span class="references__note">Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, Vol. 21, 140 (2020), 1--67.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Colin+Raffel%2C+Noam+Shazeer%2C+Adam+Roberts%2C+Katherine+Lee%2C+Sharan+Narang%2C+Michael+Matena%2C+Yanqi+Zhou%2C+Wei+Li%2C+and+Peter+J+Liu.+2020.+Exploring+the+Limits+of+Transfer+Learning+with+a+Unified+Text-to-Text+Transformer.+Journal+of+Machine+Learning+Research%2C+Vol.+21%2C+140+%282020%29%2C+1%2D%2D67."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00029" class="&#xA;                references__item&#xA;            "><span class="references__note">Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (ACL). 2556--2565.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Piyush+Sharma%2C+Nan+Ding%2C+Sebastian+Goodman%2C+and+Radu+Soricut.+2018.+Conceptual+Captions%3A+A+Cleaned%2C+Hypernymed%2C+Image+Alt-text+Dataset+For+Automatic+Image+Captioning.+In+Proceedings+of+the+56th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+%28Volume+1%3A+Long+Papers%29+%28ACL%29.+2556%2D%2D2565."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_29_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FP18-1238" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00030" class="&#xA;                references__item&#xA;            "><span class="references__note">Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Karen+Simonyan+and+Andrew+Zisserman.+2014.+Very+deep+convolutional+networks+for+large-scale+image+recognition.+arXiv+preprint+arXiv%3A1409.1556+%282014%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00031" class="&#xA;                references__item&#xA;            "><span class="references__note">Karan Singhal, Karthik Raman, and Balder ten Cate. 2019. Learning multilingual word embeddings using image-text data. arXiv preprint arXiv:1905.12260 (2019).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Karan+Singhal%2C+Karthik+Raman%2C+and+Balder+ten+Cate.+2019.+Learning+multilingual+word+embeddings+using+image-text+data.+arXiv+preprint+arXiv%3A1905.12260+%282019%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00032" class="&#xA;                references__item&#xA;            "><span class="references__note">Lucia Specia, Stella Frank, Khalil Sima'An, and Desmond Elliott. 2016. A Shared Task on Multimodal Machine Translation and Crosslingual Image Description. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers (WMT). 543--553.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Lucia+Specia%2C+Stella+Frank%2C+Khalil+Sima%27An%2C+and+Desmond+Elliott.+2016.+A+Shared+Task+on+Multimodal+Machine+Translation+and+Crosslingual+Image+Description.+In+Proceedings+of+the+First+Conference+on+Machine+Translation%3A+Volume+2%2C+Shared+Task+Papers+%28WMT%29.+543%2D%2D553."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_32_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FW16-2346" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00033" class="&#xA;                references__item&#xA;            "><span class="references__note">Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In Proceedings of the 8th International Conference on Learning Representations (ICLR).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Weijie+Su%2C+Xizhou+Zhu%2C+Yue+Cao%2C+Bin+Li%2C+Lewei+Lu%2C+Furu+Wei%2C+and+Jifeng+Dai.+2020.+VL-BERT%3A+Pre-training+of+Generic+Visual-Linguistic+Representations.+In+Proceedings+of+the+8th+International+Conference+on+Learning+Representations+%28ICLR%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00034" class="&#xA;                references__item&#xA;            "><span class="references__note">Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. 2017. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. In Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV). 843--852.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Chen+Sun%2C+Abhinav+Shrivastava%2C+Saurabh+Singh%2C+and+Abhinav+Gupta.+2017.+Revisiting+Unreasonable+Effectiveness+of+Data+in+Deep+Learning+Era.+In+Proceedings+of+the+2017+IEEE+International+Conference+on+Computer+Vision+%28ICCV%29.+843%2D%2D852."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_34_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1109%2FICCV.2017.97" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00035" class="&#xA;                references__item&#xA;            "><span class="references__note">Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2818--2826.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Christian+Szegedy%2C+Vincent+Vanhoucke%2C+Sergey+Ioffe%2C+Jon+Shlens%2C+and+Zbigniew+Wojna.+2016.+Rethinking+the+Inception+Architecture+for+Computer+Vision.+In+Proceedings+of+the+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+%28CVPR%29.+2818%2D%2D2826."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_35_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1109%2FCVPR.2016.308" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00036" class="&#xA;                references__item&#xA;            "><span class="references__note">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS). 5998--6008.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Ashish+Vaswani%2C+Noam+Shazeer%2C+Niki+Parmar%2C+Jakob+Uszkoreit%2C+Llion+Jones%2C+Aidan+N+Gomez%2C+%C5%81ukasz+Kaiser%2C+and+Illia+Polosukhin.+2017.+Attention+is+all+you+need.+In+Proceedings+of+the+31st+Conference+on+Neural+Information+Processing+Systems+%28NeurIPS%29.+5998%2D%2D6008."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00037" class="&#xA;                references__item&#xA;            "><span class="references__note">Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and Tell: A Neural Image Caption Generator. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3156--3164.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Oriol+Vinyals%2C+Alexander+Toshev%2C+Samy+Bengio%2C+and+Dumitru+Erhan.+2015.+Show+and+Tell%3A+A+Neural+Image+Caption+Generator.+In+Proceedings+of+the+IEEE+conference+on+computer+vision+and+pattern+recognition.+3156%2D%2D3164."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_37_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1109%2FCVPR.2015.7298935" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00038" class="&#xA;                references__item&#xA;            "><span class="references__note">Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 833--844.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Shijie+Wu+and+Mark+Dredze.+2019.+Beto%2C+bentz%2C+becas%3A+The+surprising+cross-lingual+effectiveness+of+BERT.+In+Proceedings+of+the+2019+Conference+on+Empirical+Methods+in+Natural+Language+Processing+and+the+9th+International+Joint+Conference+on+Natural+Language+Processing+%28EMNLP-IJCNLP%29.+833%2D%2D844."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_38_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FD19-1077" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00039" class="&#xA;                references__item&#xA;            "><span class="references__note">Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934 (2020).<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Linting+Xue%2C+Noah+Constant%2C+Adam+Roberts%2C+Mihir+Kale%2C+Rami+Al-Rfou%2C+Aditya+Siddhant%2C+Aditya+Barua%2C+and+Colin+Raffel.+2020.+mT5%3A+A+massively+multilingual+pre-trained+text-to-text+transformer.+arXiv+preprint+arXiv%3A2010.11934+%282020%29."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span></span></li><li id="ref-00040" class="&#xA;                references__item&#xA;            "><span class="references__note">Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, Vol. 2 (2014), 67--78.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Peter+Young%2C+Alice+Lai%2C+Micah+Hodosh%2C+and+Julia+Hockenmaier.+2014.+From+image+descriptions+to+visual+denotations%3A+New+similarity+metrics+for+semantic+inference+over+event+descriptions.+Transactions+of+the+Association+for+Computational+Linguistics%2C+Vol.+2+%282014%29%2C+67%2D%2D78."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_40_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1162%2Ftacl_a_00166" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00041" class="&#xA;                references__item&#xA;            "><span class="references__note">Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From Recognition to Cognition: Visual Commonsense Reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 6713--6724.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Rowan+Zellers%2C+Yonatan+Bisk%2C+Ali+Farhadi%2C+and+Yejin+Choi.+2019.+From+Recognition+to+Cognition%3A+Visual+Commonsense+Reasoning.+In+Proceedings+of+the+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+%28CVPR%29.+6713%2D%2D6724."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_41_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1109%2FCVPR.2019.00688" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00042" class="&#xA;                references__item&#xA;            "><span class="references__note">Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. 2020. Unified Vision-Language Pre-Training for Image Captioning and VQA. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). 13041--13049.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Luowei+Zhou%2C+Hamid+Palangi%2C+Lei+Zhang%2C+Houdong+Hu%2C+Jason+J+Corso%2C+and+Jianfeng+Gao.+2020.+Unified+Vision-Language+Pre-Training+for+Image+Captioning+and+VQA.+In+Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence+%28AAAI%29.+13041%2D%2D13049."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_42_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.1609%2Faaai.v34i07.7005" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00043" class="&#xA;                references__item&#xA;            "><span class="references__note">Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, and Zhou Yu. 2018. A Visual Attention Grounding Neural Model for Multimodal Machine Translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). 3643--3653.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Mingyang+Zhou%2C+Runxiang+Cheng%2C+Yong+Jae+Lee%2C+and+Zhou+Yu.+2018.+A+Visual+Attention+Grounding+Neural+Model+for+Multimodal+Machine+Translation.+In+Proceedings+of+the+2018+Conference+on+Empirical+Methods+in+Natural+Language+Processing+%28EMNLP%29.+3643%2D%2D3653."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/servlet/linkout?suffix=e_1_3_2_2_43_1&amp;dbid=16&amp;doi=10.1145%2F3404835.3463257&amp;key=10.18653%2Fv1%2FD18-1400" target="_blank"><span class="visibility-hidden">Cross Ref</span><img class="simple-tooltip__block--b" data-title="Cross Ref" alt="Cross Ref" src="/templates/jsp/_ux3/_acm/images/crossref_icon.svg"></img></a></span></span></li><li id="ref-00044" class="&#xA;                references__item&#xA;            "><span class="references__note">Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV). 19--27.<span class="references__suffix"><a class="google-scholar" href="http://scholar.google.com/scholar?hl=en&q=Yukun+Zhu%2C+Ryan+Kiros%2C+Rich+Zemel%2C+Ruslan+Salakhutdinov%2C+Raquel+Urtasun%2C+Antonio+Torralba%2C+and+Sanja+Fidler.+2015.+Aligning+books+and+movies%3A+Towards+story-like+visual+explanations+by+watching+movies+and+reading+books.+In+Proceedings+of+the+2015+IEEE+International+Conference+on+Computer+Vision+%28ICCV%29.+19%2D%2D27."target="_blank"><span class="visibility-hidden">Google Scholar</span><img src="/specs/products/acm/images/googleScholar.svg" class="simple-tooltip__block--b" alt="Google Scholar"/></a></span><span class="references__suffix"><a href="/doi/10.1109/ICCV.2015.11" target="_blank"><span class="visibility-hidden">Digital Library</span><img class="simple-tooltip__block--b" data-title="Digital Library" alt="Digital Library" src="/templates/jsp/_ux3/_acm/images/DL_icon.svg"></img></a></span></span></li></ol></div>









        <div data-widget-def="UX3TagWidget" data-widget-id="2a75f978-170a-4843-8722-e878a1b77fbc" class="citation article__section article__index-terms">
        



        
        









        <div data-widget-def="graphQueryWidget" data-widget-id="4ca2ed65-717e-4e67-bbb5-6578b8c6acac" class="colored-block__title">
        



        
        <h2 id="sec-terms" class="section__title left-bordered-title">Index Terms</h2>

        </div>
    

<ol class="rlist organizational-chart"><li><h6>WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning</h6><ol class="rlist level-1  ch-2 hasNodes"><li><div data-background="#808080" style="background-color:#808080;box-shadow:none"><p><a href="/topic/ccs2012/10010147?ContentGroupKey=10.1145%2F3404835&amp;expand=all">Computing methodologies</a></p></div><ol class="rlist level-2 ch-1 hasNodes"><li><div data-background="#808080" style="background-color:#808080;box-shadow:none"><p><a href="/topic/ccs2012/10010147.10010257?ContentGroupKey=10.1145%2F3404835&amp;expand=all">Machine learning</a></p></div><ol class="rlist level-3 ch-0"></ol></li></ol></li><li><div data-background="#808080" style="background-color:#808080;box-shadow:none"><p><a href="/topic/ccs2012/10002951?ContentGroupKey=10.1145%2F3404835&amp;expand=all">Information systems</a></p></div><ol class="rlist level-2 ch-1 hasNodes"><li><div data-background="#808080" style="background-color:#808080;box-shadow:none"><p><a href="/topic/ccs2012/10002951.10003317?ContentGroupKey=10.1145%2F3404835&amp;expand=all">Information retrieval</a></p></div><ol class="rlist level-3 ch-1 hasNodes"><li><div data-background="#808080" style="background-color:#808080;box-shadow:none"><p><a href="/topic/ccs2012/10002951.10003317.10003371?ContentGroupKey=10.1145%2F3404835&amp;expand=all">Specialized information retrieval</a></p></div><ol class="rlist level-4 ch-2 hasNodes"><li><div data-background="#808080" style="background-color:#808080;box-shadow:none"><p><a href="/topic/ccs2012/10002951.10003317.10003371.10003386?ContentGroupKey=10.1145%2F3404835&amp;expand=all">Multimedia and multimodal retrieval</a></p></div><ol class="rlist level-5 ch-1 hasNodes"><li><div data-background="#808080" style="background-color:#808080;box-shadow:none"><p><a href="/topic/ccs2012/10002951.10003317.10003371.10003386.10003387?ContentGroupKey=10.1145%2F3404835&amp;expand=all">Image search</a></p></div><ol class="rlist level-6 ch-0"></ol></li></ol></li><li><div data-background="#D3D3D3" style="background-color:#D3D3D3;box-shadow:inset 0px 0px 0px 3px #808080"><p><a href="/topic/ccs2012/10002951.10003317.10003371.10003381?ContentGroupKey=10.1145%2F3404835&amp;expand=all">Structure and multilingual text search</a></p></div><ol class="rlist level-6 ch-0"></ol></li></ol></li></ol></li></ol></li></ol></li></ol>

        </div>
    










        <div data-widget-def="graphQueryWidget" data-widget-id="91639479-279a-46e3-b7c7-f902a3fd7945" class="toc__section accordion-tabbed__tab">
        



        
        









        <div data-widget-def="UX3RelatedDigitalObject" data-widget-id="98838540-5c38-436f-9b98-0e292765fbe4" class="accordion-tabbed__content">
        



        
        

        </div>
    


        </div>
    




        
        <div class="article__section article__comments">



        
        <div class="colored-block__title"> <h2 id="sec-comments" class="section__title left-bordered-title">Comments</h2> </div>




        
        <div id="disqus_thread"></div><a href="https://dl.acm.org/doi/abs/10.1145/3404835.3463257#disqus_thread" class="disqus-count hidden"></a><script>var disqus_config = function () {
    this.page.url = window.location.href;
    this.page.identifier = '10.1145/3404835.3463257';
    this.page.remote_auth_s3 = '';
    this.page.api_key = 'MmWj4FlWmXk20FJsmWMXbTJoZT7TVzTsc9Rw0yrJp3L1ZNXZuvmxF3F5rFIF8RqQ';
};
(function () {
    var d = document, s = d.createElement('script');
    s.src = 'https://acm-prod.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();</script><noscript>Please enable JavaScript to view the<a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript><script id="dsq-count-scr" src="//acm-prod.disqus.com/count.js" async=""></script>
</div>
</div><div class="pill-content"><div id="pill-access"><div class="pill-access__content">



        
        <div class="pill-access__content"><div class="section__separator"><h3 class="left-bordered-title">Login options</h3><div class="section__content"><p class="info--text">Check if you have access through your login credentials or your institution to get full access on this article.</p><a href="/action/showLogin?redirectUri=/doi/abs/10.1145/3404835.3463257" title="Sign in" class="btn big btn--inverse">Sign in</a></div></div><div class="section__separator"><h3 class="left-bordered-title">Full Access</h3><div class="section__content"><a href="/action/publisherEcommerceHelper?doi=10.1145/3404835.3463257&amp;redirectUri=https://dl.acm.org/doi/abs/10.1145/3404835.3463257" title="Get this Publication" class="btn blue big">Get this Publication</a></div></div></div>
</div></div><div id="pill-information"><div class="tab tab--flex"><ul role="tablist" class="rlist tab__nav"><li role="presentation" class="active"><a id="pill-information__contentcon" href="#pill-information__content" aria-controls="pill-information__content" role="tab" data-toggle="tab" title="Information" aria-selected="true" data-simple-tab-id="" tabindex="0">Information</a></li><li role="presentation"><a id="pill-authors__contentcon" href="#pill-authors__content" aria-controls="pill-authors__content" role="tab" data-toggle="tab" title="Authors" aria-selected="false" data-simple-tab-id="" tabindex="-1">Contributors</a></li></ul><ul class="rlist tab__content"><li id="pill-information__content" aria-labelledby="pill-information__contentcon" role="tabpanel" aria-hidden="false" tabindex="0" class="tab__pane active"><div class="pill-information__content"><div class="section__separator"><h3 class="left-bordered-title">Published in</h3><div class="section__content cover-image-short-paper">









        <div data-widget-def="UX3CoverImage" data-widget-id="7b8bd09e-8112-4266-b136-2f71ab2b109c" class="left-side-image">
        



        
        <div class="cover-image"><div class="cover-image__image"><img src="/specs/products/acm/releasedAssets/images/Default_image_lazy.svg" data-src="/cms/asset/05c60a81-785b-43ae-abca-b7543decc302/3404835.cover.jpg" alt="ACM Conferences cover image" class="lazy"/></div><div class="cover-image__details"><div class="book-meta">SIGIR '21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</div><div class="cover-date">July 2021</div><div class="cover-pages">2998  pages</div><div class="cover-image__details-extra"><div class="flex-container"><span class="bold">ISBN:</span><span class="space">9781450380379</span></div><div class="flex-container"><span class="bold">DOI:</span><span class="space">10.1145/3404835</span></div><div class="flex-container"><div><ul class="rlist--inline loa truncate-list" title="list of authors" data-lines="2"><li class="label"><b>General Chairs: </b></li><li><a href="/profile/81100278520" title="Fernando Diaz"><img class="author-picture" src="/pb-assets/icons/DOs/default-profile-1543932446943.svg" width="24" height="24" alt="" aria-hidden="true"/><span>Fernando Diaz</span></a><span class="loa_author_inst hidden"><p data-doi="10.1145/contrib-81100278520">(Google)</p></span><span>,</span></li><li><a href="/profile/81332526845" title="Chirag Shah"><img class="author-picture" src="/do/10.1145/contrib-81332526845/rel-imgonly/chiragshah.jpg" width="24" height="24" alt="" aria-hidden="true"/><span>Chirag Shah</span></a><span class="loa_author_inst hidden"><p data-doi="10.1145/contrib-81332526845">University of Washington</p></span><span>,</span></li><li><a href="/profile/81100583468" title="Torsten Suel"><img class="author-picture" src="/pb-assets/icons/DOs/default-profile-1543932446943.svg" width="24" height="24" alt="" aria-hidden="true"/><span>Torsten Suel</span></a><span class="loa_author_inst hidden"><p data-doi="10.1145/contrib-81100583468">New York University</p></span><span>,</span></li><li class="label"><b>Program Chairs: </b></li><li><a href="/profile/81100073022" title="Pablo Castells"><img class="author-picture" src="/pb-assets/icons/DOs/default-profile-1543932446943.svg" width="24" height="24" alt="" aria-hidden="true"/><span>Pablo Castells</span></a><span class="loa_author_inst hidden"><p data-doi="10.1145/contrib-81100073022">Universidad Autónoma de Madrid, Amazon</p></span><span>,</span></li><li><a href="/profile/99659799155" title="Rosie Jones"><img class="author-picture" src="/pb-assets/icons/DOs/default-profile-1543932446943.svg" width="24" height="24" alt="" aria-hidden="true"/><span>Rosie Jones</span></a><span class="loa_author_inst hidden"><p data-doi="10.1145/contrib-99659799155">Spotify</p></span><span>,</span></li><li><a href="/profile/81100193167" title="Tetsuya Sakai"><img class="author-picture" src="/do/10.1145/contrib-81100193167/rel-imgonly/drtsakai080809.jpg" width="24" height="24" alt="" aria-hidden="true"/><span>Tetsuya Sakai</span></a><span class="loa_author_inst hidden"><p data-doi="10.1145/contrib-81100193167">Waseda University</p></span></li></ul></div></div></div><div class="pb-dropzone" data-pb-dropzone="metadataDisplayExtra" title="metadataDisplayExtra"></div></div></div>

        </div>
    
<p class="copyrights small">Copyright © 2021 Owner/Author</p></div></div><div class="section__separator proceedingInfo-sponsors hidden"><h3 class="left-bordered-title">Sponsors</h3><ul class="proceedingInfo-sponsors__list"></ul></div><div class="section__separator proceedingInfo-inCoop hidden"><h3 class="left-bordered-title">In-Cooperation</h3><ul class="proceedingInfo-inCoop__list"></ul></div><div class="section__separator"><h3 class="left-bordered-title">Publisher</h3><div class="section__content publisher"><div class="inlineBlock"></div><div class="inlineBlock"><p class="publisher__name">Association for Computing Machinery</p><p class="publisher__address">New York, NY, United States</p></div></div></div>









        <div data-widget-def="contentItemHistory" data-widget-id="6eba30da-4023-4f09-bb08-f98cf09d5aa0" class="section__separator">
        



        
            <h3 class="left-bordered-title">
                Publication History
            </h3>
        
        <div class="section__content"><ul class="rlist article-chapter-history-list"><li><span class="item_label">Online:</span> 11 July 2021</li><li><span class="item_label">Published:</span> 11 July 2021</li></ul></div>

        </div>
    




        
        <!-- rightslink drop zone-->



        
        <div class="section__separator"><h3 class="left-bordered-title"> Permissions</h3><div class="section__content"><p class="info--text">Request permissions about this article.</p><a href="/servlet/linkout?type=rightslinkBasic&amp;url=ContentIdType%3Ddoi%26isbn%3D9781450380379%26contentID%3D10.1145%252F3404835.3463257%26bookTitle%3DProceedings%2Bof%2Bthe%2B44th%2BInternational%2BACM%2BSIGIR%2BConference%2Bon%2BResearch%2Band%2BDevelopment%2Bin%2BInformation%2BRetrieval" rel="noopener" target="_blank" class="btn big stretched blue">Request Permissions</a></div></div>



<div class="section__separator"><h3 class="left-bordered-title">Author Tags</h3><div class="section__content"><div class="tags-widget"><div class="tags-widget__content"><ul class="rlist--inline"><li><a href="/keyword/machine learning?expand=all" title="machine learning" class="badge-type">machine learning</a></li><li><a href="/keyword/multilingual?expand=all" title="multilingual" class="badge-type">multilingual</a></li><li><a href="/keyword/dataset?expand=all" title="dataset" class="badge-type">dataset</a></li><li><a href="/keyword/neural networks?expand=all" title="neural networks" class="badge-type">neural networks</a></li><li><a href="/keyword/multimodal?expand=all" title="multimodal" class="badge-type">multimodal</a></li><li><a href="/keyword/image-text retrieval?expand=all" title="image-text retrieval" class="badge-type">image-text retrieval</a></li><li><a href="/keyword/wikipedia?expand=all" title="wikipedia" class="badge-type">wikipedia</a></li></ul></div></div></div></div><div class="section__separator"><h3 class="left-bordered-title">Qualifiers</h3><ul class="qualifiers__list"><li class="capitalized qualifiers__list__item">short-paper</li></ul></div><div class="section__separator proceedingInfo-conference hidden"><h3 class="left-bordered-title">Conference</h3></div>









        <div data-widget-def="UX3AcceptanceRatesWidget" data-widget-id="daac1568-1d03-438e-a4c9-c9df280908b7" class="section__separator">
        



        
        <div class="acceptance-rates__section"><h2 id="acceptance-rates" aria-expanded="true" class="left-bordered-title">Acceptance Rates</h2><div class="accordion-tabbed__content"><div class="acceptance-rates__heading"><h6>Overall Acceptance Rate<span> 526 </span>of<span> 2,561 </span>submissions,<span> 21%</span></h6></div></div></div>

        </div>
    










        <div data-widget-def="UX3UpcomingConferencesWidget" data-widget-id="452ce022-9919-435e-ab40-1386965a30bd" class="section__separator">
        



        
        

        </div>
    
<div class="section__separator funding-sources hidden"><h3 class="left-bordered-title">Funding Sources</h3><ul class="funding-list"></ul></div></div></li><li id="pill-authors__content" aria-labelledby="pill-authors__contentcon" role="tabpanel" aria-hidden="true" class="tab__pane"><div class="pill-authors__content"><div class="section__separator"><div class="pill-authors-list"><div class="expandable-accordion"><div class="pill-normalized-authors">



        
        <div class="publication-contribs-contianer"><input id="ContribsAjax" type="hidden" data-ajax="/pb/widgets/getContribs?widgetId=f6c6c0f3-1b50-42bb-a7bb-1062377c12d4&amp;pbContext=%3Btaxonomy%3Ataxonomy%3Aconference-collections%3Bwgroup%3Astring%3AACM+Publication+Websites%3BgroupTopic%3Atopic%3Aacm-pubtype%26gt%3Bproceeding%3Barticle%3Aarticle%3Adoi%5C%3A10.1145%2F3404835.3463257%3Bissue%3Aissue%3Adoi%5C%3A10.1145%2F3404835%3Bctype%3Astring%3ABook+Content%3Bpage%3Astring%3AArticle%2FChapter+View%3BsubPage%3Astring%3AAbstract%3Bcsubtype%3Astring%3AConference+Proceedings%3Bwebsite%3Awebsite%3Adl-site%3Btopic%3Atopic%3Aconference-collections%26gt%3Bir%3Bjournal%3Ajournal%3Aacmconferences%3BpageGroup%3Astring%3APublication+Pages"/><div class="loader"><img src="/specs/products/acm/releasedAssets/images/loader.gif"/></div></div>
</div><!--.pill-all-authors.authors-accordion.disable-truncate.hidden(aria-hidden="true")--><!--    if(data.item && data.item.contribsFull)--><!--        | !{data.item.contribsFull}--></div></div></div><div class="section__separator"><h3 class="left-bordered-title">Other Metrics</h3><div class="section__content badges"><a href="#pill-bibliometrics__contentcon" data-slide-target="#pill-metric" data-tab="pill-bibliometrics__content" data-ctrl-res="screen-xlg" data-full-screen="false" data-label="&lt;i class=&quot;icon-metric &quot;&gt;&lt;/i&gt;Bibliometrics &amp; Citations" class="btn big stretched btn--inverse w-slide__btn tab-link pill-content">View Article Metrics</a></div></div></div></li></ul></div></div><div id="pill-metric"><div class="tab tab--flex"><ul role="tablist" class="rlist tab__nav"><li role="presentation" class="active"><a id="pill-bibliometrics__contentcon" href="#pill-bibliometrics__content" aria-controls="pill-bibliometrics__content" role="tab" data-toggle="tab" title="Bibliometrics" aria-selected="true" data-simple-tab-id="" tabindex="0">Bibliometrics</a></li><li role="presentation"><a id="pill-citations__contentcon" href="#pill-citations__content" data-ajaxurl="/action/ajaxShowCitedBy?widgetId=f69d88a8-b404-4aae-83a9-9acea4426d78&amp;ajax=true&amp;doi=10.1145%2F3404835.3463257&amp;pbContext=%3Btaxonomy%3Ataxonomy%3Aconference-collections%3Bwgroup%3Astring%3AACM+Publication+Websites%3BgroupTopic%3Atopic%3Aacm-pubtype%26gt%3Bproceeding%3Barticle%3Aarticle%3Adoi%5C%3A10.1145%2F3404835.3463257%3Bissue%3Aissue%3Adoi%5C%3A10.1145%2F3404835%3Bctype%3Astring%3ABook+Content%3Bpage%3Astring%3AArticle%2FChapter+View%3BsubPage%3Astring%3AAbstract%3Bcsubtype%3Astring%3AConference+Proceedings%3Bwebsite%3Awebsite%3Adl-site%3Btopic%3Atopic%3Aconference-collections%26gt%3Bir%3Bjournal%3Ajournal%3Aacmconferences%3BpageGroup%3Astring%3APublication+Pages" data-component="pubAjaxContent" data-ajaxtarget="#pill-citations__content .citedBy" aria-controls="pill-citations__content" role="tab" data-toggle="tab" title="Citations" aria-selected="false" data-simple-tab-id="" tabindex="-1" class="loadAjax loadLazyAjax">Citations<span class="citation-count count-bubble">1</span></a></li></ul><ul class="rlist tab__content"><li id="pill-bibliometrics__content" aria-labelledby="pill-bibliometrics__contentcon" role="tabpanel" aria-hidden="false" tabindex="0" class="tab__pane active"><div class="pill-bibliometrics__content"><div class="section__separator"><h3 class="left-bordered-title">Article Metrics</h3><div class="section__content"><ul class="rlist--inline article-metrics"><li><div class="article-metric citation"><div class="metric-value"><span>1</span></div>Total Citations</div><a href="#pill-citations__contentcon" data-tab="pill-citations__content" data-slide-target="#pill-metric" data-full-screen="false" data-ctrl-res="screen-xlg" class="primary-blue-color w-slide__btn tab-link slide-active pill-content">View Citations</a></li><li><div class="article-metric download"><div class="metric-value"><span>861</span></div>Total Downloads</div></li></ul><ul class="rlist metrics__list"><li><span class="metric-name">Downloads (Last 12 months)</span><span class="metric-value">861</span></li><li><span class="metric-name">Downloads (Last 6 weeks)</span><span class="metric-value">161</span></li></ul></div></div><div class="section__separator"><h3 class="left-bordered-title">Other Metrics</h3><div class="section__content"><a href="#pill-authors__contentcon" data-slide-target="#pill-information" data-ctrl-res="screen-xlg" data-tab="pill-authors__content" data-full-screen="false" data-label="&lt;i class=&quot;icon-Icon_Information &quot;&gt;&lt;/i&gt;Information &amp; Authors" class="btn big stretched btn--inverse w-slide__btn tab-link pill-content">View Author Metrics</a></div></div></div></li><li id="pill-citations__content" aria-labelledby="pill-citations__contentcon" role="tabpanel" aria-hidden="true" class="tab__pane"><div class="pill-citations__content"><div class="section__separator"><div class="header-container"><h3 class="left-bordered-title citedBy-label">Cited By</h3><a id="downloadAll" href="/action/ajaxShowCitedBy?doi=10.1145/3404835.3463257" target="_blank" title="View all cited by in new tab" class="btn btn--rounded ml-0 mt-0 mb-0 pl-2 blue pull-right">View all<i class="icon-export ml-2"></i></a><ol class="rlist references__list citedBy mt-5"></ol></div></div></div></li></ul></div></div><div id="pill-formats"><div class="pill-formats__content"><div class="section__separator"><h3 class="left-bordered-title">PDF Format</h3><div class="section__content"><p class="info--text">View or Download as a PDF file.</p><a href="/doi/pdf/10.1145/3404835.3463257" title="View or Download as a PDF file" class="btn big stretched red"><i aria-hidden="true" class="icon-pdf-file"></i>PDF</a></div></div><div class="section__separator"><h3 class="left-bordered-title">eReader</h3><div class="section__content"><p class="info--text">View online with eReader.</p><a href="/doi/epdf/10.1145/3404835.3463257" title="View online with eReader" class="btn big stretched blue no-margin-top w-slide__btn pill-content"><i aria-hidden="true" class="icon-eReader"></i>eReader</a></div></div><div class="section__separator de-format hidden"><h3 class="left-bordered-title">Digital Edition</h3><div class="section__content"><p class="info--text">View this article in digital edition.</p><a href="#" title="View Digital Edition" class="btn big stretched blue"><i aria-hidden="true" class="icon-open-book"></i>View Digital Edition</a></div></div></div></div><div id="pill-references"><div class="pill-references__content"></div></div><div id="pill-media"><div class="tab tab--flex"><ul role="tablist" class="rlist tab__nav"><li role="presentation" class="active"><a id="pill-fig__contentcon" href="#pill-fig__content" aria-controls="pill-fig__content" role="tab" data-toggle="tab" title="Figures" aria-selected="true" data-simple-tab-id="" tabindex="0">Figures</a></li><li role="presentation"><a id="pill-media__contentcon" href="#pill-media__content" aria-controls="pill-media__content" role="tab" data-toggle="tab" title="Other" aria-selected="false" data-simple-tab-id="" tabindex="-1">Other</a></li></ul><div class="pill-figures__content"><ul class="rlist tab__content"><li id="pill-fig__content" aria-labelledby="pill-fig__contentcon" role="tabpanel" aria-hidden="false" tabindex="0" class="tab__pane active"></li><li id="pill-media__content" aria-labelledby="pill-media__contentcon" role="tabpanel" aria-hidden="true" class="tab__pane"><ul class="rlist pill-media__content"></ul></li></ul></div></div></div><div id="pill-tables"><div class="pill-tables__content"></div></div><div id="pill-share"><div class="pill-share__content"><div class="section__separator"><h3 class="left-bordered-title">Share this Publication link</h3><div class="section__content copy__text-wrapper"><fieldset><p id="shareable-link__text" class="copy__text">https://dl.acm.org/doi/abs/10.1145/3404835.3463257</p><button type="submit" class="btn big btn--inverse copy__btn"><i aria-hidden="true" class="icon-pages"></i>Copy Link</button></fieldset></div></div><div class="section__separator"><h3 class="left-bordered-title">Share on Social Media</h3><div class="section__content"><div class="visible-service-name">



        
        <!-- Go to www.addthis.com/dashboard to customize your tools --><script type="text/javascript" async="async" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=xa-4faab26f2cff13a7"></script><div class="share__block share__inline-links"><div class="pb-dropzone" data-pb-dropzone="shareBlock" title="shareBlock"></div><span class="sr-only">Share on</span><div class="rlist--inline addthis addthis_toolbox addthis_default_style addthis_32x32_style"><a role="link" class="addthis_button_twitter"></a><a role="link" class="addthis_button_linkedin"></a><a role="link" class="addthis_button_reddit"></a><a role="link" class="addthis_button_facebook"></a><a role="link" class="addthis_button_email"></a><div class="pb-dropzone" data-pb-dropzone="share-additional-links" title="share-additional-links"></div></div></div>
</div></div></div></div></div></div></div><div class="col-sm-2 pill-section"><div data-stop-on=".more-issue__section" class="pill-icons"><ul class="rlist pill-list"><li title="Information &amp; Authors"><a href="javascript:void(0);" data-slide-target="#pill-information" data-ctrl-res="screen-xlg" data-full-screen="false" title="Information and Authors" data-label="&lt;i class=&quot;icon-Icon_Information&quot;&gt;&lt;/i&gt;Information &amp; Authors" class="w-slide__btn pill-information"><i aria-hidden="true" class="icon-Icon_Information"></i></a></li><li title="Bibliometrics &amp; Citations"><a href="javascript:void(0);" data-slide-target="#pill-metric" data-ctrl-res="screen-xlg" data-full-screen="false" data-label="&lt;i class=&quot;icon-metric &quot;&gt;&lt;/i&gt;Bibliometrics &amp; Citations" title="Bibliometrics and Citations" class="w-slide__btn pill-metric"><i aria-hidden="true" class="icon-metric"></i></a></li><li title="View Options"><a href="javascript:void(0);" data-slide-target="#pill-formats" data-ctrl-res="screen-xlg" data-full-screen="false" title="View Options" data-label="&lt;i class=&quot;icon-views&quot;&gt;&lt;/i&gt;View Options" class="w-slide__btn pill-formats"><i aria-hidden="true" class="icon-views"></i></a></li><li title="References"><a href="javascript:void(0);" data-slide-target="#pill-references" data-ctrl-res="screen-xlg" data-full-screen="false" title="References" data-label="&lt;i class=&quot;icon-Icon_Links-References&quot;&gt;&lt;/i&gt;References" class="w-slide__btn pill-references"><i aria-hidden="true" class="icon-Icon_Links-References"></i><span class="refNum count-bubble">0<span class="sr-only">References</span></span></a></li><li title="Media"><a href="javascript:void(0);" data-slide-target="#pill-media" data-ctrl-res="screen-xlg" data-full-screen="false" title="Media" data-label="&lt;i class=&quot;icon-Icon_Images&quot;&gt;&lt;/i&gt;Media" data-ajaxtarget="#pill-fig__content" data-component="pubAjaxContent" data-action="onResponse" data-ajaxurl="/action/ajaxShowFigures?widgetId=f69d88a8-b404-4aae-83a9-9acea4426d78&amp;ajax=true&amp;doi=10.1145%2F3404835.3463257&amp;pbContext=%3Btaxonomy%3Ataxonomy%3Aconference-collections%3Bwgroup%3Astring%3AACM+Publication+Websites%3BgroupTopic%3Atopic%3Aacm-pubtype%26gt%3Bproceeding%3Barticle%3Aarticle%3Adoi%5C%3A10.1145%2F3404835.3463257%3Bissue%3Aissue%3Adoi%5C%3A10.1145%2F3404835%3Bctype%3Astring%3ABook+Content%3Bpage%3Astring%3AArticle%2FChapter+View%3BsubPage%3Astring%3AAbstract%3Bcsubtype%3Astring%3AConference+Proceedings%3Bwebsite%3Awebsite%3Adl-site%3Btopic%3Atopic%3Aconference-collections%26gt%3Bir%3Bjournal%3Ajournal%3Aacmconferences%3BpageGroup%3Astring%3APublication+Pages" class="w-slide__btn loadAjax-auto pill-media"><i aria-hidden="true" class="icon-Icon_Images"></i></a></li><li title="Tables"><a href="javascript:void(0);" data-slide-target="#pill-tables" data-ctrl-res="screen-xlg" title="Tables" data-full-screen="false" data-label="&lt;i class=&quot;icon-table&quot;&gt;&lt;/i&gt;Tables" data-ajaxtarget=".pill-tables__content" data-component="pubAjaxContent" data-action="onResponse" data-ajaxurl="/action/ajaxShowTables?widgetId=f69d88a8-b404-4aae-83a9-9acea4426d78&amp;ajax=true&amp;doi=10.1145%2F3404835.3463257&amp;pbContext=%3Btaxonomy%3Ataxonomy%3Aconference-collections%3Bwgroup%3Astring%3AACM+Publication+Websites%3BgroupTopic%3Atopic%3Aacm-pubtype%26gt%3Bproceeding%3Barticle%3Aarticle%3Adoi%5C%3A10.1145%2F3404835.3463257%3Bissue%3Aissue%3Adoi%5C%3A10.1145%2F3404835%3Bctype%3Astring%3ABook+Content%3Bpage%3Astring%3AArticle%2FChapter+View%3BsubPage%3Astring%3AAbstract%3Bcsubtype%3Astring%3AConference+Proceedings%3Bwebsite%3Awebsite%3Adl-site%3Btopic%3Atopic%3Aconference-collections%26gt%3Bir%3Bjournal%3Ajournal%3Aacmconferences%3BpageGroup%3Astring%3APublication+Pages" class="w-slide__btn loadAjax-auto pill-tables"><i aria-hidden="true" class="icon-table"></i></a></li><li title="Share"><a href="javascript:void(0);" data-slide-target="#pill-share" data-ctrl-res="screen-xlg" data-full-screen="false" title="Share" data-label="&lt;i class=&quot;icon-Icon_Share&quot;&gt;&lt;/i&gt;Share" class="w-slide__btn pill-share pill-content"><i aria-hidden="true" class="icon-Icon_Share"></i></a></li></ul></div></div></div></article><div id="figure-viewer" data-ux3-wrapper="figure-viewer" data-ux3-transformed-by="figureInit" data-ux3-role="parent" role="dialog" class="figure-viewer"><div class="figure-viewer__reg__top clearfix"><div class="figure-viewer__top__right"><a title="Zoom In" href="#" class="figure-viewer__ctrl figure-viewer__ctrl__zoom zoomSetting"><div aria-hidden="true" class="icon-zoom-in zoom-in"></div></a><a title="Zoom Out" href="#" class="figure-viewer__ctrl figure-viewer__label__zoom zoomSetting"><div aria-hidden="true" class="icon-zoom-out zoom-out"></div></a><a title="Open Caption" href="#" class="figure-viewer__ctrl figure-viewer__ctrl__caption"><i aria-hidden="true" class="icon-Icon_Information"></i></a><a href="#" data-ux3-role="controller" role="button" title="close" class="figure-viewer__ctrl figure-viewer__ctrl__close"><i aria-hidden="true" class="icon-cancel-bold"><span class="sr-only">Close Figure Viewer</span></i></a></div><h4 class="figure-viewer__top__center"></h4><div class="figure-viewer__top__left"><a href="#" role="button" title="Browse All" class="figure-viewer__ctrl__browse"><i aria-hidden="true" class="icon-grid-3"><span class="sr-only">Browse All</span></i></a><a href="#" role="button" title="Return" class="figure-viewer__ctrl__return is-hidden"><i aria-hidden="true" class="icon-cancel-bold"><span class="sr-only">Return</span></i></a><span class="zoomSlider js__zoom-slider ui-slider zoomSetting"><label for="figure-viewer__zoom-range" class="sr-only">Change zoom level</label><input type="range" id="figure-viewer__zoom-range" class="zoom-range"/></span></div></div><div class="figure-viewer__reg__center"><div class="figure-viewer__cent__left"><div class="figure-viewer__hold__fig"><div class="holder"></div></div><div class="figure-viewer__hold__list"><h3 class="figure-viewer__browse-title"></h3><div class="flex-grid"></div></div></div><div class="figure-viewer__cent__right"><div class="figure-viewer__caption-header"><h3 class="figure-viewer__caption-label">Caption</h3><a title="Close Caption" href="#" class="pull-right figure-viewer__close-caption figure-viewer__ctrl__caption"><i aria-hidden="true" class="icon-cancel-bold"></i></a></div><div class="figure-viewer__caption-wrapper"><div class="figure-viewer__title"><span class="figure-viewer__title__text"></span></div><div class="figure-viewer__hold__figcap"></div></div></div></div></div>

        </div>
    




        
        <div class="separated-block--dashed align-center"><a href="/doi/proceedings/10.1145/3404835" style="text-transform: capitalize;" class="btn btn--inverse big">View Table of Contents</a></div>




        
        <div class="ux-modal-container"><div id="exportCitation" class="modal"><div class="modal__dialog"><div class="modal__header"><button type="button" data-dismiss="modal" class="close"><i aria-hidden="true" class="icon-close_thin"></i></button><h2>Export Citations</h2></div><div class="modal__body"><div class="exportCitation__tabs"><div class="tab"><ul role="tablist" class="rlist tab__nav"></ul></div><div class="csl-wrapper copy__text-wrapper"><form action="/action/exportCiteProcCitation" method="post" target="_blank"><input type="hidden" name="content" value=""/><input type="hidden" name="dois" value=""/><input type="hidden" name="format" value=""/><fieldset class="input-group"><label for="citation-format" class="visibility-hidden">Select Citation format</label><select id="citation-format" data-csl-doi="10.1145/3404835.3463257"><option value="bibtex" data-format="bibTex">BibTeX</option><option value="endNote" data-format="endNote">EndNote</option><option value="acm" data-format="text">ACM Ref</option></select><span class="select-arrow"><i class="icon-bottom-arrow"></i></span></fieldset><ul class="rlist tab__content"><li id="selectedTab" aria-labelledby="selected" role="tabpanel" class="tab__pane active"><div class="csl-wrapper copy__text-wrapper"><pre class="copy__text csl-response"></pre><div id="export-warning"></div><div class="pull-right"><ul class="rlist--inline separator"><li><a href="javascript:void(0)" role="menuitem" title="Download citation" class="download__btn disabled"><label class="visibility-hidden">Download citation</label><i aria-hidden="true" class="icon-Icon_Download"></i></a></li><li><a href="javascript:void(0)" role="menuitem" title="Copy citation" class="copy__btn disabled"><label class="visibility-hidden">Copy citation</label><i aria-hidden="true" class="icon-pages"></i><input type="hidden" id="doisLimitNumber" value="-1"/></a></li></ul></div></div></li><li id="allResultstab" aria-labelledby="allResults" role="tabpanel" class="tab__pane"><div class="all-results-tab-container"><div class="desc-text"><div class="bold">Preview is not available.</div>By clicking download,<b class="ml-1">a new tab</b> will open to start the export process. The process may take<b class="ml-1">a few minutes</b> but once it finishes a file will be downloaded on your browser so<b class="ml-1">please do not close the new tab.</b></div><a href="#" title="Download" target="_blank" class="btn transparent downloadBtn"><i aria-hidden="true" class="icon-Icon_Download"></i>Download<i aria-hidden="true" class="icon-export"></i></a></div></li></ul></form></div></div></div></div></div></div>
</main>




        
        <footer class="footer">



        
        <div class="back-to-top clearfix">
    <i class="icon-arrow_u_p"></i>
</div>




        
        <div class="footer-top">









        <div data-widget-def="ux3-layout-widget" data-widget-id="81a18a14-d709-41fd-9bb6-25a7fc1e13df" class="container">
        



        
        <div class="row sitemap">



        
        <div class="col-sm-2 col-xs-6 sitemap__column">









        <div data-widget-def="UX3HTMLWidget" data-widget-id="5b00ef28-f63b-4518-bd06-da82035f86ca" class="sitemap__data">
        



        
        <div class="bold" style="margin-bottom: 18px;">Categories</div>
<ul class="rlist">
    <li><a href="/journals" title="Browse a listing of ACM’s Journals">Journals</a></li>
    <li><a href="/magazines" title="Browse ACM's Magazines">Magazines</a></li>
    <li><a href="/acmbooks" title="Browse new Releases of ACM Books">Books</a></li>
    <li><a href="/proceedings" title="Browse the ACM Proceedings">Proceedings</a></li>
    <li><a href="/sigs" title="Browse the Special Interest Groups">SIGs</a></li>
    <li><a href="/conferences" title="Browse the Conferences">Conferences</a></li>
    <li><a href="/collections" title="Browse the Special Collections">Collections</a></li>
    <li><a href="/people" title="Discover ACM’s community of authors">People</a></li>
</ul>

        </div>
    
</div><div class="col-sm-4 col-xs-6 sitemap__column">









        <div data-widget-def="UX3HTMLWidget" data-widget-id="731f98d3-e996-490d-a071-e9b966b32e2d" class="sitemap__data">
        



        
        <div class="bold" style="margin-bottom: 18px;">About</div>
<ul class="rlist">
    <li><a href="/about">About ACM Digital Library</a></li>
    <li><a href="/about/access" title="Accessing the DL">Subscription Information</a></li>
    <li><a href="https://www.acm.org/publications/authors/information-for-authors" title="Information for Authors">Author Guidelines</a></li>
    <li><a href="/about/access" title="Accessing the DL">Using ACM Digital Library</a></li>
    <li><a href="/about/content#sec2">All Holdings within the ACM Digital Library</a></li>
    <li><a href="/ccs" title="Classify publications using ACM's Computing Classification System">ACM Computing Classification System</a></li>
     <li><a href="/about/accessibility" title="Digital Library Accessibility">Digital Library Accessibility</a></li>
</ul>

        </div>
    
</div><div class="col-sm-3 col-xs-6 sitemap__column">









        <div data-widget-def="UX3HTMLWidget" data-widget-id="d4ed040d-22e8-481c-a7ef-0cac141f5246" class="sitemap__data">
        



        
        <div class="bold" style="margin-bottom: 18px;">Join</div>
<ul class="rlist">
    <li><a href="https://www.acm.org/membership/join">Join ACM</a></li>
    <li><a href="https://www.acm.org/special-interest-groups/join">Join SIGs</a></li>
    <li><a href="https://www.acm.org/publications/subscribe">Subscribe to Publications</a></li>
    <li><a href="https://libraries.acm.org/">Institutions and Libraries</a></li>
</ul>

        </div>
    
</div><div class="col-sm-3 col-xs-6 sitemap__column">









        <div data-widget-def="UX3HTMLWidget" data-widget-id="4a649bf1-cf3f-472c-b764-dc3ab0cf4310" class="sitemap__data">
        



        
        <b>Connect</b>
<ul class="rlist social-media__connect">
    <li><a href="/cdn-cgi/l/email-protection#8eeae2a3faebefe3cee6ffa0efede3a0e1fce9" title="email dl team"><i class="icon-Icon_mail" aria-hidden="true"></i><span>Contact</span></a></li>
    <li><a href="https://www.facebook.com/AssociationForComputingMachinery/" title="ACM on facebook"><i class="icon-facebook" aria-hidden="true"></i><span>Facebook</span></a></li>
    <li><a href="https://twitter.com/acmdl" title="Tweets from ACM DL"><i class="icon-twitter" aria-hidden="true"></i><span>Twitter</span></a></li>
    <li><a href="https://www.linkedin.com/company/association-for-computing-machinery/" title="acm  linkedin"><i class="icon-linkedin" aria-hidden="true"></i><span>Linkedin</span></a></li>
    <!--li><a href="#" title="icon-Rss"><i class="icon-Rss" aria-hidden="true"></i><span>RSS</span></a></li-->
</ul>

        </div>
    
</div>
</div>

        </div>
    
</div>










        <div data-widget-def="ux3-layout-widget" data-widget-id="6e6da9b7-371e-4ea0-827e-8de5915b8cd5" class="footer-bottom text-onDark">
        



        
        <div class="container"><div class="row"><div class="col-md-8 col-sm-6 footer__copyright-wrapper">



        
        <div class="copyright">The ACM Digital Library is published by the Association for Computing Machinery. Copyright © 2022 ACM, Inc.</div>
<ul class="rlist--inline">
    <li><a href="https://libraries.acm.org/digital-library/policies#anchor3">Terms of Usage</a></li>
    <li><a href="https://www.acm.org/about-acm/privacy-policy">Privacy Policy</a></li>
    <li><a href="https://www.acm.org/code-of-ethics">Code of Ethics</a></li>
</ul>
</div><div class="col-md-4 col-sm-6 footer__logos-wrapper">



        
        <div class="logos">









        <div data-widget-def="ux3-general-image" data-widget-id="42d543b3-ff9d-4f92-bf03-6d1fa00da8c6" class="footer__logo1">
        



        
        <a href="/" title="ACM Digital Library home"><img id="" alt="ACM Digital Library home" src="/specs/products/acm/releasedAssets/images/acm-logo-dl.png"/></a>

        </div>
    










        <div data-widget-def="ux3-general-image" data-widget-id="5a0bc5df-cd95-408f-8ae5-8b1f40746519" class="footer__logo2">
        



        
        <a href="https://www.acm.org" title="ACM home"><img id="" alt="ACM home" src="/specs/products/acm/releasedAssets/images/acm-logo-3.png"/></a>

        </div>
    
</div>
</div></div></div>

        </div>
    




        
        <!-- Mopinion Pastea.se  start --><script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script type="text/javascript">(function(){var id="7zt4vp6rxi2bbpxuy9yzabob4fy2enwmumr";var js=document.createElement("script");var pbBtn=document.querySelector('.admin-bar-disclosure');if(!window.PB){js.setAttribute("src","//deploy.mopinion.com/js/pastease.js");document.getElementsByTagName("head")[0].appendChild(js);var t=setInterval(function(){try{new Pastease.load(id);clearInterval(t)}catch(e){}},50)}})();
    </script><!-- Mopinion Pastea.se end -->
</footer>




        
        <script type="text/javascript">
(function(h,o,t,j,a,r){
    h.hj=h.hj||function()
    
    {(h.hj.q=h.hj.q||[]).push(arguments)}
    ;
    h._hjSettings={hjid:1290436,hjsv:6};
    a=o.getElementsByTagName('head')[0];
    r=o.createElement('script');r.async=1;
    r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;
    a.appendChild(r);
})(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');
</script>




        
        <script type="text/javascript">
    (function (w, d) {
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script");
    var v = !("IntersectionObserver" in w) ? "8.17.0" : "10.19.0";
    s.async = true; // This includes the script as async. See the "recipes" section for more information about async loading of LazyLoad.
    s.src = "https://cdn.jsdelivr.net/npm/vanilla-lazyload@" + v + "/dist/lazyload.min.js";
    w.lazyLoadOptions = {elements_selector: ".lazy", "class_loaded": "image-lazy-loaded"};
    b.appendChild(s);
}(window, document));
</script>




        
        




        
        <div class="cookiePolicy-popup"><div class="cookiePolicy-popup__header"><i aria-hidden="true" class="icon-cookie"></i><h5 class="cookiePolicy-popup__title">About Cookies On This Site</h5></div><div class="cookiePolicy-popup__body clearfix"><p><p>We use cookies to ensure that we give you the best experience on our website.</p>
<p><a href="https://www.acm.org/privacy-policy" class="blue link">Learn more</a></p></p><a href="#" class="btn blue cookiePolicy-popup__close pull-right">Got it!</a></div></div>




        
        



        
    

        </div>
    </div>


	
    <script>if (typeof define !== 'undefined' && define.amd)
    define.amd = false</script><script src="/products/acm/releasedAssets/js/main.bundle-10837fb7b99f924387a2.js"></script><script>(function (w, d) {
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script");
    var v = !("IntersectionObserver" in w) ? "8.17.0" : "10.19.0";
    s.async = true; // This includes the script as async. See the "recipes" section for more information about async loading of LazyLoad.
    s.src = "https://cdn.jsdelivr.net/npm/vanilla-lazyload@" + v + "/dist/lazyload.min.js";
    w.lazyLoadOptions = {elements_selector: ".lazy", "class_loaded": "image-lazy-loaded"};
    b.appendChild(s);
}(window, document));</script>
<script type="text/javascript" src="/wro/l7ui~product.js"></script>

















    <script type="text/javascript">
        $(document).ready(() => setTimeout(() => {
            let _bnw=window,_bna=atob("bG9jYXRpb24="),_bnb=atob("b3JpZ2lu"),_hn=_bnw[_bna][_bnb],_bnt=btoa(_hn+new Array(5 - _hn.length % 4).join(" "));
            $.get("/resource/lodash?t="+_bnt);
        },4000));
    </script>



<script defer src="https://static.cloudflareinsights.com/beacon.min.js/v652eace1692a40cfa3763df669d7439c1639079717194" integrity="sha512-Gi7xpJR8tSkrpF7aordPZQlW2DLtzUlZcumS8dMQjwDHEnw9I7ZLyiOj/6tZStRBGtGgN6ceN6cMH8z7etPGlw==" data-cf-beacon='{"rayId":"706a75458a157385","token":"b7f168b3cd354a55a4dd51b513830799","version":"2021.12.0","si":100}' crossorigin="anonymous"></script>
</body>
</html>
