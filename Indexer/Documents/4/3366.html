<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Analytics/Systems/Cluster/AMD GPU - Wikitech</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"9825eec8-bf52-49df-90a9-43fac637fc21","wgCSPNonce":false,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Analytics/Systems/Cluster/AMD_GPU","wgTitle":"Analytics/Systems/Cluster/AMD GPU","wgCurRevisionId":1932235,"wgRevisionId":1932235,"wgArticleId":444148,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Analytics/Systems/Cluster/AMD_GPU","wgRelevantArticleId":444148,"wgIsProbablyEditable":false,"wgRelevantPageIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[]
,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":10000,"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgULSCurrentAutonym":"English","wgEditSubmitButtonLabelPublish":true,"wgDiscussionToolsFeaturesEnabled":{"replytool":true,"newtopictool":false,"sourcemodetoolbar":true,"topicsubscription":false,"autotopicsub":false},"wgDiscussionToolsFallbackEditMode":"visual","wgULSPosition":"personal","wgULSisCompactLinksEnabled":true,"wgSiteNoticeId":"2.0"};RLSTATE={"ext.gadget.enwp-boxes":"ready","site.styles":"ready","user.styles":"ready","user":"ready","user.options":"loading","ext.pygments":"ready","ext.discussionTools.init.styles":"ready","skins.vector.styles.legacy":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.pt":"ready",
"ext.dismissableSiteNotice.styles":"ready"};RLPAGEMODULES=["site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.gadget.site","ext.discussionTools.init","ext.uls.compactlinks","ext.uls.interface","ext.dismissableSiteNotice"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1i9g4",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.discussionTools.init.styles%7Cext.dismissableSiteNotice.styles%7Cext.pygments%7Cext.uls.pt%7Cext.visualEditor.desktopArticleTarget.noscript%7Cskins.vector.styles.legacy&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.gadget.enwp-boxes&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.39.0-wmf.10"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta name="format-detection" content="telephone=no"/>
<meta property="og:title" content="Analytics/Systems/Cluster/AMD GPU - Wikitech"/>
<meta property="og:type" content="website"/>
<link rel="shortcut icon" href="/static/favicon/wikitech.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikitech (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//wikitech.wikimedia.org/w/api.php?action=rsd"/>
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Wikitech Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://wikitech.wikimedia.org/wiki/Analytics/Systems/Cluster/AMD_GPU"/>
</head>
<body class="ext-discussiontools-replytool-enabled ext-discussiontools-sourcemodetoolbar-enabled mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Analytics_Systems_Cluster_AMD_GPU rootpage-Analytics skin-vector action-view skin-vector-legacy"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"><div id="mw-dismissablenotice-anonplace"></div><script>(function(){var node=document.getElementById("mw-dismissablenotice-anonplace");if(node){node.outerHTML="\u003Cdiv class=\"mw-dismissable-notice\"\u003E\u003Cdiv class=\"mw-dismissable-notice-close\"\u003E[\u003Ca tabindex=\"0\" role=\"button\"\u003Edismiss\u003C/a\u003E]\u003C/div\u003E\u003Cdiv class=\"mw-dismissable-notice-body\"\u003E\u003Cdiv id=\"localNotice\"\u003E\u003Cdiv class=\"sitenotice\" lang=\"en\" dir=\"ltr\"\u003E\u003Cdiv class=\"center\"\u003E\u003Ca href=\"https://www.mediawiki.org/wiki/Wikimedia_Hackathon_2022\" class=\"extiw\" title=\"mw:Wikimedia Hackathon 2022\"\u003EWikimedia Hackathon 2022\u003C/a\u003E will be held \u003Ci\u003Eonline\u003C/i\u003E May 20-22, 2022.\u003C/div\u003E\u003C/div\u003E\u003C/div\u003E\u003C/div\u003E\u003C/div\u003E";}}());</script></div>
	<div class="mw-indicators">
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading">Analytics/Systems/Cluster/AMD GPU</h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Wikitech</div>
		<div id="contentSub"><span class="subpages">&lt; <a href="/wiki/Analytics" class="mw-redirect" title="Analytics">Analytics</a>&lrm; | <a href="/wiki/Analytics/Systems" title="Analytics/Systems">Systems</a>&lrm; | <a href="/wiki/Analytics/Systems/Cluster" title="Analytics/Systems/Cluster">Cluster</a></span></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content mw-content-ltr" lang="en" dir="ltr"><div class="mw-parser-output"><div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#What_GPU_model_do_we_have?_On_what_hosts?"><span class="tocnumber">1</span> <span class="toctext">What GPU model do we have? On what hosts?</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Use_the_Debian_packages"><span class="tocnumber">2</span> <span class="toctext">Use the Debian packages</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Use_the_GPU_on_the_host"><span class="tocnumber">3</span> <span class="toctext">Use the GPU on the host</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Use_tensorflow"><span class="tocnumber">4</span> <span class="toctext">Use tensorflow</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#Experimental"><span class="tocnumber">4.1</span> <span class="toctext">Experimental</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Configure_your_Tensorflow_script"><span class="tocnumber">5</span> <span class="toctext">Configure your Tensorflow script</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Check_the_version_of_ROCm_deployed_on_a_host"><span class="tocnumber">6</span> <span class="toctext">Check the version of ROCm deployed on a host</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Check_usage_of_the_GPU"><span class="tocnumber">7</span> <span class="toctext">Check usage of the GPU</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Outstanding_issues"><span class="tocnumber">8</span> <span class="toctext">Outstanding issues</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#Reset_the_GPU_state"><span class="tocnumber">9</span> <span class="toctext">Reset the GPU state</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#Upgrade_the_Debian_packages"><span class="tocnumber">10</span> <span class="toctext">Upgrade the Debian packages</span></a></li>
</ul>
</div>

<h2><span id="What_GPU_model_do_we_have.3F_On_what_hosts.3F"></span><span class="mw-headline" id="What_GPU_model_do_we_have?_On_what_hosts?" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-What_GPU_model_do_we_have?_On_what_hosts?&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-What_GPU_model_do_we_have?_On_what_hosts?"></span>What GPU model do we have? On what hosts?<span data-mw-comment-end="h-What_GPU_model_do_we_have?_On_what_hosts?"></span></span></h2>
<p>The Analytics team added a GPU to stat1005 and one to stat1008.
The model is AMD Radeon Pro WX 9100 16GB. The choice fell to AMD since they are currently the only ones releasing their software stack open source: <a rel="nofollow" class="external free" href="https://rocm.github.io/ROCmInstall.html">https://rocm.github.io/ROCmInstall.html</a>
</p><p>There is also a GPU (same model) on each of the following Hadoop worker nodes: an-worker1096 to an-worker1101.
</p>
<h2><span class="mw-headline" id="Use_the_Debian_packages" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Use_the_Debian_packages&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Use_the_Debian_packages"></span>Use the Debian packages<span data-mw-comment-end="h-Use_the_Debian_packages"></span></span></h2>
<p>See <a rel="nofollow" class="external text" href="https://github.com/wikimedia/puppet/blob/production/modules/profile/manifests/statistics/gpu.pp">profile::statistics::gpu</a> or the <a rel="nofollow" class="external text" href="https://github.com/wikimedia/puppet/tree/production/modules/amd_rocm/manifests">amd_rocm</a> module in operations/puppet.
</p>
<h2><span class="mw-headline" id="Use_the_GPU_on_the_host" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Use_the_GPU_on_the_host&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Use_the_GPU_on_the_host"></span>Use the GPU on the host<span data-mw-comment-end="h-Use_the_GPU_on_the_host"></span></span></h2>
<p>All users in <code>analytics-privatedata-users</code> are automatically granted access to the GPUs, otherwise a user need to be in the <code>gpu-testers</code> POSIX group in operations/puppet. This is a workaround to force the users in that group to be in the <code>render</code> POSIX group (available on Debian), that grants access to the GPU. Please keep in mind few things:
</p>
<ul><li>Be careful in launching multiple parallel jobs on the same GPU, see <a class="external free" href="https://phabricator.wikimedia.org/T248574">https://phabricator.wikimedia.org/T248574</a></li>
<li>Miriam Redi is currently the main point of contact to decide what the schedule of GPU usage should be. In case of doubt, before starting any test or heavy job, please follow up with her.</li></ul>
<h2><span class="mw-headline" id="Use_tensorflow" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Use_tensorflow&quot;,&quot;replies&quot;:[&quot;h-Experimental-Use_tensorflow&quot;],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Use_tensorflow"></span>Use tensorflow<span data-mw-comment-end="h-Use_tensorflow"></span></span></h2>
<p>The easiest solution is to create a Python 3 virtual environment on stat1005 or stat1008 and then pip3 install <a rel="nofollow" class="external free" href="https://pypi.org/project/tensorflow-rocm/">https://pypi.org/project/tensorflow-rocm/</a>. Please remember that every version of the package is linked against a specific version of ROCm, so it may be possible that newer versions of tensorflow-rocm don't run on our hosts since we don't have an up to date version of ROCm deployed yet.
</p><p>Upstream suggested to follow <a rel="nofollow" class="external free" href="https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/RELEASE.md">https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/RELEASE.md</a> and check every time what combination of tensorflow-rocm and ROCm is supported.
</p>
<p><b>We have only one version of ROCm deployed:</b> <b>4.2</b>. <b>Only tensorflow-rocm 2.5.0 is supported.</b></p><div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>virtualenv -p python3 test_tf
<span class="nb">source</span> test_tf/bin/activate 
pip3 install tensorflow-rocm<span class="o">==</span><span class="m">2</span>.5.0
</pre></div>
<h3><span class="mw-headline" id="Experimental" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Experimental-Use_tensorflow&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:3,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Experimental-Use_tensorflow"></span>Experimental<span data-mw-comment-end="h-Experimental-Use_tensorflow"></span></span></h3>
<p>The fact that AMD forked the <code>tensorflow</code> pypi package poses some challenges when using other Tensorflow-based packages in our infrastructure. Most of them in fact have the <code>tensorflow</code> package dependency in their setup.py configurations, and this means that pip will always try to install it (conflicting with <code>tensorflow-rocm</code>). We are testing a hack that aims to trick pip, installing an empty <code>tensorflow</code> package alongside with <code>tensorflow-rocm</code>. This is the procedure: 
</p>
<ol><li>Create an empty <code>tensorflow</code> package. The version that you use needs to be the same as <code>tensorflow-rocm</code>. A possible solution is:<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>$ mkdir <span class="nb">test</span>
$ <span class="nb">cd</span> <span class="nb">test</span>
$ cat > setup.py <span class="s">&lt;&lt; EOF</span>
<span class="s">from setuptools import setup, find_packages</span>

<span class="s">setup(</span>
<span class="s">    name = 'tensorflow', </span>
<span class="s">    version='2.5.0', </span>
<span class="s">    packages=find_packages(),</span>
<span class="s">)</span>
<span class="s">EOF</span>
$ python3 setup.py bdist_wheel
$ ls dist/tensorflow-2.5.0-py3-none-any.whl
</pre></div></li>
<li>Create your Python conda/venv environment as always.</li>
<li>pip install /path/to/dist/tensorflow-2.6-py3-none-any.whl</li>
<li>pip install tensorflow-rocm==2.5.0</li>
<li>pip install etc.. [namely all packages that require tensorflow, the ones that you are interested in]</li></ol>
<p>You may need to solve some dependency issue when pip installing in this way, since installing packages separately may induce some conflicts that you wouldn't have in "regular" installs. Some useful tools:
</p>
<ul><li>pip install pipdeptree; pipdeptree (to check your dependency tree)</li>
<li>pip install pip --upgrade (to get the latest pip version)</li></ul>
<h2><span class="mw-headline" id="Configure_your_Tensorflow_script" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Configure_your_Tensorflow_script&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Configure_your_Tensorflow_script"></span>Configure your Tensorflow script<span data-mw-comment-end="h-Configure_your_Tensorflow_script"></span></span></h2>
<p>By default, Tensorflow tasks take all available resources (both from the CPU and the GPU). In resource sharing settings, this might cause resources to saturate quickly and some process to block before execution. When using Tensorflow scripts on our GPU machines, please make sure you add to your code the following snippet:
</p><p>For Tensorflow version 2.0 and 2.1:
</p>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span> <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
 <span class="n">gpu_devices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">'GPU'</span><span class="p">)</span>
 <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">set_memory_growth</span><span class="p">(</span><span class="n">gpu_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
<p>or directly
</p>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span> <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
 <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">set_per_process_memory_growth</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<p><br/>
For prior versions:
</p>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf_config</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">()</span>
<span class="n">tf_config</span><span class="o">.</span><span class="n">gpu_options</span><span class="o">.</span><span class="n">allow_growth</span><span class="o">=</span><span class="kc">True</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
<p><br/>
Also, a good practice is to limit the number of threads used by your tensorflow code.
</p><p>For Tensorflow version 2.0 and 2.1:
</p>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">threading</span><span class="o">.</span><span class="n">set_intra_op_parallelism_threads</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="c1">#or lower values</span>
<span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">threading</span><span class="o">.</span><span class="n">set_inter_op_parallelism_threads</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="c1">#or lower values</span>
</pre></div>
<p>For prior versions:
</p>
<div class="mw-highlight mw-highlight-lang-python mw-content-ltr" dir="ltr"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="n">tf_config</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">intra_op_parallelism_threads</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">inter_op_parallelism_threads</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf_config</span><span class="p">)</span>
</pre></div>
<p><br/>
</p>
<h2><span class="mw-headline" id="Check_the_version_of_ROCm_deployed_on_a_host" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Check_the_version_of_ROCm_deployed_on_a_host&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Check_the_version_of_ROCm_deployed_on_a_host"></span>Check the version of ROCm deployed on a host<span data-mw-comment-end="h-Check_the_version_of_ROCm_deployed_on_a_host"></span></span></h2>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>elukey@stat1005:~/test$ dpkg -l rocm-dev
<span class="nv">Desired</span><span class="o">=</span>Unknown/Install/Remove/Purge/Hold
<span class="p">|</span> <span class="nv">Status</span><span class="o">=</span>Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
<span class="p">|</span>/ Err?<span class="o">=(</span>none<span class="o">)</span>/Reinst-required <span class="o">(</span>Status,Err: <span class="nv">uppercase</span><span class="o">=</span>bad<span class="o">)</span>
<span class="o">||</span>/ Name           Version      Architecture Description
+++-<span class="o">==============</span>-<span class="o">============</span>-<span class="o">============</span>-<span class="o">=================================================</span>
ii  rocm-dev       <span class="m">2</span>.7.22       amd64        Radeon Open Compute <span class="o">(</span>ROCm<span class="o">)</span> Runtime software stack
</pre></div>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>elukey@stat1008:~$ dpkg -l rocm-dev
<span class="nv">Desired</span><span class="o">=</span>Unknown/Install/Remove/Purge/Hold
<span class="p">|</span> <span class="nv">Status</span><span class="o">=</span>Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
<span class="p">|</span>/ Err?<span class="o">=(</span>none<span class="o">)</span>/Reinst-required <span class="o">(</span>Status,Err: <span class="nv">uppercase</span><span class="o">=</span>bad<span class="o">)</span>
<span class="o">||</span>/ Name           Version      Architecture Description
+++-<span class="o">==============</span>-<span class="o">============</span>-<span class="o">============</span>-<span class="o">=================================================</span>
ii  rocm-dev       <span class="m">3</span>.3.0-19     amd64        Radeon Open Compute <span class="o">(</span>ROCm<span class="o">)</span> Runtime software stack
</pre></div>
<p>Changelog in <a rel="nofollow" class="external free" href="https://rocm-documentation.readthedocs.io/en/latest/Current_Release_Notes/Current-Release-Notes.html">https://rocm-documentation.readthedocs.io/en/latest/Current_Release_Notes/Current-Release-Notes.html</a>
</p>
<h2><span class="mw-headline" id="Check_usage_of_the_GPU" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Check_usage_of_the_GPU&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Check_usage_of_the_GPU"></span>Check usage of the GPU<span data-mw-comment-end="h-Check_usage_of_the_GPU"></span></span></h2>
<p>On the host (limited to analytics-privatedata-users):
</p>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>elukey@stat1005:~$ sudo radeontop
</pre></div>
<p>In Grafana: <a class="external free" href="https://grafana.wikimedia.org/d/ZAX3zaIWz/amd-rocm-gpu">https://grafana.wikimedia.org/d/ZAX3zaIWz/amd-rocm-gpu</a>
</p><p>Code available in: <a rel="nofollow" class="external free" href="https://github.com/wikimedia/puppet/blob/production/modules/prometheus/manifests/node_amd_rocm.pp">https://github.com/wikimedia/puppet/blob/production/modules/prometheus/manifests/node_amd_rocm.pp</a>
</p>
<h2><span class="mw-headline" id="Outstanding_issues" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Outstanding_issues&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Outstanding_issues"></span>Outstanding issues<span data-mw-comment-end="h-Outstanding_issues"></span></span></h2>
<ul><li><s><a rel="nofollow" class="external free" href="https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/559">https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/559</a></s></li>
<li><s>GPUs are not correctly handling multi-tasking - <a class="external free" href="https://phabricator.wikimedia.org/T248574">https://phabricator.wikimedia.org/T248574</a></s></li>
<li><a rel="nofollow" class="external free" href="https://github.com/tensorflow/io/issues/1548">https://github.com/tensorflow/io/issues/1548</a></li></ul>
<h2><span class="mw-headline" id="Reset_the_GPU_state" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Reset_the_GPU_state&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Reset_the_GPU_state"></span>Reset the GPU state<span data-mw-comment-end="h-Reset_the_GPU_state"></span></span></h2>
<p>If the GPU gets stuck for some reason (unclean job completion, etc..) the following may happen:
</p>
<ul><li>radeontop shows steady RAM usage (90%+ for example).</li>
<li>tensorflow gets stuck when trying to execute jobs.</li></ul>
<p>Usually rebooting the host works, but the following procedure might help as well:
</p>
<ul><li>run <code>sudo /opt/rocm/bin/rocm-smi</code> and get the id of the GPU (usually 1)</li>
<li>run <code>sudo /opt/rocm/bin/rocm-smi --gpureset -d X</code> (with X equals to the id of the GPU)</li></ul>
<h2><span class="mw-headline" id="Upgrade_the_Debian_packages" data-mw-comment="{&quot;type&quot;:&quot;heading&quot;,&quot;level&quot;:0,&quot;id&quot;:&quot;h-Upgrade_the_Debian_packages&quot;,&quot;replies&quot;:[],&quot;headingLevel&quot;:2,&quot;placeholderHeading&quot;:false,&quot;name&quot;:&quot;h-&quot;}"><span data-mw-comment-start="" id="h-Upgrade_the_Debian_packages"></span>Upgrade the Debian packages<span data-mw-comment-end="h-Upgrade_the_Debian_packages"></span></span></h2>
<p>We import the Debian packages released by AMD for Ubuntu Xenial to the amd-rocm component in wikimedia-buster. Up to now (Oct 2021) there is one Debian package released by AMD that is not open source, <code>hsa-ext-rocr-dev</code>. It contains binary libraries to have a better image support in OpenCL, and we don't use it for obvious reasons. The package is sadly required by other packages, and upstream still hasn't made it optional (<a rel="nofollow" class="external free" href="https://github.com/RadeonOpenCompute/ROCm/issues/761">https://github.com/RadeonOpenCompute/ROCm/issues/761</a>).
</p><p>The solution found in <a class="external free" href="https://phabricator.wikimedia.org/T224723">https://phabricator.wikimedia.org/T224723</a> was to create a dummy package via Debian equiv to satisfy dependencies and please the apt install process. This means that every time a new ROCm release is out, the following procedure needs to be done:
</p><p>Before starting to upgrade, please:
</p>
<ul><li>Check <a rel="nofollow" class="external free" href="https://github.com/RadeonOpenCompute/ROCm">https://github.com/RadeonOpenCompute/ROCm</a>, there is a changelog for every version. Pay attention for breaking changes and OS supported.</li>
<li>Check what version of ROCm is supported by what version of tensorflow-rocm. As indicated in a previous section, upstream builds a fork of tensorflow building/linking every version with a specific ROCm library version. The file <a rel="nofollow" class="external text" href="https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/build_rocm_python3">build_rocm_python3</a> (please change the branch according to the version that you are targeting, or use this one for latest) should list a <b>ROCM_INSTALL_DIR</b> value, that will tell you what version of ROCm was used when releasing.</li>
<li>At this point, it is better to involve people that use Tensorflow before targeting a specific release, to choose the best combination for our use cases. Miriam Redi is a good point of contact.</li></ul>
<p>Once you have a target release in mind: 
</p><p>1) Check <a rel="nofollow" class="external free" href="http://repo.radeon.com/rocm/apt/">http://repo.radeon.com/rocm/apt/</a> and see if a new version is out. If so, create a new component like:
</p>
<div class="mw-highlight mw-highlight-lang-puppet mw-content-ltr" dir="ltr"><pre><span></span><span class="na">Name</span><span class="p">:</span> <span class="na">amd</span><span class="o">-</span><span class="na">rocmXX</span>
<span class="na">Method</span><span class="p">:</span> <span class="na">http</span><span class="p">:</span><span class="o">//</span><span class="na">repo</span><span class="err">.</span><span class="na">radeon</span><span class="err">.</span><span class="na">com</span><span class="o">/</span><span class="na">rocm</span><span class="o">/</span><span class="na">apt</span><span class="o">/</span><span class="na">X</span><span class="err">.</span><span class="na">X</span><span class="o">/</span>
<span class="na">Suite</span><span class="p">:</span> <span class="na">xenial</span>
<span class="na">Components</span><span class="p">:</span> <span class="na">main</span><span class="o">></span><span class="na">thirdparty</span><span class="o">/</span><span class="na">amd</span><span class="o">-</span><span class="na">rocmXX</span>
<span class="na">UDebComponents</span><span class="p">:</span>
<span class="na">Architectures</span><span class="p">:</span> <span class="na">amd64</span>
<span class="na">VerifyRelease</span><span class="p">:</span> <span class="mi">9386</span><span class="na">B48A1A693C5C</span>
<span class="na">ListShellHook</span><span class="p">:</span> <span class="na">grep</span><span class="o">-</span><span class="na">dctrl</span> <span class="o">-</span><span class="na">e</span> <span class="o">-</span><span class="na">S</span> <span class="s">'^([..cut..])$'</span> <span class="o">||</span> <span class="p">[</span> <span class="nv">$?</span> <span class="o">-</span><span class="na">eq</span> <span class="mi">1</span> <span class="p">]</span>
</pre></div>
<p>Replace the XX wildcards with the version number of course.
</p><p>2) ssh to apt1001, run puppet and check for updates (remember to replace the XX wildcards):
</p>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>root@apt1001:/srv/wikimedia# reprepro --noskipold --ignore<span class="o">=</span>forbiddenchar --component thirdparty/amd-rocmXX checkupdate buster-wikimedia
Calculating packages to get...
Updates needed <span class="k">for</span> <span class="s1">'buster-wikimedia|thirdparty/amd-rocm|amd64'</span>:
<span class="o">[</span>..<span class="o">]</span>
<span class="s1">'hsa-rocr-dev'</span>: newly installed as <span class="s1">'1.1.9-87-g1566fdd'</span> <span class="o">(</span>from <span class="s1">'amd-rocm'</span><span class="o">)</span>:
 files needed: pool/thirdparty/amd-rocm/h/hsa-rocr-dev/hsa-rocr-dev_1.1.9-87-g1566fdd_amd64.deb
<span class="o">[</span>..<span class="o">]</span>
</pre></div>
<p>3) find the new version of <code>hsa-rocr-dev</code>, since it is the only package in ROCm that requires a precise version of the <code>hsa-ext-rocr-dev</code> package (namely its version).
</p><p>3) create a control file like the following on boron:
</p>
<div class="mw-highlight mw-highlight-lang-text mw-content-ltr" dir="ltr"><pre><span></span>### Commented entries have reasonable defaults.
### Uncomment to edit them.
# Source: &lt;source package name; defaults to package name>
Section: devel
Priority: optional
# Homepage: &lt;enter URL here; no default>
Standards-Version: 3.9.2

Package: hsa-ext-rocr-dev
Version: 1.1.9-87-g1566fdd
Maintainer: Luca Toscano &lt;ltoscano@wikimedia.org>
# Pre-Depends: &lt;comma-separated list of packages>
# Depends: &lt;comma-separated list of packages>
# Recommends: &lt;comma-separated list of packages>
# Suggests: &lt;comma-separated list of packages>
# Provides: &lt;comma-separated list of packages>
# Replaces: &lt;comma-separated list of packages>
Architecture: amd64
# Multi-Arch: &lt;one of: foreign|same|allowed>
# Copyright: &lt;copyright file; defaults to GPL2>
# Changelog: &lt;changelog file; defaults to a generic changelog>
# Readme: &lt;README.Debian file; defaults to a generic one>
# Extra-Files: &lt;comma-separated list of additional files for the doc directory>
# Files: &lt;pair of space-separated paths; First is file to include, second is destination>
#  &lt;more pairs, if there's more than one file to include. Notice the starting space>
Description: dummy package to satisfy dependencies for hsa-rocr-dev
 hsa-rocr-dev-ext contains binary only and non open-source libraries
 .
</pre></div>
<p>Make sure the Version is the new one of <code>hsa-rocr-dev</code> and save.
</p><p>4) build the package with <code>equivs-build control</code>
</p><p>5) upload the package to reprepro (remember to replace the XX wildcards):
</p>
<div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>reprepro -C thirdparty/amd-rocmXX includedeb buster-wikimedia /home/elukey/hsa-ext-rocr-dev_1.1.9-87-g1566fdd_amd64.deb
</pre></div><p>6) Update the <code>thirdparty/rocmXX</code> component (remember to replace the XX wildcards):</p><div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>reprepro --noskipold --ignore<span class="o">=</span>forbiddenchar --component thirdparty/amd-rocmXX update buster-wikimedia
</pre></div><p>7) Update the versions supported by the amd_rocm module in operations/puppet.
8) On the host that you want to upgrade:</p><div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>sudo apt autoremove -y rocm-smi-lib migraphx miopengemm rocminfo hsakmt-roct rocrand hsa-rocr-dev rocm-cmake hsa-ext-rocr-dev rocm-device-libs hip_base hip_samples llvm-amdgpu comgr rocm-gdb rocm-dbgapi mivisionx
</pre></div><p>And then run puppet to install the new packages. Some quick tests to see if the GPU is properly recognized:</p><div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>elukey@stat1005:~$ /opt/rocm/bin/rocminfo
<span class="o">[</span>..<span class="o">]</span>

elukey@stat1005:~$ /opt/rocm/opencl/bin/clinfo
<span class="o">[</span>..<span class="o">]</span>

elukey@stat1005:~$ /opt/rocm/bin/hipconfig
<span class="o">[</span>..<span class="o">]</span>
</pre></div><div class="mw-highlight mw-highlight-lang-bash mw-content-ltr" dir="ltr"><pre><span></span>elukey@stat1005:~$ <span class="nb">export</span> <span class="nv">https_proxy</span><span class="o">=</span>http://webproxy:8080

elukey@stat1005:~$ virtualenv -p python3 <span class="nb">test</span>

elukey@stat1005:~$ <span class="nb">source</span> test/bin/activate

<span class="o">(</span><span class="nb">test</span><span class="o">)</span> elukey@stat1005:~$ pip3 install tensorflow-rocm

elukey@stat1008:~$ cat gpu_test.py
import tensorflow as tf
<span class="c1"># Creates a graph.</span>
with tf.device<span class="o">(</span><span class="s1">'/device:GPU:0'</span><span class="o">)</span>:
  <span class="nv">a</span> <span class="o">=</span> tf.constant<span class="o">([</span><span class="m">1</span>.0, <span class="m">2</span>.0, <span class="m">3</span>.0, <span class="m">4</span>.0, <span class="m">5</span>.0, <span class="m">6</span>.0<span class="o">]</span>, <span class="nv">shape</span><span class="o">=[</span><span class="m">2</span>, <span class="m">3</span><span class="o">]</span>, <span class="nv">name</span><span class="o">=</span><span class="s1">'a'</span><span class="o">)</span>
  <span class="nv">b</span> <span class="o">=</span> tf.constant<span class="o">([</span><span class="m">1</span>.0, <span class="m">2</span>.0, <span class="m">3</span>.0, <span class="m">4</span>.0, <span class="m">5</span>.0, <span class="m">6</span>.0<span class="o">]</span>, <span class="nv">shape</span><span class="o">=[</span><span class="m">3</span>, <span class="m">2</span><span class="o">]</span>, <span class="nv">name</span><span class="o">=</span><span class="s1">'b'</span><span class="o">)</span>
  <span class="nv">c</span> <span class="o">=</span> tf.matmul<span class="o">(</span>a, b<span class="o">)</span>
<span class="c1"># Runs the op.</span>
print<span class="o">(</span>c<span class="o">)</span>

<span class="o">(</span><span class="nb">test</span><span class="o">)</span> elukey@stat1008:~$ python gpu_test.py
tf.Tensor<span class="o">(</span>
<span class="o">[[</span><span class="m">22</span>. <span class="m">28</span>.<span class="o">]</span>
 <span class="o">[</span><span class="m">49</span>. <span class="m">64</span>.<span class="o">]]</span>, <span class="nv">shape</span><span class="o">=(</span><span class="m">2</span>, <span class="m">2</span><span class="o">)</span>, <span class="nv">dtype</span><span class="o">=</span>float32<span class="o">)</span>
</pre></div>
<!-- 
NewPP limit report
Parsed by labweb1001
Cached time: 20220421090220
Cache expiry: 1814400
Reduced expiry: false
Complications: []
DiscussionTools time usage: 0.025 seconds
CPU time usage: 0.116 seconds
Real time usage: 3.562 seconds
Preprocessor visited node count: 106/1000000
Post‐expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/100
Expensive parser function count: 0/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 15917/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key labswiki:pcache:idhash:444148-0!canonical and timestamp 20220421090216 and revision id 1932235. Serialized with JSON.
 -->
</div>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://wikitech.wikimedia.org/w/index.php?title=Analytics/Systems/Cluster/AMD_GPU&amp;oldid=1932235">https://wikitech.wikimedia.org/w/index.php?title=Analytics/Systems/Cluster/AMD_GPU&amp;oldid=1932235</a>"</div></div>
		<div id="catlinks" class="catlinks catlinks-allhidden" data-mw="interface"></div>
	</div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label" role="navigation" 
	 >
	<label id="p-personal-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Personal tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="pt-uls" class="mw-list-item active"><a class="uls-trigger" href="#"><span>English</span></a></li><li id="pt-createaccount" class="mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Analytics%2FSystems%2FCluster%2FAMD+GPU" title="You are encouraged to create an account and log in; however, it is not mandatory"><span>Create account</span></a></li><li id="pt-login" class="mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Analytics%2FSystems%2FCluster%2FAMD+GPU" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o"><span>Log in</span></a></li></ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" aria-labelledby="p-namespaces-label" role="navigation" 
	 >
	<label id="p-namespaces-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Namespaces</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-nstab-main" class="selected mw-list-item"><a href="/wiki/Analytics/Systems/Cluster/AMD_GPU" title="View the content page [c]" accesskey="c"><span>Page</span></a></li><li id="ca-talk" class="new mw-list-item"><a href="/w/index.php?title=Talk:Analytics/Systems/Cluster/AMD_GPU&amp;action=edit&amp;redlink=1" rel="discussion" title="Discussion about the content page (page does not exist) [t]" accesskey="t"><span>Discussion</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-variants-label" role="navigation" 
	 >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox" aria-labelledby="p-variants-label" />
	<label id="p-variants-label" aria-label="Change language variant" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">English</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" aria-labelledby="p-views-label" role="navigation" 
	 >
	<label id="p-views-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Views</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-view" class="selected mw-list-item"><a href="/wiki/Analytics/Systems/Cluster/AMD_GPU"><span>Read</span></a></li><li id="ca-viewsource" class="mw-list-item"><a href="/w/index.php?title=Analytics/Systems/Cluster/AMD_GPU&amp;action=edit" title="This page is protected.&#10;You can view its source [e]" accesskey="e"><span>View source</span></a></li><li id="ca-history" class="mw-list-item"><a href="/w/index.php?title=Analytics/Systems/Cluster/AMD_GPU&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-cactions-label" role="navigation"  title="More options"
	 >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox" aria-labelledby="p-cactions-label" />
	<label id="p-cactions-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">More</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<div>
			<h3 >
				<label for="searchInput">Search</label>
			</h3>
		<form action="/w/index.php" id="searchform"
			class="vector-search-box-form">
			<div id="simpleSearch"
				class="vector-search-box-inner"
				 data-search-loc="header-navigation">
				<input class="vector-search-box-input"
					 type="search" name="search" placeholder="Search Wikitech" aria-label="Search Wikitech" autocapitalize="sentences" title="Search Wikitech [f]" accesskey="f" id="searchInput"
				/>
				<input type="hidden" name="title" value="Special:Search"/>
				<input id="mw-searchButton"
					 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search the pages for this text" value="Search" />
				<input id="searchButton"
					 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go" />
			</div>
		</form>
	</div>
</div>

		</div>
	</div>
	

<div id="mw-panel">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Main_Page"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal" aria-labelledby="p-navigation-label" role="navigation" 
	 >
	<label id="p-navigation-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Navigation</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" icon="home" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" icon="recentChanges" title="A list of recent changes in the wiki [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-Server-admin-log:-Prod" class="mw-list-item"><a href="/wiki/Server_Admin_Log"><span>Server admin log: Prod</span></a></li><li id="n-Admin-log:-RelEng" class="mw-list-item"><a href="/wiki/Release_Engineering/SAL"><span>Admin log: RelEng</span></a></li><li id="n-Incident-status" class="mw-list-item"><a href="/wiki/Incident_status"><span>Incident status</span></a></li><li id="n-Deployments" class="mw-list-item"><a href="/wiki/Deployments"><span>Deployments</span></a></li><li id="n-SRE/Operations-Help" class="mw-list-item"><a href="/wiki/SRE_Team_requests"><span>SRE/Operations Help</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-Cloud_VPS_&amp;_Toolforge" class="mw-portlet mw-portlet-Cloud_VPS_Toolforge vector-menu vector-menu-portal portal" aria-labelledby="p-Cloud_VPS_&amp;_Toolforge-label" role="navigation" 
	 >
	<label id="p-Cloud_VPS_&amp;_Toolforge-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Cloud VPS &amp; Toolforge</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-Cloud-VPS-portal" class="mw-list-item"><a href="/wiki/Portal:Cloud_VPS"><span>Cloud VPS portal</span></a></li><li id="n-Toolforge-portal" class="mw-list-item"><a href="/wiki/Portal:Toolforge"><span>Toolforge portal</span></a></li><li id="n-Request-VPS-project" class="mw-list-item"><a href="https://phabricator.wikimedia.org/project/view/2875/"><span>Request VPS project</span></a></li><li id="n-Admin-log:-Cloud-VPS" class="mw-list-item"><a href="/wiki/Cloud_VPS_Server_Admin_Log"><span>Admin log: Cloud VPS</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" aria-labelledby="p-tb-label" role="navigation" 
	 >
	<label id="p-tb-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Analytics/Systems/Cluster/AMD_GPU" title="A list of all wiki pages that link here [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Analytics/Systems/Cluster/AMD_GPU" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Analytics/Systems/Cluster/AMD_GPU&amp;oldid=1932235" title="Permanent link to this revision of the page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Analytics/Systems/Cluster/AMD_GPU&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Analytics%2FSystems%2FCluster%2FAMD_GPU&amp;id=1932235&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-coll-print_export" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" aria-labelledby="p-coll-print_export-label" role="navigation" 
	 >
	<label id="p-coll-print_export-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Print/export</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="coll-create_a_book" class="mw-list-item"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Analytics%2FSystems%2FCluster%2FAMD+GPU"><span>Create a book</span></a></li><li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Analytics%2FSystems%2FCluster%2FAMD_GPU&amp;action=show-download-screen"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Analytics/Systems/Cluster/AMD_GPU&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li></ul>
		
	</div>
</nav>

	
</div>

</div>

<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 11 November 2021, at 07:03.</li>
	<li id="footer-info-copyright">Text is available under the <a href="https://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike License</a>;
additional terms may apply.
See <a href="https://foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> for details.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Main_Page" title="Main Page">About Wikitech</a></li>
	<li id="footer-places-disclaimer"><a href="https://foundation.wikimedia.org/wiki/General_disclaimer" class="extiw" title="wikimedia:General disclaimer">Disclaimers</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-mobileview"><a href="//wikitech.wikimedia.org/w/index.php?title=Analytics/Systems/Cluster/AMD_GPU&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/wikitech.wikimedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"/></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"discussiontools":{"limitreport-timeusage":"0.025"},"limitreport":{"cputime":"0.116","walltime":"3.562","ppvisitednodes":{"value":106,"limit":1000000},"postexpandincludesize":{"value":0,"limit":2097152},"templateargumentsize":{"value":0,"limit":2097152},"expansiondepth":{"value":2,"limit":100},"expensivefunctioncount":{"value":0,"limit":500},"unstrip-depth":{"value":0,"limit":20},"unstrip-size":{"value":15917,"limit":5000000},"timingprofile":["100.00%    0.000      1 -total"]},"cachereport":{"origin":"labweb1001","timestamp":"20220421090220","ttl":1814400,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":138,"wgHostname":"labweb1001"});});</script>
</body>
</html>