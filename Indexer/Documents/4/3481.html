<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 1,431 results for author: <span class="mathjax">Li, C</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Li%2C+C">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Li, C">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Li%2C+C&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Li, C">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01469">arXiv:2205.01469</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01469">pdf</a>, <a href="https://arxiv.org/format/2205.01469">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Convergence of Fictitious Play: A Decomposition Approach
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yurong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+X">Xiaotie Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenchen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mguni%2C+D">David Mguni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+X">Xiang Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yaodong Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01469v1-abstract-short" style="display: inline;">
        Fictitious play (FP) is one of the most fundamental game-theoretical learning frameworks for computing Nash equilibrium in $n$-player games, which builds the foundation for modern multi-agent learning algorithms. Although FP has provable convergence guarantees on zero-sum games and potential games, many real-world problems are often a mixture of both and the convergence property of FP has not been&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01469v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01469v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01469v1-abstract-full" style="display: none;">
        Fictitious play (FP) is one of the most fundamental game-theoretical learning frameworks for computing Nash equilibrium in $n$-player games, which builds the foundation for modern multi-agent learning algorithms. Although FP has provable convergence guarantees on zero-sum games and potential games, many real-world problems are often a mixture of both and the convergence property of FP has not been fully studied yet. In this paper, we extend the convergence results of FP to the combinations of such games and beyond. Specifically, we derive new conditions for FP to converge by leveraging game decomposition techniques. We further develop a linear relationship unifying cooperation and competition in the sense that these two classes of games are mutually transferable. Finally, we analyze a non-convergent example of FP, the Shapley game, and develop sufficient conditions for FP to converge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01469v1-abstract-full').style.display = 'none'; document.getElementById('2205.01469v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01286">arXiv:2205.01286</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01286">pdf</a>, <a href="https://arxiv.org/format/2205.01286">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When Multi-Level Meets Multi-Interest: A Multi-Grained Neural Model for Sequential Recommendation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+Y">Yu Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+J">Jianxin Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niu%2C+Y">Yannan Niu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yang Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenliang Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01286v1-abstract-short" style="display: inline;">
        Sequential recommendation aims at identifying the next item that is preferred by a user based on their behavioral history. Compared to conventional sequential models that leverage attention mechanisms and RNNs, recent efforts mainly follow two directions for improvement: multi-interest learning and graph convolutional aggregation. Specifically, multi-interest methods such as ComiRec and MIMN, focu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01286v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01286v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01286v1-abstract-full" style="display: none;">
        Sequential recommendation aims at identifying the next item that is preferred by a user based on their behavioral history. Compared to conventional sequential models that leverage attention mechanisms and RNNs, recent efforts mainly follow two directions for improvement: multi-interest learning and graph convolutional aggregation. Specifically, multi-interest methods such as ComiRec and MIMN, focus on extracting different interests for a user by performing historical item clustering, while graph convolution methods including TGSRec and SURGE elect to refine user preferences based on multi-level correlations between historical items. Unfortunately, neither of them realizes that these two types of solutions can mutually complement each other, by aggregating multi-level user preference to achieve more precise multi-interest extraction for a better recommendation. To this end, in this paper, we propose a unified multi-grained neural model(named MGNM) via a combination of multi-interest learning and graph convolutional aggregation. Concretely, MGNM first learns the graph structure and information aggregation paths of the historical items for a user. It then performs graph convolution to derive item representations in an iterative fashion, in which the complex preferences at different levels can be well captured. Afterwards, a novel sequential capsule network is proposed to inject the sequential patterns into the multi-interest extraction process, leading to a more precise interest learning in a multi-grained manner.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01286v1-abstract-full').style.display = 'none'; document.getElementById('2205.01286v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00976">arXiv:2205.00976</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00976">pdf</a>, <a href="https://arxiv.org/format/2205.00976">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3477495.3532009">10.1145/3477495.3532009 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Knowledge Graph Contrastive Learning for Recommendation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yuhao Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+C">Chao Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+L">Lianghao Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenliang Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00976v1-abstract-short" style="display: inline;">
        Knowledge Graphs (KGs) have been utilized as useful side information to improve recommendation quality. In those recommender systems, knowledge graph information often contains fruitful facts and inherent semantic relatedness among items. However, the success of such methods relies on the high quality knowledge graphs, and may not learn quality representations with two challenges: i) The long-tail&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00976v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00976v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00976v1-abstract-full" style="display: none;">
        Knowledge Graphs (KGs) have been utilized as useful side information to improve recommendation quality. In those recommender systems, knowledge graph information often contains fruitful facts and inherent semantic relatedness among items. However, the success of such methods relies on the high quality knowledge graphs, and may not learn quality representations with two challenges: i) The long-tail distribution of entities results in sparse supervision signals for KG-enhanced item representation; ii) Real-world knowledge graphs are often noisy and contain topic-irrelevant connections between items and entities. Such KG sparsity and noise make the item-entity dependent relations deviate from reflecting their true characteristics, which significantly amplifies the noise effect and hinders the accurate representation of user&#39;s preference.
  To fill this research gap, we design a general Knowledge Graph Contrastive Learning framework (KGCL) that alleviates the information noise for knowledge graph-enhanced recommender systems. Specifically, we propose a knowledge graph augmentation schema to suppress KG noise in information aggregation, and derive more robust knowledge-aware representations for items. In addition, we exploit additional supervision signals from the KG augmentation process to guide a cross-view contrastive learning paradigm, giving a greater role to unbiased user-item interactions in gradient descent and further suppressing the noise. Extensive experiments on three public datasets demonstrate the consistent superiority of our KGCL over state-of-the-art techniques. KGCL also achieves strong performance in recommendation scenarios with sparse user-item interactions, long-tail and noisy KG entities. Our implementation codes are available at https://github.com/yuh-yang/KGCL-SIGIR22
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00976v1-abstract-full').style.display = 'none'; document.getElementById('2205.00976v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been published as a full paper at SIGIR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00313">arXiv:2205.00313</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00313">pdf</a>, <a href="https://arxiv.org/format/2205.00313">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3495163">10.1145/3495163 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FairSR: Fairness-aware Sequential Recommendation through Multi-Task Learning with Preference Graph Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cheng-Te Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hsu%2C+C">Cheng Hsu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00313v1-abstract-short" style="display: inline;">
        Sequential recommendation (SR) learns from the temporal dynamics of user-item interactions to predict the next ones. Fairness-aware recommendation mitigates a variety of algorithmic biases in the learning of user preferences. This paper aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interacti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00313v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00313v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00313v1-abstract-full" style="display: none;">
        Sequential recommendation (SR) learns from the temporal dynamics of user-item interactions to predict the next ones. Fairness-aware recommendation mitigates a variety of algorithmic biases in the learning of user preferences. This paper aims at bringing a marriage between SR and algorithmic fairness. We propose a novel fairness-aware sequential recommendation task, in which a new metric, interaction fairness, is defined to estimate how recommended items are fairly interacted by users with different protected attribute groups. We propose a multi-task learning based deep end-to-end model, FairSR, which consists of two parts. One is to learn and distill personalized sequential features from the given user and her item sequence for SR. The other is fairness-aware preference graph embedding (FPGE). The aim of FPGE is two-fold: incorporating the knowledge of users&#39; and items&#39; attributes and their correlation into entity representations, and alleviating the unfair distributions of user attributes on items. Extensive experiments conducted on three datasets show FairSR can outperform state-of-the-art SR models in recommendation performance. In addition, the recommended items by FairSR also exhibit promising interaction fairness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00313v1-abstract-full').style.display = 'none'; document.getElementById('2205.00313v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACM Trans. Intell. Syst. Technol. (TIST) 2022</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          H.3.3
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00159">arXiv:2205.00159</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00159">pdf</a>, <a href="https://arxiv.org/format/2205.00159">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SVTR: Scene Text Recognition with a Single Visual Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+Y">Yongkun Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhineng Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+C">Caiyan Jia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yin%2C+X">Xiaoting Yin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+T">Tianlun Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenxia Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+Y">Yuning Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+Y">Yu-Gang Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00159v1-abstract-short" style="display: inline;">
        Dominant scene text recognition models commonly contain two building blocks, a visual model for feature extraction and a sequence model for text transcription. This hybrid architecture, although accurate, is complex and less efficient. In this study, we propose a Single Visual model for Scene Text recognition within the patch-wise image tokenization framework, which dispenses with the sequential m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00159v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00159v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00159v1-abstract-full" style="display: none;">
        Dominant scene text recognition models commonly contain two building blocks, a visual model for feature extraction and a sequence model for text transcription. This hybrid architecture, although accurate, is complex and less efficient. In this study, we propose a Single Visual model for Scene Text recognition within the patch-wise image tokenization framework, which dispenses with the sequential modeling entirely. The method, termed SVTR, firstly decomposes an image text into small patches named character components. Afterward, hierarchical stages are recurrently carried out by component-level mixing, merging and/or combining. Global and local mixing blocks are devised to perceive the inter-character and intra-character patterns, leading to a multi-grained character component perception. Thus, characters are recognized by a simple linear prediction. Experimental results on both English and Chinese scene text recognition tasks demonstrate the effectiveness of SVTR. SVTR-L (Large) achieves highly competitive accuracy in English and outperforms existing methods by a large margin in Chinese, while running faster. In addition, SVTR-T (Tiny) is an effective and much smaller model, which shows appealing speed at inference. The code is publicly available at https://github.com/PaddlePaddle/PaddleOCR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00159v1-abstract-full').style.display = 'none'; document.getElementById('2205.00159v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7pages,6figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IJCAI 2022
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13538">arXiv:2204.13538</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13538">pdf</a>, <a href="https://arxiv.org/format/2204.13538">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Combinatorics">math.CO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Asymptotically Optimal Quasi-Complementary Code Sets from Multivariate Functions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sarkar%2C+P">Palash Sarkar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunlei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Majhi%2C+S">Sudhan Majhi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zilong Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13538v1-abstract-short" style="display: inline;">
        Owing to the more significant set size properties as compared to the set of complete complementary codes (CCCs), quasi-complementary code sets (QCCSs) are more convenient to support a large number of users in multicarrier code-division multiple-access (MC-CDMA) system over CCCs. Besides set size, it is also desirable to have a low maximum aperiodic correlation magnitude and small alphabet size. Th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13538v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13538v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13538v1-abstract-full" style="display: none;">
        Owing to the more significant set size properties as compared to the set of complete complementary codes (CCCs), quasi-complementary code sets (QCCSs) are more convenient to support a large number of users in multicarrier code-division multiple-access (MC-CDMA) system over CCCs. Besides set size, it is also desirable to have a low maximum aperiodic correlation magnitude and small alphabet size. This paper aims to construct asymptotically optimal and near-optimal aperiodic QCCSs having a small alphabet size and low maximum correlation magnitude. Using multivariate functions and its associated graph, we propose a family of QCCSs consisting of multiple sets of CCCs and determine the parameters of the proposed QCCSs. Unlike the existing constructions of aperiodic QCCSs, the proposed construction can maintain a small alphabet size irrespective of the increasing sequence length and large set size.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13538v1-abstract-full').style.display = 'none'; document.getElementById('2204.13538v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12884">arXiv:2204.12884</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12884">pdf</a>, <a href="https://arxiv.org/format/2204.12884">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gleo-Det: Deep Convolution Feature-Guided Detector with Local Entropy Optimization for Salient Points
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=You%2C+Y">Yanan You</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+W">Wenli Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12884v1-abstract-short" style="display: inline;">
        Feature detection is an important procedure for image matching, where unsupervised feature detection methods are the detection approaches that have been mostly studied recently, including the ones that are based on repeatability requirement to define loss functions, and the ones that attempt to use descriptor matching to drive the optimization of the pipelines. For the former type, mean square err&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12884v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12884v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12884v1-abstract-full" style="display: none;">
        Feature detection is an important procedure for image matching, where unsupervised feature detection methods are the detection approaches that have been mostly studied recently, including the ones that are based on repeatability requirement to define loss functions, and the ones that attempt to use descriptor matching to drive the optimization of the pipelines. For the former type, mean square error (MSE) is usually used which cannot provide strong constraint for training and can make the model easy to be stuck into the collapsed solution. For the later one, due to the down sampling operation and the expansion of receptive fields, the details can be lost for local descriptors can be lost, making the constraint not fine enough. Considering the issues above, we propose to combine both ideas, which including three aspects. 1) We propose to achieve fine constraint based on the requirement of repeatability while coarse constraint with guidance of deep convolution features. 2) To address the issue that optimization with MSE is limited, entropy-based cost function is utilized, both soft cross-entropy and self-information. 3) With the guidance of convolution features, we define the cost function from both positive and negative sides. Finally, we study the effect of each modification proposed and experiments demonstrate that our method achieves competitive results over the state-of-the-art approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12884v1-abstract-full').style.display = 'none'; document.getElementById('2204.12884v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12881">arXiv:2204.12881</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12881">pdf</a>, <a href="https://arxiv.org/format/2204.12881">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LiftPool: Lifting-based Graph Pooling for Hierarchical Graph Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Mingxing Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+W">Wenrui Dai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenglin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+J">Junni Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiong%2C+H">Hongkai Xiong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12881v1-abstract-short" style="display: inline;">
        Graph pooling has been increasingly considered for graph neural networks (GNNs) to facilitate hierarchical graph representation learning. Existing graph pooling methods commonly consist of two stages, i.e., selecting the top-ranked nodes and removing the rest nodes to construct a coarsened graph representation. However, local structural information of the removed nodes would be inevitably dropped&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12881v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12881v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12881v1-abstract-full" style="display: none;">
        Graph pooling has been increasingly considered for graph neural networks (GNNs) to facilitate hierarchical graph representation learning. Existing graph pooling methods commonly consist of two stages, i.e., selecting the top-ranked nodes and removing the rest nodes to construct a coarsened graph representation. However, local structural information of the removed nodes would be inevitably dropped in these methods, due to the inherent coupling of nodes (location) and their features (signals). In this paper, we propose an enhanced three-stage method via lifting, named LiftPool, to improve hierarchical graph representation by maximally preserving the local structural information in graph pooling. LiftPool introduces an additional stage of graph lifting before graph coarsening to preserve the local information of the removed nodes and decouple the processes of node removing and feature reduction. Specifically, for each node to be removed, its local information is obtained by subtracting the global information aggregated from its neighboring preserved nodes. Subsequently, this local information is aligned and propagated to the preserved nodes to alleviate information loss in graph coarsening. Furthermore, we demonstrate that the proposed LiftPool is localized and permutation-invariant. The proposed graph lifting structure is general to be integrated with existing downsampling-based graph pooling methods. Evaluations on benchmark graph datasets show that LiftPool substantially outperforms the state-of-the-art graph pooling methods in the task of graph classification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12881v1-abstract-full').style.display = 'none'; document.getElementById('2204.12881v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12496">arXiv:2204.12496</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12496">pdf</a>, <a href="https://arxiv.org/format/2204.12496">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Information Bottleneck for Deep Multi-View Subspace Clustering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+S">Shiye Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Changsheng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yanming Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+Y">Ye Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Guoren Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12496v2-abstract-short" style="display: inline;">
        In this paper, we explore the problem of deep multi-view subspace clustering framework from an information-theoretic point of view. We extend the traditional information bottleneck principle to learn common information among different views in a self-supervised manner, and accordingly establish a new framework called Self-supervised Information Bottleneck based Multi-view Subspace Clustering (SIB-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12496v2-abstract-full').style.display = 'inline'; document.getElementById('2204.12496v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12496v2-abstract-full" style="display: none;">
        In this paper, we explore the problem of deep multi-view subspace clustering framework from an information-theoretic point of view. We extend the traditional information bottleneck principle to learn common information among different views in a self-supervised manner, and accordingly establish a new framework called Self-supervised Information Bottleneck based Multi-view Subspace Clustering (SIB-MSC). Inheriting the advantages from information bottleneck, SIB-MSC can learn a latent space for each view to capture common information among the latent representations of different views by removing superfluous information from the view itself while retaining sufficient information for the latent representations of other views. Actually, the latent representation of each view provides a kind of self-supervised signal for training the latent representations of other views. Moreover, SIB-MSC attempts to learn the other latent space for each view to capture the view-specific information by introducing mutual information based regularization terms, so as to further improve the performance of multi-view subspace clustering. To the best of our knowledge, this is the first work to explore information bottleneck for multi-view subspace clustering. Extensive experiments on real-world multi-view data demonstrate that our method achieves superior performance over the related state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12496v2-abstract-full').style.display = 'none'; document.getElementById('2204.12496v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.11640">arXiv:2204.11640</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.11640">pdf</a>, <a href="https://arxiv.org/format/2204.11640">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hybrid ISTA: Unfolding ISTA With Convergence Guarantees Using Free-Form Deep Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Z">Ziyang Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+W">Wenrui Dai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+D">Duoduo Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenglin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+J">Junni Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiong%2C+H">Hongkai Xiong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.11640v1-abstract-short" style="display: inline;">
        It is promising to solve linear inverse problems by unfolding iterative algorithms (e.g., iterative shrinkage thresholding algorithm (ISTA)) as deep neural networks (DNNs) with learnable parameters. However, existing ISTA-based unfolded algorithms restrict the network architectures for iterative updates with the partial weight coupling structure to guarantee convergence. In this paper, we propose&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11640v1-abstract-full').style.display = 'inline'; document.getElementById('2204.11640v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.11640v1-abstract-full" style="display: none;">
        It is promising to solve linear inverse problems by unfolding iterative algorithms (e.g., iterative shrinkage thresholding algorithm (ISTA)) as deep neural networks (DNNs) with learnable parameters. However, existing ISTA-based unfolded algorithms restrict the network architectures for iterative updates with the partial weight coupling structure to guarantee convergence. In this paper, we propose hybrid ISTA to unfold ISTA with both pre-computed and learned parameters by incorporating free-form DNNs (i.e., DNNs with arbitrary feasible and reasonable network architectures), while ensuring theoretical convergence. We first develop HCISTA to improve the efficiency and flexibility of classical ISTA (with pre-computed parameters) without compromising the convergence rate in theory. Furthermore, the DNN-based hybrid algorithm is generalized to popular variants of learned ISTA, dubbed HLISTA, to enable a free architecture of learned parameters with a guarantee of linear convergence. To our best knowledge, this paper is the first to provide a convergence-provable framework that enables free-form DNNs in ISTA-based unfolded algorithms. This framework is general to endow arbitrary DNNs for solving linear inverse problems with convergence guarantees. Extensive experiments demonstrate that hybrid ISTA can reduce the reconstruction error with an improved convergence rate in the tasks of sparse recovery and compressive sensing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.11640v1-abstract-full').style.display = 'none'; document.getElementById('2204.11640v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This is a draft paper. The final version has been accepted by IEEE/ACM Transactions on Pattern Analysis and Machine Intelligence (TPAMI), and will be published in the future</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.10994">arXiv:2204.10994</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.10994">pdf</a>, <a href="https://arxiv.org/format/2204.10994">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yue Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhenghua Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bao%2C+Z">Zuyi Bao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jiacheng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+B">Bo Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+F">Fei Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Min Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.10994v3-abstract-short" style="display: inline;">
        This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10994v3-abstract-full').style.display = 'inline'; document.getElementById('2204.10994v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.10994v3-abstract-full" style="display: none;">
        This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence model and the sequence-to-edit model, both enhanced with large pretrained language models, achieving competitive benchmark performance on previous and our datasets. We also discuss CGEC evaluation methodologies, including the effect of multiple references and using a char-based metric. Our annotation guidelines, data, and code are available at \url{https://github.com/HillZhang1999/MuCGEC}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10994v3-abstract-full').style.display = 'none'; document.getElementById('2204.10994v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NAACL2022 (main conference)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.09222">arXiv:2204.09222</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.09222">pdf</a>, <a href="https://arxiv.org/format/2204.09222">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        K-LITE: Learning Transferable Visual Models with External Knowledge
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+S">Sheng Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunyuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+X">Xiaowei Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+Y">Yujia Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianwei Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+P">Pengchuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rohrbach%2C+A">Anna Rohrbach</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gan%2C+Z">Zhe Gan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+L">Lijuan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+L">Lu Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Ce Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Keutzer%2C+K">Kurt Keutzer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Darrell%2C+T">Trevor Darrell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+J">Jianfeng Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.09222v1-abstract-short" style="display: inline;">
        Recent state-of-the-art computer vision systems are trained from natural language supervision, ranging from simple object category names to descriptive captions. This free form of supervision ensures high generality and usability of the learned visual models, based on extensive heuristics on data collection to cover as many visual concepts as possible. Alternatively, learning with external knowled&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09222v1-abstract-full').style.display = 'inline'; document.getElementById('2204.09222v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.09222v1-abstract-full" style="display: none;">
        Recent state-of-the-art computer vision systems are trained from natural language supervision, ranging from simple object category names to descriptive captions. This free form of supervision ensures high generality and usability of the learned visual models, based on extensive heuristics on data collection to cover as many visual concepts as possible. Alternatively, learning with external knowledge about images is a promising way which leverages a much more structured source of supervision. In this paper, we propose K-LITE (Knowledge-augmented Language-Image Training and Evaluation), a simple strategy to leverage external knowledge to build transferable visual systems: In training, it enriches entities in natural language with WordNet and Wiktionary knowledge, leading to an efficient and scalable approach to learning image representations that can understand both visual concepts and their knowledge; In evaluation, the natural language is also augmented with external knowledge and then used to reference learned visual concepts (or describe new ones) to enable zero-shot and few-shot transfer of the pre-trained models. We study the performance of K-LITE on two important computer vision problems, image classification and object detection, benchmarking on 20 and 13 different existing datasets, respectively. The proposed knowledge-augmented models show significant improvement in transfer learning performance over existing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.09222v1-abstract-full').style.display = 'none'; document.getElementById('2204.09222v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint. The first three authors contribute equally</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08917">arXiv:2204.08917</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08917">pdf</a>, <a href="https://arxiv.org/format/2204.08917">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Global-and-Local Collaborative Learning for Co-Salient Object Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cong%2C+R">Runmin Cong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+N">Ning Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chongyi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+H">Huazhu Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yao Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Q">Qingming Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kwong%2C+S">Sam Kwong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08917v1-abstract-short" style="display: inline;">
        The goal of co-salient object detection (CoSOD) is to discover salient objects that commonly appear in a query group containing two or more relevant images. Therefore, how to effectively extract inter-image correspondence is crucial for the CoSOD task. In this paper, we propose a global-and-local collaborative learning architecture, which includes a global correspondence modeling (GCM) and a local&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08917v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08917v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08917v1-abstract-full" style="display: none;">
        The goal of co-salient object detection (CoSOD) is to discover salient objects that commonly appear in a query group containing two or more relevant images. Therefore, how to effectively extract inter-image correspondence is crucial for the CoSOD task. In this paper, we propose a global-and-local collaborative learning architecture, which includes a global correspondence modeling (GCM) and a local correspondence modeling (LCM) to capture comprehensive inter-image corresponding relationship among different images from the global and local perspectives. Firstly, we treat different images as different time slices and use 3D convolution to integrate all intra features intuitively, which can more fully extract the global group semantics. Secondly, we design a pairwise correlation transformation (PCT) to explore similarity correspondence between pairwise images and combine the multiple local pairwise correspondences to generate the local inter-image relationship. Thirdly, the inter-image relationships of the GCM and LCM are integrated through a global-and-local correspondence aggregation (GLA) module to explore more comprehensive inter-image collaboration cues. Finally, the intra- and inter-features are adaptively integrated by an intra-and-inter weighting fusion (AEWF) module to learn co-saliency features and predict the co-saliency map. The proposed GLNet is evaluated on three prevailing CoSOD benchmark datasets, demonstrating that our model trained on a small dataset (about 3k images) still outperforms eleven state-of-the-art competitors trained on some large datasets (about 8k-200k images).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08917v1-abstract-full').style.display = 'none'; document.getElementById('2204.08917v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE Transactions on Cybernetics 2022, project page: https://rmcong.github.io/proj_GLNet.html</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08790">arXiv:2204.08790</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08790">pdf</a>, <a href="https://arxiv.org/format/2204.08790">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunyuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+H">Haotian Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L+H">Liunian Harold Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+P">Pengchuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aneja%2C+J">Jyoti Aneja</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianwei Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+P">Ping Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+Y+J">Yong Jae Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+H">Houdong Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zicheng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+J">Jianfeng Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08790v2-abstract-short" style="display: inline;">
        Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate strong transferability to a variety of datasets/tasks. However, it remains a challenge to evaluate the transferablity of these foundation models due to the lack of easy-to-use toolkits for fair benchmarkin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08790v2-abstract-full').style.display = 'inline'; document.getElementById('2204.08790v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08790v2-abstract-full" style="display: none;">
        Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate strong transferability to a variety of datasets/tasks. However, it remains a challenge to evaluate the transferablity of these foundation models due to the lack of easy-to-use toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation of Language-augmented Visual Task-level Transfer), the first benchmark to compare and evaluate pre-trained language-augmented visual models. Several highlights include: (i) Datasets. As downstream evaluation suites, it consists of 20 image classification datasets and 35 object detection datasets, each of which is augmented with external knowledge. (ii) Toolkit. An automatic hyper-parameter tuning toolkit is developed to ensure the fairness in model adaption. To leverage the full power of language-augmented visual models, novel language-aware initialization methods are proposed to significantly improve the adaption performance. (iii) Metrics. A variety of evaluation metrics are used, including sample-efficiency (zero-shot and few-shot) and parameter-efficiency (linear probing and full model fine-tuning). We will release our toolkit and evaluation platforms for the research community.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08790v2-abstract-full').style.display = 'none'; document.getElementById('2204.08790v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint. The first two authors contribute equally</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08760">arXiv:2204.08760</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08760">pdf</a>, <a href="https://arxiv.org/ps/2204.08760">ps</a>, <a href="https://arxiv.org/format/2204.08760">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low c-differentially uniform functions via an extension of Dillon&#39;s switching method
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunlei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Riera%2C+C">Constanza Riera</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stanica%2C+P">Pantelimon Stanica</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08760v1-abstract-short" style="display: inline;">
        In this paper we generalize Dillon&#39;s switching method to characterize the exact $c$-differential uniformity of functions constructed via this method. More precisely, we modify some PcN/APcN and other functions with known $c$-differential uniformity in a controllable number of coordinates to render more such functions. We present several applications of the method in constructing PcN and APcN funct&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08760v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08760v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08760v1-abstract-full" style="display: none;">
        In this paper we generalize Dillon&#39;s switching method to characterize the exact $c$-differential uniformity of functions constructed via this method. More precisely, we modify some PcN/APcN and other functions with known $c$-differential uniformity in a controllable number of coordinates to render more such functions. We present several applications of the method in constructing PcN and APcN functions with respect to all $c\neq 1$. As a byproduct, we generalize some result of [Y. Wu, N. Li, X. Zeng, {\em New PcN and APcN functions over finite fields}, Designs Codes Crypt. 89 (2021), 2637--2651]. Computational results rendering functions with low differential uniformity, as well as, other good cryptographic properties are sprinkled throughout the paper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08760v1-abstract-full').style.display = 'none'; document.getElementById('2204.08760v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">21 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08477">arXiv:2204.08477</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08477">pdf</a>, <a href="https://arxiv.org/format/2204.08477">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self Supervised Lesion Recognition For Breast Ultrasound Diagnosis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+Y">Yuanfan Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+C">Canqian Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+T">Tiancheng Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunxiao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+R">Rui Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+Y">Yi Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08477v1-abstract-short" style="display: inline;">
        Previous deep learning based Computer Aided Diagnosis (CAD) system treats multiple views of the same lesion as independent images. Since an ultrasound image only describes a partial 2D projection of a 3D lesion, such paradigm ignores the semantic relationship between different views of a lesion, which is inconsistent with the traditional diagnosis where sonographers analyze a lesion from at least&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08477v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08477v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08477v1-abstract-full" style="display: none;">
        Previous deep learning based Computer Aided Diagnosis (CAD) system treats multiple views of the same lesion as independent images. Since an ultrasound image only describes a partial 2D projection of a 3D lesion, such paradigm ignores the semantic relationship between different views of a lesion, which is inconsistent with the traditional diagnosis where sonographers analyze a lesion from at least two views. In this paper, we propose a multi-task framework that complements Benign/Malignant classification task with lesion recognition (LR) which helps leveraging relationship among multiple views of a single lesion to learn a complete representation of the lesion. To be specific, LR task employs contrastive learning to encourage representation that pulls multiple views of the same lesion and repels those of different lesions. The task therefore facilitates a representation that is not only invariant to the view change of the lesion, but also capturing fine-grained features to distinguish between different lesions. Experiments show that the proposed multi-task framework boosts the performance of Benign/Malignant classification as two sub-tasks complement each other and enhance the learned representation of ultrasound images.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08477v1-abstract-full').style.display = 'none'; document.getElementById('2204.08477v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4pages. Accepted by ISBI2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08382">arXiv:2204.08382</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08382">pdf</a>, <a href="https://arxiv.org/ps/2204.08382">ps</a>, <a href="https://arxiv.org/format/2204.08382">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Subspace Nonnegative Matrix Factorization for Feature Representation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Junhang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+J">Jiao Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tong%2C+C">Can Tong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+T">Tingting Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yuchen Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qi%2C+S">Shouliang Qi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+Y">Yudong Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Teng%2C+Y">Yueyang Teng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08382v1-abstract-short" style="display: inline;">
        Traditional nonnegative matrix factorization (NMF) learns a new feature representation on the whole data space, which means treating all features equally. However, a subspace is often sufficient for accurate representation in practical applications, and redundant features can be invalid or even harmful. For example, if a camera has some sensors destroyed, then the corresponding pixels in the photo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08382v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08382v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08382v1-abstract-full" style="display: none;">
        Traditional nonnegative matrix factorization (NMF) learns a new feature representation on the whole data space, which means treating all features equally. However, a subspace is often sufficient for accurate representation in practical applications, and redundant features can be invalid or even harmful. For example, if a camera has some sensors destroyed, then the corresponding pixels in the photos from this camera are not helpful to identify the content, which means only the subspace consisting of remaining pixels is worthy of attention. This paper proposes a new NMF method by introducing adaptive weights to identify key features in the original space so that only a subspace involves generating the new representation. Two strategies are proposed to achieve this: the fuzzier weighted technique and entropy regularized weighted technique, both of which result in an iterative solution with a simple form. Experimental results on several real-world datasets demonstrated that the proposed methods can generate a more accurate feature representation than existing methods. The code developed in this study is available at https://github.com/WNMF1/FWNMF-ERWNMF.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08382v1-abstract-full').style.display = 'none'; document.getElementById('2204.08382v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08311">arXiv:2204.08311</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08311">pdf</a>, <a href="https://arxiv.org/format/2204.08311">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Application of Transfer Learning and Ensemble Learning in Image-level Classification for Breast Histopathology
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Y">Yuchao Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+X">Xiaomin Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Haoyuan Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Hao Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yixin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Haiqing Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xiaoyan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+H">Hongzan Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+X">Xinyu Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grzegorzek%2C+M">Marcin Grzegorzek</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08311v1-abstract-short" style="display: inline;">
        Background: Breast cancer has the highest prevalence in women globally. The classification and diagnosis of breast cancer and its histopathological images have always been a hot spot of clinical concern. In Computer-Aided Diagnosis (CAD), traditional classification models mostly use a single network to extract features, which has significant limitations. On the other hand, many networks are traine&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08311v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08311v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08311v1-abstract-full" style="display: none;">
        Background: Breast cancer has the highest prevalence in women globally. The classification and diagnosis of breast cancer and its histopathological images have always been a hot spot of clinical concern. In Computer-Aided Diagnosis (CAD), traditional classification models mostly use a single network to extract features, which has significant limitations. On the other hand, many networks are trained and optimized on patient-level datasets, ignoring the application of lower-level data labels.
  Method: This paper proposes a deep ensemble model based on image-level labels for the binary classification of benign and malignant lesions of breast histopathological images. First, the BreakHis dataset is randomly divided into a training, validation and test set. Then, data augmentation techniques are used to balance the number of benign and malignant samples. Thirdly, considering the performance of transfer learning and the complementarity between each network, VGG-16, Xception, Resnet-50, DenseNet-201 are selected as the base classifiers.
  Result: In the ensemble network model with accuracy as the weight, the image-level binary classification achieves an accuracy of $98.90\%$. In order to verify the capabilities of our method, the latest Transformer and Multilayer Perception (MLP) models have been experimentally compared on the same dataset. Our model wins with a $5\%-20\%$ advantage, emphasizing the ensemble model&#39;s far-reaching significance in classification tasks.
  Conclusion: This research focuses on improving the model&#39;s classification performance with an ensemble algorithm. Transfer learning plays an essential role in small datasets, improving training speed and accuracy. Our model has outperformed many existing approaches in accuracy, providing a method for the field of auxiliary medical diagnosis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08311v1-abstract-full').style.display = 'none'; document.getElementById('2204.08311v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08166">arXiv:2204.08166</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08166">pdf</a>, <a href="https://arxiv.org/format/2204.08166">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TOD-CNN: An Effective Convolutional Neural Network for Tiny Object Detection in Sperm Videos
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+S">Shuojia Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+H">Hongzan Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+P">Peng Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jiawei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+P">Pingli Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+Y">Yudong Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+X">Xinyu Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grzegorzek%2C+M">Marcin Grzegorzek</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08166v1-abstract-short" style="display: inline;">
        The detection of tiny objects in microscopic videos is a problematic point, especially in large-scale experiments. For tiny objects (such as sperms) in microscopic videos, current detection methods face challenges in fuzzy, irregular, and precise positioning of objects. In contrast, we present a convolutional neural network for tiny object detection (TOD-CNN) with an underlying data set of high-qu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08166v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08166v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08166v1-abstract-full" style="display: none;">
        The detection of tiny objects in microscopic videos is a problematic point, especially in large-scale experiments. For tiny objects (such as sperms) in microscopic videos, current detection methods face challenges in fuzzy, irregular, and precise positioning of objects. In contrast, we present a convolutional neural network for tiny object detection (TOD-CNN) with an underlying data set of high-quality sperm microscopic videos (111 videos, $&gt;$ 278,000 annotated objects), and a graphical user interface (GUI) is designed to employ and test the proposed model effectively. TOD-CNN is highly accurate, achieving $85.60\%$ AP$_{50}$ in the task of real-time sperm detection in microscopic videos. To demonstrate the importance of sperm detection technology in sperm quality analysis, we carry out relevant sperm quality evaluation metrics and compare them with the diagnosis results from medical doctors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08166v1-abstract-full').style.display = 'none'; document.getElementById('2204.08166v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 12 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08127">arXiv:2204.08127</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08127">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Parallel Network with Channel Attention and Post-Processing for Carotid Arteries Vulnerable Plaque Segmentation in Ultrasound Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+Y">Yanchao Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cancheng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+L">Lu Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+K">Ke Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hua%2C+Y">Yang Hua</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jicong Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08127v1-abstract-short" style="display: inline;">
        Carotid arteries vulnerable plaques are a crucial factor in the screening of atherosclerosis by ultrasound technique. However, the plaques are contaminated by various noises such as artifact, speckle noise, and manual segmentation may be time-consuming. This paper proposes an automatic convolutional neural network (CNN) method for plaque segmentation in carotid ultrasound images using a small data&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08127v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08127v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08127v1-abstract-full" style="display: none;">
        Carotid arteries vulnerable plaques are a crucial factor in the screening of atherosclerosis by ultrasound technique. However, the plaques are contaminated by various noises such as artifact, speckle noise, and manual segmentation may be time-consuming. This paper proposes an automatic convolutional neural network (CNN) method for plaque segmentation in carotid ultrasound images using a small dataset. First, a parallel network with three independent scale decoders is utilized as our base segmentation network, pyramid dilation convolutions are used to enlarge receptive fields in the three segmentation sub-networks. Subsequently, the three decoders are merged to be rectified in channels by SENet. Thirdly, in test stage, the initially segmented plaque is refined by the max contour morphology post-processing to obtain the final plaque. Moreover, three loss function Dice loss, SSIM loss and cross-entropy loss are compared to segment plaques. Test results show that the proposed method with dice loss function yields a Dice value of 0.820, an IoU of 0.701, Acc of 0.969, and modified Hausdorff distance (MHD) of 1.43 for 30 vulnerable cases of plaques, it outperforms some of the conventional CNN-based methods on these metrics. Additionally, we apply an ablation experiment to show the validity of each proposed module. Our study provides some reference for similar researches and may be useful in actual applications for plaque segmentation of ultrasound carotid arteries.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08127v1-abstract-full').style.display = 'none'; document.getElementById('2204.08127v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages,6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08118">arXiv:2204.08118</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08118">pdf</a>, <a href="https://arxiv.org/ps/2204.08118">ps</a>, <a href="https://arxiv.org/format/2204.08118">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Differential Properties of the Power Mapping $x^{p^m+2}$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Man%2C+Y">Yuying Man</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+Y">Yongbo Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunlei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Helleseth%2C+T">Tor Helleseth</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08118v1-abstract-short" style="display: inline;">
        Let $m$ be a positive integer and $p$ a prime. In this paper, we investigate the differential properties of the power mapping $x^{p^m+2}$ over $\mathbb{F}_{p^n}$, where $n=2m$ or $n=2m-1$. For the case $n=2m$, by transforming the derivative equation of $x^{p^m+2}$ and studying some related equations, we completely determine the differential spectrum of this power mapping. For the case $n=2m-1$, th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08118v1-abstract-full').style.display = 'inline'; document.getElementById('2204.08118v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08118v1-abstract-full" style="display: none;">
        Let $m$ be a positive integer and $p$ a prime. In this paper, we investigate the differential properties of the power mapping $x^{p^m+2}$ over $\mathbb{F}_{p^n}$, where $n=2m$ or $n=2m-1$. For the case $n=2m$, by transforming the derivative equation of $x^{p^m+2}$ and studying some related equations, we completely determine the differential spectrum of this power mapping. For the case $n=2m-1$, the derivative equation can be transformed to a polynomial of degree $p+3$. The problem is more difficult and we obtain partial results about the differential spectrum of $x^{p^m+2}$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08118v1-abstract-full').style.display = 'none'; document.getElementById('2204.08118v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07963">arXiv:2204.07963</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07963">pdf</a>, <a href="https://arxiv.org/format/2204.07963">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AFSC: Adaptive Fourier Space Compression for Anomaly Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Haote Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yunlong Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+L">Liyan Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenxin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yue Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ding%2C+X">Xinghao Ding</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07963v1-abstract-short" style="display: inline;">
        Anomaly Detection (AD) on medical images enables a model to recognize any type of anomaly pattern without lesion-specific supervised learning. Data augmentation based methods construct pseudo-healthy images by &#34;pasting&#34; fake lesions on real healthy ones, and a network is trained to predict healthy images in a supervised manner. The lesion can be found by difference between the unhealthy input and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07963v1-abstract-full').style.display = 'inline'; document.getElementById('2204.07963v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07963v1-abstract-full" style="display: none;">
        Anomaly Detection (AD) on medical images enables a model to recognize any type of anomaly pattern without lesion-specific supervised learning. Data augmentation based methods construct pseudo-healthy images by &#34;pasting&#34; fake lesions on real healthy ones, and a network is trained to predict healthy images in a supervised manner. The lesion can be found by difference between the unhealthy input and pseudo-healthy output. However, using only manually designed fake lesions fail to approximate to irregular real lesions, hence limiting the model generalization. We assume by exploring the intrinsic data property within images, we can distinguish previously unseen lesions from healthy regions in an unhealthy image. In this study, we propose an Adaptive Fourier Space Compression (AFSC) module to distill healthy feature for AD. The compression of both magnitude and phase in frequency domain addresses the hyper intensity and diverse position of lesions. Experimental results on the BraTS and MS-SEG datasets demonstrate an AFSC baseline is able to produce promising detection results, and an AFSC module can be effectively embedded into existing AD methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07963v1-abstract-full').style.display = 'none'; document.getElementById('2204.07963v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07429">arXiv:2204.07429</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07429">pdf</a>, <a href="https://arxiv.org/format/2204.07429">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Experimentally realized memristive memory augmented neural network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mao%2C+R">Ruibin Mao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+B">Bo Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yahui Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kazemi%2C+A">Arman Kazemi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Laguna%2C+A+F">Ann Franchesca Laguna</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Neimier%2C+M">Michael Neimier</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+X+S">X. Sharon Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sheng%2C+X">Xia Sheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Graves%2C+C+E">Catherine E. Graves</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Strachan%2C+J+P">John Paul Strachan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Can Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07429v1-abstract-short" style="display: inline;">
        Lifelong on-device learning is a key challenge for machine intelligence, and this requires learning from few, often single, samples. Memory augmented neural network has been proposed to achieve the goal, but the memory module has to be stored in an off-chip memory due to its size. Therefore the practical use has been heavily limited. Previous works on emerging memory-based implementation have diff&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07429v1-abstract-full').style.display = 'inline'; document.getElementById('2204.07429v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07429v1-abstract-full" style="display: none;">
        Lifelong on-device learning is a key challenge for machine intelligence, and this requires learning from few, often single, samples. Memory augmented neural network has been proposed to achieve the goal, but the memory module has to be stored in an off-chip memory due to its size. Therefore the practical use has been heavily limited. Previous works on emerging memory-based implementation have difficulties in scaling up because different modules with various structures are difficult to integrate on the same chip and the small sense margin of the content addressable memory for the memory module heavily limited the degree of mismatch calculation. In this work, we implement the entire memory augmented neural network architecture in a fully integrated memristive crossbar platform and achieve an accuracy that closely matches standard software on digital hardware for the Omniglot dataset. The successful demonstration is supported by implementing new functions in crossbars in addition to widely reported matrix multiplications. For example, the locality-sensitive hashing operation is implemented in crossbar arrays by exploiting the intrinsic stochasticity of memristor devices. Besides, the content-addressable memory module is realized in crossbars, which also supports the degree of mismatches. Simulations based on experimentally validated models show such an implementation can be efficiently scaled up for one-shot learning on the Mini-ImageNet dataset. The successful demonstration paves the way for practical on-device lifelong learning and opens possibilities for novel attention-based algorithms not possible in conventional hardware.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07429v1-abstract-full').style.display = 'none'; document.getElementById('2204.07429v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">54 pages, 21 figures, 3 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07321">arXiv:2204.07321</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07321">pdf</a>, <a href="https://arxiv.org/format/2204.07321">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Graph Pooling for Graph Neural Networks: Progress, Challenges, and Opportunities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Chuang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhan%2C+Y">Yibing Zhan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+B">Bo Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jia Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+W">Wenbin Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tao%2C+D">Dacheng Tao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07321v1-abstract-short" style="display: inline;">
        Graph neural networks have emerged as a leading architecture for many graph-level tasks such as graph classification and graph generation with a notable improvement. Among these tasks, graph pooling is an essential component of graph neural network architectures for obtaining a holistic graph-level representation of the entire graph. Although a great variety of methods have been proposed in this p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07321v1-abstract-full').style.display = 'inline'; document.getElementById('2204.07321v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07321v1-abstract-full" style="display: none;">
        Graph neural networks have emerged as a leading architecture for many graph-level tasks such as graph classification and graph generation with a notable improvement. Among these tasks, graph pooling is an essential component of graph neural network architectures for obtaining a holistic graph-level representation of the entire graph. Although a great variety of methods have been proposed in this promising and fast-developing research field, to the best of our knowledge, little effort has been made to systematically summarize these methods. To set the stage for the development of future works, in this paper, we attempt to fill this gap by providing a broad review of recent methods on graph pooling. Specifically, 1) we first propose a taxonomy of existing graph pooling methods and provide a mathematical summary for each category; 2) next, we provide an overview of the libraries related to graph pooling, including the commonly used datasets, model architectures for downstream tasks, and open-source implementations; 3) then, we further outline in brief the applications that incorporate the idea of graph pooling in a number of domains; 4) and finally, we discuss some critical challenges faced by the current studies and share our insights on potential directions for improving graph pooling in the future.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07321v1-abstract-full').style.display = 'none'; document.getElementById('2204.07321v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07114">arXiv:2204.07114</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07114">pdf</a>, <a href="https://arxiv.org/format/2204.07114">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Look Back and Forth: Video Super-Resolution with Explicit Temporal Difference Modeling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Isobe%2C+T">Takashi Isobe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+X">Xu Jia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tao%2C+X">Xin Tao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Changlin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+R">Ruihuang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+Y">Yongjie Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mu%2C+J">Jing Mu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+H">Huchuan Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tai%2C+Y">Yu-Wing Tai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07114v1-abstract-short" style="display: inline;">
        Temporal modeling is crucial for video super-resolution. Most of the video super-resolution methods adopt the optical flow or deformable convolution for explicitly motion compensation. However, such temporal modeling techniques increase the model complexity and might fail in case of occlusion or complex motion, resulting in serious distortion and artifacts. In this paper, we propose to explore the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07114v1-abstract-full').style.display = 'inline'; document.getElementById('2204.07114v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07114v1-abstract-full" style="display: none;">
        Temporal modeling is crucial for video super-resolution. Most of the video super-resolution methods adopt the optical flow or deformable convolution for explicitly motion compensation. However, such temporal modeling techniques increase the model complexity and might fail in case of occlusion or complex motion, resulting in serious distortion and artifacts. In this paper, we propose to explore the role of explicit temporal difference modeling in both LR and HR space. Instead of directly feeding consecutive frames into a VSR model, we propose to compute the temporal difference between frames and divide those pixels into two subsets according to the level of difference. They are separately processed with two branches of different receptive fields in order to better extract complementary information. To further enhance the super-resolution result, not only spatial residual features are extracted, but the difference between consecutive frames in high-frequency domain is also computed. It allows the model to exploit intermediate SR results in both future and past to refine the current SR output. The difference at different time steps could be cached such that information from further distance in time could be propagated to the current frame for refinement. Experiments on several video super-resolution benchmark datasets demonstrate the effectiveness of the proposed method and its favorable performance against state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07114v1-abstract-full').style.display = 'none'; document.getElementById('2204.07114v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06455">arXiv:2204.06455</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06455">pdf</a>, <a href="https://arxiv.org/format/2204.06455">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WSSS4LUAD: Grand Challenge on Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+C">Chu Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+X">Xipeng Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+L">Lixu Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+H">Huan Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Bingbing Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+S">Su Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+S">Shanshan Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+Z">Zhenwei Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mai%2C+J">Jinhai Mai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+J">Jiatai Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+B">Bingchao Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+Z">Zeyan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhizhen Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yumeng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Huihui Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+C">Chao Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+C">Chunhui Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mao%2C+L">Lijian Mao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+M">Min Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Duan%2C+L">Luwen Duan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+J">Jingsong Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+D">Dong Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fang%2C+Z">Zijie Fang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yang Chen</a>
      , et al. (18 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06455v2-abstract-short" style="display: inline;">
        Lung cancer is the leading cause of cancer death worldwide, and adenocarcinoma (LUAD) is the most common subtype. Exploiting the potential value of the histopathology images can promote precision medicine in oncology. Tissue segmentation is the basic upstream task of histopathology image analysis. Existing deep learning models have achieved superior segmentation performance but require sufficient&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06455v2-abstract-full').style.display = 'inline'; document.getElementById('2204.06455v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06455v2-abstract-full" style="display: none;">
        Lung cancer is the leading cause of cancer death worldwide, and adenocarcinoma (LUAD) is the most common subtype. Exploiting the potential value of the histopathology images can promote precision medicine in oncology. Tissue segmentation is the basic upstream task of histopathology image analysis. Existing deep learning models have achieved superior segmentation performance but require sufficient pixel-level annotations, which is time-consuming and expensive. To enrich the label resources of LUAD and to alleviate the annotation efforts, we organize this challenge WSSS4LUAD to call for the outstanding weakly-supervised semantic segmentation (WSSS) techniques for histopathology images of LUAD. Participants have to design the algorithm to segment tumor epithelial, tumor-associated stroma and normal tissue with only patch-level labels. This challenge includes 10,091 patch-level annotations (the training set) and over 130 million labeled pixels (the validation and test sets), from 87 WSIs (67 from GDPH, 20 from TCGA). All the labels were generated by a pathologist-in-the-loop pipeline with the help of AI models and checked by the label review board. Among 532 registrations, 28 teams submitted the results in the test phase with over 1,000 submissions. Finally, the first place team achieved mIoU of 0.8413 (tumor: 0.8389, stroma: 0.7931, normal: 0.8919). According to the technical reports of the top-tier teams, CAM is still the most popular approach in WSSS. Cutmix data augmentation has been widely adopted to generate more reliable samples. With the success of this challenge, we believe that WSSS approaches with patch-level annotations can be a complement to the traditional pixel annotations while reducing the annotation efforts. The entire dataset has been released to encourage more researches on computational pathology in LUAD and more novel WSSS techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06455v2-abstract-full').style.display = 'none'; document.getElementById('2204.06455v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.05941">arXiv:2204.05941</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.05941">pdf</a>, <a href="https://arxiv.org/format/2204.05941">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+M">Minbin Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Z">Zhijian Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Changlin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Hang Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Zhenguo Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+X">Xiaodan Liang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.05941v1-abstract-short" style="display: inline;">
        Neural Architecture Search (NAS) aims to find efficient models for multiple tasks. Beyond seeking solutions for a single task, there are surging interests in transferring network design knowledge across multiple tasks. In this line of research, effectively modeling task correlations is vital yet highly neglected. Therefore, we propose \textbf{Arch-Graph}, a transferable NAS method that predicts ta&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05941v1-abstract-full').style.display = 'inline'; document.getElementById('2204.05941v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.05941v1-abstract-full" style="display: none;">
        Neural Architecture Search (NAS) aims to find efficient models for multiple tasks. Beyond seeking solutions for a single task, there are surging interests in transferring network design knowledge across multiple tasks. In this line of research, effectively modeling task correlations is vital yet highly neglected. Therefore, we propose \textbf{Arch-Graph}, a transferable NAS method that predicts task-specific optimal architectures with respect to given task embeddings. It leverages correlations across multiple tasks by using their embeddings as a part of the predictor&#39;s input for fast adaptation. We also formulate NAS as an architecture relation graph prediction problem, with the relational graph constructed by treating candidate architectures as nodes and their pairwise relations as edges. To enforce some basic properties such as acyclicity in the relational graph, we add additional constraints to the optimization process, converting NAS into the problem of finding a Maximal Weighted Acyclic Subgraph (MWAS). Our algorithm then strives to eliminate cycles and only establish edges in the graph if the rank results can be trusted. Through MWAS, Arch-Graph can effectively rank candidate models for each task with only a small budget to finetune the predictor. With extensive experiments on TransNAS-Bench-101, we show Arch-Graph&#39;s transferability and high sample efficiency across numerous tasks, beating many NAS methods designed for both single-task and multi-task search. It is able to find top 0.16\% and 0.29\% architectures on average on two search spaces under the budget of only 50 models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05941v1-abstract-full').style.display = 'none'; document.getElementById('2204.05941v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.05927">arXiv:2204.05927</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.05927">pdf</a>, <a href="https://arxiv.org/format/2204.05927">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Recommender May Not Favor Loyal Users
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+Y">Yitong Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+A">Aixin Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jie Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chenliang Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.05927v1-abstract-short" style="display: inline;">
        In academic research, recommender systems are often evaluated on benchmark datasets, without much consideration about the global timeline. Hence, we are unable to answer questions like: Do loyal users enjoy better recommendations than non-loyal users? Loyalty can be defined by the time period a user has been active in a recommender system, or by the number of historical interactions a user has. In&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05927v1-abstract-full').style.display = 'inline'; document.getElementById('2204.05927v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.05927v1-abstract-full" style="display: none;">
        In academic research, recommender systems are often evaluated on benchmark datasets, without much consideration about the global timeline. Hence, we are unable to answer questions like: Do loyal users enjoy better recommendations than non-loyal users? Loyalty can be defined by the time period a user has been active in a recommender system, or by the number of historical interactions a user has. In this paper, we offer a comprehensive analysis of recommendation results along global timeline. We conduct experiments with five widely used models, i.e., BPR, NeuMF, LightGCN, SASRec and TiSASRec, on four benchmark datasets, i.e., MovieLens-25M, Yelp, Amazon-music, and Amazon-electronic. Our experiment results give an answer &#34;No&#34; to the above question. Users with many historical interactions suffer from relatively poorer recommendations. Users who stay with the system for a short time period enjoy better recommendations. Both findings are counter-intuitive. Interestingly, users who have recently interacted with the system, with respect to the time point of the test instance, enjoy better recommendations. The finding on recency applies to all users, regardless of users&#39; loyalty. Our study offers a different perspective to understand recommender performance, and our findings could trigger a revisit of recommender model design.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.05927v1-abstract-full').style.display = 'none'; document.getElementById('2204.05927v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04681">arXiv:2204.04681</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04681">pdf</a>, <a href="https://arxiv.org/format/2204.04681">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing the Robustness, Efficiency, and Diversity of Differentiable Architecture Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ning%2C+J">Jia Ning</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+H">Han Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+K">Kun He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04681v1-abstract-short" style="display: inline;">
        Differentiable architecture search (DARTS) has attracted much attention due to its simplicity and significant improvement in efficiency. However, the excessive accumulation of the skip connection makes it suffer from long-term weak stability and low robustness. Many works attempt to restrict the accumulation of skip connections by indicators or manual design, however, these methods are susceptible&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04681v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04681v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04681v1-abstract-full" style="display: none;">
        Differentiable architecture search (DARTS) has attracted much attention due to its simplicity and significant improvement in efficiency. However, the excessive accumulation of the skip connection makes it suffer from long-term weak stability and low robustness. Many works attempt to restrict the accumulation of skip connections by indicators or manual design, however, these methods are susceptible to thresholds and human priors. In this work, we suggest a more subtle and direct approach that removes skip connections from the operation space. Then, by introducing an adaptive channel allocation strategy, we redesign the DARTS framework to automatically refill the skip connections in the evaluation stage, resolving the performance degradation caused by the absence of skip connections. Our method, dubbed Adaptive-Channel-Allocation-DARTS (ACA-DRATS), could eliminate the inconsistency in operation strength and significantly expand the architecture diversity. We continue to explore smaller search space under our framework, and offer a direct search on the entire ImageNet dataset. Experiments show that ACA-DRATS improves the search stability and significantly speeds up DARTS by more than ten times while yielding higher accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04681v1-abstract-full').style.display = 'none'; document.getElementById('2204.04681v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04377">arXiv:2204.04377</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04377">pdf</a>, <a href="https://arxiv.org/format/2204.04377">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robotic Surgery Remote Mentoring via AR with 3D Scene Streaming and Hand Interaction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Long%2C+Y">Yonghao Long</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chengkun Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dou%2C+Q">Qi Dou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04377v1-abstract-short" style="display: inline;">
        With the growing popularity of robotic surgery, education becomes increasingly important and urgently needed for the sake of patient safety. However, experienced surgeons have limited accessibility due to their busy clinical schedule or working in a distant city, thus can hardly provide sufficient education resources for novices. Remote mentoring, as an effective way, can help solve this problem,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04377v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04377v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04377v1-abstract-full" style="display: none;">
        With the growing popularity of robotic surgery, education becomes increasingly important and urgently needed for the sake of patient safety. However, experienced surgeons have limited accessibility due to their busy clinical schedule or working in a distant city, thus can hardly provide sufficient education resources for novices. Remote mentoring, as an effective way, can help solve this problem, but traditional methods are limited to plain text, audio, or 2D video, which are not intuitive nor vivid. Augmented reality (AR), a thriving technique being widely used for various education scenarios, is promising to offer new possibilities of visual experience and interactive teaching. In this paper, we propose a novel AR-based robotic surgery remote mentoring system with efficient 3D scene visualization and natural 3D hand interaction. Using a head-mounted display (i.e., HoloLens), the mentor can remotely monitor the procedure streamed from the trainee&#39;s operation side. The mentor can also provide feedback directly with hand gestures, which is in-turn transmitted to the trainee and viewed in surgical console as guidance. We comprehensively validate the system on both real surgery stereo videos and ex-vivo scenarios of common robotic training tasks (i.e., peg-transfer and suturing). Promising results are demonstrated regarding the fidelity of streamed scene visualization, the accuracy of feedback with hand interaction, and the low-latency of each component in the entire remote mentoring system. This work showcases the feasibility of leveraging AR technology for reliable, flexible and low-cost solutions to robotic surgical education, and holds great potential for clinical applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04377v1-abstract-full').style.display = 'none'; document.getElementById('2204.04377v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.03929">arXiv:2204.03929</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.03929">pdf</a>, <a href="https://arxiv.org/format/2204.03929">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Hyperspectral-Depth Reconstruction Using Single Color-Dot Projection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunyu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Monno%2C+Y">Yusuke Monno</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Okutomi%2C+M">Masatoshi Okutomi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.03929v1-abstract-short" style="display: inline;">
        Depth reconstruction and hyperspectral reflectance reconstruction are two active research topics in computer vision and image processing. Conventionally, these two topics have been studied separately using independent imaging setups and there is no existing method which can acquire depth and spectral reflectance simultaneously in one shot without using special hardware. In this paper, we propose a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03929v1-abstract-full').style.display = 'inline'; document.getElementById('2204.03929v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.03929v1-abstract-full" style="display: none;">
        Depth reconstruction and hyperspectral reflectance reconstruction are two active research topics in computer vision and image processing. Conventionally, these two topics have been studied separately using independent imaging setups and there is no existing method which can acquire depth and spectral reflectance simultaneously in one shot without using special hardware. In this paper, we propose a novel single-shot hyperspectral-depth reconstruction method using an off-the-shelf RGB camera and projector. Our method is based on a single color-dot projection, which simultaneously acts as structured light for depth reconstruction and spatially-varying color illuminations for hyperspectral reflectance reconstruction. To jointly reconstruct the depth and the hyperspectral reflectance from a single color-dot image, we propose a novel end-to-end network architecture that effectively incorporates a geometric color-dot pattern loss and a photometric hyperspectral reflectance loss. Through the experiments, we demonstrate that our hyperspectral-depth reconstruction method outperforms the combination of an existing state-of-the-art single-shot hyperspectral reflectance reconstruction method and depth reconstruction method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03929v1-abstract-full').style.display = 'none'; document.getElementById('2204.03929v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2022. Project homepage: http://www.ok.sc.e.titech.ac.jp/res/DHD/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.03610">arXiv:2204.03610</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.03610">pdf</a>, <a href="https://arxiv.org/format/2204.03610">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unified Contrastive Learning in Image-Text-Label Space
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianwei Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunyuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+P">Pengchuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+B">Bin Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Ce Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+L">Lu Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+J">Jianfeng Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.03610v1-abstract-short" style="display: inline;">
        Visual recognition is recently learned via either supervised learning on human-annotated image-label data or language-image contrastive learning with webly-crawled image-text pairs. While supervised learning may result in a more discriminative representation, language-image pretraining shows unprecedented zero-shot recognition capability, largely due to the different properties of data sources and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03610v1-abstract-full').style.display = 'inline'; document.getElementById('2204.03610v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.03610v1-abstract-full" style="display: none;">
        Visual recognition is recently learned via either supervised learning on human-annotated image-label data or language-image contrastive learning with webly-crawled image-text pairs. While supervised learning may result in a more discriminative representation, language-image pretraining shows unprecedented zero-shot recognition capability, largely due to the different properties of data sources and learning objectives. In this work, we introduce a new formulation by combining the two data sources into a common image-text-label space. In this space, we propose a new learning paradigm, called Unified Contrastive Learning (UniCL) with a single learning objective to seamlessly prompt the synergy of two data types. Extensive experiments show that our UniCL is an effective way of learning semantically rich yet discriminative representations, universally for image recognition in zero-shot, linear-probe, fully finetuning and transfer learning scenarios. Particularly, it attains gains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over the language-image contrastive learning and supervised learning methods, respectively. In linear probe setting, it also boosts the performance over the two methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL stand-alone is a good learner on pure image-label data, rivaling the supervised learning methods across three image classification datasets and two types of vision backbones, ResNet and Swin Transformer. Code is available at https://github.com/microsoft/UniCL.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03610v1-abstract-full').style.display = 'none'; document.getElementById('2204.03610v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.01341">arXiv:2204.01341</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.01341">pdf</a>, <a href="https://arxiv.org/format/2204.01341">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jiawei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+N">Ning Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rahaman%2C+M+M">Md Mamunur Rahaman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+Y">Yu-Dong Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yu-Hao Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jinghua Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+T">Tao Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qin%2C+W">Wenjun Qin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grzegorzek%2C+M">Marcin Grzegorzek</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.01341v1-abstract-short" style="display: inline;">
        This paper proposes a novel pixel interval down-sampling network (PID-Net) for dense tiny objects (yeast cells) counting tasks with higher accuracy. The PID-Net is an end-to-end CNN model with encoder to decoder architecture. The pixel interval down-sampling operations are concatenated with max-pooling operations to combine the sparse and dense features. It addresses the limitation of contour cong&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01341v1-abstract-full').style.display = 'inline'; document.getElementById('2204.01341v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.01341v1-abstract-full" style="display: none;">
        This paper proposes a novel pixel interval down-sampling network (PID-Net) for dense tiny objects (yeast cells) counting tasks with higher accuracy. The PID-Net is an end-to-end CNN model with encoder to decoder architecture. The pixel interval down-sampling operations are concatenated with max-pooling operations to combine the sparse and dense features. It addresses the limitation of contour conglutination of dense objects while counting. Evaluation was done using classical segmentation metrics (Dice, Jaccard, Hausdorff distance) as well as counting metrics. Experimental result shows that the proposed PID-Net has the best performance and potential for dense tiny objects counting tasks, which achieves 96.97% counting accuracy on the dataset with 2448 yeast cell images. By comparing with the state-of-the-art approaches like Attention U-Net, Swin U-Net and Trans U-Net, the proposed PID-Net can segment the dense tiny objects with clearer boundaries and fewer incorrect debris, which shows the great potential of PID-Net in the task of accurate counting tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01341v1-abstract-full').style.display = 'none'; document.getElementById('2204.01341v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.00327">arXiv:2204.00327</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.00327">pdf</a>, <a href="https://arxiv.org/format/2204.00327">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Diverse Preference Augmentation with Multiple Domains for Cold-start Recommendations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Changyu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Hui Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Duan%2C+L">Lixin Duan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yin%2C+H">Hongzhi Yin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+J">Jie Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.00327v1-abstract-short" style="display: inline;">
        Cold-start issues have been more and more challenging for providing accurate recommendations with the fast increase of users and items. Most existing approaches attempt to solve the intractable problems via content-aware recommendations based on auxiliary information and/or cross-domain recommendations with transfer learning. Their performances are often constrained by the extremely sparse user-it&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00327v1-abstract-full').style.display = 'inline'; document.getElementById('2204.00327v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.00327v1-abstract-full" style="display: none;">
        Cold-start issues have been more and more challenging for providing accurate recommendations with the fast increase of users and items. Most existing approaches attempt to solve the intractable problems via content-aware recommendations based on auxiliary information and/or cross-domain recommendations with transfer learning. Their performances are often constrained by the extremely sparse user-item interactions, unavailable side information, or very limited domain-shared users. Recently, meta-learners with meta-augmentation by adding noises to labels have been proven to be effective to avoid overfitting and shown good performance on new tasks. Motivated by the idea of meta-augmentation, in this paper, by treating a user&#39;s preference over items as a task, we propose a so-called Diverse Preference Augmentation framework with multiple source domains based on meta-learning (referred to as MetaDPA) to i) generate diverse ratings in a new domain of interest (known as target domain) to handle overfitting on the case of sparse interactions, and to ii) learn a preference model in the target domain via a meta-learning scheme to alleviate cold-start issues. Specifically, we first conduct multi-source domain adaptation by dual conditional variational autoencoders and impose a Multi-domain InfoMax (MDI) constraint on the latent representations to learn domain-shared and domain-specific preference properties. To avoid overfitting, we add a Mutually-Exclusive (ME) constraint on the output of decoders to generate diverse ratings given content data. Finally, these generated diverse ratings and the original ratings are introduced into the meta-training procedure to learn a preference meta-learner, which produces good generalization ability on cold-start recommendation tasks. Experiments on real-world datasets show our proposed MetaDPA clearly outperforms the current state-of-the-art baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00327v1-abstract-full').style.display = 'none'; document.getElementById('2204.00327v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.16530">arXiv:2203.16530</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.16530">pdf</a>, <a href="https://arxiv.org/format/2203.16530">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Instance-Specific Adaptation for Cross-Domain Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+Y">Yuliang Zou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zizhao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chun-Liang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Han Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pfister%2C+T">Tomas Pfister</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+J">Jia-Bin Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.16530v1-abstract-short" style="display: inline;">
        We propose a test-time adaptation method for cross-domain image segmentation. Our method is simple: Given a new unseen instance at test time, we adapt a pre-trained model by conducting instance-specific BatchNorm (statistics) calibration. Our approach has two core components. First, we replace the manually designed BatchNorm calibration rule with a learnable module. Second, we leverage strong data&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.16530v1-abstract-full').style.display = 'inline'; document.getElementById('2203.16530v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.16530v1-abstract-full" style="display: none;">
        We propose a test-time adaptation method for cross-domain image segmentation. Our method is simple: Given a new unseen instance at test time, we adapt a pre-trained model by conducting instance-specific BatchNorm (statistics) calibration. Our approach has two core components. First, we replace the manually designed BatchNorm calibration rule with a learnable module. Second, we leverage strong data augmentation to simulate random domain shifts for learning the calibration rule. In contrast to existing domain adaptation methods, our method does not require accessing the target domain data at training time or conducting computationally expensive test-time model training/optimization. Equipping our method with models trained by standard recipes achieves significant improvement, comparing favorably with several state-of-the-art domain generalization and one-shot unsupervised domain adaptation approaches. Combining our method with the domain generalization methods further improves performance, reaching a new state of the art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.16530v1-abstract-full').style.display = 'none'; document.getElementById('2203.16530v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://yuliang.vision/InstCal/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.16329">arXiv:2203.16329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.16329">pdf</a>, <a href="https://arxiv.org/format/2203.16329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Parameter-efficient Fine-tuning for Vision Transformers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+X">Xuehai He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunyuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+P">Pengchuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianwei Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X+E">Xin Eric Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.16329v1-abstract-short" style="display: inline;">
        In computer vision, it has achieved great success in adapting large-scale pretrained vision models (e.g., Vision Transformer) to downstream tasks via fine-tuning. Common approaches for fine-tuning either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient fine-tuning strategies for Vision Transformers on vision tasks. We formulate efficient fin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.16329v1-abstract-full').style.display = 'inline'; document.getElementById('2203.16329v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.16329v1-abstract-full" style="display: none;">
        In computer vision, it has achieved great success in adapting large-scale pretrained vision models (e.g., Vision Transformer) to downstream tasks via fine-tuning. Common approaches for fine-tuning either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient fine-tuning strategies for Vision Transformers on vision tasks. We formulate efficient fine-tuning as a subspace training problem and perform a comprehensive benchmarking over different efficient fine-tuning methods. We conduct an empirical study on each efficient fine-tuning method focusing on its performance alongside parameter cost. Furthermore, we also propose a parameter-efficient fine-tuning framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation method. We analyze and compare our method with a diverse set of baseline fine-tuning methods (including state-of-the-art methods for pretrained language models). Our method performs the best in terms of the tradeoff between accuracy and parameter efficiency across three commonly used image classification datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.16329v1-abstract-full').style.display = 'none'; document.getElementById('2203.16329v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15943">arXiv:2203.15943</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15943">pdf</a>, <a href="https://arxiv.org/format/2203.15943">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Leaf Segmentation under Complex Lighting Conditions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+X">Xufeng Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chang-Tsun Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Adams%2C+S">Scott Adams</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kouzani%2C+A">Abbas Kouzani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+R">Richard Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+L">Ligang He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Y">Yongjian Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vernon%2C+M">Michael Vernon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Doeven%2C+E">Egan Doeven</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Webb%2C+L">Lawrence Webb</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mcclellan%2C+T">Todd Mcclellan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guskic%2C+A">Adam Guskic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15943v1-abstract-short" style="display: inline;">
        As an essential prerequisite task in image-based plant phenotyping, leaf segmentation has garnered increasing attention in recent years. While self-supervised learning is emerging as an effective alternative to various computer vision tasks, its adaptation for image-based plant phenotyping remains rather unexplored. In this work, we present a self-supervised leaf segmentation framework consisting&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15943v1-abstract-full').style.display = 'inline'; document.getElementById('2203.15943v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15943v1-abstract-full" style="display: none;">
        As an essential prerequisite task in image-based plant phenotyping, leaf segmentation has garnered increasing attention in recent years. While self-supervised learning is emerging as an effective alternative to various computer vision tasks, its adaptation for image-based plant phenotyping remains rather unexplored. In this work, we present a self-supervised leaf segmentation framework consisting of a self-supervised semantic segmentation model, a color-based leaf segmentation algorithm, and a self-supervised color correction model. The self-supervised semantic segmentation model groups the semantically similar pixels by iteratively referring to the self-contained information, allowing the pixels of the same semantic object to be jointly considered by the color-based leaf segmentation algorithm for identifying the leaf regions. Additionally, we propose to use a self-supervised color correction model for images taken under complex illumination conditions. Experimental results on datasets of different plant species demonstrate the potential of the proposed self-supervised framework in achieving effective and generalizable leaf segmentation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15943v1-abstract-full').style.display = 'none'; document.getElementById('2203.15943v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15336">arXiv:2203.15336</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15336">pdf</a>, <a href="https://arxiv.org/format/2203.15336">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        End-to-End Compressed Video Representation Learning for Generic Event Boundary Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Congcong Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xinyao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+L">Longyin Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hong%2C+D">Dexiang Hong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+T">Tiejian Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+L">Libo Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15336v1-abstract-short" style="display: inline;">
        Generic event boundary detection aims to localize the generic, taxonomy-free event boundaries that segment videos into chunks. Existing methods typically require video frames to be decoded before feeding into the network, which demands considerable computational power and storage space. To that end, we propose a new end-to-end compressed video representation learning for event boundary detection t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15336v1-abstract-full').style.display = 'inline'; document.getElementById('2203.15336v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15336v1-abstract-full" style="display: none;">
        Generic event boundary detection aims to localize the generic, taxonomy-free event boundaries that segment videos into chunks. Existing methods typically require video frames to be decoded before feeding into the network, which demands considerable computational power and storage space. To that end, we propose a new end-to-end compressed video representation learning for event boundary detection that leverages the rich information in the compressed domain, i.e., RGB, motion vectors, residuals, and the internal group of pictures (GOP) structure, without fully decoding the video. Specifically, we first use the ConvNets to extract features of the I-frames in the GOPs. After that, a light-weight spatial-channel compressed encoder is designed to compute the feature representations of the P-frames based on the motion vectors, residuals and representations of their dependent I-frames. A temporal contrastive module is proposed to determine the event boundaries of video sequences. To remedy the ambiguities of annotations and speed up the training process, we use the Gaussian kernel to preprocess the ground-truth event boundaries. Extensive experiments conducted on the Kinetics-GEBD dataset demonstrate that the proposed method achieves comparable results to the state-of-the-art methods with $4.5\times$ faster running speed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15336v1-abstract-full').style.display = 'none'; document.getElementById('2203.15336v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14957">arXiv:2203.14957</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14957">pdf</a>, <a href="https://arxiv.org/format/2203.14957">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Frame-wise Action Representations for Long Videos via Sequence Contrastive Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+M">Minghao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+F">Fangyun Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chong Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+D">Deng Cai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14957v1-abstract-short" style="display: inline;">
        Prior works on action representation learning mainly focus on designing various architectures to extract the global representations for short video clips. In contrast, many practical applications such as video alignment have strong demand for learning dense representations for long videos. In this paper, we introduce a novel contrastive action representation learning (CARL) framework to learn fram&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14957v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14957v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14957v1-abstract-full" style="display: none;">
        Prior works on action representation learning mainly focus on designing various architectures to extract the global representations for short video clips. In contrast, many practical applications such as video alignment have strong demand for learning dense representations for long videos. In this paper, we introduce a novel contrastive action representation learning (CARL) framework to learn frame-wise action representations, especially for long videos, in a self-supervised manner. Concretely, we introduce a simple yet efficient video encoder that considers spatio-temporal context to extract frame-wise representations. Inspired by the recent progress of self-supervised learning, we present a novel sequence contrastive loss (SCL) applied on two correlated views obtained through a series of spatio-temporal data augmentations. SCL optimizes the embedding space by minimizing the KL-divergence between the sequence similarity of two augmented views and a prior Gaussian distribution of timestamp distance. Experiments on FineGym, PennAction and Pouring datasets show that our method outperforms previous state-of-the-art by a large margin for downstream fine-grained action classification. Surprisingly, although without training on paired videos, our approach also shows outstanding performance on video alignment and fine-grained frame retrieval tasks. Code and models are available at https://github.com/minghchen/CARL_code.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14957v1-abstract-full').style.display = 'none'; document.getElementById('2203.14957v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14875">arXiv:2203.14875</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14875">pdf</a>, <a href="https://arxiv.org/ps/2203.14875">ps</a>, <a href="https://arxiv.org/format/2203.14875">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FLDP: Flexible strategy for local differential privacy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+D">Dan Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+S">Suyun Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+R">Ruixuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cuiping Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+W">Wenjuan Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Hong Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14875v1-abstract-short" style="display: inline;">
        Local differential privacy (LDP), a technique applying unbiased statistical estimations instead of real data, is often adopted in data collection. In particular, this technique is used with frequency oracles (FO) because it can protect each user&#39;s privacy and prevent leakage of sensitive information. However, the definition of LDP is so conservative that it requires all inputs to be indistinguisha&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14875v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14875v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14875v1-abstract-full" style="display: none;">
        Local differential privacy (LDP), a technique applying unbiased statistical estimations instead of real data, is often adopted in data collection. In particular, this technique is used with frequency oracles (FO) because it can protect each user&#39;s privacy and prevent leakage of sensitive information. However, the definition of LDP is so conservative that it requires all inputs to be indistinguishable after perturbation. Indeed, LDP protects each value; however, it is rarely used in practical scenarios owing to its cost in terms of accuracy. In this paper, we address the challenge of providing weakened but flexible protection where each value only needs to be indistinguishable from part of the domain after perturbation. First, we present this weakened but flexible LDP (FLDP) notion. We then prove the association with LDP and DP. Second, we design an FHR approach for the common FO issue while satisfying FLDP. The proposed approach balances communication cost, computational complexity, and estimation accuracy. Finally, experimental results using practical and synthetic datasets verify the effectiveness and efficiency of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14875v1-abstract-full').style.display = 'none'; document.getElementById('2203.14875v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25pages, 5 figures, This is the proper field for the extended version of ICASSP conference</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education (RenminUniver-sity), Beijing 100872
        

        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14849">arXiv:2203.14849</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14849">pdf</a>, <a href="https://arxiv.org/format/2203.14849">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Safe Active Learning for Multi-Output Gaussian Processes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Cen-You Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rakitsch%2C+B">Barbara Rakitsch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zimmer%2C+C">Christoph Zimmer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14849v1-abstract-short" style="display: inline;">
        Multi-output regression problems are commonly encountered in science and engineering. In particular, multi-output Gaussian processes have been emerged as a promising tool for modeling these complex systems since they can exploit the inherent correlations and provide reliable uncertainty estimates. In many applications, however, acquiring the data is expensive and safety concerns might arise (e.g.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14849v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14849v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14849v1-abstract-full" style="display: none;">
        Multi-output regression problems are commonly encountered in science and engineering. In particular, multi-output Gaussian processes have been emerged as a promising tool for modeling these complex systems since they can exploit the inherent correlations and provide reliable uncertainty estimates. In many applications, however, acquiring the data is expensive and safety concerns might arise (e.g. robotics, engineering). We propose a safe active learning approach for multi-output Gaussian process regression. This approach queries the most informative data or output taking the relatedness between the regressors and safety constraints into account. We prove the effectiveness of our approach by providing theoretical analysis and by demonstrating empirical results on simulated datasets and on a real-world engineering dataset. On all datasets, our approach shows improved convergence compared to its competitors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14849v1-abstract-full').style.display = 'none'; document.getElementById('2203.14849v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at AISTATS 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14509">arXiv:2203.14509</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14509">pdf</a>, <a href="https://arxiv.org/format/2203.14509">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automated Progressive Learning for Efficient Training of Vision Transformers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Changlin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuang%2C+B">Bohan Zhuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Guangrun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+X">Xiaodan Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+X">Xiaojun Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yi Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14509v1-abstract-short" style="display: inline;">
        Recent advances in vision Transformers (ViTs) have come with a voracious appetite for computing power, high-lighting the urgent need to develop efficient training methods for ViTs. Progressive learning, a training scheme where the model capacity grows progressively during training, has started showing its ability in efficient training. In this paper, we take a practical step towards efficient trai&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14509v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14509v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14509v1-abstract-full" style="display: none;">
        Recent advances in vision Transformers (ViTs) have come with a voracious appetite for computing power, high-lighting the urgent need to develop efficient training methods for ViTs. Progressive learning, a training scheme where the model capacity grows progressively during training, has started showing its ability in efficient training. In this paper, we take a practical step towards efficient training of ViTs by customizing and automating progressive learning. First, we develop a strong manual baseline for progressive learning of ViTs, by introducing momentum growth (MoGrow) to bridge the gap brought by model growth. Then, we propose automated progressive learning (AutoProg), an efficient training scheme that aims to achieve lossless acceleration by automatically increasing the training overload on-the-fly; this is achieved by adaptively deciding whether, where and how much should the model grow during progressive learning. Specifically, we first relax the optimization of the growth schedule to sub-network architecture optimization problem, then propose one-shot estimation of the sub-network performance via an elastic supernet. The searching overhead is reduced to minimal by recycling the parameters of the supernet. Extensive experiments of efficient training on ImageNet with two representative ViT models, DeiT and VOLO, demonstrate that AutoProg can accelerate ViTs training by up to 85.1% with no performance drop. Code: https://github.com/changlin31/AutoProg
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14509v1-abstract-full').style.display = 'none'; document.getElementById('2203.14509v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.14387">arXiv:2203.14387</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.14387">pdf</a>, <a href="https://arxiv.org/format/2203.14387">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Domain Generalization in Object Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xingxuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+Z">Zekai Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+R">Renzhe Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiashuo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cui%2C+P">Peng Cui</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wan%2C+W">Weitao Wan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+C">Chong Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chen Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.14387v1-abstract-short" style="display: inline;">
        Despite the striking performance achieved by modern detectors when training and test data are sampled from the same or similar distribution, the generalization ability of detectors under unknown distribution shifts remains hardly studied. Recently several works discussed the detectors&#39; adaptation ability to a specific target domain which are not readily applicable in real-world applications since&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14387v1-abstract-full').style.display = 'inline'; document.getElementById('2203.14387v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.14387v1-abstract-full" style="display: none;">
        Despite the striking performance achieved by modern detectors when training and test data are sampled from the same or similar distribution, the generalization ability of detectors under unknown distribution shifts remains hardly studied. Recently several works discussed the detectors&#39; adaptation ability to a specific target domain which are not readily applicable in real-world applications since detectors may encounter various environments or situations while pre-collecting all of them before training is inconceivable. In this paper, we study the critical problem, domain generalization in object detection (DGOD), where detectors are trained with source domains and evaluated on unknown target domains. To thoroughly evaluate detectors under unknown distribution shifts, we formulate the DGOD problem and propose a comprehensive evaluation benchmark to fill the vacancy. Moreover, we propose a novel method named Region Aware Proposal reweighTing (RAPT) to eliminate dependence within RoI features. Extensive experiments demonstrate that current DG methods fail to address the DGOD problem and our method outperforms other state-of-the-art counterparts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.14387v1-abstract-full').style.display = 'none'; document.getElementById('2203.14387v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13397">arXiv:2203.13397</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13397">pdf</a>, <a href="https://arxiv.org/format/2203.13397">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Changye Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Knopman%2C+D">David Knopman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+W">Weizhe Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cohen%2C+T">Trevor Cohen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pakhomov%2C+S">Serguei Pakhomov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13397v1-abstract-short" style="display: inline;">
        Deep learning (DL) techniques involving fine-tuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer&#39;s disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research. As an alt&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13397v1-abstract-full').style.display = 'inline'; document.getElementById('2203.13397v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13397v1-abstract-full" style="display: none;">
        Deep learning (DL) techniques involving fine-tuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer&#39;s disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research. As an alternative to fitting model parameters directly, we propose a novel method by which a Transformer DL model (GPT-2) pre-trained on general English text is paired with an artificially degraded version of itself (GPT-D), to compute the ratio between these two models&#39; \textit{perplexities} on language from cognitively healthy and impaired individuals. This technique approaches state-of-the-art performance on text data from a widely used &#34;Cookie Theft&#34; picture description task, and unlike established alternatives also generalizes well to spontaneous conversations. Furthermore, GPT-D generates text with characteristics known to be associated with AD, demonstrating the induction of dementia-related linguistic anomalies. Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models, the language that they produce, and the deleterious effects of dementia on human speech and language characteristics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13397v1-abstract-full').style.display = 'none'; document.getElementById('2203.13397v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted by ACL 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13395">arXiv:2203.13395</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13395">pdf</a>, <a href="https://arxiv.org/format/2203.13395">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Using Reinforcement Learning to Study Platform Economies under Market Shocks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xintong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+G+Q">Gary Qiurui Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eden%2C+A">Alon Eden</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Clara Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Trott%2C+A">Alexander Trott</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+S">Stephan Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Parkes%2C+D+C">David C. Parkes</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13395v1-abstract-short" style="display: inline;">
        Driven by rapid digitization and expansive internet access, market-driven platforms (e.g., Amazon, DoorDash, Uber, TaskRabbit) are increasingly prevalent and becoming key drivers of the economy. Across many industries, platforms leverage digital infrastructure to efficiently match producers and consumers, dynamically set prices, and enable economies of scale. This increasing prominence makes it im&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13395v1-abstract-full').style.display = 'inline'; document.getElementById('2203.13395v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13395v1-abstract-full" style="display: none;">
        Driven by rapid digitization and expansive internet access, market-driven platforms (e.g., Amazon, DoorDash, Uber, TaskRabbit) are increasingly prevalent and becoming key drivers of the economy. Across many industries, platforms leverage digital infrastructure to efficiently match producers and consumers, dynamically set prices, and enable economies of scale. This increasing prominence makes it important to understand the behavior of platforms, which induces complex phenomenon especially in the presence of severe market shocks (e.g., during pandemics). In this work, we develop a multi-agent simulation environment to capture key elements of a platform economy, including the kinds of economic shocks that disrupt a traditional, off-platform market. We use deep reinforcement learning (RL) to model the pricing and matching behavior of a platform that optimizes for revenue and various socially-aware objectives. We start with tractable motivating examples to establish intuitions about the dynamics and function of optimal platform policies. We then conduct extensive empirical simulations on multi-period environments, including settings with market shocks. We characterize the effect of a platform on the efficiency and resilience of an economic system under different platform design objectives. We further analyze the consequences of regulation fixing platform fees, and study the alignment of a revenue-maximizing platform with social welfare under different platform matching policies. As such, our RL-based framework provides a foundation for understanding platform economies under different designs and for yielding new economic insights that are beyond analytical tractability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13395v1-abstract-full').style.display = 'none'; document.getElementById('2203.13395v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.12856">arXiv:2203.12856</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.12856">pdf</a>, <a href="https://arxiv.org/format/2203.12856">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Fixation: Dynamic Window Visual Transformer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+P">Pengzhen Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Changlin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Guangrun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+Y">Yun Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+Q">Qing Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+X">Xiaodan Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+X">Xiaojun Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.12856v2-abstract-short" style="display: inline;">
        Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. However, this may limit the modeling potential of these window-based models for multi-scale information. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12856v2-abstract-full').style.display = 'inline'; document.getElementById('2203.12856v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.12856v2-abstract-full" style="display: none;">
        Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. However, this may limit the modeling potential of these window-based models for multi-scale information. In this paper, we propose a novel method, named Dynamic Window Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT goes beyond the model that employs a fixed single window setting. To the best of our knowledge, we are the first to use dynamic multi-scale windows to explore the upper limit of the effect of window settings on model performance. In DW-ViT, multi-scale information is obtained by assigning windows of different sizes to different head groups of window multi-head self-attention. Then, the information is dynamically fused by assigning different weights to the multi-scale window branches. We conducted a detailed performance evaluation on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related state-of-the-art (SoTA) methods, DW-ViT obtains the best performance. Specifically, compared with the current SoTA Swin Transformers \cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements on all three datasets with similar parameters and computational costs. In addition, DW-ViT exhibits good scalability and can be easily inserted into any window-based visual transformers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12856v2-abstract-full').style.display = 'none'; document.getElementById('2203.12856v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        CVPR2022
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11926">arXiv:2203.11926</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11926">pdf</a>, <a href="https://arxiv.org/format/2203.11926">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Focal Modulation Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianwei Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chunyuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+J">Jianfeng Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11926v1-abstract-short" style="display: inline;">
        In this work, we propose focal modulation network (FocalNet in short), where self-attention (SA) is completely replaced by a focal modulation module that is more effective and efficient for modeling token interactions. Focal modulation comprises three components: $(i)$ hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11926v1-abstract-full').style.display = 'inline'; document.getElementById('2203.11926v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11926v1-abstract-full" style="display: none;">
        In this work, we propose focal modulation network (FocalNet in short), where self-attention (SA) is completely replaced by a focal modulation module that is more effective and efficient for modeling token interactions. Focal modulation comprises three components: $(i)$ hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges at different granularity levels, $(ii)$ gated aggregation to selectively aggregate context features for each visual token (query) based on its content, and $(iii)$ modulation or element-wise affine transformation to fuse the aggregated features into the query vector. Extensive experiments show that FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin Transformers) with similar time and memory cost on the tasks of image classification, object detection, and semantic segmentation. Specifically, our FocalNets with tiny and base sizes achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224$\times$224 and 384$\times$384, respectively. FocalNets exhibit remarkable superiority when transferred to downstream tasks. For object detection with Mask R-CNN, our FocalNet base trained with 1$\times$ already surpasses Swin trained with 3$\times$ schedule (49.0 v.s. 48.5). For semantic segmentation with UperNet, FocalNet base evaluated at single-scale outperforms Swin evaluated at multi-scale (50.5 v.s. 49.7). These results render focal modulation a favorable alternative to SA for effective and efficient visual modeling in real-world applications. Code is available at https://github.com/microsoft/FocalNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11926v1-abstract-full').style.display = 'none'; document.getElementById('2203.11926v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">technical report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11425">arXiv:2203.11425</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11425">pdf</a>, <a href="https://arxiv.org/format/2203.11425">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Abstractive Grounded Summarization of Podcast Transcripts
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+K">Kaiqiang Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chen Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaoyang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+D">Dong Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+F">Fei Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11425v1-abstract-short" style="display: inline;">
        Podcasts have recently shown a rapid rise in popularity. Summarization of podcast transcripts is of practical benefit to both content providers and consumers. It helps consumers to quickly decide whether they will listen to the podcasts and reduces the cognitive load of content providers to write summaries. Nevertheless, podcast summarization faces significant challenges including factual inconsis&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11425v1-abstract-full').style.display = 'inline'; document.getElementById('2203.11425v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11425v1-abstract-full" style="display: none;">
        Podcasts have recently shown a rapid rise in popularity. Summarization of podcast transcripts is of practical benefit to both content providers and consumers. It helps consumers to quickly decide whether they will listen to the podcasts and reduces the cognitive load of content providers to write summaries. Nevertheless, podcast summarization faces significant challenges including factual inconsistencies with respect to the inputs. The problem is exacerbated by speech disfluencies and recognition errors in transcripts of spoken language. In this paper, we explore a novel abstractive summarization method to alleviate these challenges. Specifically, our approach learns to produce an abstractive summary while grounding summary segments in specific portions of the transcript to allow for full inspection of summary details. We conduct a series of analyses of the proposed approach on a large podcast dataset and show that the approach can achieve promising results. Grounded summaries bring clear benefits in locating the summary and transcript segments that contain inconsistent information, and hence significantly improve summarization quality in both automatic and human evaluation metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11425v1-abstract-full').style.display = 'none'; document.getElementById('2203.11425v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10852">arXiv:2203.10852</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10852">pdf</a>, <a href="https://arxiv.org/format/2203.10852">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-modal learning for predicting the genotype of glioma
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Y">Yiran Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+L">Lei Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+L">Lipei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sch%C3%B6nlieb%2C+C">Carola-Bibiane Schnlieb</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Price%2C+S+J">Stephen J. Price</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Chao Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10852v1-abstract-short" style="display: inline;">
        The isocitrate dehydrogenase (IDH) gene mutation is an essential biomarker for the diagnosis and prognosis of glioma. It is promising to better predict glioma genotype by integrating focal tumor image and geometric features with brain network features derived from MRI. Convolutions neural networks show reasonable performance in predicting IDH mutation, which, however, cannot learn from non-Euclide&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10852v1-abstract-full').style.display = 'inline'; document.getElementById('2203.10852v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10852v1-abstract-full" style="display: none;">
        The isocitrate dehydrogenase (IDH) gene mutation is an essential biomarker for the diagnosis and prognosis of glioma. It is promising to better predict glioma genotype by integrating focal tumor image and geometric features with brain network features derived from MRI. Convolutions neural networks show reasonable performance in predicting IDH mutation, which, however, cannot learn from non-Euclidean data, e.g., geometric and network data. In this study, we propose a multi-modal learning framework using three separate encoders to extract features of focal tumor image, tumor geometrics and global brain networks. To mitigate the limited availability of diffusion MRI, we develop a self-supervised approach to generate brain networks from anatomical multi-sequence MRI. Moreover, to extract tumor-related features from the brain network, we design a hierarchical attention module for the brain network encoder. Further, we design a bi-level multi-modal contrastive loss to align the multi-modal features and tackle the domain gap at the focal tumor and global brain. Finally, we propose a weighted population graph to integrate the multi-modal features for genotype prediction. Experimental results on the testing set show that the proposed model outperforms the baseline deep learning models. The ablation experiments validate the performance of different components of the framework. The visualized interpretation corresponds to clinical knowledge with further validation. In conclusion, the proposed learning framework provides a novel approach for predicting the genotype of glioma.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10852v1-abstract-full').style.display = 'none'; document.getElementById('2203.10852v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.09915">arXiv:2203.09915</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.09915">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Collaborative Driving: Learning- Aided Joint Topology Formulation and Beamforming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+C">Changle Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luan%2C+T+H">Tom H. Luan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+C+Y+Y">Chau Yuen Yuchuan Fu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.09915v1-abstract-short" style="display: inline;">
        Currently, autonomous vehicles are able to drive more naturally based on the driving policies learned from millions of driving miles in real environments. However, to further improve the automation level of vehicles is a challenging task, especially in the case of multi-vehicle cooperation. In recent heated discussions of 6G, millimeter-wave (mmWave) and terahertz (THz) bands are deemed to play im&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09915v1-abstract-full').style.display = 'inline'; document.getElementById('2203.09915v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.09915v1-abstract-full" style="display: none;">
        Currently, autonomous vehicles are able to drive more naturally based on the driving policies learned from millions of driving miles in real environments. However, to further improve the automation level of vehicles is a challenging task, especially in the case of multi-vehicle cooperation. In recent heated discussions of 6G, millimeter-wave (mmWave) and terahertz (THz) bands are deemed to play important roles in new radio communication architectures and algorithms. To enable reliable autonomous driving in 6G, in this paper, we envision collaborative autonomous driving, a new framework that jointly controls driving topology and formulate vehicular networks in the mmWave/THz bands. As a swarm intelligence system, the collaborative driving scheme goes beyond existing autonomous driving patterns based on single-vehicle intelligence in terms of safety and efficiency. With efficient data sharing, the proposed framework is able to achieve cooperative sensing and load balancing so that improve sensing efficiency with saved computational resources. To deal with the new challenges in the collaborative driving framework, we further illustrate two promising approaches for mmWave/THz-based vehicle-to-vehicle (V2V) communications. Finally, we discuss several potential open research problems for the proposed collaborative driving scheme.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09915v1-abstract-full').style.display = 'none'; document.getElementById('2203.09915v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Vehicular Technology Magazine, 2022
      </p>
    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Li%2C+C&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>