<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 247 results for author: <span class="mathjax">Dong, X</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Dong%2C+X">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Dong, X">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Dong%2C+X&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Dong, X">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=200"
            class="pagination-link "
            aria-label="Page 5"
            aria-current="page">5
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01902">arXiv:2205.01902</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01902">pdf</a>, <a href="https://arxiv.org/format/2205.01902">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pik-Fix: Restoring and Colorizing Old Photo
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+R">Runsheng Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+Z">Zhengzhong Tu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+Y">Yuanqi Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaoyu Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jinlong Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meng%2C+Z">Zibo Meng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+J">Jiaqi Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bovik%2C+A">Alan Bovik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+H">Hongkai Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01902v1-abstract-short" style="display: inline;">
        Restoring and inpainting the visual memories that are present, but often impaired, in old photos remains an intriguing but unsolved research topic. Decades-old photos often suffer from severe and commingled degradation such as cracks, defocus, and color-fading, which are difficult to treat individually and harder to repair when they interact. Deep learning presents a plausible avenue, but the lack&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01902v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01902v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01902v1-abstract-full" style="display: none;">
        Restoring and inpainting the visual memories that are present, but often impaired, in old photos remains an intriguing but unsolved research topic. Decades-old photos often suffer from severe and commingled degradation such as cracks, defocus, and color-fading, which are difficult to treat individually and harder to repair when they interact. Deep learning presents a plausible avenue, but the lack of large-scale datasets of old photos makes addressing this restoration task very challenging. Here we present a novel reference-based end-to-end learning framework that is able to both repair and colorize old and degraded pictures. Our proposed framework consists of three modules: a restoration sub-network that conducts restoration from degradations, a similarity sub-network that performs color histogram matching and color transfer, and a colorization subnet that learns to predict the chroma elements of images that have been conditioned on chromatic reference signals. The overall system makes uses of color histogram priors from reference images, which greatly reduces the need for large-scale training data. We have also created a first-of-a-kind public dataset of real old photos that are paired with ground truth &#34;pristine&#34; photos that have been that have been manually restored by PhotoShop experts. We conducted extensive experiments on this dataset and synthetic datasets, and found that our method significantly outperforms previous state-of-the-art models using both qualitative comparisons and quantitative measurements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01902v1-abstract-full').style.display = 'none'; document.getElementById('2205.01902v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2202.02606</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13874">arXiv:2204.13874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13874">pdf</a>, <a href="https://arxiv.org/format/2204.13874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3485447.3512035">10.1145/3485447.3512035 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OA-Mine: Open-World Attribute Mining for E-Commerce Products with Weak Supervision
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xinyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Chenwei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X+L">Xin Luna Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shang%2C+J">Jingbo Shang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Faloutsos%2C+C">Christos Faloutsos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jiawei Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13874v1-abstract-short" style="display: inline;">
        Automatic extraction of product attributes from their textual descriptions is essential for online shopper experience. One inherent challenge of this task is the emerging nature of e-commerce products -- we see new types of products with their unique set of new attributes constantly. Most prior works on this matter mine new values for a set of known attributes but cannot handle new attributes that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13874v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13874v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13874v1-abstract-full" style="display: none;">
        Automatic extraction of product attributes from their textual descriptions is essential for online shopper experience. One inherent challenge of this task is the emerging nature of e-commerce products -- we see new types of products with their unique set of new attributes constantly. Most prior works on this matter mine new values for a set of known attributes but cannot handle new attributes that arose from constantly changing data. In this work, we study the attribute mining problem in an open-world setting to extract novel attributes and their values. Instead of providing comprehensive training data, the user only needs to provide a few examples for a few known attribute types as weak supervision. We propose a principled framework that first generates attribute value candidates and then groups them into clusters of attributes. The candidate generation step probes a pre-trained language model to extract phrases from product titles. Then, an attribute-aware fine-tuning method optimizes a multitask objective and shapes the language model representation to be attribute-discriminative. Finally, we discover new attributes and values through the self-ensemble of our framework, which handles the open-world challenge. We run extensive experiments on a large distantly annotated development set and a gold standard human-annotated test set that we collected. Our model significantly outperforms strong baselines and can generalize to unseen attributes and product types.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13874v1-abstract-full').style.display = 'none'; document.getElementById('2204.13874v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">WWW 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13767">arXiv:2204.13767</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13767">pdf</a>, <a href="https://arxiv.org/format/2204.13767">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting--Full Version
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cirstea%2C+R">Razvan-Gabriel Cirstea</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+C">Chenjuan Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+B">Bin Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kieu%2C+T">Tung Kieu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xuanyi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+S">Shirui Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13767v1-abstract-short" style="display: inline;">
        A variety of real-world applications rely on far future information to make decisions, thus calling for efficient and accurate long sequence multivariate time series forecasting. While recent attention-based forecasting models show strong abilities in capturing long-term dependencies, they still suffer from two key limitations. First, canonical self attention has a quadratic complexity w.r.t. the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13767v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13767v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13767v1-abstract-full" style="display: none;">
        A variety of real-world applications rely on far future information to make decisions, thus calling for efficient and accurate long sequence multivariate time series forecasting. While recent attention-based forecasting models show strong abilities in capturing long-term dependencies, they still suffer from two key limitations. First, canonical self attention has a quadratic complexity w.r.t. the input time series length, thus falling short in efficiency. Second, different variables&#39; time series often have distinct temporal dynamics, which existing studies fail to capture, as they use the same model parameter space, e.g., projection matrices, for all variables&#39; time series, thus falling short in accuracy. To ensure high efficiency and accuracy, we propose Triformer, a triangular, variable-specific attention. (i) Linear complexity: we introduce a novel patch attention with linear complexity. When stacking multiple layers of the patch attentions, a triangular structure is proposed such that the layer sizes shrink exponentially, thus maintaining linear complexity. (ii) Variable-specific parameters: we propose a light-weight method to enable distinct sets of model parameters for different variables&#39; time series to enhance accuracy without compromising efficiency and memory usage. Strong empirical evidence on four datasets from multiple domains justifies our design choices, and it demonstrates that Triformer outperforms state-of-the-art methods w.r.t. both accuracy and efficiency. This is an extended version of &#34;Triformer: Triangular, Variable-Specific Attentions for Long Sequence Multivariate Time Series Forecasting&#34;, to appear in IJCAI 2022 [Cirstea et al., 2022a], including additional experimental results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13767v1-abstract-full').style.display = 'none'; document.getElementById('2204.13767v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13109">arXiv:2204.13109</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13109">pdf</a>, <a href="https://arxiv.org/format/2204.13109">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Worst-Case Dynamic Power Distribution Network Noise Prediction Using Convolutional Neural Network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiao Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yufei Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yin%2C+X">Xunzhao Yin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhuo%2C+C">Cheng Zhuo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13109v1-abstract-short" style="display: inline;">
        Worst-case dynamic PDN noise analysis is an essential step in PDN sign-off to ensure the performance and reliability of chips. However, with the growing PDN size and increasing scenarios to be validated, it becomes very time- and resource-consuming to conduct full-stack PDN simulation to check the worst-case noise for different test vectors. Recently, various works have proposed machine learning b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13109v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13109v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13109v1-abstract-full" style="display: none;">
        Worst-case dynamic PDN noise analysis is an essential step in PDN sign-off to ensure the performance and reliability of chips. However, with the growing PDN size and increasing scenarios to be validated, it becomes very time- and resource-consuming to conduct full-stack PDN simulation to check the worst-case noise for different test vectors. Recently, various works have proposed machine learning based methods for supply noise prediction, many of which still suffer from large training overhead, inefficiency, or non-scalability. Thus, this paper proposed an efficient and scalable framework for the worst-case dynamic PDN noise prediction. The framework first reduces the spatial and temporal redundancy in the PDN and input current vector, and then employs efficient feature extraction as well as a novel convolutional neural network architecture to predict the worst-case dynamic PDN noise. Experimental results show that the proposed framework consistently outperforms the commercial tool and the state-of-the-art machine learning method with only 0.63-1.02% mean relative error and 25-69$\times$ speedup.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13109v1-abstract-full').style.display = 'none'; document.getElementById('2204.13109v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07446">arXiv:2204.07446</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07446">pdf</a>, <a href="https://arxiv.org/format/2204.07446">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Wi-Fi and Bluetooth Contact Tracing Without User Intervention
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yuen%2C+B">Brosnan Yuen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bie%2C+Y">Yifeng Bie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cairns%2C+D">Duncan Cairns</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Harper%2C+G">Geoffrey Harper</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+J">Jason Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chang%2C+C">Charles Chang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaodai Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+T">Tao Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07446v1-abstract-short" style="display: inline;">
        A custom Wi-Fi and Bluetooth indoor contact tracing system is created to find detailed paths of infected individuals without any user intervention. The system tracks smartphones, but it does not require smartphone applications, connecting to the routers, or any other extraneous devices on the users. A custom Turtlebot3 is used for site surveying, where it simulates mobile device movement and packe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07446v1-abstract-full').style.display = 'inline'; document.getElementById('2204.07446v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07446v1-abstract-full" style="display: none;">
        A custom Wi-Fi and Bluetooth indoor contact tracing system is created to find detailed paths of infected individuals without any user intervention. The system tracks smartphones, but it does not require smartphone applications, connecting to the routers, or any other extraneous devices on the users. A custom Turtlebot3 is used for site surveying, where it simulates mobile device movement and packet transmission. Transmit power, receive power, and round trip time are collected by a custom ESP32C3 router. MAC randomization is defeated to identify unique smartphones. Subsequently, the wireless parameters above are converted to signal path loss and time of flight. Bidirectional long short term memory takes the wireless parameters and predicts the detailed paths of the users within 1 m. Public health authorities can use the contact tracing website to find the detailed paths of the suspected cases using the smartphone models and initial positions of confirm cases. The system can also track indirect contact transmissions originating from surfaces and droplets due to having absolute positions of users.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07446v1-abstract-full').style.display = 'none'; document.getElementById('2204.07446v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04705">arXiv:2204.04705</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04705">pdf</a>, <a href="https://arxiv.org/format/2204.04705">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xin Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=De+Salvo%2C+B">Barbara De Salvo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+M">Meng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Chiao Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qu%2C+Z">Zhongnan Qu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kung%2C+H+T">H. T. Kung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Z">Ziyun Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04705v1-abstract-short" style="display: inline;">
        We design deep neural networks (DNNs) and corresponding networks&#39; splittings to distribute DNNs&#39; workload to camera sensors and a centralized aggregator on head mounted devices to meet system performance targets in inference accuracy and latency under the given hardware resource constraints. To achieve an optimal balance among computation, communication, and performance, a split-aware neural archi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04705v1-abstract-full').style.display = 'inline'; document.getElementById('2204.04705v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04705v1-abstract-full" style="display: none;">
        We design deep neural networks (DNNs) and corresponding networks&#39; splittings to distribute DNNs&#39; workload to camera sensors and a centralized aggregator on head mounted devices to meet system performance targets in inference accuracy and latency under the given hardware resource constraints. To achieve an optimal balance among computation, communication, and performance, a split-aware neural architecture search framework, SplitNets, is introduced to conduct model designing, splitting, and communication reduction simultaneously. We further extend the framework to multi-view systems for learning to fuse inputs from multiple camera sensors with optimal performance and systemic efficiency. We validate SplitNets for single-view system on ImageNet as well as multi-view system on 3D classification, and show that the SplitNets framework achieves state-of-the-art (SOTA) performance and system latency compared with existing approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04705v1-abstract-full').style.display = 'none'; document.getElementById('2204.04705v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15470">arXiv:2203.15470</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15470">pdf</a>, <a href="https://arxiv.org/format/2203.15470">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistical Finance">q-fin.ST</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applications">stat.AP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Graph similarity learning for change-point detection in dynamic networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sulem%2C+D">Deborah Sulem</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kenlay%2C+H">Henry Kenlay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cucuringu%2C+M">Mihai Cucuringu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowen Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15470v1-abstract-short" style="display: inline;">
        Dynamic networks are ubiquitous for modelling sequential graph-structured data, e.g., brain connectome, population flows and messages exchanges. In this work, we consider dynamic networks that are temporal sequences of graph snapshots, and aim at detecting abrupt changes in their structure. This task is often termed network change-point detection and has numerous applications, such as fraud detect&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15470v1-abstract-full').style.display = 'inline'; document.getElementById('2203.15470v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15470v1-abstract-full" style="display: none;">
        Dynamic networks are ubiquitous for modelling sequential graph-structured data, e.g., brain connectome, population flows and messages exchanges. In this work, we consider dynamic networks that are temporal sequences of graph snapshots, and aim at detecting abrupt changes in their structure. This task is often termed network change-point detection and has numerous applications, such as fraud detection or physical motion monitoring. Leveraging a graph neural network model, we design a method to perform online network change-point detection that can adapt to the specific network domain and localise changes with no delay. The main novelty of our method is to use a siamese graph neural network architecture for learning a data-driven graph similarity function, which allows to effectively compare the current graph and its recent history. Importantly, our method does not require prior knowledge on the network generative distribution and is agnostic to the type of change-points; moreover, it can be applied to a large variety of networks, that include for instance edge weights and node attributes. We show on synthetic and real data that our method enjoys a number of benefits: it is able to learn an adequate graph similarity function for performing online network change-point detection in diverse types of change-point settings, and requires a shorter data history to detect changes than most existing state-of-the-art baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15470v1-abstract-full').style.display = 'none'; document.getElementById('2203.15470v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">33 pages, 21 figures, 5 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15320">arXiv:2203.15320</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15320">pdf</a>, <a href="https://arxiv.org/format/2203.15320">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dressing in the Wild by Watching Dance Videos
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xin Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+F">Fuwei Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+Z">Zhenyu Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xijin Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+D+K">Daniel K. Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+M">Min Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Long%2C+X">Xiang Long</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+X">Xiaodan Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianchao Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15320v1-abstract-short" style="display: inline;">
        While significant progress has been made in garment transfer, one of the most applicable directions of human-centric image generation, existing works overlook the in-the-wild imagery, presenting severe garment-person misalignment as well as noticeable degradation in fine texture details. This paper, therefore, attends to virtual try-on in real-world scenes and brings essential improvements in auth&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15320v1-abstract-full').style.display = 'inline'; document.getElementById('2203.15320v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15320v1-abstract-full" style="display: none;">
        While significant progress has been made in garment transfer, one of the most applicable directions of human-centric image generation, existing works overlook the in-the-wild imagery, presenting severe garment-person misalignment as well as noticeable degradation in fine texture details. This paper, therefore, attends to virtual try-on in real-world scenes and brings essential improvements in authenticity and naturalness especially for loose garment (e.g., skirts, formal dresses), challenging poses (e.g., cross arms, bent legs), and cluttered backgrounds. Specifically, we find that the pixel flow excels at handling loose garments whereas the vertex flow is preferred for hard poses, and by combining their advantages we propose a novel generative network called wFlow that can effectively push up garment transfer to in-the-wild context. Moreover, former approaches require paired images for training. Instead, we cut down the laboriousness by working on a newly constructed large-scale video dataset named Dance50k with self-supervised cross-frame training and an online cycle optimization. The proposed Dance50k can boost real-world virtual dressing by covering a wide variety of garments under dancing poses. Extensive experiments demonstrate the superiority of our wFlow in generating realistic garment transfer results for in-the-wild images without resorting to expensive paired datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15320v1-abstract-full').style.display = 'none'; document.getElementById('2203.15320v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CVPR2022, Project: https://awesome-wflow.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.09811">arXiv:2203.09811</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.09811">pdf</a>, <a href="https://arxiv.org/format/2203.09811">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xingning Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gan%2C+T">Tian Gan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+X">Xuemeng Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jianlong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+Y">Yuan Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nie%2C+L">Liqiang Nie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.09811v2-abstract-short" style="display: inline;">
        Scene Graph Generation, which generally follows a regular encoder-decoder pipeline, aims to first encode the visual contents within the given image and then parse them into a compact summary graph. Existing SGG approaches generally not only neglect the insufficient modality fusion between vision and language, but also fail to provide informative predicates due to the biased relationship prediction&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09811v2-abstract-full').style.display = 'inline'; document.getElementById('2203.09811v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.09811v2-abstract-full" style="display: none;">
        Scene Graph Generation, which generally follows a regular encoder-decoder pipeline, aims to first encode the visual contents within the given image and then parse them into a compact summary graph. Existing SGG approaches generally not only neglect the insufficient modality fusion between vision and language, but also fail to provide informative predicates due to the biased relationship predictions, leading SGG far from practical. Towards this end, in this paper, we first present a novel Stacked Hybrid-Attention network, which facilitates the intra-modal refinement as well as the inter-modal interaction, to serve as the encoder. We then devise an innovative Group Collaborative Learning strategy to optimize the decoder. Particularly, based upon the observation that the recognition capability of one classifier is limited towards an extremely unbalanced dataset, we first deploy a group of classifiers that are expert in distinguishing different subsets of classes, and then cooperatively optimize them from two aspects to promote the unbiased SGG. Experiments conducted on VG and GQA datasets demonstrate that, we not only establish a new state-of-the-art in the unbiased metric, but also nearly double the performance compared with two baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09811v2-abstract-full').style.display = 'none'; document.getElementById('2203.09811v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2022, the code is available at https://github.com/dongxingning/SHA-GCL-for-SGG</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.09424">arXiv:2203.09424</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.09424">pdf</a>, <a href="https://arxiv.org/format/2203.09424">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        elBERto: Self-supervised Commonsense Learning for Question Answering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhan%2C+X">Xunlin Zhan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yuan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiao Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+X">Xiaodan Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Z">Zhiting Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Carin%2C+L">Lawrence Carin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.09424v1-abstract-short" style="display: inline;">
        Commonsense question answering requires reasoning about everyday situations and causes and effects implicit in context. Typically, existing approaches first retrieve external evidence and then perform commonsense reasoning using these evidence. In this paper, we propose a Self-supervised Bidirectional Encoder Representation Learning of Commonsense (elBERto) framework, which is compatible with off-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09424v1-abstract-full').style.display = 'inline'; document.getElementById('2203.09424v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.09424v1-abstract-full" style="display: none;">
        Commonsense question answering requires reasoning about everyday situations and causes and effects implicit in context. Typically, existing approaches first retrieve external evidence and then perform commonsense reasoning using these evidence. In this paper, we propose a Self-supervised Bidirectional Encoder Representation Learning of Commonsense (elBERto) framework, which is compatible with off-the-shelf QA model architectures. The framework comprises five self-supervised tasks to force the model to fully exploit the additional training signals from contexts containing rich commonsense. The tasks include a novel Contrastive Relation Learning task to encourage the model to distinguish between logically contrastive contexts, a new Jigsaw Puzzle task that requires the model to infer logical chains in long contexts, and three classic SSL tasks to maintain pre-trained models language encoding ability. On the representative WIQA, CosmosQA, and ReClor datasets, elBERto outperforms all other methods, including those utilizing explicit graph reasoning and external knowledge retrieval. Moreover, elBERto achieves substantial improvements on out-of-paragraph and no-effect questions where simple lexical similarity comparison does not help, indicating that it successfully learns commonsense and is able to leverage it when given dynamic context.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09424v1-abstract-full').style.display = 'none'; document.getElementById('2203.09424v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.04042">arXiv:2203.04042</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.04042">pdf</a>, <a href="https://arxiv.org/format/2203.04042">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Abandoning the Bayer-Filter to See in the Dark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xingbo Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+W">Wanyan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Miao%2C+Z">Zhihui Miao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+L">Lan Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jiewen Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+Z">Zhe Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Teoh%2C+A+B+J">Andrew Beng Jin Teoh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+J">Jiajun Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.04042v2-abstract-short" style="display: inline;">
        Low-light image enhancement - a pervasive but challenging problem, plays a central role in enhancing the visibility of an image captured in a poor illumination environment. Due to the fact that not all photons can pass the Bayer-Filter on the sensor of the color camera, in this work, we first present a De-Bayer-Filter simulator based on deep neural networks to generate a monochrome raw image from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04042v2-abstract-full').style.display = 'inline'; document.getElementById('2203.04042v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.04042v2-abstract-full" style="display: none;">
        Low-light image enhancement - a pervasive but challenging problem, plays a central role in enhancing the visibility of an image captured in a poor illumination environment. Due to the fact that not all photons can pass the Bayer-Filter on the sensor of the color camera, in this work, we first present a De-Bayer-Filter simulator based on deep neural networks to generate a monochrome raw image from the colored raw image. Next, a fully convolutional network is proposed to achieve the low-light image enhancement by fusing colored raw data with synthesized monochrome raw data. Channel-wise attention is also introduced to the fusion process to establish a complementary interaction between features from colored and monochrome raw images. To train the convolutional networks, we propose a dataset with monochrome and color raw pairs named Mono-Colored Raw paired dataset (MCR) collected by using a monochrome camera without Bayer-Filter and a color camera with Bayer-Filter. The proposed pipeline take advantages of the fusion of the virtual monochrome and the color raw images and our extensive experiments indicate that significant improvement can be achieved by leveraging raw sensor data and data-driven learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04042v2-abstract-full').style.display = 'none'; document.getElementById('2203.04042v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.04041">arXiv:2203.04041</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.04041">pdf</a>, <a href="https://arxiv.org/format/2203.04041">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Shape-invariant 3D Adversarial Point Clouds
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Q">Qidong Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaoyi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Dongdong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+H">Hang Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+W">Weiming Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+N">Nenghai Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.04041v2-abstract-short" style="display: inline;">
        Adversary and invisibility are two fundamental but conflict characters of adversarial perturbations. Previous adversarial attacks on 3D point cloud recognition have often been criticized for their noticeable point outliers, since they just involve an &#34;implicit constrain&#34; like global distance loss in the time-consuming optimization to limit the generated noise. While point cloud is a highly structu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04041v2-abstract-full').style.display = 'inline'; document.getElementById('2203.04041v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.04041v2-abstract-full" style="display: none;">
        Adversary and invisibility are two fundamental but conflict characters of adversarial perturbations. Previous adversarial attacks on 3D point cloud recognition have often been criticized for their noticeable point outliers, since they just involve an &#34;implicit constrain&#34; like global distance loss in the time-consuming optimization to limit the generated noise. While point cloud is a highly structured data format, it is hard to constrain its perturbation with a simple loss or metric properly. In this paper, we propose a novel Point-Cloud Sensitivity Map to boost both the efficiency and imperceptibility of point perturbations. This map reveals the vulnerability of point cloud recognition models when encountering shape-invariant adversarial noises. These noises are designed along the shape surface with an &#34;explicit constrain&#34; instead of extra distance loss. Specifically, we first apply a reversible coordinate transformation on each point of the point cloud input, to reduce one degree of point freedom and limit its movement on the tangent plane. Then we calculate the best attacking direction with the gradients of the transformed point cloud obtained on the white-box model. Finally we assign each point with a non-negative score to construct the sensitivity map, which benefits both white-box adversarial invisibility and black-box query-efficiency extended in our work. Extensive evaluations prove that our method can achieve the superior performance on various point cloud recognition models, with its satisfying adversarial imperceptibility and strong resistance to different point cloud defense settings. Our code is available at: https://github.com/shikiw/SI-Adv.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04041v2-abstract-full').style.display = 'none'; document.getElementById('2203.04041v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.01584">arXiv:2203.01584</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.01584">pdf</a>, <a href="https://arxiv.org/format/2203.01584">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhibo Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowei Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+H">Henry Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhifei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chiu%2C+W">Weifeng Chiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+T">Tao Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+K">Kui Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.01584v1-abstract-short" style="display: inline;">
        Prioritizing fairness is of central importance in artificial intelligence (AI) systems, especially for those societal applications, e.g., hiring systems should recommend applicants equally from different demographic groups, and risk assessment systems must eliminate racism in criminal justice. Existing efforts towards the ethical development of AI systems have leveraged data science to mitigate bi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01584v1-abstract-full').style.display = 'inline'; document.getElementById('2203.01584v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.01584v1-abstract-full" style="display: none;">
        Prioritizing fairness is of central importance in artificial intelligence (AI) systems, especially for those societal applications, e.g., hiring systems should recommend applicants equally from different demographic groups, and risk assessment systems must eliminate racism in criminal justice. Existing efforts towards the ethical development of AI systems have leveraged data science to mitigate biases in the training set or introduced fairness principles into the training process. For a deployed AI system, however, it may not allow for retraining or tuning in practice. By contrast, we propose a more flexible approach, i.e., fairness-aware adversarial perturbation (FAAP), which learns to perturb input data to blind deployed models on fairness-related features, e.g., gender and ethnicity. The key advantage is that FAAP does not modify deployed models in terms of parameters and structures. To achieve this, we design a discriminator to distinguish fairness-related attributes based on latent representations from deployed models. Meanwhile, a perturbation generator is trained against the discriminator, such that no fairness-related features could be extracted from perturbed inputs. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed FAAP. In addition, FAAP is validated on real-world commercial deployments (inaccessible to model parameters), which shows the transferability of FAAP, foreseeing the potential of black-box adaptation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01584v1-abstract-full').style.display = 'none'; document.getElementById('2203.01584v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.01318">arXiv:2203.01318</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.01318">pdf</a>, <a href="https://arxiv.org/format/2203.01318">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Protecting Celebrities from DeepFake with Identity Consistency Transformer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaoyi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bao%2C+J">Jianmin Bao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Dongdong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+T">Ting Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+W">Weiming Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+N">Nenghai Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Dong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+F">Fang Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+B">Baining Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.01318v3-abstract-short" style="display: inline;">
        In this work we propose Identity Consistency Transformer, a novel face forgery detection method that focuses on high-level semantics, specifically identity information, and detecting a suspect face by finding identity inconsistency in inner and outer face regions. The Identity Consistency Transformer incorporates a consistency loss for identity consistency determination. We show that Identity Cons&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01318v3-abstract-full').style.display = 'inline'; document.getElementById('2203.01318v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.01318v3-abstract-full" style="display: none;">
        In this work we propose Identity Consistency Transformer, a novel face forgery detection method that focuses on high-level semantics, specifically identity information, and detecting a suspect face by finding identity inconsistency in inner and outer face regions. The Identity Consistency Transformer incorporates a consistency loss for identity consistency determination. We show that Identity Consistency Transformer exhibits superior generalization ability not only across different datasets but also across various types of image degradation forms found in real-world applications including deepfake videos. The Identity Consistency Transformer can be easily enhanced with additional identity information when such information is available, and for this reason it is especially well-suited for detecting face forgeries involving celebrities. Code will be released at \url{https://github.com/LightDXY/ICT_DeepFake}
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01318v3-abstract-full').style.display = 'none'; document.getElementById('2203.01318v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To Appear at CVPR 2022, code is available at https://github.com/LightDXY/ICT_DeepFake</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.01095">arXiv:2203.01095</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.01095">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Generalized Approach for Cancellable Template and Its Realization for Minutia Cylinder-Code
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xingbo Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+Z">Zhe Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wong%2C+K">KokSheik Wong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.01095v1-abstract-short" style="display: inline;">
        Hashing technology gains much attention in protecting the biometric template lately. For instance, Index-of-Max (IoM), a recent reported hashing technique, is a ranking-based locality sensitive hashing technique, which illustrates the feasibility to protect the ordered and fixed-length biometric template. However, biometric templates are not always in the form of ordered and fixed-length, rather i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01095v1-abstract-full').style.display = 'inline'; document.getElementById('2203.01095v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.01095v1-abstract-full" style="display: none;">
        Hashing technology gains much attention in protecting the biometric template lately. For instance, Index-of-Max (IoM), a recent reported hashing technique, is a ranking-based locality sensitive hashing technique, which illustrates the feasibility to protect the ordered and fixed-length biometric template. However, biometric templates are not always in the form of ordered and fixed-length, rather it may be an unordered and variable size point set e.g. fingerprint minutiae, which restricts the usage of the traditional hashing technology. In this paper, we proposed a generalized version of IoM hashing namely gIoM, and therefore the unordered and variable size biometric template can be used. We demonstrate a realization using a well-known variable size feature vector, fingerprint Minutia Cylinder-Code (MCC). The gIoM transforms MCC into index domain to form indexing-based feature representation. Consequently, the inversion of MCC from the transformed representation is computational infeasible, thus to achieve non-invertibility while the performance is preserved. Public fingerprint databases FVC2002 and FVC2004 are employed for experiment as benchmark to demonstrate a fair comparison with other methods. Moreover, the security and privacy analysis suggest that gIoM meets the criteria of template protection: non-invertibility, revocability, and non-linkability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01095v1-abstract-full').style.display = 'none'; document.getElementById('2203.01095v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.09747">arXiv:2202.09747</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.09747">pdf</a>, <a href="https://arxiv.org/format/2202.09747">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PGE: Robust Product Graph Embedding Learning for Error Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+K">Kewei Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+Y+E">Yifan Ethan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X+L">Xin Luna Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+Y">Yizhou Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.09747v1-abstract-short" style="display: inline;">
        Although product graphs (PGs) have gained increasing attentions in recent years for their successful applications in product search and recommendations, the extensive power of PGs can be limited by the inevitable involvement of various kinds of errors. Thus, it is critical to validate the correctness of triples in PGs to improve their reliability. Knowledge graph (KG) embedding methods have strong&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.09747v1-abstract-full').style.display = 'inline'; document.getElementById('2202.09747v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.09747v1-abstract-full" style="display: none;">
        Although product graphs (PGs) have gained increasing attentions in recent years for their successful applications in product search and recommendations, the extensive power of PGs can be limited by the inevitable involvement of various kinds of errors. Thus, it is critical to validate the correctness of triples in PGs to improve their reliability. Knowledge graph (KG) embedding methods have strong error detection abilities. Yet, existing KG embedding methods may not be directly applicable to a PG due to its distinct characteristics: (1) PG contains rich textual signals, which necessitates a joint exploration of both text information and graph structure; (2) PG contains a large number of attribute triples, in which attribute values are represented by free texts. Since free texts are too flexible to define entities in KGs, traditional way to map entities to their embeddings using ids is no longer appropriate for attribute value representation; (3) Noisy triples in a PG mislead the embedding learning and significantly hurt the performance of error detection. To address the aforementioned challenges, we propose an end-to-end noise-tolerant embedding learning framework, PGE, to jointly leverage both text information and graph structure in PG to learn embeddings for error detection. Experimental results on real-world product graph demonstrate the effectiveness of the proposed framework comparing with the state-of-the-art approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.09747v1-abstract-full').style.display = 'none'; document.getElementById('2202.09747v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.08069">arXiv:2202.08069</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.08069">pdf</a>, <a href="https://arxiv.org/ps/2202.08069">ps</a>, <a href="https://arxiv.org/format/2202.08069">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VLDB 2021: Designing a Hybrid Conference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=T%C3%B6z%C3%BCn%2C+P">Pınar Tözün</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Naumann%2C+F">Felix Naumann</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bonnet%2C+P">Philippe Bonnet</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X+L">Xin Luna Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.08069v1-abstract-short" style="display: inline;">
        In 2020, while main database conferences one by one had to adopt a virtual format as a result of the ongoing COVID-19 pandemic, we decided to hold VLDB 2021 in hybrid format. This paper describes how we defined the hybrid format for VLDB 2021 going through the key design decisions. In addition, we list the lessons learned from running such a conference. Our goal is to share this knowledge with fel&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.08069v1-abstract-full').style.display = 'inline'; document.getElementById('2202.08069v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.08069v1-abstract-full" style="display: none;">
        In 2020, while main database conferences one by one had to adopt a virtual format as a result of the ongoing COVID-19 pandemic, we decided to hold VLDB 2021 in hybrid format. This paper describes how we defined the hybrid format for VLDB 2021 going through the key design decisions. In addition, we list the lessons learned from running such a conference. Our goal is to share this knowledge with fellow conference organizers who target a hybrid conference format as well, which is on its way to becoming the norm rather than the exception. For readers who are more interested in the highlights rather than details, a short version of this report appears in SIGMOD Record.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.08069v1-abstract-full').style.display = 'none'; document.getElementById('2202.08069v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.02606">arXiv:2202.02606</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ROMNet: Renovate the Old Memories
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+R">Runsheng Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+Z">Zhengzhong Tu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+Y">Yuanqi Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaoyu Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jinlong Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meng%2C+Z">Zibo Meng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+J">Jiaqi Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=YU%2C+H">Hongkai YU</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.02606v2-abstract-short" style="display: inline;">
        Renovating the memories in old photos is an intriguing research topic in computer vision fields. These legacy images often suffer from severe and commingled degradations such as cracks, noise, and color-fading, while lack of large-scale paired old photo datasets makes this restoration task very challenging. In this work, we present a novel reference-based end-to-end learning framework that can joi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.02606v2-abstract-full').style.display = 'inline'; document.getElementById('2202.02606v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.02606v2-abstract-full" style="display: none;">
        Renovating the memories in old photos is an intriguing research topic in computer vision fields. These legacy images often suffer from severe and commingled degradations such as cracks, noise, and color-fading, while lack of large-scale paired old photo datasets makes this restoration task very challenging. In this work, we present a novel reference-based end-to-end learning framework that can jointly repair and colorize the degraded legacy pictures. Specifically, the proposed framework consists of three modules: a restoration sub-network for degradation restoration, a similarity sub-network for color histogram matching and transfer, and a colorization subnet that learns to predict the chroma elements of the images conditioned on chromatic reference signals. The whole system takes advantage of the color histogram priors in a given reference image, which vastly reduces the dependency on large-scale training data. Apart from the proposed method, we also create, to our knowledge, the first public and real-world old photo dataset with paired ground truth for evaluating old photo restoration models, wherein each old photo is paired with a manually restored pristine image by PhotoShop experts. Our extensive experiments conducted on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-arts both quantitatively and qualitatively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.02606v2-abstract-full').style.display = 'none'; document.getElementById('2202.02606v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper major revision</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.09490">arXiv:2201.09490</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.09490">pdf</a>, <a href="https://arxiv.org/format/2201.09490">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dual Preference Distribution Learning for Item Recommendation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xue Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+X">Xuemeng Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+N">Na Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Y">Yinwei Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Z">Zhongzhou Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+H">Hongjun Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.09490v1-abstract-short" style="display: inline;">
        Recommender systems can automatically recommend users items that they probably like, for which the goal is to represent the user and item as well as model their interaction. Existing methods have primarily learned the user&#39;s preferences and item&#39;s features with vectorized representations, and modeled the user-item interaction by the similarity of their representations. In fact, the user&#39;s differen&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.09490v1-abstract-full').style.display = 'inline'; document.getElementById('2201.09490v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.09490v1-abstract-full" style="display: none;">
        Recommender systems can automatically recommend users items that they probably like, for which the goal is to represent the user and item as well as model their interaction. Existing methods have primarily learned the user&#39;s preferences and item&#39;s features with vectorized representations, and modeled the user-item interaction by the similarity of their representations. In fact, the user&#39;s different preferences are related and capturing such relations could better understand the user&#39;s preferences for a better recommendation. Toward this end, we propose to represent the user&#39;s preference with multi-variant Gaussian distribution, and model the user-item interaction by calculating the probability density at the item in the user&#39;s preference distribution. In this manner, the mean vector of the Gaussian distribution is able to capture the center of the user&#39;s preferences, while its covariance matrix captures the relations of these preferences. In particular, in this work, we propose a dual preference distribution learning framework (DUPLE), which captures the user&#39;s preferences to both the items and attributes by a Gaussian distribution, respectively. As a byproduct, identifying the user&#39;s preference to specific attributes enables us to provide the explanation of recommending an item to the user. Extensive quantitative and qualitative experiments on six public datasets show that DUPLE achieves the best performance over all state-of-the-art recommendation methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.09490v1-abstract-full').style.display = 'none'; document.getElementById('2201.09490v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 5 figures. This manuscript has been submitted to IEEE TKDE</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          H.3
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.04729">arXiv:2201.04729</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.04729">pdf</a>, <a href="https://arxiv.org/format/2201.04729">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Local2Global: A distributed approach for scaling representation learning on graphs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jeub%2C+L+G+S">Lucas G. S. Jeub</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Colavizza%2C+G">Giovanni Colavizza</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowen Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bazzi%2C+M">Marya Bazzi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cucuringu%2C+M">Mihai Cucuringu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.04729v1-abstract-short" style="display: inline;">
        We propose a decentralised &#34;local2global&#34;&#39; approach to graph representation learning, that one can a-priori use to scale any embedding technique. Our local2global approach proceeds by first dividing the input graph into overlapping subgraphs (or &#34;patches&#34;) and training local representations for each patch independently. In a second step, we combine the local representations into a globally consist&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.04729v1-abstract-full').style.display = 'inline'; document.getElementById('2201.04729v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.04729v1-abstract-full" style="display: none;">
        We propose a decentralised &#34;local2global&#34;&#39; approach to graph representation learning, that one can a-priori use to scale any embedding technique. Our local2global approach proceeds by first dividing the input graph into overlapping subgraphs (or &#34;patches&#34;) and training local representations for each patch independently. In a second step, we combine the local representations into a globally consistent representation by estimating the set of rigid motions that best align the local representations using information from the patch overlaps, via group synchronization. A key distinguishing feature of local2global relative to existing work is that patches are trained independently without the need for the often costly parameter synchronization during distributed training. This allows local2global to scale to large-scale industrial applications, where the input graph may not even fit into memory and may be stored in a distributed manner. We apply local2global on data sets of different sizes and show that our approach achieves a good trade-off between scale and accuracy on edge reconstruction and semi-supervised classification. We also consider the downstream task of anomaly detection and show how one can use local2global to highlight anomalies in cybersecurity networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.04729v1-abstract-full').style.display = 'none'; document.getElementById('2201.04729v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages, 6 figures. arXiv admin note: substantial text overlap with arXiv:2107.12224</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.02048">arXiv:2201.02048</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.02048">pdf</a>, <a href="https://arxiv.org/format/2201.02048">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Integrating Human-in-the-loop into Swarm Learning for Decentralized Fake News Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xishuang Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+L">Lijun Qian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.02048v1-abstract-short" style="display: inline;">
        Social media has become an effective platform to generate and spread fake news that can mislead people and even distort public opinion. Centralized methods for fake news detection, however, cannot effectively protect user privacy during the process of centralized data collection for training models. Moreover, it cannot fully involve user feedback in the loop of learning detection models for furthe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.02048v1-abstract-full').style.display = 'inline'; document.getElementById('2201.02048v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.02048v1-abstract-full" style="display: none;">
        Social media has become an effective platform to generate and spread fake news that can mislead people and even distort public opinion. Centralized methods for fake news detection, however, cannot effectively protect user privacy during the process of centralized data collection for training models. Moreover, it cannot fully involve user feedback in the loop of learning detection models for further enhancing fake news detection. To overcome these challenges, this paper proposed a novel decentralized method, Human-in-the-loop Based Swarm Learning (HBSL), to integrate user feedback into the loop of learning and inference for recognizing fake news without violating user privacy in a decentralized manner. It consists of distributed nodes that are able to independently learn and detect fake news on local data. Furthermore, detection models trained on these nodes can be enhanced through decentralized model merging. Experimental results demonstrate that the proposed method outperforms the state-of-the-art decentralized method in regard of detecting fake news on a benchmark dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.02048v1-abstract-full').style.display = 'none'; document.getElementById('2201.02048v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.11668">arXiv:2112.11668</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.11668">pdf</a>, <a href="https://arxiv.org/format/2112.11668">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xinhsuai Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+M">Min Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+S">Shuicheng Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Hanwang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.11668v1-abstract-short" style="display: inline;">
        The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.11668v1-abstract-full').style.display = 'inline'; document.getElementById('2112.11668v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.11668v1-abstract-full" style="display: none;">
        The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.11668v1-abstract-full').style.display = 'none'; document.getElementById('2112.11668v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NeurIPS-2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.09245">arXiv:2112.09245</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.09245">pdf</a>, <a href="https://arxiv.org/format/2112.09245">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automated Deep Learning: Neural Architecture Search Is Not the End
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xuanyi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kedziora%2C+D+J">David Jacob Kedziora</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Musial%2C+K">Katarzyna Musial</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gabrys%2C+B">Bogdan Gabrys</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.09245v2-abstract-short" style="display: inline;">
        Deep learning (DL) has proven to be a highly effective approach for developing models in diverse contexts, including visual perception, speech recognition, and machine translation. However, the end-to-end process for applying DL is not trivial. It requires grappling with problem formulation and context understanding, data engineering, model development, deployment, continuous monitoring and mainte&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.09245v2-abstract-full').style.display = 'inline'; document.getElementById('2112.09245v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.09245v2-abstract-full" style="display: none;">
        Deep learning (DL) has proven to be a highly effective approach for developing models in diverse contexts, including visual perception, speech recognition, and machine translation. However, the end-to-end process for applying DL is not trivial. It requires grappling with problem formulation and context understanding, data engineering, model development, deployment, continuous monitoring and maintenance, and so on. Moreover, each of these steps typically relies heavily on humans, in terms of both knowledge and interactions, which impedes the further advancement and democratization of DL. Consequently, in response to these issues, a new field has emerged over the last few years: automated deep learning (AutoDL). This endeavor seeks to minimize the need for human involvement and is best known for its achievements in neural architecture search (NAS), a topic that has been the focus of several surveys. That stated, NAS is not the be-all and end-all of AutoDL. Accordingly, this review adopts an overarching perspective, examining research efforts into automation across the entirety of an archetypal DL workflow. In so doing, this work also proposes a comprehensive set of ten criteria by which to assess existing work in both individual publications and broader research areas. These criteria are: novelty, solution quality, efficiency, stability, interpretability, reproducibility, engineering quality, scalability, generalizability, and eco-friendliness. Thus, ultimately, this review provides an evaluative overview of AutoDL in the early 2020s, identifying where future opportunities for progress may exist.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.09245v2-abstract-full').style.display = 'none'; document.getElementById('2112.09245v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">65 pages, 9 tables, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.14522">arXiv:2111.14522</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.14522">pdf</a>, <a href="https://arxiv.org/format/2111.14522">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding over-squashing and bottlenecks on graphs via curvature
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Topping%2C+J">Jake Topping</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Di+Giovanni%2C+F">Francesco Di Giovanni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chamberlain%2C+B+P">Benjamin Paul Chamberlain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowen Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bronstein%2C+M+M">Michael M. Bronstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.14522v2-abstract-short" style="display: inline;">
        Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as &#39;over-squashing&#39;, has been heuristically attributed to gr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.14522v2-abstract-full').style.display = 'inline'; document.getElementById('2111.14522v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.14522v2-abstract-full" style="display: none;">
        Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as &#39;over-squashing&#39;, has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.14522v2-abstract-full').style.display = 'none'; document.getElementById('2111.14522v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.14281">arXiv:2111.14281</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.14281">pdf</a>, <a href="https://arxiv.org/format/2111.14281">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Passive Indoor Localization with WiFi Fingerprints
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hoang%2C+M+T">Minh Tu Hoang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuen%2C+B">Brosnan Yuen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+K">Kai Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Elmoogy%2C+A">Ahmed Elmoogy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaodai Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+T">Tao Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Westendorp%2C+R">Robert Westendorp</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tarimala%2C+K+R">Kishore Reddy Tarimala</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.14281v1-abstract-short" style="display: inline;">
        This paper proposes passive WiFi indoor localization. Instead of using WiFi signals received by mobile devices as fingerprints, we use signals received by routers to locate the mobile carrier. Consequently, software installation on the mobile device is not required. To resolve the data insufficiency problem, flow control signals such as request to send (RTS) and clear to send (CTS) are utilized. I&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.14281v1-abstract-full').style.display = 'inline'; document.getElementById('2111.14281v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.14281v1-abstract-full" style="display: none;">
        This paper proposes passive WiFi indoor localization. Instead of using WiFi signals received by mobile devices as fingerprints, we use signals received by routers to locate the mobile carrier. Consequently, software installation on the mobile device is not required. To resolve the data insufficiency problem, flow control signals such as request to send (RTS) and clear to send (CTS) are utilized. In our model, received signal strength indicator (RSSI) and channel state information (CSI) are used as fingerprints for several algorithms, including deterministic, probabilistic and neural networks localization algorithms. We further investigated localization algorithms performance through extensive on-site experiments with various models of phones at hundreds of testing locations. We demonstrate that our passive scheme achieves an average localization error of 0.8 m when the phone is actively transmitting data frames and 1.5 m when it is not transmitting data frames.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.14281v1-abstract-full').style.display = 'none'; document.getElementById('2111.14281v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 9 figures, data is availabe in IEEE portal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.12710">arXiv:2111.12710</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.12710">pdf</a>, <a href="https://arxiv.org/format/2111.12710">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaoyi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bao%2C+J">Jianmin Bao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+T">Ting Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Dongdong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+W">Weiming Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+L">Lu Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Dong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+F">Fang Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+N">Nenghai Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.12710v2-abstract-short" style="display: inline;">
        This paper explores a better codebook for BERT pre-training of vision transformers. The recent work BEiT successfully transfers BERT pre-training from NLP to the vision field. It directly adopts one simple discrete VAE as the visual tokenizer, but has not considered the semantic level of the resulting visual tokens. By contrast, the discrete tokens in NLP field are naturally highly semantic. This&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12710v2-abstract-full').style.display = 'inline'; document.getElementById('2111.12710v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.12710v2-abstract-full" style="display: none;">
        This paper explores a better codebook for BERT pre-training of vision transformers. The recent work BEiT successfully transfers BERT pre-training from NLP to the vision field. It directly adopts one simple discrete VAE as the visual tokenizer, but has not considered the semantic level of the resulting visual tokens. By contrast, the discrete tokens in NLP field are naturally highly semantic. This difference motivates us to learn a perceptual codebook. And we surprisingly find one simple yet effective idea: enforcing perceptual similarity during the dVAE training. We demonstrate that the visual tokens generated by the proposed perceptual codebook do exhibit better semantic meanings, and subsequently help pre-training achieve superior transfer performance in various downstream tasks. For example, we achieve 84.5% Top-1 accuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive method BEiT by +1.3 with the same pre-training epochs. It can also improve the performance of object detection and segmentation tasks on COCO val by +1.3 box AP and +1.0 mask AP, semantic segmentation on ADE20k by +1.0 mIoU. Equipped with a larger backbone ViT-H, we achieve the state-of-the-art performance (88.3% Top-1 accuracy) among the methods using only ImageNet-1K data. The code and models will be available at https://github.com/microsoft/PeCo.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12710v2-abstract-full').style.display = 'none'; document.getElementById('2111.12710v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Updated SOTA 88.3% Top-1 Acc on ViT-H model, and ablation about tokenzier size</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.12334">arXiv:2111.12334</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.12334">pdf</a>, <a href="https://arxiv.org/format/2111.12334">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MobileXNet: An Efficient Convolutional Neural Network for Monocular Depth Estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xingshuai Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garratt%2C+M+A">Matthew A. Garratt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anavatti%2C+S+G">Sreenatha G. Anavatti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abbass%2C+H+A">Hussein A. Abbass</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.12334v1-abstract-short" style="display: inline;">
        Depth is a vital piece of information for autonomous vehicles to perceive obstacles. Due to the relatively low price and small size of monocular cameras, depth estimation from a single RGB image has attracted great interest in the research community. In recent years, the application of Deep Neural Networks (DNNs) has significantly boosted the accuracy of monocular depth estimation (MDE). State-of-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12334v1-abstract-full').style.display = 'inline'; document.getElementById('2111.12334v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.12334v1-abstract-full" style="display: none;">
        Depth is a vital piece of information for autonomous vehicles to perceive obstacles. Due to the relatively low price and small size of monocular cameras, depth estimation from a single RGB image has attracted great interest in the research community. In recent years, the application of Deep Neural Networks (DNNs) has significantly boosted the accuracy of monocular depth estimation (MDE). State-of-the-art methods are usually designed on top of complex and extremely deep network architectures, which require more computational resources and cannot run in real-time without using high-end GPUs. Although some researchers tried to accelerate the running speed, the accuracy of depth estimation is degraded because the compressed model does not represent images well. In addition, the inherent characteristic of the feature extractor used by the existing approaches results in severe spatial information loss in the produced feature maps, which also impairs the accuracy of depth estimation on small sized images. In this study, we are motivated to design a novel and efficient Convolutional Neural Network (CNN) that assembles two shallow encoder-decoder style subnetworks in succession to address these problems. In particular, we place our emphasis on the trade-off between the accuracy and speed of MDE. Extensive experiments have been conducted on the NYU depth v2, KITTI, Make3D and Unreal data sets. Compared with the state-of-the-art approaches which have an extremely deep and complex architecture, the proposed network not only achieves comparable performance but also runs at a much faster speed on a single, less powerful GPU.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12334v1-abstract-full').style.display = 'none'; document.getElementById('2111.12334v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.12128">arXiv:2111.12128</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.12128">pdf</a>, <a href="https://arxiv.org/format/2111.12128">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Unreasonable Effectiveness of Feature propagation in Learning on Graphs with Missing Node Features
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Rossi%2C+E">Emanuele Rossi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kenlay%2C+H">Henry Kenlay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gorinova%2C+M+I">Maria I. Gorinova</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chamberlain%2C+B+P">Benjamin Paul Chamberlain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowen Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bronstein%2C+M">Michael Bronstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.12128v2-abstract-short" style="display: inline;">
        While Graph Neural Networks (GNNs) have recently become the de facto standard for modeling relational data, they impose a strong assumption on the availability of the node or edge features of the graph. In many real-world applications, however, features are only partially available; for example, in social networks, age and gender are available only for a small subset of users. We present a general&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12128v2-abstract-full').style.display = 'inline'; document.getElementById('2111.12128v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.12128v2-abstract-full" style="display: none;">
        While Graph Neural Networks (GNNs) have recently become the de facto standard for modeling relational data, they impose a strong assumption on the availability of the node or edge features of the graph. In many real-world applications, however, features are only partially available; for example, in social networks, age and gender are available only for a small subset of users. We present a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which we call Feature Propagation. We experimentally show that the proposed approach outperforms previous methods on seven common node-classification benchmarks and can withstand surprisingly high rates of missing features: on average we observe only around 4% relative accuracy drop when 99% of the features are missing. Moreover, it takes only 10 seconds to run on a graph with $\sim$2.5M nodes and $\sim$123M edges on a single GPU.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12128v2-abstract-full').style.display = 'none'; document.getElementById('2111.12128v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.08600">arXiv:2111.08600</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.08600">pdf</a>, <a href="https://arxiv.org/format/2111.08600">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Real-Time Monocular Depth Estimation for Robotics: A Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xingshuai Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garratt%2C+M+A">Matthew A. Garratt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anavatti%2C+S+G">Sreenatha G. Anavatti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abbass%2C+H+A">Hussein A. Abbass</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.08600v1-abstract-short" style="display: inline;">
        As an essential component for many autonomous driving and robotic activities such as ego-motion estimation, obstacle avoidance and scene understanding, monocular depth estimation (MDE) has attracted great attention from the computer vision and robotics communities. Over the past decades, a large number of methods have been developed. To the best of our knowledge, however, there is not a comprehens&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08600v1-abstract-full').style.display = 'inline'; document.getElementById('2111.08600v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.08600v1-abstract-full" style="display: none;">
        As an essential component for many autonomous driving and robotic activities such as ego-motion estimation, obstacle avoidance and scene understanding, monocular depth estimation (MDE) has attracted great attention from the computer vision and robotics communities. Over the past decades, a large number of methods have been developed. To the best of our knowledge, however, there is not a comprehensive survey of MDE. This paper aims to bridge this gap by reviewing 197 relevant articles published between 1970 and 2021. In particular, we provide a comprehensive survey of MDE covering various methods, introduce the popular performance evaluation metrics and summarize publically available datasets. We also summarize available open-source implementations of some representative methods and compare their performances. Furthermore, we review the application of MDE in some important robotic tasks. Finally, we conclude this paper by presenting some promising directions for future research. This survey is expected to assist readers to navigate this research field.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08600v1-abstract-full').style.display = 'none'; document.getElementById('2111.08600v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.05897">arXiv:2111.05897</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.05897">pdf</a>, <a href="https://arxiv.org/format/2111.05897">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lian%2C+X">Xiangru Lian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+B">Binhang Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+X">Xuefeng Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yulong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Y">Yongjun He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Honghuan Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+L">Lei Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+H">Haodong Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Chengjun Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xing Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+Y">Yiqiao Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+M">Mingnan Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+C">Congfei Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+J">Jingru Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Haonan Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+L">Lei Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+R">Renjie Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+J">Jianying Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shu%2C+C">Chengchun Shu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiu%2C+X">Xuezhong Qiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhishan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kong%2C+D">Dongying Kong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+L">Lei Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+H">Hai Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+S">Sen Yang</a>
      , et al. (2 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.05897v3-abstract-short" style="display: inline;">
        Deep learning based models have dominated the current landscape of production recommender systems. Furthermore, recent years have witnessed an exponential growth of the model scale--from Google&#39;s 2016 model with 1 billion parameters to the latest Facebook&#39;s model with 12 trillion parameters. Significant quality boost has come with each jump of the model capacity, which makes us believe the era of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.05897v3-abstract-full').style.display = 'inline'; document.getElementById('2111.05897v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.05897v3-abstract-full" style="display: none;">
        Deep learning based models have dominated the current landscape of production recommender systems. Furthermore, recent years have witnessed an exponential growth of the model scale--from Google&#39;s 2016 model with 1 billion parameters to the latest Facebook&#39;s model with 12 trillion parameters. Significant quality boost has come with each jump of the model capacity, which makes us believe the era of 100 trillion parameters is around the corner. However, the training of such models is challenging even within industrial scale data centers. This difficulty is inherited from the staggering heterogeneity of the training computation--the model&#39;s embedding layer could include more than 99.99% of the total model size, which is extremely memory-intensive; while the rest neural network is increasingly computation-intensive. To support the training of such huge models, an efficient distributed training system is in urgent need. In this paper, we resolve this challenge by careful co-design of both the optimization algorithm and the distributed system architecture. Specifically, in order to ensure both the training efficiency and the training accuracy, we design a novel hybrid training algorithm, where the embedding layer and the dense neural network are handled by different synchronization mechanisms; then we build a system called Persia (short for parallel recommendation training system with hybrid acceleration) to support this hybrid training algorithm. Both theoretical demonstration and empirical study up to 100 trillion parameters have conducted to justified the system design and implementation of Persia. We make Persia publicly available (at https://github.com/PersiaML/Persia) so that anyone would be able to easily train a recommender model at the scale of 100 trillion parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.05897v3-abstract-full').style.display = 'none'; document.getElementById('2111.05897v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.03281">arXiv:2111.03281</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.03281">pdf</a>, <a href="https://arxiv.org/format/2111.03281">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Recognizing Vector Graphics without Rasterization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+X">Xinyang Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+L">Lu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shan%2C+C">Caihua Shan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yifei Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xuanyi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+D">Dongsheng Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.03281v3-abstract-short" style="display: inline;">
        In this paper, we consider a different data format for images: vector graphics. In contrast to raster graphics which are widely used in image recognition, vector graphics can be scaled up or down into any resolution without aliasing or information loss, due to the analytic representation of the primitives in the document. Furthermore, vector graphics are able to give extra structural information o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03281v3-abstract-full').style.display = 'inline'; document.getElementById('2111.03281v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.03281v3-abstract-full" style="display: none;">
        In this paper, we consider a different data format for images: vector graphics. In contrast to raster graphics which are widely used in image recognition, vector graphics can be scaled up or down into any resolution without aliasing or information loss, due to the analytic representation of the primitives in the document. Furthermore, vector graphics are able to give extra structural information on how low-level elements group together to form high level shapes or structures. These merits of graphic vectors have not been fully leveraged in existing methods. To explore this data format, we target on the fundamental recognition tasks: object localization and classification. We propose an efficient CNN-free pipeline that does not render the graphic into pixels (i.e. rasterization), and takes textual document of the vector graphics as input, called YOLaT (You Only Look at Text). YOLaT builds multi-graphs to model the structural and spatial information in vector graphics, and a dual-stream graph neural network is proposed to detect objects from the graph. Our experiments show that by directly operating on vector graphics, YOLaT out-performs raster-graphic based object detection baselines in terms of both average precision and efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03281v3-abstract-full').style.display = 'none'; document.getElementById('2111.03281v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NeurIPS2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.02842">arXiv:2111.02842</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.02842">pdf</a>, <a href="https://arxiv.org/format/2111.02842">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks on Graph Classification via Bayesian Optimisation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wan%2C+X">Xingchen Wan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kenlay%2C+H">Henry Kenlay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ru%2C+B">Binxin Ru</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Blaas%2C+A">Arno Blaas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Osborne%2C+M+A">Michael A. Osborne</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowen Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.02842v1-abstract-short" style="display: inline;">
        Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous re&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.02842v1-abstract-full').style.display = 'inline'; document.getElementById('2111.02842v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.02842v1-abstract-full" style="display: none;">
        Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.02842v1-abstract-full').style.display = 'none'; document.getElementById('2111.02842v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021. 11 pages, 8 figures, 2 tables (24 pages, 17 figures, 8 tables including references and appendices)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.15720">arXiv:2110.15720</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.15720">pdf</a>, <a href="https://arxiv.org/format/2110.15720">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Weakly Supervised Concept Map Generation through Task-Guided Graph Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+J">Jiaying Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiangjue Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+C">Carl Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.15720v2-abstract-short" style="display: inline;">
        Recent years have witnessed the rapid development of concept map generation techniques due to their advantages in providing well-structured summarization of knowledge from free texts. Traditional unsupervised methods do not generate task-oriented concept maps, whereas deep generative models require large amounts of training data. In this work, we present GT-D2G (Graph Translation based Document-To&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15720v2-abstract-full').style.display = 'inline'; document.getElementById('2110.15720v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.15720v2-abstract-full" style="display: none;">
        Recent years have witnessed the rapid development of concept map generation techniques due to their advantages in providing well-structured summarization of knowledge from free texts. Traditional unsupervised methods do not generate task-oriented concept maps, whereas deep generative models require large amounts of training data. In this work, we present GT-D2G (Graph Translation based Document-To-Graph), an automatic concept map generation framework that leverages generalized NLP pipelines to derive semantic-rich initial graphs, and translates them into more concise structures under the weak supervision of document labels. The quality and interpretability of such concept maps are validated through human evaluation on three real-world corpora, and their utility in the downstream task is further demonstrated in the controlled experiments with scarce document labels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15720v2-abstract-full').style.display = 'none'; document.getElementById('2110.15720v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.15268">arXiv:2110.15268</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.15268">pdf</a>, <a href="https://arxiv.org/format/2110.15268">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Continuous Face Representation with Explicit Functions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+L">Liping Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Weijun Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+L">Linjun Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+L">Lina Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ning%2C+X">Xin Ning</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaoli Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+J">Jian Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qin%2C+H">Hong Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.15268v1-abstract-short" style="display: inline;">
        How to represent a face pattern? While it is presented in a continuous way in our visual system, computers often store and process the face image in a discrete manner with 2D arrays of pixels. In this study, we attempt to learn a continuous representation for face images with explicit functions. First, we propose an explicit model (EmFace) for human face representation in the form of a finite sum&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15268v1-abstract-full').style.display = 'inline'; document.getElementById('2110.15268v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.15268v1-abstract-full" style="display: none;">
        How to represent a face pattern? While it is presented in a continuous way in our visual system, computers often store and process the face image in a discrete manner with 2D arrays of pixels. In this study, we attempt to learn a continuous representation for face images with explicit functions. First, we propose an explicit model (EmFace) for human face representation in the form of a finite sum of mathematical terms, where each term is an analytic function element. Further, to estimate the unknown parameters of EmFace, a novel neural network, EmNet, is designed with an encoder-decoder structure and trained using the backpropagation algorithm, where the encoder is defined by a deep convolutional neural network and the decoder is an explicit mathematical expression of EmFace. Experimental results show that EmFace has a higher representation performance on faces with various expressions, postures, and other factors, compared to that of other methods. Furthermore, EmFace achieves reasonable performance on several face image processing tasks, including face image restoration, denoising, and transformation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15268v1-abstract-full').style.display = 'none'; document.getElementById('2110.15268v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.14509">arXiv:2110.14509</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.14509">pdf</a>, <a href="https://arxiv.org/format/2110.14509">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Transfer Learning for Multi-source Entity Linkage via Domain Adaptation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+D">Di Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sisman%2C+B">Bunyamin Sisman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+H">Hao Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X+L">Xin Luna Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Koutra%2C+D">Danai Koutra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.14509v1-abstract-short" style="display: inline;">
        Multi-source entity linkage focuses on integrating knowledge from multiple sources by linking the records that represent the same real world entity. This is critical in high-impact applications such as data cleaning and user stitching. The state-of-the-art entity linkage pipelines mainly depend on supervised learning that requires abundant amounts of training data. However, collecting well-labeled&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.14509v1-abstract-full').style.display = 'inline'; document.getElementById('2110.14509v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.14509v1-abstract-full" style="display: none;">
        Multi-source entity linkage focuses on integrating knowledge from multiple sources by linking the records that represent the same real world entity. This is critical in high-impact applications such as data cleaning and user stitching. The state-of-the-art entity linkage pipelines mainly depend on supervised learning that requires abundant amounts of training data. However, collecting well-labeled training data becomes expensive when the data from many sources arrives incrementally over time. Moreover, the trained models can easily overfit to specific data sources, and thus fail to generalize to new sources due to significant differences in data and label distributions. To address these challenges, we present AdaMEL, a deep transfer learning framework that learns generic high-level knowledge to perform multi-source entity linkage. AdaMEL models the attribute importance that is used to match entities through an attribute-level self-attention mechanism, and leverages the massive unlabeled data from new data sources through domain adaptation to make it generic and data-source agnostic. In addition, AdaMEL is capable of incorporating an additional set of labeled data to more accurately integrate data sources with different attribute importance. Extensive experiments show that our framework achieves state-of-the-art results with 8.21% improvement on average over methods based on supervised learning. Besides, it is more stable in handling different sets of data sources in less runtime.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.14509v1-abstract-full').style.display = 'none'; document.getElementById('2110.14509v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.12752">arXiv:2110.12752</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.12752">pdf</a>, <a href="https://arxiv.org/format/2110.12752">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive Gaussian Processes on Graphs via Spectral Graph Wavelets
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Opolka%2C+F+L">Felix L. Opolka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhi%2C+Y">Yin-Cong Zhi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%C3%B2%2C+P">Pietro Liò</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowen Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.12752v2-abstract-short" style="display: inline;">
        Graph-based models require aggregating information in the graph from neighbourhoods of different sizes. In particular, when the data exhibit varying levels of smoothness on the graph, a multi-scale approach is required to capture the relevant information. In this work, we propose a Gaussian process model using spectral graph wavelets, which can naturally aggregate neighbourhood information at diff&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.12752v2-abstract-full').style.display = 'inline'; document.getElementById('2110.12752v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.12752v2-abstract-full" style="display: none;">
        Graph-based models require aggregating information in the graph from neighbourhoods of different sizes. In particular, when the data exhibit varying levels of smoothness on the graph, a multi-scale approach is required to capture the relevant information. In this work, we propose a Gaussian process model using spectral graph wavelets, which can naturally aggregate neighbourhood information at different scales. Through maximum likelihood optimisation of the model hyperparameters, the wavelets automatically adapt to the different frequencies in the data, and as a result our model goes beyond capturing low frequency information. We achieve scalability to larger graphs by using a spectrum-adaptive polynomial approximation of the filter function, which is designed to yield a low approximation error in dense areas of the graph spectrum. Synthetic and real-world experiments demonstrate the ability of our model to infer scales accurately and produce competitive performances against state-of-the-art models in graph-based learning tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.12752v2-abstract-full').style.display = 'none'; document.getElementById('2110.12752v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.09807">arXiv:2110.09807</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.09807">pdf</a>, <a href="https://arxiv.org/format/2110.09807">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to Learn Graph Topologies
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pu%2C+X">Xingyue Pu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+T">Tianyue Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiaoyun Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowen Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+S">Siheng Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.09807v1-abstract-short" style="display: inline;">
        Learning a graph topology to reveal the underlying relationship between data entities plays an important role in various machine learning and data analysis tasks. Under the assumption that structured data vary smoothly over a graph, the problem can be formulated as a regularised convex optimisation over a positive semidefinite cone and solved by iterative algorithms. Classic methods require an exp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09807v1-abstract-full').style.display = 'inline'; document.getElementById('2110.09807v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.09807v1-abstract-full" style="display: none;">
        Learning a graph topology to reveal the underlying relationship between data entities plays an important role in various machine learning and data analysis tasks. Under the assumption that structured data vary smoothly over a graph, the problem can be formulated as a regularised convex optimisation over a positive semidefinite cone and solved by iterative algorithms. Classic methods require an explicit convex function to reflect generic topological priors, e.g. the $\ell_1$ penalty for enforcing sparsity, which limits the flexibility and expressiveness in learning rich topological structures. We propose to learn a mapping from node data to the graph structure based on the idea of learning to optimise (L2O). Specifically, our model first unrolls an iterative primal-dual splitting algorithm into a neural network. The key structural proximal projection is replaced with a variational autoencoder that refines the estimated graph with enhanced topological properties. The model is trained in an end-to-end fashion with pairs of node data and graph samples. Experiments on both synthetic and real-world data demonstrate that our model is more efficient than classic iterative algorithms in learning a graph with specific topological properties.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09807v1-abstract-full').style.display = 'none'; document.getElementById('2110.09807v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.09443">arXiv:2110.09443</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.09443">pdf</a>, <a href="https://arxiv.org/format/2110.09443">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beltrami Flow and Neural Diffusion on Graphs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chamberlain%2C+B+P">Benjamin Paul Chamberlain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rowbottom%2C+J">James Rowbottom</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eynard%2C+D">Davide Eynard</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Di+Giovanni%2C+F">Francesco Di Giovanni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaowen Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bronstein%2C+M+M">Michael M Bronstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.09443v1-abstract-short" style="display: inline;">
        We propose a novel class of graph neural networks based on the discretised Beltrami flow, a non-Euclidean diffusion PDE. In our model, node features are supplemented with positional encodings derived from the graph topology and jointly evolved by the Beltrami flow, producing simultaneously continuous feature learning and topology evolution. The resulting model generalises many popular graph neural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09443v1-abstract-full').style.display = 'inline'; document.getElementById('2110.09443v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.09443v1-abstract-full" style="display: none;">
        We propose a novel class of graph neural networks based on the discretised Beltrami flow, a non-Euclidean diffusion PDE. In our model, node features are supplemented with positional encodings derived from the graph topology and jointly evolved by the Beltrami flow, producing simultaneously continuous feature learning and topology evolution. The resulting model generalises many popular graph neural networks and achieves state-of-the-art results on several benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09443v1-abstract-full').style.display = 'none'; document.getElementById('2110.09443v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">21 pages, 5 figures. Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.08870">arXiv:2109.08870</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.08870">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast query-by-example speech search using separable model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yuguang Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+Y">Yu Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xin Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Minqiang Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.08870v1-abstract-short" style="display: inline;">
        Traditional Query-by-Example (QbE) speech search approaches usually use methods based on frame-level features, while state-of-the-art approaches tend to use models based on acoustic word embeddings (AWEs) to transform variable length audio signals into fixed length feature vector representations. However, these approaches cannot meet the requirements of the search quality as well as speed at the s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.08870v1-abstract-full').style.display = 'inline'; document.getElementById('2109.08870v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.08870v1-abstract-full" style="display: none;">
        Traditional Query-by-Example (QbE) speech search approaches usually use methods based on frame-level features, while state-of-the-art approaches tend to use models based on acoustic word embeddings (AWEs) to transform variable length audio signals into fixed length feature vector representations. However, these approaches cannot meet the requirements of the search quality as well as speed at the same time. In this paper, we propose a novel fast QbE speech search method based on separable models to fix this problem. First, a QbE speech search training framework is introduced. Second, we design a novel model inference scheme based on RepVGG which can efficiently improve the QbE search quality. Third, we modify and improve our QbE speech search model according to the proposed model inference scheme. Experiments on keywords dataset shows that our proposed method can improve the GPU Real-time Factor (RTF) from 1/150 to 1/2300 by just applying separable model scheme and outperforms other state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.08870v1-abstract-full').style.display = 'none'; document.getElementById('2109.08870v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.07202">arXiv:2109.07202</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.07202">pdf</a>, <a href="https://arxiv.org/format/2109.07202">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep 3D Mesh Watermarking with Self-Adaptive Robustness
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Feng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+H">Hang Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fang%2C+H">Han Fang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaojuan Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+W">Weiming Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+X">Xi Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+N">Nenghai Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.07202v1-abstract-short" style="display: inline;">
        Robust 3D mesh watermarking is a traditional research topic in computer graphics, which provides an efficient solution to the copyright protection for 3D meshes. Traditionally, researchers need manually design watermarking algorithms to achieve sufficient robustness for the actual application scenarios. In this paper, we propose the first deep learning-based 3D mesh watermarking framework, which c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.07202v1-abstract-full').style.display = 'inline'; document.getElementById('2109.07202v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.07202v1-abstract-full" style="display: none;">
        Robust 3D mesh watermarking is a traditional research topic in computer graphics, which provides an efficient solution to the copyright protection for 3D meshes. Traditionally, researchers need manually design watermarking algorithms to achieve sufficient robustness for the actual application scenarios. In this paper, we propose the first deep learning-based 3D mesh watermarking framework, which can solve this problem once for all. In detail, we propose an end-to-end network, consisting of a watermark embedding sub-network, a watermark extracting sub-network and attack layers. We adopt the topology-agnostic graph convolutional network (GCN) as the basic convolution operation for 3D meshes, so our network is not limited by registered meshes (which share a fixed topology). For the specific application scenario, we can integrate the corresponding attack layers to guarantee adaptive robustness against possible attacks. To ensure the visual quality of watermarked 3D meshes, we design a curvature-based loss function to constrain the local geometry smoothness of watermarked meshes. Experimental results show that the proposed method can achieve more universal robustness and faster watermark embedding than baseline methods while guaranteeing comparable visual quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.07202v1-abstract-full').style.display = 'none'; document.getElementById('2109.07202v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.05460">arXiv:2109.05460</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.05460">pdf</a>, <a href="https://arxiv.org/format/2109.05460">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        End-to-End Conversational Search for Online Shopping with Utterance Transfer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+L">Liqiang Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma2%2C+J">Jun Ma2</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X+L">Xin Luna Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Martinez-Gomez%2C+P">Pascual Martinez-Gomez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zalmout%2C+N">Nasser Zalmout</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+W">Wei Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+T">Tong Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+H">Hao He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+Y">Yaohui Jin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.05460v1-abstract-short" style="display: inline;">
        Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data.In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dia&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.05460v1-abstract-full').style.display = 'inline'; document.getElementById('2109.05460v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.05460v1-abstract-full" style="display: none;">
        Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data.In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e-commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd-sourcing, and the conversational search system significantly outperformed the best tested baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.05460v1-abstract-full').style.display = 'none'; document.getElementById('2109.05460v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.04275">arXiv:2109.04275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.04275">pdf</a>, <a href="https://arxiv.org/format/2109.04275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiao Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhan%2C+X">Xunlin Zhan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yangxin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Y">Yunchao Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kampffmeyer%2C+M+C">Michael C. Kampffmeyer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+X">Xiaoyong Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+M">Minlong Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yaowei Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+X">Xiaodan Liang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.04275v5-abstract-short" style="display: inline;">
        Despite the potential of multi-modal pre-training to learn highly discriminative feature representations from complementary data modalities, current progress is being slowed by the lack of large-scale modality-diverse datasets. By leveraging the natural suitability of E-commerce, where different modalities capture complementary semantic information, we contribute a large-scale multi-modal pre-trai&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.04275v5-abstract-full').style.display = 'inline'; document.getElementById('2109.04275v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.04275v5-abstract-full" style="display: none;">
        Despite the potential of multi-modal pre-training to learn highly discriminative feature representations from complementary data modalities, current progress is being slowed by the lack of large-scale modality-diverse datasets. By leveraging the natural suitability of E-commerce, where different modalities capture complementary semantic information, we contribute a large-scale multi-modal pre-training dataset M5Product. The dataset comprises 5 modalities (image, text, table, video, and audio), covers over 6,000 categories and 5,000 attributes, and is 500 larger than the largest publicly available dataset with a similar number of modalities. Furthermore, M5Product contains incomplete modality pairs and noise while also having a long-tailed distribution, resembling most real-world problems. We further propose Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework that integrates the different modalities into a unified model through an adaptive feature fusion mechanism, where the importance of each modality is learned directly from the modality embeddings and impacts the inter-modality contrastive learning and masked tasks within a multi-modal transformer model. We evaluate the current multi-modal pre-training state-of-the-art approaches and benchmark their ability to learn from unlabeled data when faced with the large number of modalities in the M5Product dataset. We conduct extensive experiments on four downstream tasks and demonstrate the superiority of our SCALE model, providing insights into the importance of dataset scale and diversity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.04275v5-abstract-full').style.display = 'none'; document.getElementById('2109.04275v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 September, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.13958">arXiv:2108.13958</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.13958">pdf</a>, <a href="https://arxiv.org/format/2108.13958">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Novel Dataset for Keypoint Detection of quadruped Animals from Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Banik%2C+P">Prianka Banik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L">Lin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xishuang Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.13958v1-abstract-short" style="display: inline;">
        In this paper, we studied the problem of localizing a generic set of keypoints across multiple quadruped or four-legged animal species from images. Due to the lack of large scale animal keypoint dataset with ground truth annotations, we developed a novel dataset, AwA Pose, for keypoint detection of quadruped animals from images. Our dataset contains significantly more keypoints per animal and has&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13958v1-abstract-full').style.display = 'inline'; document.getElementById('2108.13958v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.13958v1-abstract-full" style="display: none;">
        In this paper, we studied the problem of localizing a generic set of keypoints across multiple quadruped or four-legged animal species from images. Due to the lack of large scale animal keypoint dataset with ground truth annotations, we developed a novel dataset, AwA Pose, for keypoint detection of quadruped animals from images. Our dataset contains significantly more keypoints per animal and has much more diverse animals than the existing datasets for animal keypoint detection. We benchmarked the dataset with a state-of-the-art deep learning model for different keypoint detection tasks, including both seen and unseen animal cases. Experimental results showed the effectiveness of the dataset. We believe that this dataset will help the computer vision community in the design and evaluation of improved models for the generalized quadruped animal keypoint detection problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13958v1-abstract-full').style.display = 'none'; document.getElementById('2108.13958v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.13465">arXiv:2108.13465</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.13465">pdf</a>, <a href="https://arxiv.org/format/2108.13465">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Full-Cycle Energy Consumption Benchmark for Low-Carbon Computer Vision
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+X">Xinyang Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bai%2C+D">Donglin Bai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yuge Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+N">Ningxin Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xuanyi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+L">Lu Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Y">Yuqing Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+D">Dongsheng Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.13465v2-abstract-short" style="display: inline;">
        The energy consumption of deep learning models is increasing at a breathtaking rate, which raises concerns due to potential negative effects on carbon neutrality in the context of global warming and climate change. With the progress of efficient deep learning techniques, e.g., model compression, researchers can obtain efficient models with fewer parameters and smaller latency. However, most of the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13465v2-abstract-full').style.display = 'inline'; document.getElementById('2108.13465v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.13465v2-abstract-full" style="display: none;">
        The energy consumption of deep learning models is increasing at a breathtaking rate, which raises concerns due to potential negative effects on carbon neutrality in the context of global warming and climate change. With the progress of efficient deep learning techniques, e.g., model compression, researchers can obtain efficient models with fewer parameters and smaller latency. However, most of the existing efficient deep learning methods do not explicitly consider energy consumption as a key performance indicator. Furthermore, existing methods mostly focus on the inference costs of the resulting efficient models, but neglect the notable energy consumption throughout the entire life cycle of the algorithm. In this paper, we present the first large-scale energy consumption benchmark for efficient computer vision models, where a new metric is proposed to explicitly evaluate the full-cycle energy consumption under different model usage intensity. The benchmark can provide insights for low carbon emission when selecting efficient deep learning algorithms in different model usage scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13465v2-abstract-full').style.display = 'none'; document.getElementById('2108.13465v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 October, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ArXiv Preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.10038">arXiv:2108.10038</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.10038">pdf</a>, <a href="https://arxiv.org/format/2108.10038">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Event Extraction by Associating Event Types and Argument Roles
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Qian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+S">Shu Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jia Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jianxin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sheng%2C+J">Jiawei Sheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+L">Lihong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaohan Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+H">Hao Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.10038v1-abstract-short" style="display: inline;">
        Event extraction (EE), which acquires structural event knowledge from texts, can be divided into two sub-tasks: event type classification and element extraction (namely identifying triggers and arguments under different role patterns). As different event types always own distinct extraction schemas (i.e., role patterns), previous work on EE usually follows an isolated learning paradigm, performing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.10038v1-abstract-full').style.display = 'inline'; document.getElementById('2108.10038v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.10038v1-abstract-full" style="display: none;">
        Event extraction (EE), which acquires structural event knowledge from texts, can be divided into two sub-tasks: event type classification and element extraction (namely identifying triggers and arguments under different role patterns). As different event types always own distinct extraction schemas (i.e., role patterns), previous work on EE usually follows an isolated learning paradigm, performing element extraction independently for different event types. It ignores meaningful associations among event types and argument roles, leading to relatively poor performance for less frequent types/roles. This paper proposes a novel neural association framework for the EE task. Given a document, it first performs type classification via constructing a document-level graph to associate sentence nodes of different types, and adopting a graph attention network to learn sentence embeddings. Then, element extraction is achieved by building a universal schema of argument roles, with a parameter inheritance mechanism to enhance role preference for extracted elements. As such, our model takes into account type and role associations during EE, enabling implicit information sharing among them. Experimental results show that our approach consistently outperforms most state-of-the-art EE methods in both sub-tasks. Particularly, for types/roles with less training data, the performance is superior to the existing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.10038v1-abstract-full').style.display = 'none'; document.getElementById('2108.10038v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.05895">arXiv:2108.05895</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.05895">pdf</a>, <a href="https://arxiv.org/format/2108.05895">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mobile-Former: Bridging MobileNet and Transformer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yinpeng Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dai%2C+X">Xiyang Dai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+D">Dongdong Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+M">Mengchen Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaoyi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+L">Lu Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zicheng Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.05895v3-abstract-short" style="display: inline;">
        We present Mobile-Former, a parallel design of MobileNet and transformer with a two-way bridge in between. This structure leverages the advantages of MobileNet at local processing and transformer at global interaction. And the bridge enables bidirectional fusion of local and global features. Different from recent works on vision transformer, the transformer in Mobile-Former contains very few token&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.05895v3-abstract-full').style.display = 'inline'; document.getElementById('2108.05895v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.05895v3-abstract-full" style="display: none;">
        We present Mobile-Former, a parallel design of MobileNet and transformer with a two-way bridge in between. This structure leverages the advantages of MobileNet at local processing and transformer at global interaction. And the bridge enables bidirectional fusion of local and global features. Different from recent works on vision transformer, the transformer in Mobile-Former contains very few tokens (e.g. 6 or fewer tokens) that are randomly initialized to learn global priors, resulting in low computational cost. Combining with the proposed light-weight cross attention to model the bridge, Mobile-Former is not only computationally efficient, but also has more representation power. It outperforms MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet classification. For instance, Mobile-Former achieves 77.9\% top-1 accuracy at 294M FLOPs, gaining 1.3\% over MobileNetV3 but saving 17\% of computations. When transferring to object detection, Mobile-Former outperforms MobileNetV3 by 8.6 AP in RetinaNet framework. Furthermore, we build an efficient end-to-end detector by replacing backbone, encoder and decoder in DETR with Mobile-Former, which outperforms DETR by 1.1 AP but saves 52\% of computational cost and 36\% of parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.05895v3-abstract-full').style.display = 'none'; document.getElementById('2108.05895v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To Appear at CVPR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.02947">arXiv:2108.02947</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.02947">pdf</a>, <a href="https://arxiv.org/format/2108.02947">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.tifs.2022.02.017">10.1016/j.tifs.2022.02.017 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A review on vision-based analysis for automatic dietary assessment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Min%2C+W">Weiqing Min</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+T">Tianhao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiaoxiao Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Haisheng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+S">Shuqiang Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.02947v2-abstract-short" style="display: inline;">
        Background: Maintaining a healthy diet is vital to avoid health-related issues, e.g., undernutrition, obesity and many non-communicable diseases. An indispensable part of the health diet is dietary assessment. Traditional manual recording methods are not only burdensome but time-consuming, and contain substantial biases and errors. Recent advances in Artificial Intelligence (AI), especially comput&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02947v2-abstract-full').style.display = 'inline'; document.getElementById('2108.02947v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.02947v2-abstract-full" style="display: none;">
        Background: Maintaining a healthy diet is vital to avoid health-related issues, e.g., undernutrition, obesity and many non-communicable diseases. An indispensable part of the health diet is dietary assessment. Traditional manual recording methods are not only burdensome but time-consuming, and contain substantial biases and errors. Recent advances in Artificial Intelligence (AI), especially computer vision technologies, have made it possible to develop automatic dietary assessment solutions, which are more convenient, less time-consuming and even more accurate to monitor daily food intake. Scope and approach: This review presents Vision-Based Dietary Assessment (VBDA) architectures, including multi-stage architecture and end-to-end one. The multi-stage dietary assessment generally consists of three stages: food image analysis, volume estimation and nutrient derivation. The prosperity of deep learning makes VBDA gradually move to an end-to-end implementation, which applies food images to a single network to directly estimate the nutrition. The recently proposed end-to-end methods are also discussed. We further analyze existing dietary assessment datasets, indicating that one large-scale benchmark is urgently needed, and finally highlight critical challenges and future trends for VBDA. Key findings and conclusions: After thorough exploration, we find that multi-task end-to-end deep learning approaches are one important trend of VBDA. Despite considerable research progress, many challenges remain for VBDA due to the meal complexity. We also provide the latest ideas for future development of VBDA, e.g., fine-grained food analysis and accurate volume estimation. This review aims to encourage researchers to propose more practical solutions for VBDA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02947v2-abstract-full').style.display = 'none'; document.getElementById('2108.02947v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Trends in Food Science &amp; Technology</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.02366">arXiv:2108.02366</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.02366">pdf</a>, <a href="https://arxiv.org/format/2108.02366">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xinzhi Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Long%2C+C">Chengjiang Long</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+W">Wenju Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+C">Chunxia Xiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.02366v1-abstract-short" style="display: inline;">
        Existing image captioning methods just focus on understanding the relationship between objects or instances in a single image, without exploring the contextual correlation existed among contextual image. In this paper, we propose Dual Graph Convolutional Networks (Dual-GCN) with transformer and curriculum learning for image captioning. In particular, we not only use an object-level GCN to capture&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02366v1-abstract-full').style.display = 'inline'; document.getElementById('2108.02366v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.02366v1-abstract-full" style="display: none;">
        Existing image captioning methods just focus on understanding the relationship between objects or instances in a single image, without exploring the contextual correlation existed among contextual image. In this paper, we propose Dual Graph Convolutional Networks (Dual-GCN) with transformer and curriculum learning for image captioning. In particular, we not only use an object-level GCN to capture the object to object spatial relation within a single image, but also adopt an image-level GCN to capture the feature information provided by similar images. With the well-designed Dual-GCN, we can make the linguistic transformer better understand the relationship between different objects in a single image and make full use of similar images as auxiliary information to generate a reasonable caption description for a single image. Meanwhile, with a cross-review strategy introduced to determine difficulty levels, we adopt curriculum learning as the training strategy to increase the robustness and generalization of our proposed model. We conduct extensive experiments on the large-scale MS COCO dataset, and the experimental results powerfully demonstrate that our proposed method outperforms recent state-of-the-art approaches. It achieves a BLEU-1 score of 82.2 and a BLEU-2 score of 67.6. Our source code is available at {\em \color{magenta}{\url{https://github.com/Unbear430/DGCN-for-image-captioning}}}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02366v1-abstract-full').style.display = 'none'; document.getElementById('2108.02366v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper was accepted to the 29th ACM International Conference on Multimedia (ACM MM), Chengdu, Sichuan, China, Oct 20-24, 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.00577">arXiv:2108.00577</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.00577">pdf</a>, <a href="https://arxiv.org/format/2108.00577">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Logic-Consistency Text Generation from Semantic Parses
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shu%2C+C">Chang Shu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yusen Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiangyu Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+P">Peng Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+T">Tao Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+R">Rui Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.00577v1-abstract-short" style="display: inline;">
        Text generation from semantic parses is to generate textual descriptions for formal representation inputs such as logic forms and SQL queries. This is challenging due to two reasons: (1) the complex and intensive inner logic with the data scarcity constraint, (2) the lack of automatic evaluation metrics for logic consistency. To address these two challenges, this paper first proposes SNOWBALL, a f&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.00577v1-abstract-full').style.display = 'inline'; document.getElementById('2108.00577v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.00577v1-abstract-full" style="display: none;">
        Text generation from semantic parses is to generate textual descriptions for formal representation inputs such as logic forms and SQL queries. This is challenging due to two reasons: (1) the complex and intensive inner logic with the data scarcity constraint, (2) the lack of automatic evaluation metrics for logic consistency. To address these two challenges, this paper first proposes SNOWBALL, a framework for logic consistent text generation from semantic parses that employs an iterative training procedure by recursively augmenting the training set with quality control. Second, we propose a novel automatic metric, BLEC, for evaluating the logical consistency between the semantic parses and generated texts. The experimental results on two benchmark datasets, Logic2Text and Spider, demonstrate the SNOWBALL framework enhances the logic consistency on both BLEC and human evaluation. Furthermore, our statistical analysis reveals that BLEC is more logically consistent with human evaluation than general-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data and code are available at https://github.com/Ciaranshu/relogic.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.00577v1-abstract-full').style.display = 'none'; document.getElementById('2108.00577v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL Findings, 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.14572">arXiv:2107.14572</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.14572">pdf</a>, <a href="https://arxiv.org/format/2107.14572">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhan%2C+X">Xunlin Zhan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yangxin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+X">Xiao Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Y">Yunchao Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+M">Minlong Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yichi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+H">Hang Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+X">Xiaodan Liang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.14572v2-abstract-short" style="display: inline;">
        Nowadays, customer&#39;s demands for E-commerce are more diversified, which introduces more complications to the product retrieval industry. Previous methods are either subject to single-modal input or perform supervised image-level product retrieval, thus fail to accommodate real-life scenarios where enormous weakly annotated multi-modal data are present. In this paper, we investigate a more realisti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.14572v2-abstract-full').style.display = 'inline'; document.getElementById('2107.14572v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.14572v2-abstract-full" style="display: none;">
        Nowadays, customer&#39;s demands for E-commerce are more diversified, which introduces more complications to the product retrieval industry. Previous methods are either subject to single-modal input or perform supervised image-level product retrieval, thus fail to accommodate real-life scenarios where enormous weakly annotated multi-modal data are present. In this paper, we investigate a more realistic setting that aims to perform weakly-supervised multi-modal instance-level product retrieval among fine-grained product categories. To promote the study of this challenging task, we contribute Product1M, one of the largest multi-modal cosmetic datasets for real-world instance-level retrieval. Notably, Product1M contains over 1 million image-caption pairs and consists of two sample types, i.e., single-product and multi-product samples, which encompass a wide variety of cosmetics brands. In addition to the great diversity, Product1M enjoys several appealing characteristics including fine-grained categories, complex combinations, and fuzzy correspondence that well mimic the real-world scenes. Moreover, we propose a novel model named Cross-modal contrAstive Product Transformer for instance-level prodUct REtrieval (CAPTURE), that excels in capturing the potential synergy between multi-modal inputs via a hybrid-stream transformer in a self-supervised manner.CAPTURE generates discriminative instance features via masked multi-modal learning as well as cross-modal contrastive pretraining and it outperforms several SOTA cross-modal baselines. Extensive ablation studies well demonstrate the effectiveness and the generalization capacity of our model. Dataset and codes are available at https: //github.com/zhanxlin/Product1M.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.14572v2-abstract-full').style.display = 'none'; document.getElementById('2107.14572v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Dong%2C+X&amp;start=200"
            class="pagination-link "
            aria-label="Page 5"
            aria-current="page">5
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>