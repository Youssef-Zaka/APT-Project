<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 420 results for author: <span class="mathjax">Bengio, Y</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Bengio, Y">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Bengio%2C+Y&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Bengio, Y">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.00666">arXiv:2205.00666</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.00666">pdf</a>, <a href="https://arxiv.org/format/2205.00666">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="General Economics">econ.GN</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        (Private)-Retroactive Carbon Pricing [(P)ReCaP]: A Market-based Approach for Climate Finance and Risk Assessment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gupta%2C+P">Prateek Gupta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Radovic%2C+D">Dylan Radovic</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Scholl%2C+M">Maarten Scholl</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Williams%2C+A">Andrew Williams</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Witt%2C+C+S">Christian Schroeder de Witt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+T">Tianyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.00666v1-abstract-short" style="display: inline;">
        Insufficient Social Cost of Carbon (SCC) estimation methods and short-term decision-making horizons have hindered the ability of carbon emitters to properly correct for the negative externalities of climate change, as well as the capacity of nations to balance economic and climate policy. To overcome these limitations, we introduce Retrospective Social Cost of Carbon Updating (ReSCCU), a novel mec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00666v1-abstract-full').style.display = 'inline'; document.getElementById('2205.00666v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.00666v1-abstract-full" style="display: none;">
        Insufficient Social Cost of Carbon (SCC) estimation methods and short-term decision-making horizons have hindered the ability of carbon emitters to properly correct for the negative externalities of climate change, as well as the capacity of nations to balance economic and climate policy. To overcome these limitations, we introduce Retrospective Social Cost of Carbon Updating (ReSCCU), a novel mechanism that corrects for these limitations as empirically measured evidence is collected. To implement ReSCCU in the context of carbon taxation, we propose Retroactive Carbon Pricing (ReCaP), a market mechanism in which polluters offload the payment of ReSCCU adjustments to insurers. To alleviate systematic risks and minimize government involvement, we introduce the Private ReCaP (PReCaP) prediction market, which could see real-world implementation based on the engagement of a few high net-worth individuals or independent institutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.00666v1-abstract-full').style.display = 'none'; document.getElementById('2205.00666v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          91B18 (Primary) 91B76; 91G40 (Secondary)
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          J.4
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11369">arXiv:2203.11369</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11369">pdf</a>, <a href="https://arxiv.org/format/2203.11369">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Temporal Abstractions-Augmented Temporally Contrastive Learning: An Alternative to the Laplacian in RL
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Erraqabi%2C+A">Akram Erraqabi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Machado%2C+M+C">Marlos C. Machado</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+M">Mingde Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sukhbaatar%2C+S">Sainbayar Sukhbaatar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lazaric%2C+A">Alessandro Lazaric</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Denoyer%2C+L">Ludovic Denoyer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11369v1-abstract-short" style="display: inline;">
        In reinforcement learning, the graph Laplacian has proved to be a valuable tool in the task-agnostic setting, with applications ranging from skill discovery to reward shaping. Recently, learning the Laplacian representation has been framed as the optimization of a temporally-contrastive objective to overcome its computational limitations in large (or continuous) state spaces. However, this approac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11369v1-abstract-full').style.display = 'inline'; document.getElementById('2203.11369v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11369v1-abstract-full" style="display: none;">
        In reinforcement learning, the graph Laplacian has proved to be a valuable tool in the task-agnostic setting, with applications ranging from skill discovery to reward shaping. Recently, learning the Laplacian representation has been framed as the optimization of a temporally-contrastive objective to overcome its computational limitations in large (or continuous) state spaces. However, this approach requires uniform access to all states in the state space, overlooking the exploration problem that emerges during the representation learning process. In this work, we propose an alternative method that is able to recover, in a non-uniform-prior setting, the expressiveness and the desired properties of the Laplacian representation. We do so by combining the representation learning with a skill-based covering policy, which provides a better training distribution to extend and refine the representation. We also show that a simple augmentation of the representation objective with the learned temporal abstractions improves dynamics-awareness and helps exploration. We find that our method succeeds as an alternative to the Laplacian in the non-uniform setting and scales to challenging continuous control environments. Finally, even if our method is not optimized for skill discovery, the learned skills can successfully solve difficult continuous navigation tasks with sparse rewards, where standard skill discovery approaches are no so effective.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11369v1-abstract-full').style.display = 'none'; document.getElementById('2203.11369v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.04115">arXiv:2203.04115</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.04115">pdf</a>, <a href="https://arxiv.org/format/2203.04115">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Biomolecules">q-bio.BM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Biological Sequence Design with GFlowNets
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jain%2C+M">Moksh Jain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+E">Emmanuel Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garcia%2C+A">Alex-Hernandez Garcia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rector-Brooks%2C+J">Jarrid Rector-Brooks</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dossou%2C+B+F+P">Bonaventure F. P. Dossou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ekbote%2C+C">Chanakya Ekbote</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+J">Jie Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+T">Tianyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kilgour%2C+M">Micheal Kilgour</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+D">Dinghuai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simine%2C+L">Lena Simine</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Das%2C+P">Payel Das</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.04115v1-abstract-short" style="display: inline;">
        Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key con&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04115v1-abstract-full').style.display = 'inline'; document.getElementById('2203.04115v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.04115v1-abstract-full" style="display: none;">
        Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04115v1-abstract-full').style.display = 'none'; document.getElementById('2203.04115v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 3 figures. Code available at: https://github.com/MJ10/BioSeq-GFN-AL</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.03724">arXiv:2203.03724</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.03724">pdf</a>, <a href="https://arxiv.org/format/2203.03724">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A New Era: Intelligent Tutoring Systems Will Transform Online Learning for Millions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=St-Hilaire%2C+F">Francois St-Hilaire</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vu%2C+D+D">Dung Do Vu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Frau%2C+A">Antoine Frau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burns%2C+N">Nathan Burns</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Faraji%2C+F">Farid Faraji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Potochny%2C+J">Joseph Potochny</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Robert%2C+S">Stephane Robert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roussel%2C+A">Arnaud Roussel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+S">Selene Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Glazier%2C+T">Taylor Glazier</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Romano%2C+J+V">Junfel Vincent Romano</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Belfer%2C+R">Robert Belfer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shayan%2C+M">Muhammad Shayan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Smofsky%2C+A">Ariella Smofsky</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Delarosbil%2C+T">Tommy Delarosbil</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ahn%2C+S">Seulmin Ahn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eden-Walker%2C+S">Simon Eden-Walker</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sony%2C+K">Kritika Sony</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ching%2C+A+O">Ansona Onyi Ching</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Elkins%2C+S">Sabina Elkins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stepanyan%2C+A">Anush Stepanyan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Matajova%2C+A">Adela Matajova</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+V">Victor Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sahraei%2C+H">Hossein Sahraei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Larson%2C+R">Robert Larson</a>
      , et al. (6 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.03724v1-abstract-short" style="display: inline;">
        Despite artificial intelligence (AI) having transformed major aspects of our society, less than a fraction of its potential has been explored, let alone deployed, for education. AI-powered learning can provide millions of learners with a highly personalized, active and practical learning experience, which is key to successful learning. This is especially relevant in the context of online learning&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03724v1-abstract-full').style.display = 'inline'; document.getElementById('2203.03724v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.03724v1-abstract-full" style="display: none;">
        Despite artificial intelligence (AI) having transformed major aspects of our society, less than a fraction of its potential has been explored, let alone deployed, for education. AI-powered learning can provide millions of learners with a highly personalized, active and practical learning experience, which is key to successful learning. This is especially relevant in the context of online learning platforms. In this paper, we present the results of a comparative head-to-head study on learning outcomes for two popular online learning platforms (n=199 participants): A MOOC platform following a traditional model delivering content using lecture videos and multiple-choice quizzes, and the Korbit learning platform providing a highly personalized, active and practical learning experience. We observe a huge and statistically significant increase in the learning outcomes, with students on the Korbit platform providing full feedback resulting in higher course completion rates and achieving learning gains 2 to 2.5 times higher than both students on the MOOC platform and students in a control group who don&#39;t receive personalized feedback on the Korbit platform. The results demonstrate the tremendous impact that can be achieved with a personalized, active learning AI-powered system. Making this technology and learning experience available to millions of learners around the world will represent a significant leap forward towards the democratization of education.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03724v1-abstract-full').style.display = 'none'; document.getElementById('2203.03724v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 6 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.0; K.3.1; K.4.0
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.01443">arXiv:2203.01443</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.01443">pdf</a>, <a href="https://arxiv.org/format/2203.01443">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Continuous-Time Meta-Learning with Forward Mode Differentiation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Deleu%2C+T">Tristan Deleu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kanaa%2C+D">David Kanaa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+L">Leo Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kerg%2C+G">Giancarlo Kerg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lajoie%2C+G">Guillaume Lajoie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bacon%2C+P">Pierre-Luc Bacon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.01443v1-abstract-short" style="display: inline;">
        Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differenti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01443v1-abstract-full').style.display = 'inline'; document.getElementById('2203.01443v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.01443v1-abstract-full" style="display: none;">
        Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01443v1-abstract-full').style.display = 'none'; document.getElementById('2203.01443v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.13914">arXiv:2202.13914</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.13914">pdf</a>, <a href="https://arxiv.org/format/2202.13914">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Combining Modular Skills in Multitask Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ponti%2C+E+M">Edoardo M. Ponti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sordoni%2C+A">Alessandro Sordoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reddy%2C+S">Siva Reddy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.13914v2-abstract-short" style="display: inline;">
        A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent discrete skills from a (potentially small) inventory. In turn, skills correspond to parameter-efficient (sparse / low-rank) model parameterisations. By jointly learning these&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.13914v2-abstract-full').style.display = 'inline'; document.getElementById('2202.13914v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.13914v2-abstract-full" style="display: none;">
        A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent discrete skills from a (potentially small) inventory. In turn, skills correspond to parameter-efficient (sparse / low-rank) model parameterisations. By jointly learning these and a task-skill allocation matrix, the network for each task is instantiated as the average of the parameters of active skills. To favour non-trivial soft partitions of skills across tasks, we experiment with a series of inductive biases, such as an Indian Buffet Process prior and a two-speed learning rate. We evaluate our latent-skill model on two main settings: 1) multitask reinforcement learning for grounded instruction following on 8 levels of the BabyAI platform; and 2) few-shot adaptation of pre-trained text-to-text generative models on CrossFit, a benchmark comprising 160 NLP tasks. We find that the modular design of a network significantly increases sample efficiency in reinforcement learning and few-shot generalisation in supervised learning, compared to baselines with fully shared, task-specific, or conditionally generated parameters where knowledge is entangled across tasks. In addition, we show how discrete skills help interpretability, as they yield an explicit hierarchy of tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.13914v2-abstract-full').style.display = 'none'; document.getElementById('2202.13914v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.13903">arXiv:2202.13903</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.13903">pdf</a>, <a href="https://arxiv.org/ps/2202.13903">ps</a>, <a href="https://arxiv.org/format/2202.13903">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bayesian Structure Learning with Generative Flow Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Deleu%2C+T">Tristan Deleu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=G%C3%B3is%2C+A">António Góis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Emezue%2C+C">Chris Emezue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rankawat%2C+M">Mansi Rankawat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lacoste-Julien%2C+S">Simon Lacoste-Julien</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bauer%2C+S">Stefan Bauer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.13903v1-abstract-short" style="display: inline;">
        In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.13903v1-abstract-full').style.display = 'inline'; document.getElementById('2202.13903v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.13903v1-abstract-full" style="display: none;">
        In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.13903v1-abstract-full').style.display = 'none'; document.getElementById('2202.13903v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.04202">arXiv:2202.04202</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.04202">pdf</a>, <a href="https://arxiv.org/format/2202.04202">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RECOVER: sequential model optimization platform for combination drug repurposing identifies novel synergistic compounds in vitro
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bertin%2C+P">Paul Bertin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rector-Brooks%2C+J">Jarrid Rector-Brooks</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sharma%2C+D">Deepak Sharma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gaudelet%2C+T">Thomas Gaudelet</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anighoro%2C+A">Andrew Anighoro</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gross%2C+T">Torsten Gross</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Martinez-Pena%2C+F">Francisco Martinez-Pena</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+E+L">Eileen L. Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=S%2C+S+M">Suraj M S</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Regep%2C+C">Cristian Regep</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hayter%2C+J">Jeremy Hayter</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Korablyov%2C+M">Maksym Korablyov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Valiante%2C+N">Nicholas Valiante</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=van+der+Sloot%2C+A">Almer van der Sloot</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tyers%2C+M">Mike Tyers</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roberts%2C+C">Charles Roberts</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bronstein%2C+M+M">Michael M. Bronstein</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lairson%2C+L+L">Luke L. Lairson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Taylor-King%2C+J+P">Jake P. Taylor-King</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.04202v1-abstract-short" style="display: inline;">
        Selecting optimal drug repurposing combinations for further preclinical development is a challenging technical feat. Due to the toxicity of many therapeutic agents (e.g., chemotherapy), practitioners have favoured selection of synergistic compounds whereby lower doses can be used whilst maintaining high efficacy. For a fixed small molecule library, an exhaustive combinatorial chemical screen becom&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.04202v1-abstract-full').style.display = 'inline'; document.getElementById('2202.04202v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.04202v1-abstract-full" style="display: none;">
        Selecting optimal drug repurposing combinations for further preclinical development is a challenging technical feat. Due to the toxicity of many therapeutic agents (e.g., chemotherapy), practitioners have favoured selection of synergistic compounds whereby lower doses can be used whilst maintaining high efficacy. For a fixed small molecule library, an exhaustive combinatorial chemical screen becomes infeasible to perform for academic and industry laboratories alike. Deep learning models have achieved state-of-the-art results in silico for the prediction of synergy scores. However, databases of drug combinations are highly biased towards synergistic agents and these results do not necessarily generalise out of distribution. We employ a sequential model optimization search applied to a deep learning model to quickly discover highly synergistic drug combinations active against a cancer cell line, while requiring substantially less screening than an exhaustive evaluation. Through iteratively adapting the model to newly acquired data, after only 3 rounds of ML-guided experimentation (including a calibration round), we find that the set of combinations queried by our model is enriched for highly synergistic combinations. Remarkably, we rediscovered a synergistic drug combination that was later confirmed to be under study within clinical trials.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.04202v1-abstract-full').style.display = 'none'; document.getElementById('2202.04202v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.01361">arXiv:2202.01361</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.01361">pdf</a>, <a href="https://arxiv.org/format/2202.01361">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generative Flow Networks for Discrete Probabilistic Modeling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+D">Dinghuai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Malkin%2C+N">Nikolay Malkin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhen Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Volokhova%2C+A">Alexandra Volokhova</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.01361v1-abstract-short" style="display: inline;">
        We present energy-based generative flow networks (EB-GFN), a novel probabilistic modeling algorithm for high-dimensional discrete data. Building upon the theory of generative flow networks (GFlowNets), we model the generation process by a stochastic data construction policy and thus amortize expensive MCMC exploration into a fixed number of actions sampled from a GFlowNet. We show how GFlowNets ca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.01361v1-abstract-full').style.display = 'inline'; document.getElementById('2202.01361v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.01361v1-abstract-full" style="display: none;">
        We present energy-based generative flow networks (EB-GFN), a novel probabilistic modeling algorithm for high-dimensional discrete data. Building upon the theory of generative flow networks (GFlowNets), we model the generation process by a stochastic data construction policy and thus amortize expensive MCMC exploration into a fixed number of actions sampled from a GFlowNet. We show how GFlowNets can approximately perform large-block Gibbs sampling to mix between modes. We propose a framework to jointly train a GFlowNet with an energy function, so that the GFlowNet learns to sample from the energy distribution, while the energy learns with an approximate MLE objective with negative samples from the GFlowNet. We demonstrate EB-GFN&#39;s effectiveness on various probabilistic modeling tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.01361v1-abstract-full').style.display = 'none'; document.getElementById('2202.01361v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages; code: https://github.com/zdhNarsil/EB_GFN</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.01334">arXiv:2202.01334</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.01334">pdf</a>, <a href="https://arxiv.org/format/2202.01334">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+D">Dianbo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lamb%2C+A">Alex Lamb</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+X">Xu Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Notsawo%2C+P">Pascal Notsawo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mozer%2C+M">Mike Mozer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.01334v1-abstract-short" style="display: inline;">
        Vector Quantization (VQ) is a method for discretizing latent representations and has become a major part of the deep learning toolkit. It has been theoretically and empirically shown that discretization of representations leads to improved generalization, including in reinforcement learning where discretization can be used to bottleneck multi-agent communication to promote agent specialization and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.01334v1-abstract-full').style.display = 'inline'; document.getElementById('2202.01334v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.01334v1-abstract-full" style="display: none;">
        Vector Quantization (VQ) is a method for discretizing latent representations and has become a major part of the deep learning toolkit. It has been theoretically and empirically shown that discretization of representations leads to improved generalization, including in reinforcement learning where discretization can be used to bottleneck multi-agent communication to promote agent specialization and robustness. The discretization tightness of most VQ-based methods is defined by the number of discrete codes in the representation vector and the codebook size, which are fixed as hyperparameters. In this work, we propose learning to dynamically select discretization tightness conditioned on inputs, based on the hypothesis that data naturally contains variations in complexity that call for different levels of representational coarseness. We show that dynamically varying tightness in communication bottlenecks can improve model performance on visual reasoning and reinforcement learning tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.01334v1-abstract-full').style.display = 'none'; document.getElementById('2202.01334v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.13415">arXiv:2201.13415</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.13415">pdf</a>, <a href="https://arxiv.org/format/2201.13415">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Scaling Difference Target Propagation by Learning Backprop Targets
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ernoult%2C+M">Maxence Ernoult</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Normandin%2C+F">Fabrice Normandin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moudgil%2C+A">Abhinav Moudgil</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Spinney%2C+S">Sean Spinney</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Belilovsky%2C+E">Eugene Belilovsky</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rish%2C+I">Irina Rish</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Richards%2C+B">Blake Richards</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.13415v1-abstract-short" style="display: inline;">
        The development of biologically-plausible learning algorithms is important for understanding learning in the brain, but most of them fail to scale-up to real-world tasks, limiting their potential as explanations for learning by real brains. As such, it is important to explore learning algorithms that come with strong theoretical guarantees and can match the performance of backpropagation (BP) on c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.13415v1-abstract-full').style.display = 'inline'; document.getElementById('2201.13415v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.13415v1-abstract-full" style="display: none;">
        The development of biologically-plausible learning algorithms is important for understanding learning in the brain, but most of them fail to scale-up to real-world tasks, limiting their potential as explanations for learning by real brains. As such, it is important to explore learning algorithms that come with strong theoretical guarantees and can match the performance of backpropagation (BP) on complex tasks. One such algorithm is Difference Target Propagation (DTP), a biologically-plausible learning algorithm whose close relation with Gauss-Newton (GN) optimization has been recently established. However, the conditions under which this connection rigorously holds preclude layer-wise training of the feedback pathway synaptic weights (which is more biologically plausible). Moreover, good alignment between DTP weight updates and loss gradients is only loosely guaranteed and under very specific conditions for the architecture being trained. In this paper, we propose a novel feedback weight training scheme that ensures both that DTP approximates BP and that layer-wise feedback weight training can be restored without sacrificing any theoretical guarantees. Our theory is corroborated by experimental results and we report the best performance ever achieved by DTP on CIFAR-10 and ImageNet 32$\times$32
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.13415v1-abstract-full').style.display = 'none'; document.getElementById('2201.13415v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.13259">arXiv:2201.13259</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.13259">pdf</a>, <a href="https://arxiv.org/format/2201.13259">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Trajectory Balance: Improved Credit Assignment in GFlowNets
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Malkin%2C+N">Nikolay Malkin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jain%2C+M">Moksh Jain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+E">Emmanuel Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+C">Chen Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.13259v1-abstract-short" style="display: inline;">
        Generative Flow Networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. Prior temporal difference-like learning objectives for training GFlowNets, such as flow matching and detailed balance, are pron&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.13259v1-abstract-full').style.display = 'inline'; document.getElementById('2201.13259v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.13259v1-abstract-full" style="display: none;">
        Generative Flow Networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. Prior temporal difference-like learning objectives for training GFlowNets, such as flow matching and detailed balance, are prone to inefficient credit propagation across action sequences, particularly in the case of long sequences. We propose a new learning objective for GFlowNets, trajectory balance, as a more efficient alternative to previously used objectives. We prove that any global minimizer of the trajectory balance objective can define a policy that samples exactly from the target distribution. In experiments on four distinct domains, we empirically demonstrate the benefits of the trajectory balance objective for GFlowNet convergence, diversity of generated samples, and robustness to long action sequences and large action spaces.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.13259v1-abstract-full').style.display = 'none'; document.getElementById('2201.13259v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.11783">arXiv:2201.11783</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.11783">pdf</a>, <a href="https://arxiv.org/format/2201.11783">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rethinking Learning Dynamics in RL using Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+R">Ramnath Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deleu%2C+T">Tristan Deleu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.11783v1-abstract-short" style="display: inline;">
        We present a learning mechanism for reinforcement learning of closely related skills parameterized via a skill embedding space. Our approach is grounded on the intuition that nothing makes you learn better than a coevolving adversary. The main contribution of our work is to formulate an adversarial training regime for reinforcement learning with the help of entropy-regularized policy gradient form&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11783v1-abstract-full').style.display = 'inline'; document.getElementById('2201.11783v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.11783v1-abstract-full" style="display: none;">
        We present a learning mechanism for reinforcement learning of closely related skills parameterized via a skill embedding space. Our approach is grounded on the intuition that nothing makes you learn better than a coevolving adversary. The main contribution of our work is to formulate an adversarial training regime for reinforcement learning with the help of entropy-regularized policy gradient formulation. We also adapt existing measures of causal attribution to draw insights from the skills learned. Our experiments demonstrate that the adversarial process leads to a better exploration of multiple solutions and understanding the minimum number of different skills necessary to solve a given set of tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11783v1-abstract-full').style.display = 'none'; document.getElementById('2201.11783v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.11775">arXiv:2201.11775</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.11775">pdf</a>, <a href="https://arxiv.org/format/2201.11775">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Effect of Diversity in Meta-Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+R">Ramnath Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deleu%2C+T">Tristan Deleu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.11775v1-abstract-short" style="display: inline;">
        Few-shot learning aims to learn representations that can tackle novel tasks given a small number of examples. Recent studies show that task distribution plays a vital role in the model&#39;s performance. Conventional wisdom is that task diversity should improve the performance of meta-learning. In this work, we find evidence to the contrary; we study different task distributions on a myriad of models&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11775v1-abstract-full').style.display = 'inline'; document.getElementById('2201.11775v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.11775v1-abstract-full" style="display: none;">
        Few-shot learning aims to learn representations that can tackle novel tasks given a small number of examples. Recent studies show that task distribution plays a vital role in the model&#39;s performance. Conventional wisdom is that task diversity should improve the performance of meta-learning. In this work, we find evidence to the contrary; we study different task distributions on a myriad of models and datasets to evaluate the effect of task diversity on meta-learning algorithms. For this experiment, we train on multiple datasets, and with three broad classes of meta-learning models - Metric-based (i.e., Protonet, Matching Networks), Optimization-based (i.e., MAML, Reptile, and MetaOptNet), and Bayesian meta-learning models (i.e., CNAPs). Our experiments demonstrate that the effect of task diversity on all these algorithms follows a similar trend, and task diversity does not seem to offer any benefits to the learning of the model. Furthermore, we also demonstrate that even a handful of tasks, repeated over multiple batches, would be sufficient to achieve a performance similar to uniform sampling and draws into question the need for additional tasks to create better models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11775v1-abstract-full').style.display = 'none'; document.getElementById('2201.11775v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.13734">arXiv:2112.13734</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.13734">pdf</a>, <a href="https://arxiv.org/ps/2112.13734">ps</a>, <a href="https://arxiv.org/format/2112.13734">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Domain Balanced Sampling Improves Out-of-Distribution Generalization of Chest X-ray Pathology Prediction Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tetteh%2C+E">Enoch Tetteh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Viviano%2C+J">Joseph Viviano</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Krueger%2C+D">David Krueger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cohen%2C+J+P">Joseph Paul Cohen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.13734v2-abstract-short" style="display: inline;">
        Learning models that generalize under different distribution shifts in medical imaging has been a long-standing research challenge. There have been several proposals for efficient and robust visual representation learning among vision research practitioners, especially in the sensitive and critical biomedical domain. In this paper, we propose an idea for out-of-distribution generalization of chest&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.13734v2-abstract-full').style.display = 'inline'; document.getElementById('2112.13734v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.13734v2-abstract-full" style="display: none;">
        Learning models that generalize under different distribution shifts in medical imaging has been a long-standing research challenge. There have been several proposals for efficient and robust visual representation learning among vision research practitioners, especially in the sensitive and critical biomedical domain. In this paper, we propose an idea for out-of-distribution generalization of chest X-ray pathologies that uses a simple balanced batch sampling technique. We observed that balanced sampling between the multiple training datasets improves the performance over baseline models trained without balancing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.13734v2-abstract-full').style.display = 'none'; document.getElementById('2112.13734v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 December, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">MED-NEURIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.03215">arXiv:2112.03215</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.03215">pdf</a>, <a href="https://arxiv.org/format/2112.03215">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-scale Feature Learning Dynamics: Insights for Double Descent
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pezeshki%2C+M">Mohammad Pezeshki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mitra%2C+A">Amartya Mitra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lajoie%2C+G">Guillaume Lajoie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.03215v1-abstract-short" style="display: inline;">
        A key challenge in building theoretical foundations for deep learning is the complex optimization dynamics of neural networks, resulting from the high-dimensional interactions between the large number of network parameters. Such non-trivial dynamics lead to intriguing behaviors such as the phenomenon of &#34;double descent&#34; of the generalization error. The more commonly studied aspect of this phenomen&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.03215v1-abstract-full').style.display = 'inline'; document.getElementById('2112.03215v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.03215v1-abstract-full" style="display: none;">
        A key challenge in building theoretical foundations for deep learning is the complex optimization dynamics of neural networks, resulting from the high-dimensional interactions between the large number of network parameters. Such non-trivial dynamics lead to intriguing behaviors such as the phenomenon of &#34;double descent&#34; of the generalization error. The more commonly studied aspect of this phenomenon corresponds to model-wise double descent where the test error exhibits a second descent with increasing model complexity, beyond the classical U-shaped error curve. In this work, we investigate the origins of the less studied epoch-wise double descent in which the test error undergoes two non-monotonous transitions, or descents as the training time increases. By leveraging tools from statistical physics, we study a linear teacher-student setup exhibiting epoch-wise double descent similar to that in deep neural networks. In this setting, we derive closed-form analytical expressions for the evolution of generalization error over training. We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slower-learning features start to fit, resulting in a second descent in test error. We validate our findings through numerical experiments where our theory accurately predicts empirical findings and remains consistent with observations in deep neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.03215v1-abstract-full').style.display = 'none'; document.getElementById('2112.03215v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.09266">arXiv:2111.09266</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.09266">pdf</a>, <a href="https://arxiv.org/format/2111.09266">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GFlowNet Foundations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deleu%2C+T">Tristan Deleu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+E+J">Edward J. Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lahlou%2C+S">Salem Lahlou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tiwari%2C+M">Mo Tiwari</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+E">Emmanuel Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.09266v2-abstract-short" style="display: inline;">
        Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.09266v2-abstract-full').style.display = 'inline'; document.getElementById('2111.09266v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.09266v2-abstract-full" style="display: none;">
        Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.09266v2-abstract-full').style.display = 'none'; document.getElementById('2111.09266v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.15796">arXiv:2110.15796</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.15796">pdf</a>, <a href="https://arxiv.org/format/2110.15796">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Properties from Mechanisms: An Equivariance Perspective on Identifiable Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ahuja%2C+K">Kartik Ahuja</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hartford%2C+J">Jason Hartford</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.15796v1-abstract-short" style="display: inline;">
        A key goal of unsupervised representation learning is &#34;inverting&#34; a data generating process to recover its latent properties. Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the problem and ask, &#34;Can we instead i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15796v1-abstract-full').style.display = 'inline'; document.getElementById('2110.15796v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.15796v1-abstract-full" style="display: none;">
        A key goal of unsupervised representation learning is &#34;inverting&#34; a data generating process to recover its latent properties. Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the problem and ask, &#34;Can we instead identify latent properties by leveraging knowledge of the mechanisms that govern their evolution?&#34; We provide a complete characterization of the sources of non-identifiability as we vary knowledge about a set of possible mechanisms. In particular, we prove that if we know the exact mechanisms under which the latent properties evolve, then identification can be achieved up to any equivariances that are shared by the underlying mechanisms. We generalize this characterization to settings where we only know some hypothesis class over possible mechanisms, as well as settings where the mechanisms are stochastic. We demonstrate the power of this mechanism-based perspective by showing that we can leverage our results to generalize existing identifiable representation learning results. These results suggest that by exploiting inductive biases on mechanisms, it is possible to design a range of new identifiable representation learning approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15796v1-abstract-full').style.display = 'none'; document.getElementById('2110.15796v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.15245">arXiv:2110.15245</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.15245">pdf</a>, <a href="https://arxiv.org/ps/2110.15245">ps</a>, <a href="https://arxiv.org/format/2110.15245">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Roy%2C+N">Nicholas Roy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Posner%2C+I">Ingmar Posner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barfoot%2C+T">Tim Barfoot</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Beaudoin%2C+P">Philippe Beaudoin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bohg%2C+J">Jeannette Bohg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brock%2C+O">Oliver Brock</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Depatie%2C+I">Isabelle Depatie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fox%2C+D">Dieter Fox</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Koditschek%2C+D">Dan Koditschek</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lozano-Perez%2C+T">Tomas Lozano-Perez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mansinghka%2C+V">Vikash Mansinghka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pal%2C+C">Christopher Pal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Richards%2C+B">Blake Richards</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sadigh%2C+D">Dorsa Sadigh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schaal%2C+S">Stefan Schaal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sukhatme%2C+G">Gaurav Sukhatme</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Therien%2C+D">Denis Therien</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Toussaint%2C+M">Marc Toussaint</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Van+de+Panne%2C+M">Michiel Van de Panne</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.15245v1-abstract-short" style="display: inline;">
        Machine learning has long since become a keystone technology, accelerating science and applications in a broad range of domains. Consequently, the notion of applying learning methods to a particular problem set has become an established and valuable modus operandi to advance a particular field. In this article we argue that such an approach does not straightforwardly extended to robotics -- or to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15245v1-abstract-full').style.display = 'inline'; document.getElementById('2110.15245v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.15245v1-abstract-full" style="display: none;">
        Machine learning has long since become a keystone technology, accelerating science and applications in a broad range of domains. Consequently, the notion of applying learning methods to a particular problem set has become an established and valuable modus operandi to advance a particular field. In this article we argue that such an approach does not straightforwardly extended to robotics -- or to embodied intelligence more generally: systems which engage in a purposeful exchange of energy and information with a physical environment. In particular, the purview of embodied intelligent agents extends significantly beyond the typical considerations of main-stream machine learning approaches, which typically (i) do not consider operation under conditions significantly different from those encountered during training; (ii) do not consider the often substantial, long-lasting and potentially safety-critical nature of interactions during learning and deployment; (iii) do not require ready adaptation to novel tasks while at the same time (iv) effectively and efficiently curating and extending their models of the world through targeted and deliberate actions. In reality, therefore, these limitations result in learning-based systems which suffer from many of the same operational shortcomings as more traditional, engineering-based approaches when deployed on a robot outside a well defined, and often narrow operating envelope. Contrary to viewing embodied intelligence as another application domain for machine learning, here we argue that it is in fact a key driver for the advancement of machine learning technology. In this article our goal is to highlight challenges and opportunities that are specific to embodied intelligence and to propose research directions which may significantly advance the state-of-the-art in robot learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15245v1-abstract-full').style.display = 'none'; document.getElementById('2110.15245v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.10139">arXiv:2110.10139</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.10139">pdf</a>, <a href="https://arxiv.org/format/2110.10139">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Chunked Autoregressive GAN for Conditional Waveform Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Morrison%2C+M">Max Morrison</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+R">Rithesh Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+K">Kundan Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Seetharaman%2C+P">Prem Seetharaman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.10139v2-abstract-short" style="display: inline;">
        Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. Ho&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.10139v2-abstract-full').style.display = 'inline'; document.getElementById('2110.10139v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.10139v2-abstract-full" style="display: none;">
        Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast generation speed suitable for real-time or interactive applications, and maintains or improves subjective quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.10139v2-abstract-full').style.display = 'none'; document.getElementById('2110.10139v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.09419">arXiv:2110.09419</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.09419">pdf</a>, <a href="https://arxiv.org/format/2110.09419">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compositional Attention: Disentangling Search and Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mittal%2C+S">Sarthak Mittal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Raparthy%2C+S+C">Sharath Chandra Raparthy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rish%2C+I">Irina Rish</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lajoie%2C+G">Guillaume Lajoie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.09419v2-abstract-short" style="display: inline;">
        Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09419v2-abstract-full').style.display = 'inline'; document.getElementById('2110.09419v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.09419v2-abstract-full" style="display: none;">
        Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09419v2-abstract-full').style.display = 'none'; document.getElementById('2110.09419v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.07875">arXiv:2110.07875</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.07875">pdf</a>, <a href="https://arxiv.org/format/2110.07875">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Graph Neural Networks with Learnable Structural and Positional Representations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dwivedi%2C+V+P">Vijay Prakash Dwivedi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luu%2C+A+T">Anh Tuan Luu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Laurent%2C+T">Thomas Laurent</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bresson%2C+X">Xavier Bresson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.07875v2-abstract-short" style="display: inline;">
        Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.07875v2-abstract-full').style.display = 'inline'; document.getElementById('2110.07875v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.07875v2-abstract-full" style="display: none;">
        Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.07875v2-abstract-full').style.display = 'none'; document.getElementById('2110.07875v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code at https://github.com/vijaydwivedi75/gnn-lspe</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ICLR 2022 (https://openreview.net/pdf?id=wTTjnvGphYj)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.06399">arXiv:2110.06399</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.06399">pdf</a>, <a href="https://arxiv.org/format/2110.06399">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Inference with Neural Interpreters
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Rahaman%2C+N">Nasim Rahaman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gondal%2C+M+W">Muhammad Waleed Gondal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Joshi%2C+S">Shruti Joshi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gehler%2C+P">Peter Gehler</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Locatello%2C+F">Francesco Locatello</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sch%C3%B6lkopf%2C+B">Bernhard Schölkopf</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.06399v1-abstract-short" style="display: inline;">
        Modern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorize&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.06399v1-abstract-full').style.display = 'inline'; document.getElementById('2110.06399v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.06399v1-abstract-full" style="display: none;">
        Modern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call \emph{functions}. Inputs to the model are routed through a sequence of functions in a way that is end-to-end learned. The proposed architecture can flexibly compose computation along width and depth, and lends itself well to capacity extension after training. To demonstrate the versatility of Neural Interpreters, we evaluate it in two distinct settings: image classification and visual abstract reasoning on Raven Progressive Matrices. In the former, we show that Neural Interpreters perform on par with the vision transformer using fewer parameters, while being transferrable to a new task in a sample efficient manner. In the latter, we find that Neural Interpreters are competitive with respect to the state-of-the-art in terms of systematic generalization
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.06399v1-abstract-full').style.display = 'none'; document.getElementById('2110.06399v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.03372">arXiv:2110.03372</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.03372">pdf</a>, <a href="https://arxiv.org/format/2110.03372">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Biomolecules">q-bio.BM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unifying Likelihood-free Inference with Black-box Optimization and Beyond
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+D">Dinghuai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+J">Jie Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.03372v2-abstract-short" style="display: inline;">
        Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.03372v2-abstract-full').style.display = 'inline'; document.getElementById('2110.03372v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.03372v2-abstract-full" style="display: none;">
        Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing various sequence design methods based on this framework. We show how previous optimization approaches can be &#34;reinvented&#34; in our framework, and further propose new probabilistic black-box optimization algorithms. Extensive experiments on sequence design application illustrate the benefits of the proposed methodology.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.03372v2-abstract-full').style.display = 'none'; document.getElementById('2110.03372v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2022 spotlight</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.02871">arXiv:2110.02871</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.02871">pdf</a>, <a href="https://arxiv.org/format/2110.02871">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Schmidt%2C+V">Victor Schmidt</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luccioni%2C+A+S">Alexandra Sasha Luccioni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Teng%2C+M">Mélisande Teng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+T">Tianyu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reynaud%2C+A">Alexia Reynaud</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Raghupathi%2C+S">Sunand Raghupathi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cosne%2C+G">Gautier Cosne</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Juraver%2C+A">Adrien Juraver</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vardanyan%2C+V">Vahe Vardanyan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hernandez-Garcia%2C+A">Alex Hernandez-Garcia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.02871v1-abstract-short" style="display: inline;">
        Climate change is a major threat to humanity, and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding the effects of climate change, even though they may seem abstract and distant. Projecting the potential consequences of extreme climate events such as flooding in familiar places&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.02871v1-abstract-full').style.display = 'inline'; document.getElementById('2110.02871v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.02871v1-abstract-full" style="display: none;">
        Climate change is a major threat to humanity, and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding the effects of climate change, even though they may seem abstract and distant. Projecting the potential consequences of extreme climate events such as flooding in familiar places can help make the abstract impacts of climate change more concrete and encourage action. As part of a larger initiative to build a website that projects extreme climate events onto user-chosen photos, we present our solution to simulate photo-realistic floods on authentic images. To address this complex task in the absence of suitable training data, we propose ClimateGAN, a model that leverages both simulated and real data for unsupervised domain adaptation and conditional image generation. In this paper, we describe the details of our framework, thoroughly evaluate components of our architecture and demonstrate that our model is capable of robustly generating photo-realistic flooding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.02871v1-abstract-full').style.display = 'none'; document.getElementById('2110.02871v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.02429">arXiv:2109.02429</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.02429">pdf</a>, <a href="https://arxiv.org/format/2109.02429">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Neural Causal Models with Active Interventions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Scherrer%2C+N">Nino Scherrer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bilaniuk%2C+O">Olexa Bilaniuk</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Annadani%2C+Y">Yashas Annadani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schwab%2C+P">Patrick Schwab</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sch%C3%B6lkopf%2C+B">Bernhard Schölkopf</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mozer%2C+M+C">Michael C. Mozer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bauer%2C+S">Stefan Bauer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+N+R">Nan Rosemary Ke</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.02429v2-abstract-short" style="display: inline;">
        Discovering causal structures from data is a challenging inference problem of fundamental importance in all areas of science. The appealing properties of neural networks have recently led to a surge of interest in differentiable neural network-based methods for learning causal structures from data. So far, differentiable causal discovery has focused on static datasets of observational or fixed int&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.02429v2-abstract-full').style.display = 'inline'; document.getElementById('2109.02429v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.02429v2-abstract-full" style="display: none;">
        Discovering causal structures from data is a challenging inference problem of fundamental importance in all areas of science. The appealing properties of neural networks have recently led to a surge of interest in differentiable neural network-based methods for learning causal structures from data. So far, differentiable causal discovery has focused on static datasets of observational or fixed interventional origin. In this work, we introduce an active intervention targeting (AIT) method which enables a quick identification of the underlying causal structure of the data-generating process. Our method significantly reduces the required number of interactions compared with random intervention targeting and is applicable for both discrete and continuous optimization formulations of learning the underlying directed acyclic graph (DAG) from data. We examine the proposed method across multiple frameworks in a wide range of settings and demonstrate superior performance on multiple benchmarks from simulated to real-world data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.02429v2-abstract-full').style.display = 'none'; document.getElementById('2109.02429v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 September, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.02367">arXiv:2107.02367</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.02367">pdf</a>, <a href="https://arxiv.org/format/2107.02367">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discrete-Valued Neural Communication
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+D">Dianbo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lamb%2C+A">Alex Lamb</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+C">Chen Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mozer%2C+M+C">Michael Curtis Mozer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.02367v3-abstract-short" style="display: inline;">
        Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. In structured models, an interesting question is how to conduct dynamic and possibly sparse communication among the separate components. Here, we explore&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.02367v3-abstract-full').style.display = 'inline'; document.getElementById('2107.02367v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.02367v3-abstract-full" style="display: none;">
        Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. In structured models, an interesting question is how to conduct dynamic and possibly sparse communication among the separate components. Here, we explore the hypothesis that restricting the transmitted information among components to discrete representations is a beneficial bottleneck. The motivating intuition is human language in which communication occurs through discrete symbols. Even though individuals have different understandings of what a &#34;cat&#34; is based on their specific experiences, the shared discrete token makes it possible for communication among individuals to be unimpeded by individual differences in internal representation. To discretize the values of concepts dynamically communicated among specialist components, we extend the quantization mechanism from the Vector-Quantized Variational Autoencoder to multi-headed discretization with shared codebooks and use it for discrete-valued neural communication (DVNC). Our experiments show that DVNC substantially improves systematic generalization in a variety of architectures -- transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method very useful in practice. Moreover, we establish a theoretical justification of our discretization process, proving that it has the ability to increase noise robustness and reduce the underlying dimensionality of the model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.02367v3-abstract-full').style.display = 'none'; document.getElementById('2107.02367v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.00848">arXiv:2107.00848</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.00848">pdf</a>, <a href="https://arxiv.org/format/2107.00848">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+N+R">Nan Rosemary Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Didolkar%2C+A">Aniket Didolkar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mittal%2C+S">Sarthak Mittal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lajoie%2C+G">Guillaume Lajoie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bauer%2C+S">Stefan Bauer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rezende%2C+D">Danilo Rezende</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mozer%2C+M">Michael Mozer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pal%2C+C">Christopher Pal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.00848v1-abstract-short" style="display: inline;">
        Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00848v1-abstract-full').style.display = 'inline'; document.getElementById('2107.00848v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.00848v1-abstract-full" style="display: none;">
        Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. However, we note that existing environments for studying causal induction are poorly suited for this objective because they have complicated task-specific causal graphs which are impossible to manipulate parametrically (e.g., number of nodes, sparsity, causal chain length, etc.). In this work, our goal is to facilitate research in learning representations of high-level variables as well as causal structures among them. In order to systematically probe the ability of methods to identify these variables and structures, we design a suite of benchmarking RL environments. We evaluate various representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00848v1-abstract-full').style.display = 'none'; document.getElementById('2107.00848v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.00793">arXiv:2107.00793</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.00793">pdf</a>, <a href="https://arxiv.org/format/2107.00793">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Causal-Neural Connection: Expressiveness, Learnability, and Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+K">Kevin Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+K">Kai-Zhan Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bareinboim%2C+E">Elias Bareinboim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.00793v2-abstract-short" style="display: inline;">
        One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00793v2-abstract-full').style.display = 'inline'; document.getElementById('2107.00793v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.00793v2-abstract-full" style="display: none;">
        One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00793v2-abstract-full').style.display = 'none'; document.getElementById('2107.00793v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages main body (53 total pages with references and appendix), 5 figures in main body (20 total figures including appendix)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.07635">arXiv:2106.07635</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.07635">pdf</a>, <a href="https://arxiv.org/format/2106.07635">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Variational Causal Networks: Approximate Bayesian Inference over Causal Structures
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Annadani%2C+Y">Yashas Annadani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rothfuss%2C+J">Jonas Rothfuss</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lacoste%2C+A">Alexandre Lacoste</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Scherrer%2C+N">Nino Scherrer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bauer%2C+S">Stefan Bauer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.07635v1-abstract-short" style="display: inline;">
        Learning the causal structure that underlies data is a crucial step towards robust real-world decision making. The majority of existing work in causal inference focuses on determining a single directed acyclic graph (DAG) or a Markov equivalence class thereof. However, a crucial aspect to acting intelligently upon the knowledge about causal structure which has been inferred from finite data demand&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07635v1-abstract-full').style.display = 'inline'; document.getElementById('2106.07635v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.07635v1-abstract-full" style="display: none;">
        Learning the causal structure that underlies data is a crucial step towards robust real-world decision making. The majority of existing work in causal inference focuses on determining a single directed acyclic graph (DAG) or a Markov equivalence class thereof. However, a crucial aspect to acting intelligently upon the knowledge about causal structure which has been inferred from finite data demands reasoning about its uncertainty. For instance, planning interventions to find out more about the causal mechanisms that govern our data requires quantifying epistemic uncertainty over DAGs. While Bayesian causal inference allows to do so, the posterior over DAGs becomes intractable even for a small number of variables. Aiming to overcome this issue, we propose a form of variational inference over the graphs of Structural Causal Models (SCMs). To this end, we introduce a parametric variational family modelled by an autoregressive distribution over the space of discrete DAGs. Its number of parameters does not grow exponentially with the number of variables and can be tractably learned by maximising an Evidence Lower Bound (ELBO). In our experiments, we demonstrate that the proposed variational posterior is able to provide a good approximation of the true posterior.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.07635v1-abstract-full').style.display = 'none'; document.getElementById('2106.07635v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.06607">arXiv:2106.06607</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.06607">pdf</a>, <a href="https://arxiv.org/format/2106.06607">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ahuja%2C+K">Kartik Ahuja</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Caballero%2C+E">Ethan Caballero</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+D">Dinghuai Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mitliagkas%2C+I">Ioannis Mitliagkas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rish%2C+I">Irina Rish</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.06607v1-abstract-short" style="display: inline;">
        The invariance principle from causality is at the heart of notable approaches such as invariant risk minimization (IRM) that seek to address out-of-distribution (OOD) generalization failures. Despite the promising theory, invariance principle-based approaches fail in common classification tasks, where invariant (causal) features capture all the information about the label. Are these failures due t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.06607v1-abstract-full').style.display = 'inline'; document.getElementById('2106.06607v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.06607v1-abstract-full" style="display: none;">
        The invariance principle from causality is at the heart of notable approaches such as invariant risk minimization (IRM) that seek to address out-of-distribution (OOD) generalization failures. Despite the promising theory, invariance principle-based approaches fail in common classification tasks, where invariant (causal) features capture all the information about the label. Are these failures due to the methods failing to capture the invariance? Or is the invariance principle itself insufficient? To answer these questions, we revisit the fundamental assumptions in linear regression tasks, where invariance-based approaches were shown to provably generalize OOD. In contrast to the linear regression tasks, we show that for linear classification tasks we need much stronger restrictions on the distribution shifts, or otherwise OOD generalization is impossible. Furthermore, even with appropriate restrictions on distribution shifts in place, we show that the invariance principle alone is insufficient. We prove that a form of the information bottleneck constraint along with invariance helps address key failures when invariant features capture all the information about the label and also retains the existing success when they do not. We propose an approach that incorporates both of these principles and demonstrate its effectiveness in several experiments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.06607v1-abstract-full').style.display = 'none'; document.getElementById('2106.06607v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04624">arXiv:2106.04624</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04624">pdf</a>, <a href="https://arxiv.org/format/2106.04624">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SpeechBrain: A General-Purpose Speech Toolkit
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Parcollet%2C+T">Titouan Parcollet</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Plantinga%2C+P">Peter Plantinga</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rouhe%2C+A">Aku Rouhe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cornell%2C+S">Samuele Cornell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lugosch%2C+L">Loren Lugosch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Subakan%2C+C">Cem Subakan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dawalatabad%2C+N">Nauman Dawalatabad</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Heba%2C+A">Abdelwahab Heba</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+J">Jianyuan Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chou%2C+J">Ju-Chieh Chou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yeh%2C+S">Sung-Lin Yeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+S">Szu-Wei Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+C">Chien-Feng Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rastorgueva%2C+E">Elena Rastorgueva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grondin%2C+F">François Grondin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aris%2C+W">William Aris</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Na%2C+H">Hwidong Na</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Y">Yan Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=De+Mori%2C+R">Renato De Mori</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04624v1-abstract-short" style="display: inline;">
        SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04624v1-abstract-full').style.display = 'inline'; document.getElementById('2106.04624v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04624v1-abstract-full" style="display: none;">
        SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04624v1-abstract-full').style.display = 'none'; document.getElementById('2106.04624v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04399">arXiv:2106.04399</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04399">pdf</a>, <a href="https://arxiv.org/format/2106.04399">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+E">Emmanuel Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jain%2C+M">Moksh Jain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Korablyov%2C+M">Maksym Korablyov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Precup%2C+D">Doina Precup</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04399v2-abstract-short" style="display: inline;">
        This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diver&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04399v2-abstract-full').style.display = 'inline'; document.getElementById('2106.04399v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04399v2-abstract-full" style="display: none;">
        This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04399v2-abstract-full').style.display = 'none'; document.getElementById('2106.04399v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.02097">arXiv:2106.02097</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.02097">pdf</a>, <a href="https://arxiv.org/format/2106.02097">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+M">Mingde Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhen Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luan%2C+S">Sitao Luan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+S">Shuyuan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Precup%2C+D">Doina Precup</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.02097v3-abstract-short" style="display: inline;">
        We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends at each planning step to be small. In experiments, we investigate the bottleneck mechanism with several sets of customized en&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02097v3-abstract-full').style.display = 'inline'; document.getElementById('2106.02097v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.02097v3-abstract-full" style="display: none;">
        We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends at each planning step to be small. In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring different challenges. We consistently observe that the design allows the planning agents to generalize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution generalization performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.02097v3-abstract-full').style.display = 'none'; document.getElementById('2106.02097v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS camera-ready version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.08710">arXiv:2105.08710</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.08710">pdf</a>, <a href="https://arxiv.org/format/2105.08710">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast and Slow Learning of Recurrent Independent Mechanisms
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Madan%2C+K">Kanika Madan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+N+R">Nan Rosemary Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sch%C3%B6lkopf%2C+B">Bernhard Schölkopf</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.08710v2-abstract-short" style="display: inline;">
        Decomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08710v2-abstract-full').style.display = 'inline'; document.getElementById('2105.08710v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.08710v2-abstract-full" style="display: none;">
        Decomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic manner to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the selected modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08710v2-abstract-full').style.display = 'none'; document.getElementById('2105.08710v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICLR&#39;21</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.07246">arXiv:2105.07246</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.07246">pdf</a>, <a href="https://arxiv.org/format/2105.07246">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Biomolecules">q-bio.BM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Minkai Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wujie Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+S">Shitong Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+C">Chence Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gomez-Bombarelli%2C+R">Rafael Gomez-Bombarelli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+J">Jian Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.07246v2-abstract-short" style="display: inline;">
        Predicting molecular conformations (or 3D structures) from molecular graphs is a fundamental problem in many applications. Most existing approaches are usually divided into two steps by first predicting the distances between atoms and then generating a 3D structure through optimizing a distance geometry problem. However, the distances predicted with such two-stage approaches may not be able to con&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07246v2-abstract-full').style.display = 'inline'; document.getElementById('2105.07246v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.07246v2-abstract-full" style="display: none;">
        Predicting molecular conformations (or 3D structures) from molecular graphs is a fundamental problem in many applications. Most existing approaches are usually divided into two steps by first predicting the distances between atoms and then generating a 3D structure through optimizing a distance geometry problem. However, the distances predicted with such two-stage approaches may not be able to consistently preserve the geometry of local atomic neighborhoods, making the generated structures unsatisfying. In this paper, we propose an end-to-end solution for molecular conformation prediction called ConfVAE based on the conditional variational autoencoder framework. Specifically, the molecular graph is first encoded in a latent space, and then the 3D structures are generated by solving a principled bilevel optimization program. Extensive experiments on several benchmark data sets prove the effectiveness of our proposed approach over existing state-of-the-art approaches. Code is available at \url{https://github.com/MinkaiXu/ConfVAE-ICML21}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07246v2-abstract-full').style.display = 'none'; document.getElementById('2105.07246v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICML 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.07763">arXiv:2104.07763</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.07763">pdf</a>, <a href="https://arxiv.org/format/2104.07763">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comparative Study of Learning Outcomes for Online Learning Platforms
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=St-Hilaire%2C+F">Francois St-Hilaire</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burns%2C+N">Nathan Burns</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Belfer%2C+R">Robert Belfer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shayan%2C+M">Muhammad Shayan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Smofsky%2C+A">Ariella Smofsky</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vu%2C+D+D">Dung Do Vu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Frau%2C+A">Antoine Frau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Potochny%2C+J">Joseph Potochny</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Faraji%2C+F">Farid Faraji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pavero%2C+V">Vincent Pavero</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ko%2C+N">Neroli Ko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ching%2C+A+O">Ansona Onyi Ching</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Elkins%2C+S">Sabina Elkins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stepanyan%2C+A">Anush Stepanyan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Matajova%2C+A">Adela Matajova</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Charlin%2C+L">Laurent Charlin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Serban%2C+I+V">Iulian Vlad Serban</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kochmar%2C+E">Ekaterina Kochmar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.07763v1-abstract-short" style="display: inline;">
        Personalization and active learning are key aspects to successful learning. These aspects are important to address in intelligent educational applications, as they help systems to adapt and close the gap between students with varying abilities, which becomes increasingly important in the context of online and distance learning. We run a comparative head-to-head study of learning outcomes for two p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.07763v1-abstract-full').style.display = 'inline'; document.getElementById('2104.07763v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.07763v1-abstract-full" style="display: none;">
        Personalization and active learning are key aspects to successful learning. These aspects are important to address in intelligent educational applications, as they help systems to adapt and close the gap between students with varying abilities, which becomes increasingly important in the context of online and distance learning. We run a comparative head-to-head study of learning outcomes for two popular online learning platforms: Platform A, which follows a traditional model delivering content over a series of lecture videos and multiple-choice quizzes, and Platform B, which creates a personalized learning environment and provides problem-solving exercises and personalized feedback. We report on the results of our study using pre- and post-assessment quizzes with participants taking courses on an introductory data science topic on two platforms. We observe a statistically significant increase in the learning outcomes on Platform B, highlighting the impact of well-designed and well-engineered technology supporting active learning and problem-based learning in online education. Moreover, the results of the self-assessment questionnaire, where participants reported on perceived learning gains, suggest that participants using Platform B improve their metacognition.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.07763v1-abstract-full').style.display = 'none'; document.getElementById('2104.07763v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 3 figures, 2 tables, accepted at AIED 2021 (2021 Conference on Artificial Intelligence in Education)</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.0; I.2.1; I.2.7; K.3.1; G.4
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02242">arXiv:2104.02242</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02242">pdf</a>, <a href="https://arxiv.org/format/2104.02242">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HBert + BiasCorp -- Fighting Racism on the Web
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Onabola%2C+O">Olawale Onabola</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+Z">Zhuang Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+Y">Yang Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Akera%2C+B">Benjamin Akera</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ibraheem%2C+A">Abdulrahman Ibraheem</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+J">Jia Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+D">Dianbo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02242v3-abstract-short" style="display: inline;">
        Subtle and overt racism is still present both in physical and online communities today and has impacted many lives in different segments of the society. In this short piece of work, we present how we&#39;re tackling this societal issue with Natural Language Processing. We are releasing BiasCorp, a dataset containing 139,090 comments and news segment from three specific sources - Fox News, BreitbartNew&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02242v3-abstract-full').style.display = 'inline'; document.getElementById('2104.02242v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02242v3-abstract-full" style="display: none;">
        Subtle and overt racism is still present both in physical and online communities today and has impacted many lives in different segments of the society. In this short piece of work, we present how we&#39;re tackling this societal issue with Natural Language Processing. We are releasing BiasCorp, a dataset containing 139,090 comments and news segment from three specific sources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually annotated) is ready for publication. We are currently in the final phase of manually labeling the remaining dataset using Amazon Mechanical Turk. BERT has been used widely in several downstream tasks. In this work, we present hBERT, where we modify certain layers of the pretrained BERT model with the new Hopfield Layer. hBert generalizes well across different distributions with the added advantage of a reduced model complexity. We are also releasing a JavaScript library and a Chrome Extension Application, to help developers make use of our trained model in web applications (say chat application) and for users to identify and report racially biased contents on the web respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02242v3-abstract-full').style.display = 'none'; document.getElementById('2104.02242v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 October, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          2021.ltedi-1.4 2021.ltedi-1.4
        

        

        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ltedi-1. 4 (2021) 26-33
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.01937">arXiv:2103.01937</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.01937">pdf</a>, <a href="https://arxiv.org/format/2103.01937">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Neural Production Systems: Learning Rule-Governed Visual Dynamics
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Didolkar%2C+A">Aniket Didolkar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+N+R">Nan Rosemary Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Blundell%2C+C">Charles Blundell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Beaudoin%2C+P">Philippe Beaudoin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Heess%2C+N">Nicolas Heess</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mozer%2C+M">Michael Mozer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.01937v3-abstract-short" style="display: inline;">
        Visual environments are structured, consisting of distinct objects or entities. These entities have properties -- both visible and latent -- that determine the manner in which they interact with one another. To partition images into entities, deep-learning researchers have proposed structural inductive biases such as slot-based architectures. To model interactions among entities, equivariant graph&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01937v3-abstract-full').style.display = 'inline'; document.getElementById('2103.01937v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.01937v3-abstract-full" style="display: none;">
        Visual environments are structured, consisting of distinct objects or entities. These entities have properties -- both visible and latent -- that determine the manner in which they interact with one another. To partition images into entities, deep-learning researchers have proposed structural inductive biases such as slot-based architectures. To model interactions among entities, equivariant graph neural nets (GNNs) are used, but these are not particularly well suited to the task for two reasons. First, GNNs do not predispose interactions to be sparse, as relationships among independent entities are likely to be. Second, GNNs do not factorize knowledge about interactions in an entity-conditional manner. As an alternative, we take inspiration from cognitive science and resurrect a classic approach, production systems, which consist of a set of rule templates that are applied by binding placeholder variables in the rules to specific entities. Rules are scored on their match to entities, and the best fitting rules are applied to update entity properties. In a series of experiments, we demonstrate that this architecture achieves a flexible, dynamic flow of control and serves to factorize entity-specific and rule-based information. This disentangling of knowledge achieves robust future-state prediction in rich visual environments, outperforming state-of-the-art methods using GNNs, and allows for the extrapolation from simple (few object) environments to more complex environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01937v3-abstract-full').style.display = 'none'; document.getElementById('2103.01937v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS&#39;21</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.01197">arXiv:2103.01197</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.01197">pdf</a>, <a href="https://arxiv.org/format/2103.01197">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Coordination Among Neural Modules Through a Shared Global Workspace
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Didolkar%2C+A">Aniket Didolkar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lamb%2C+A">Alex Lamb</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Badola%2C+K">Kartikeya Badola</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+N+R">Nan Rosemary Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rahaman%2C+N">Nasim Rahaman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Binas%2C+J">Jonathan Binas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Blundell%2C+C">Charles Blundell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mozer%2C+M">Michael Mozer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.01197v2-abstract-short" style="display: inline;">
        Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01197v2-abstract-full').style.display = 'inline'; document.getElementById('2103.01197v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.01197v2-abstract-full" style="display: none;">
        Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions; object-centric architectures make use of graph neural networks to model interactions among entities. However, pairwise interactions may not achieve global coordination or a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.01197v2-abstract-full').style.display = 'none'; document.getElementById('2103.01197v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR&#39;22 accepted paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.00336">arXiv:2103.00336</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.00336">pdf</a>, <a href="https://arxiv.org/format/2103.00336">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transformers with Competitive Ensembles of Independent Mechanisms
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lamb%2C+A">Alex Lamb</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+D">Di He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+G">Guolin Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+C">Chien-Feng Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.00336v1-abstract-short" style="display: inline;">
        An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated. This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as ir&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00336v1-abstract-full').style.display = 'inline'; document.getElementById('2103.00336v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.00336v1-abstract-full" style="display: none;">
        An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated. This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed. For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically. In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation. This potentially throws unrelated sources of information together, and limits the Transformer&#39;s ability to capture independent mechanisms. To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention. Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent. We study TIM on a large-scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00336v1-abstract-full').style.display = 'none'; document.getElementById('2103.00336v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under Review, ICML 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.11107">arXiv:2102.11107</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.11107">pdf</a>, <a href="https://arxiv.org/format/2102.11107">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Causal Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sch%C3%B6lkopf%2C+B">Bernhard Schölkopf</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Locatello%2C+F">Francesco Locatello</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bauer%2C+S">Stefan Bauer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+N+R">Nan Rosemary Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kalchbrenner%2C+N">Nal Kalchbrenner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.11107v1-abstract-short" style="display: inline;">
        The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assay&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11107v1-abstract-full').style.display = 'inline'; document.getElementById('2102.11107v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.11107v1-abstract-full" style="display: none;">
        The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11107v1-abstract-full').style.display = 'none'; document.getElementById('2102.11107v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Special Issue of Proceedings of the IEEE - Advances in Machine Learning and Deep Neural Networks</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.10240">arXiv:2102.10240</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.10240">pdf</a>, <a href="https://arxiv.org/format/2102.10240">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Chemical Physics">physics.chem-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Neural Generative Dynamics for Molecular Conformation Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+M">Minkai Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+S">Shitong Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+J">Jian Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+J">Jian Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.10240v3-abstract-short" style="display: inline;">
        We study how to generate molecule conformations (i.e., 3D structures) from a molecular graph. Traditional methods, such as molecular dynamics, sample conformations via computationally expensive simulations. Recently, machine learning methods have shown great potential by training on a large collection of conformation data. Challenges arise from the limited model capacity for capturing complex dist&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10240v3-abstract-full').style.display = 'inline'; document.getElementById('2102.10240v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.10240v3-abstract-full" style="display: none;">
        We study how to generate molecule conformations (i.e., 3D structures) from a molecular graph. Traditional methods, such as molecular dynamics, sample conformations via computationally expensive simulations. Recently, machine learning methods have shown great potential by training on a large collection of conformation data. Challenges arise from the limited model capacity for capturing complex distributions of conformations and the difficulty in modeling long-range dependencies between atoms. Inspired by the recent progress in deep generative models, in this paper, we propose a novel probabilistic framework to generate valid and diverse conformations given a molecular graph. We propose a method combining the advantages of both flow-based and energy-based models, enjoying: (1) a high model capacity to estimate the multimodal conformation distribution; (2) explicitly capturing the complex long-range dependencies between atoms in the observation space. Extensive experiments demonstrate the superior performance of the proposed method on several benchmarks, including conformation generation and distance modeling tasks, with a significant improvement over existing generative models for molecular conformation sampling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.10240v3-abstract-full').style.display = 'none'; document.getElementById('2102.10240v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICLR 2021. Code is available at \url{https://github.com/DeepGraphLearning/CGCF-ConfGen}</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.08501">arXiv:2102.08501</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.08501">pdf</a>, <a href="https://arxiv.org/format/2102.08501">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DEUP: Direct Epistemic Uncertainty Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lahlou%2C+S">Salem Lahlou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jain%2C+M">Moksh Jain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nekoei%2C+H">Hadi Nekoei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Butoi%2C+V">Victor Butoi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bertin%2C+P">Paul Bertin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rector-Brooks%2C+J">Jarrid Rector-Brooks</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Korablyov%2C+M">Maksym Korablyov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.08501v2-abstract-short" style="display: inline;">
        Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of knowledge of the learner. Whereas previous work was focusing on model variance, we propose a principled approach for directly estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of epis&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.08501v2-abstract-full').style.display = 'inline'; document.getElementById('2102.08501v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.08501v2-abstract-full" style="display: none;">
        Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of knowledge of the learner. Whereas previous work was focusing on model variance, we propose a principled approach for directly estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of epistemic uncertainty includes the effect of model bias (or misspecification) and is useful in interactive learning environments arising in active learning or reinforcement learning. In addition to discussing these properties of Direct Epistemic Uncertainty Prediction (DEUP), we illustrate its advantage against existing methods for uncertainty estimation on downstream tasks including sequential model optimization and reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic classification of images and for estimating uncertainty about synergistic drug combinations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.08501v2-abstract-full').style.display = 'none'; document.getElementById('2102.08501v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.03869">arXiv:2102.03869</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.03869">pdf</a>, <a href="https://arxiv.org/format/2102.03869">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Structured Sparsity Inducing Adaptive Optimizers for Deep Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Deleu%2C+T">Tristan Deleu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.03869v1-abstract-short" style="display: inline;">
        The parameters of a neural network are naturally organized in groups, some of which might not contribute to its overall performance. To prune out unimportant groups of parameters, we can include some non-differentiable penalty to the objective function, and minimize it using proximal gradient methods. In this paper, we derive the weighted proximal operator, which is a necessary component of these&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03869v1-abstract-full').style.display = 'inline'; document.getElementById('2102.03869v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.03869v1-abstract-full" style="display: none;">
        The parameters of a neural network are naturally organized in groups, some of which might not contribute to its overall performance. To prune out unimportant groups of parameters, we can include some non-differentiable penalty to the objective function, and minimize it using proximal gradient methods. In this paper, we derive the weighted proximal operator, which is a necessary component of these proximal methods, of two structured sparsity inducing penalties. Moreover, they can be approximated efficiently with a numerical solver, and despite this approximation, we prove that existing convergence guarantees are preserved when these operators are integrated as part of a generic adaptive proximal method. Finally, we show that this adaptive method, together with the weighted proximal operators derived here, is indeed capable of finding solutions with structure in their sparsity patterns, on representative examples from computer vision and natural language processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.03869v1-abstract-full').style.display = 'none'; document.getElementById('2102.03869v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.05536">arXiv:2101.05536</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.05536">pdf</a>, <a href="https://arxiv.org/format/2101.05536">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling Equilibrium Propagation to Deep ConvNets by Drastically Reducing its Gradient Estimator Bias
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Laborieux%2C+A">Axel Laborieux</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ernoult%2C+M">Maxence Ernoult</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Scellier%2C+B">Benjamin Scellier</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grollier%2C+J">Julie Grollier</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Querlioz%2C+D">Damien Querlioz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.05536v1-abstract-short" style="display: inline;">
        Equilibrium Propagation (EP) is a biologically-inspired counterpart of Backpropagation Through Time (BPTT) which, owing to its strong theoretical guarantees and the locality in space of its learning rule, fosters the design of energy-efficient hardware dedicated to learning. In practice, however, EP does not scale to visual tasks harder than MNIST. In this work, we show that a bias in the gradient&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05536v1-abstract-full').style.display = 'inline'; document.getElementById('2101.05536v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.05536v1-abstract-full" style="display: none;">
        Equilibrium Propagation (EP) is a biologically-inspired counterpart of Backpropagation Through Time (BPTT) which, owing to its strong theoretical guarantees and the locality in space of its learning rule, fosters the design of energy-efficient hardware dedicated to learning. In practice, however, EP does not scale to visual tasks harder than MNIST. In this work, we show that a bias in the gradient estimate of EP, inherent in the use of finite nudging, is responsible for this phenomenon and that cancelling it allows training deep ConvNets by EP, including architectures with distinct forward and backward connections. These results highlight EP as a scalable approach to compute error gradients in deep neural networks, thereby motivating its hardware implementation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05536v1-abstract-full').style.display = 'none'; document.getElementById('2101.05536v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2020 Workshop : &#34;Beyond Backpropagation Novel Ideas for Training Neural Architectures&#34;. arXiv admin note: substantial text overlap with arXiv:2006.03824</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.05013">arXiv:2012.05013</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.05013">pdf</a>, <a href="https://arxiv.org/format/2012.05013">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Machine Learning for Glacier Monitoring in the Hindu Kush Himalaya
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Baraka%2C+S">Shimaa Baraka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Akera%2C+B">Benjamin Akera</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aryal%2C+B">Bibek Aryal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sherpa%2C+T">Tenzing Sherpa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shresta%2C+F">Finu Shresta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ortiz%2C+A">Anthony Ortiz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sankaran%2C+K">Kris Sankaran</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ferres%2C+J+L">Juan Lavista Ferres</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Matin%2C+M">Mir Matin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.05013v1-abstract-short" style="display: inline;">
        Glacier mapping is key to ecological monitoring in the hkh region. Climate change poses a risk to individuals whose livelihoods depend on the health of glacier ecosystems. In this work, we present a machine learning based approach to support ecological monitoring, with a focus on glaciers. Our approach is based on semi-automated mapping from satellite images. We utilize readily available remote se&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.05013v1-abstract-full').style.display = 'inline'; document.getElementById('2012.05013v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.05013v1-abstract-full" style="display: none;">
        Glacier mapping is key to ecological monitoring in the hkh region. Climate change poses a risk to individuals whose livelihoods depend on the health of glacier ecosystems. In this work, we present a machine learning based approach to support ecological monitoring, with a focus on glaciers. Our approach is based on semi-automated mapping from satellite images. We utilize readily available remote sensing data to create a model to identify and outline both clean ice and debris-covered glaciers from satellite imagery. We also release data and develop a web tool that allows experts to visualize and correct model predictions, with the ultimate aim of accelerating the glacier mapping process.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.05013v1-abstract-full').style.display = 'none'; document.getElementById('2012.05013v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for a spotlight talk and a poster at the Tackling Climate Change with Machine Learning workshop at NeurIPS 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.15091">arXiv:2011.15091</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.15091">pdf</a>, <a href="https://arxiv.org/ps/2011.15091">ps</a>, <a href="https://arxiv.org/format/2011.15091">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Inductive Biases for Deep Learning of Higher-Level Cognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.15091v3-abstract-short" style="display: inline;">
        A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behavior of complex systems like brains&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.15091v3-abstract-full').style.display = 'inline'; document.getElementById('2011.15091v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.15091v3-abstract-full" style="display: none;">
        A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behavior of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans&#39; abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.15091v3-abstract-full').style.display = 'none'; document.getElementById('2011.15091v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This document contains a review of authors research as part of the requirement of AG&#39;s predoctoral exam, an overview of the main contributions of the authors few recent papers (co-authored with several other co-authors) as well as a vision of proposed future research</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.13042">arXiv:2011.13042</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.13042">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RetroGNN: Approximating Retrosynthesis by Graph Neural Networks for De Novo Drug Design
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+C">Cheng-Hao Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Korablyov%2C+M">Maksym Korablyov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jastrz%C4%99bski%2C+S">Stanisław Jastrzębski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=W%C5%82odarczyk-Pruszy%C5%84ski%2C+P">Paweł Włodarczyk-Pruszyński</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Segler%2C+M+H+S">Marwin H. S. Segler</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.13042v1-abstract-short" style="display: inline;">
        De novo molecule generation often results in chemically unfeasible molecules. A natural idea to mitigate this problem is to bias the search process towards more easily synthesizable molecules using a proxy for synthetic accessibility. However, using currently available proxies still results in highly unrealistic compounds. We investigate the feasibility of training deep graph neural networks to ap&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.13042v1-abstract-full').style.display = 'inline'; document.getElementById('2011.13042v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.13042v1-abstract-full" style="display: none;">
        De novo molecule generation often results in chemically unfeasible molecules. A natural idea to mitigate this problem is to bias the search process towards more easily synthesizable molecules using a proxy for synthetic accessibility. However, using currently available proxies still results in highly unrealistic compounds. We investigate the feasibility of training deep graph neural networks to approximate the outputs of a retrosynthesis planning software, and their use to bias the search process. We evaluate our method on a benchmark involving searching for drug-like molecules with antibiotic properties. Compared to enumerating over five million existing molecules from the ZINC database, our approach finds molecules predicted to be more likely to be antibiotics while maintaining good drug-like properties and being easily synthesizable. Importantly, our deep neural network can successfully filter out hard to synthesize molecules while achieving a $10^5$ times speed-up over using the retrosynthesis planning software.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.13042v1-abstract-full').style.display = 'none'; document.getElementById('2011.13042v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Machine Learning for Molecules Workshop at NeurIPS 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.09468">arXiv:2011.09468</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.09468">pdf</a>, <a href="https://arxiv.org/format/2011.09468">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Dynamical Systems">math.DS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gradient Starvation: A Learning Proclivity in Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pezeshki%2C+M">Mohammad Pezeshki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaba%2C+S">Sékou-Oumar Kaba</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Precup%2C+D">Doina Precup</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lajoie%2C+G">Guillaume Lajoie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.09468v4-abstract-short" style="display: inline;">
        We identify and formalize a fundamental gradient descent phenomenon resulting in a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This work provides a theoretical explanation for the e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.09468v4-abstract-full').style.display = 'inline'; document.getElementById('2011.09468v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.09468v4-abstract-full" style="display: none;">
        We identify and formalize a fundamental gradient descent phenomenon resulting in a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This work provides a theoretical explanation for the emergence of such feature imbalance in neural networks. Using tools from Dynamical Systems theory, we identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on our proposed formalism, we develop guarantees for a novel regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation. We illustrate our findings with simple and real-world out-of-distribution (OOD) generalization experiments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.09468v4-abstract-full').style.display = 'none'; document.getElementById('2011.09468v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceeding of NeurIPS 2021</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>