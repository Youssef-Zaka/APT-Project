<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;31 of 31 results for author: <span class="mathjax">Bahdanau, D</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Bahdanau%2C+D">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Bahdanau, D">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Bahdanau%2C+D&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Bahdanau, D">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.01959">arXiv:2204.01959</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.01959">pdf</a>, <a href="https://arxiv.org/format/2204.01959">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data Augmentation for Intent Classification with Off-the-shelf Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Sahu%2C+G">Gaurav Sahu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rodriguez%2C+P">Pau Rodriguez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Laradji%2C+I+H">Issam H. Laradji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Atighehchian%2C+P">Parmida Atighehchian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vazquez%2C+D">David Vazquez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.01959v1-abstract-short" style="display: inline;">
        Data augmentation is a widely employed technique to alleviate the problem of data scarcity. In this work, we propose a prompting-based approach to generate labelled training data for intent classification with off-the-shelf language models (LMs) such as GPT-3. An advantage of this method is that no task-specific LM-fine-tuning for data generation is required; hence the method requires no hyper-par&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01959v1-abstract-full').style.display = 'inline'; document.getElementById('2204.01959v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.01959v1-abstract-full" style="display: none;">
        Data augmentation is a widely employed technique to alleviate the problem of data scarcity. In this work, we propose a prompting-based approach to generate labelled training data for intent classification with off-the-shelf language models (LMs) such as GPT-3. An advantage of this method is that no task-specific LM-fine-tuning for data generation is required; hence the method requires no hyper-parameter tuning and is applicable even when the available training data is very scarce. We evaluate the proposed method in a few-shot setting on four diverse intent classification tasks. We find that GPT-generated data significantly boosts the performance of intent classifiers when intents in consideration are sufficiently distinct from each other. In tasks with semantically close intents, we observe that the generated data is less helpful. Our analysis shows that this is because GPT often generates utterances that belong to a closely-related intent instead of the desired one. We present preliminary evidence that a prompting-based GPT classifier could be helpful in filtering the generated data to enhance its quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01959v1-abstract-full').style.display = 'none'; document.getElementById('2204.01959v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to 4th Workshop on NLP for Conversational AI, ACL 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.00498">arXiv:2204.00498</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.00498">pdf</a>, <a href="https://arxiv.org/format/2204.00498">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluating the Text-to-SQL Capabilities of Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Rajkumar%2C+N">Nitarshan Rajkumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+R">Raymond Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.00498v1-abstract-short" style="display: inline;">
        We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00498v1-abstract-full').style.display = 'inline'; document.getElementById('2204.00498v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.00498v1-abstract-full" style="display: none;">
        We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00498v1-abstract-full').style.display = 'none'; document.getElementById('2204.00498v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.00578">arXiv:2112.00578</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.00578">pdf</a>, <a href="https://arxiv.org/format/2112.00578">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Systematic Generalization with Edge Transformers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bergen%2C+L">Leon Bergen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Donnell%2C+T+J">Timothy J. O&#39;Donnell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.00578v1-abstract-short" style="display: inline;">
        Recent research suggests that systematic generalization in natural language understanding remains a challenge for state-of-the-art neural models such as Transformers and Graph Neural Networks. To tackle this challenge, we propose Edge Transformer, a new model that combines inspiration from Transformers and rule-based symbolic AI. The first key idea in Edge Transformers is to associate vector state&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.00578v1-abstract-full').style.display = 'inline'; document.getElementById('2112.00578v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.00578v1-abstract-full" style="display: none;">
        Recent research suggests that systematic generalization in natural language understanding remains a challenge for state-of-the-art neural models such as Transformers and Graph Neural Networks. To tackle this challenge, we propose Edge Transformer, a new model that combines inspiration from Transformers and rule-based symbolic AI. The first key idea in Edge Transformers is to associate vector states with every edge, that is, with every pair of input nodes -- as opposed to just every node, as it is done in the Transformer model. The second major innovation is a triangular attention mechanism that updates edge representations in a way that is inspired by unification from logic programming. We evaluate Edge Transformer on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing. In all three settings, the Edge Transformer outperforms Relation-aware, Universal and classical Transformer baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.00578v1-abstract-full').style.display = 'none'; document.getElementById('2112.00578v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted as a conference paper at NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.07572">arXiv:2110.07572</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.07572">pdf</a>, <a href="https://arxiv.org/format/2110.07572">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jambor%2C+D">Dora Jambor</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.07572v1-abstract-short" style="display: inline;">
        Semantic parsing is the task of producing a structured meaning representation for natural language utterances or questions. Recent research has pointed out that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to generalize systematically, i.e. to handle examples that require recombining known knowledge in novel settings. In this work, we show that better systematic gener&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.07572v1-abstract-full').style.display = 'inline'; document.getElementById('2110.07572v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.07572v1-abstract-full" style="display: none;">
        Semantic parsing is the task of producing a structured meaning representation for natural language utterances or questions. Recent research has pointed out that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to generalize systematically, i.e. to handle examples that require recombining known knowledge in novel settings. In this work, we show that better systematic generalization can be achieved by producing the meaning representation (MR) directly as a graph and not as a sequence. To this end we propose LAGr, the Labeling Aligned Graphs algorithm that produces semantic parses by predicting node and edge labels for a complete multi-layer input-aligned graph. The strongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas weakly-supervised LAGr infers alignments for originally unaligned target graphs using an approximate MAP inference procedure. On the COGS and CFQ compositional generalization benchmarks the strongly- and weakly- supervised LAGr algorithms achieve significant improvements upon the baseline seq2seq parsers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.07572v1-abstract-full').style.display = 'none'; document.getElementById('2110.07572v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.06843">arXiv:2110.06843</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.06843">pdf</a>, <a href="https://arxiv.org/format/2110.06843">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compositional Generalization in Dependency Parsing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goodwin%2C+E">Emily Goodwin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reddy%2C+S">Siva Reddy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Donnell%2C+T+J">Timothy J. O&#39;Donnell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.06843v2-abstract-short" style="display: inline;">
        Compositionality -- the ability to combine familiar units like words into novel phrases and sentences -- has been the focus of intense interest in artificial intelligence in recent years. To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). This dataset maximizes the similarity between the test and train distributions over&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.06843v2-abstract-full').style.display = 'inline'; document.getElementById('2110.06843v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.06843v2-abstract-full" style="display: none;">
        Compositionality -- the ability to combine familiar units like words into novel phrases and sentences -- has been the focus of intense interest in artificial intelligence in recent years. To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). This dataset maximizes the similarity between the test and train distributions over primitive units, like words, while maximizing the compound divergence: the dissimilarity between test and train distributions over larger structures, like phrases. Dependency parsing, however, lacks a compositional generalization benchmark. In this work, we introduce a gold-standard set of dependency parses for CFQ, and use this to analyze the behavior of a state-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We find that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance. Additionally, we find the performance of the dependency parser does not uniformly degrade relative to compound divergence, and the parser performs differently on different splits with the same compound divergence. We explore a number of hypotheses for what causes the non-uniform degradation in dependency parsing performance, and identify a number of syntactic structures that drive the dependency parser&#39;s lower performance on the most challenging splits.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.06843v2-abstract-full').style.display = 'none'; document.getElementById('2110.06843v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.05093">arXiv:2109.05093</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.05093">pdf</a>, <a href="https://arxiv.org/format/2109.05093">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Scholak%2C+T">Torsten Scholak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schucher%2C+N">Nathan Schucher</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.05093v1-abstract-short" style="display: inline;">
        Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code and trained models available at https://github.com/ElementAI/picard), a method for c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.05093v1-abstract-full').style.display = 'inline'; document.getElementById('2109.05093v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.05093v1-abstract-full" style="display: none;">
        Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code and trained models available at https://github.com/ElementAI/picard), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.05093v1-abstract-full').style.display = 'none'; document.getElementById('2109.05093v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to EMNLP 2021. 7 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.03519">arXiv:2105.03519</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.03519">pdf</a>, <a href="https://arxiv.org/format/2105.03519">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding by Understanding Not: Modeling Negation in Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hosseini%2C+A">Arian Hosseini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Reddy%2C+S">Siva Reddy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hjelm%2C+R+D">R Devon Hjelm</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sordoni%2C+A">Alessandro Sordoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.03519v1-abstract-short" style="display: inline;">
        Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03519v1-abstract-full').style.display = 'inline'; document.getElementById('2105.03519v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.03519v1-abstract-full" style="display: none;">
        Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the resulting combined objective we reduce the mean top~1 error rate to 4% on the negated LAMA dataset. We also see some improvements on the negated NLI benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03519v1-abstract-full').style.display = 'none'; document.getElementById('2105.03519v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.06645">arXiv:2104.06645</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.06645">pdf</a>, <a href="https://arxiv.org/format/2104.06645">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Jointly Learning Truth-Conditional Denotations and Groundings using Parallel Attention
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bergen%2C+L">Leon Bergen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Donnell%2C+T+J">Timothy J. O&#39;Donnell</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.06645v1-abstract-short" style="display: inline;">
        We present a model that jointly learns the denotations of words together with their groundings using a truth-conditional semantics. Our model builds on the neurosymbolic approach of Mao et al. (2019), learning to ground objects in the CLEVR dataset (Johnson et al., 2017) using a novel parallel attention mechanism. The model achieves state of the art performance on visual question answering, learni&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.06645v1-abstract-full').style.display = 'inline'; document.getElementById('2104.06645v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.06645v1-abstract-full" style="display: none;">
        We present a model that jointly learns the denotations of words together with their groundings using a truth-conditional semantics. Our model builds on the neurosymbolic approach of Mao et al. (2019), learning to ground objects in the CLEVR dataset (Johnson et al., 2017) using a novel parallel attention mechanism. The model achieves state of the art performance on visual question answering, learning to detect and ground objects with question performance as the only training signal. We also show that the model is able to learn flexible non-canonical groundings just by adjusting answers to questions in the training set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.06645v1-abstract-full').style.display = 'none'; document.getElementById('2104.06645v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.11119">arXiv:2010.11119</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.11119">pdf</a>, <a href="https://arxiv.org/format/2010.11119">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/2021.naacl-main.103">10.18653/v1/2021.naacl-main.103 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DuoRAT: Towards Simpler Text-to-SQL Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Scholak%2C+T">Torsten Scholak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+R">Raymond Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Vries%2C+H">Harm de Vries</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pal%2C+C">Chris Pal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.11119v2-abstract-short" style="display: inline;">
        Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building DuoRAT, a re-implementation of the state-of-the-art RAT-SQL&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.11119v2-abstract-full').style.display = 'inline'; document.getElementById('2010.11119v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.11119v2-abstract-full" style="display: none;">
        Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building DuoRAT, a re-implementation of the state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building blocks. We perform several ablation experiments using DuoRAT as the baseline model. Our experiments confirm the usefulness of some techniques and point out the redundancy of others, including structural SQL features and features that link the question with the schema.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.11119v2-abstract-full').style.display = 'none'; document.getElementById('2010.11119v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NAACL 2021. 9 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.14435">arXiv:2007.14435</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.14435">pdf</a>, <a href="https://arxiv.org/format/2007.14435">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Ecologically Valid Research on Language User Interfaces
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=de+Vries%2C+H">Harm de Vries</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Manning%2C+C">Christopher Manning</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.14435v1-abstract-short" style="display: inline;">
        Language User Interfaces (LUIs) could improve human-machine interaction for a wide variety of tasks, such as playing music, getting insights from databases, or instructing domestic robots. In contrast to traditional hand-crafted approaches, recent work attempts to build LUIs in a data-driven way using modern deep learning methods. To satisfy the data needs of such learning algorithms, researchers&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.14435v1-abstract-full').style.display = 'inline'; document.getElementById('2007.14435v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.14435v1-abstract-full" style="display: none;">
        Language User Interfaces (LUIs) could improve human-machine interaction for a wide variety of tasks, such as playing music, getting insights from databases, or instructing domestic robots. In contrast to traditional hand-crafted approaches, recent work attempts to build LUIs in a data-driven way using modern deep learning methods. To satisfy the data needs of such learning algorithms, researchers have constructed benchmarks that emphasize the quantity of collected data at the cost of its naturalness and relevance to real-world LUI use cases. As a consequence, research findings on such benchmarks might not be relevant for developing practical LUIs. The goal of this paper is to bootstrap the discussion around this issue, which we refer to as the benchmarks&#39; low ecological validity. To this end, we describe what we deem an ideal methodology for machine learning research on LUIs and categorize five common ways in which recent benchmarks deviate from it. We give concrete examples of the five kinds of deviations and their consequences. Lastly, we offer a number of recommendations as to how to increase the ecological validity of machine learning research on LUIs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.14435v1-abstract-full').style.display = 'none'; document.getElementById('2007.14435v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.12770">arXiv:2007.12770</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.12770">pdf</a>, <a href="https://arxiv.org/format/2007.12770">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BabyAI 1.1
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hui%2C+D+Y">David Yu-Tung Hui</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chevalier-Boisvert%2C+M">Maxime Chevalier-Boisvert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.12770v1-abstract-short" style="display: inline;">
        The BabyAI platform is designed to measure the sample efficiency of training an agent to follow grounded-language instructions. BabyAI 1.0 presents baseline results of an agent trained by deep imitation or reinforcement learning. BabyAI 1.1 improves the agent&#39;s architecture in three minor ways. This increases reinforcement learning sample efficiency by up to 3 times and improves imitation learning&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.12770v1-abstract-full').style.display = 'inline'; document.getElementById('2007.12770v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.12770v1-abstract-full" style="display: none;">
        The BabyAI platform is designed to measure the sample efficiency of training an agent to follow grounded-language instructions. BabyAI 1.0 presents baseline results of an agent trained by deep imitation or reinforcement learning. BabyAI 1.1 improves the agent&#39;s architecture in three minor ways. This increases reinforcement learning sample efficiency by up to 3 times and improves imitation learning performance on the hardest level from 77 % to 90.4 %. We hope that these improvements increase the computational efficiency of BabyAI experiments and help users design better agents.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.12770v1-abstract-full').style.display = 'none'; document.getElementById('2007.12770v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 1 figure, technical report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.00412">arXiv:2002.00412</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.00412">pdf</a>, <a href="https://arxiv.org/format/2002.00412">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Combating False Negatives in Adversarial Imitation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zolna%2C+K">Konrad Zolna</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saharia%2C+C">Chitwan Saharia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boussioux%2C+L">Leonard Boussioux</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hui%2C+D+Y">David Yu-Tung Hui</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chevalier-Boisvert%2C+M">Maxime Chevalier-Boisvert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.00412v1-abstract-short" style="display: inline;">
        In adversarial imitation learning, a discriminator is trained to differentiate agent episodes from expert demonstrations representing the desired behavior. However, as the trained policy learns to be more successful, the negative examples (the ones produced by the agent) become increasingly similar to expert ones. Despite the fact that the task is successfully accomplished in some of the agent&#39;s t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.00412v1-abstract-full').style.display = 'inline'; document.getElementById('2002.00412v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.00412v1-abstract-full" style="display: none;">
        In adversarial imitation learning, a discriminator is trained to differentiate agent episodes from expert demonstrations representing the desired behavior. However, as the trained policy learns to be more successful, the negative examples (the ones produced by the agent) become increasingly similar to expert ones. Despite the fact that the task is successfully accomplished in some of the agent&#39;s trajectories, the discriminator is trained to output low values for them. We hypothesize that this inconsistent training signal for the discriminator can impede its learning, and consequently leads to worse overall performance of the agent. We show experimental evidence for this hypothesis and that the &#39;False Negatives&#39; (i.e. successful agent episodes) significantly hinder adversarial imitation learning, which is the first contribution of this paper. Then, we propose a method to alleviate the impact of false negatives and test it on the BabyAI environment. This method consistently improves sample efficiency over the baselines by at least an order of magnitude.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.00412v1-abstract-full').style.display = 'none'; document.getElementById('2002.00412v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 February, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This is an extended version of the student abstract published at 34th AAAI Conference on Artificial Intelligence</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.05783">arXiv:1912.05783</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.05783">pdf</a>, <a href="https://arxiv.org/format/1912.05783">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CLOSURE: Assessing Systematic Generalization of CLEVR Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Vries%2C+H">Harm de Vries</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Donnell%2C+T+J">Timothy J. O&#39;Donnell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Murty%2C+S">Shikhar Murty</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Beaudoin%2C+P">Philippe Beaudoin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.05783v2-abstract-short" style="display: inline;">
        The CLEVR dataset of natural-looking questions about 3D-rendered scenes has recently received much attention from the research community. A number of models have been proposed for this task, many of which achieved very high accuracies of around 97-99%. In this work, we study how systematic the generalization of such models is, that is to which extent they are capable of handling novel combinations&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.05783v2-abstract-full').style.display = 'inline'; document.getElementById('1912.05783v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.05783v2-abstract-full" style="display: none;">
        The CLEVR dataset of natural-looking questions about 3D-rendered scenes has recently received much attention from the research community. A number of models have been proposed for this task, many of which achieved very high accuracies of around 97-99%. In this work, we study how systematic the generalization of such models is, that is to which extent they are capable of handling novel combinations of known linguistic constructs. To this end, we test models&#39; understanding of referring expressions based on matching object properties (such as e.g. &#34;another cube that is the same size as the brown cube&#34;) in novel contexts. Our experiments on the thereby constructed CLOSURE benchmark show that state-of-the-art models often do not exhibit systematicity after being trained on CLEVR. Surprisingly, we find that an explicitly compositional Neural Module Network model also generalizes badly on CLOSURE, even when it has access to the ground-truth programs at test time. We improve the NMN&#39;s systematic generalization by developing a novel Vector-NMN module architecture with vector-valued inputs and outputs. Lastly, we investigate how much few-shot transfer learning can help models that are pretrained on CLEVR to adapt to CLOSURE. Our few-shot learning experiments contrast the adaptation behavior of the models with intermediate discrete programs with that of the end-to-end continuous models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.05783v2-abstract-full').style.display = 'none'; document.getElementById('1912.05783v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 December, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.00444">arXiv:1912.00444</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.00444">pdf</a>, <a href="https://arxiv.org/format/1912.00444">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automated curriculum generation for Policy Gradients from Demonstrations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Srinivasan%2C+A">Anirudh Srinivasan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chevalier-Boisvert%2C+M">Maxime Chevalier-Boisvert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.00444v1-abstract-short" style="display: inline;">
        In this paper, we present a technique that improves the process of training an agent (using RL) for instruction following. We develop a training curriculum that uses a nominal number of expert demonstrations and trains the agent in a manner that draws parallels from one of the ways in which humans learn to perform complex tasks, i.e by starting from the goal and working backwards. We test our meth&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.00444v1-abstract-full').style.display = 'inline'; document.getElementById('1912.00444v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.00444v1-abstract-full" style="display: none;">
        In this paper, we present a technique that improves the process of training an agent (using RL) for instruction following. We develop a training curriculum that uses a nominal number of expert demonstrations and trains the agent in a manner that draws parallels from one of the ways in which humans learn to perform complex tasks, i.e by starting from the goal and working backwards. We test our method on the BabyAI platform and show an improvement in sample efficiency for some of its tasks compared to a PPO (proximal policy optimization) baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.00444v1-abstract-full').style.display = 'none'; document.getElementById('1912.00444v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 December, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Deep RL Workshop at NeurIPS 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.12889">arXiv:1811.12889</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.12889">pdf</a>, <a href="https://arxiv.org/format/1811.12889">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Systematic Generalization: What Is Required and Can It Be Learned?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Murty%2C+S">Shikhar Murty</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Noukhovitch%2C+M">Michael Noukhovitch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+T+H">Thien Huu Nguyen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Vries%2C+H">Harm de Vries</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.12889v3-abstract-short" style="display: inline;">
        Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.12889v3-abstract-full').style.display = 'inline'; document.getElementById('1811.12889v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.12889v3-abstract-full" style="display: none;">
        Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.12889v3-abstract-full').style.display = 'none'; document.getElementById('1811.12889v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 November, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.08272">arXiv:1810.08272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.08272">pdf</a>, <a href="https://arxiv.org/format/1810.08272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chevalier-Boisvert%2C+M">Maxime Chevalier-Boisvert</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lahlou%2C+S">Salem Lahlou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Willems%2C+L">Lucas Willems</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saharia%2C+C">Chitwan Saharia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+T+H">Thien Huu Nguyen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.08272v4-abstract-short" style="display: inline;">
        Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded languag&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.08272v4-abstract-full').style.display = 'inline'; document.getElementById('1810.08272v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.08272v4-abstract-full" style="display: none;">
        Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.08272v4-abstract-full').style.display = 'none'; document.getElementById('1810.08272v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 December, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICLR 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.01946">arXiv:1806.01946</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.01946">pdf</a>, <a href="https://arxiv.org/format/1806.01946">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to Understand Goal Specifications by Modelling Reward
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hill%2C+F">Felix Hill</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Leike%2C+J">Jan Leike</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hughes%2C+E">Edward Hughes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hosseini%2C+A">Arian Hosseini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kohli%2C+P">Pushmeet Kohli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grefenstette%2C+E">Edward Grefenstette</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.01946v4-abstract-short" style="display: inline;">
        Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we prese&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.01946v4-abstract-full').style.display = 'inline'; document.getElementById('1806.01946v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.01946v4-abstract-full" style="display: none;">
        Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.01946v4-abstract-full').style.display = 'none'; document.getElementById('1806.01946v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 December, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 June, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.09259">arXiv:1804.09259</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.09259">pdf</a>, <a href="https://arxiv.org/format/1804.09259">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Commonsense mining as knowledge base completion? A study on the impact of novelty
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jastrz%C4%99bski%2C+S">Stanisław Jastrzębski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hosseini%2C+S">Seyedarian Hosseini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Noukhovitch%2C+M">Michael Noukhovitch</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheung%2C+J+C+K">Jackie Chi Kit Cheung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.09259v1-abstract-short" style="display: inline;">
        Commonsense knowledge bases such as ConceptNet represent knowledge in the form of relational triples. Inspired by the recent work by Li et al., we analyse if knowledge base completion models can be used to mine commonsense knowledge from raw text. We propose novelty of predicted triples with respect to the training set as an important factor in interpreting results. We critically analyse the diffi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.09259v1-abstract-full').style.display = 'inline'; document.getElementById('1804.09259v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.09259v1-abstract-full" style="display: none;">
        Commonsense knowledge bases such as ConceptNet represent knowledge in the form of relational triples. Inspired by the recent work by Li et al., we analyse if knowledge base completion models can be used to mine commonsense knowledge from raw text. We propose novelty of predicted triples with respect to the training set as an important factor in interpreting results. We critically analyse the difficulty of mining novel commonsense knowledge, and show that a simple baseline method outperforms the previous state of the art on predicting more novel.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.09259v1-abstract-full').style.display = 'none'; document.getElementById('1804.09259v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in Workshop on New Forms of Generalization in Deep Learning and Natural Language Processing (NAACL 2018)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1706.00286">arXiv:1706.00286</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1706.00286">pdf</a>, <a href="https://arxiv.org/format/1706.00286">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to Compute Word Embeddings On the Fly
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bosc%2C+T">Tom Bosc</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jastrz%C4%99bski%2C+S">Stanisław Jastrzębski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grefenstette%2C+E">Edward Grefenstette</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vincent%2C+P">Pascal Vincent</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1706.00286v3-abstract-short" style="display: inline;">
        Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the &#34;long tail&#34; of this distribution requires enormous amounts of data. Representations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1706.00286v3-abstract-full').style.display = 'inline'; document.getElementById('1706.00286v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1706.00286v3-abstract-full" style="display: none;">
        Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the &#34;long tail&#34; of this distribution requires enormous amounts of data. Representations of rare words trained directly on end tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end-to-end for the downstream task. We show that this improves results against baselines where embeddings are trained on the end task for reading comprehension, recognizing textual entailment and language modeling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1706.00286v3-abstract-full').style.display = 'none'; document.getElementById('1706.00286v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 March, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 June, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1611.02796">arXiv:1611.02796</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1611.02796">pdf</a>, <a href="https://arxiv.org/format/1611.02796">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jaques%2C+N">Natasha Jaques</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+S">Shixiang Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hern%C3%A1ndez-Lobato%2C+J+M">José Miguel Hernández-Lobato</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Turner%2C+R+E">Richard E. Turner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eck%2C+D">Douglas Eck</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1611.02796v9-abstract-short" style="display: inline;">
        This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.02796v9-abstract-full').style.display = 'inline'; document.getElementById('1611.02796v9-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1611.02796v9-abstract-full" style="display: none;">
        This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.02796v9-abstract-full').style.display = 'none'; document.getElementById('1611.02796v9-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 October, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 November, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Add supplementary material</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.07086">arXiv:1607.07086</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.07086">pdf</a>, <a href="https://arxiv.org/format/1607.07086">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Actor-Critic Algorithm for Sequence Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brakel%2C+P">Philemon Brakel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+K">Kelvin Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lowe%2C+R">Ryan Lowe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pineau%2C+J">Joelle Pineau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.07086v3-abstract-short" style="display: inline;">
        We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \texti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.07086v3-abstract-full').style.display = 'inline'; document.getElementById('1607.07086v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.07086v3-abstract-full" style="display: none;">
        We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \textit{critic} network that is trained to predict the value of an output token, given the policy of an \textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.07086v3-abstract-full').style.display = 'none'; document.getElementById('1607.07086v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.02688">arXiv:1605.02688</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.02688">pdf</a>, <a href="https://arxiv.org/format/1605.02688">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Symbolic Computation">cs.SC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Theano: A Python framework for fast computation of mathematical expressions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=The+Theano+Development+Team"> The Theano Development Team</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Al-Rfou%2C+R">Rami Al-Rfou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alain%2C+G">Guillaume Alain</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Almahairi%2C+A">Amjad Almahairi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Angermueller%2C+C">Christof Angermueller</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ballas%2C+N">Nicolas Ballas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bastien%2C+F">Frédéric Bastien</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bayer%2C+J">Justin Bayer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Belikov%2C+A">Anatoly Belikov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Belopolsky%2C+A">Alexander Belopolsky</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bergeron%2C+A">Arnaud Bergeron</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bergstra%2C+J">James Bergstra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bisson%2C+V">Valentin Bisson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Snyder%2C+J+B">Josh Bleecher Snyder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bouchard%2C+N">Nicolas Bouchard</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Boulanger-Lewandowski%2C+N">Nicolas Boulanger-Lewandowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bouthillier%2C+X">Xavier Bouthillier</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Br%C3%A9bisson%2C+A">Alexandre de Brébisson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Breuleux%2C+O">Olivier Breuleux</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Carrier%2C+P">Pierre-Luc Carrier</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chorowski%2C+J">Jan Chorowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Christiano%2C+P">Paul Christiano</a>
      , et al. (88 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.02688v1-abstract-short" style="display: inline;">
        Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, mu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.02688v1-abstract-full').style.display = 'inline'; document.getElementById('1605.02688v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.02688v1-abstract-full" style="display: none;">
        Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models.
  The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.02688v1-abstract-full').style.display = 'none'; document.getElementById('1605.02688v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 May, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.06456">arXiv:1511.06456</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.06456">pdf</a>, <a href="https://arxiv.org/format/1511.06456">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Task Loss Estimation for Sequence Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Serdyuk%2C+D">Dmitriy Serdyuk</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brakel%2C+P">Philémon Brakel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ke%2C+N+R">Nan Rosemary Ke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chorowski%2C+J">Jan Chorowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.06456v4-abstract-short" style="display: inline;">
        Often, the performance on a supervised machine learning task is evaluated with a emph{task loss} function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a emph{surrogate loss} function, such as for instance cross-entropy or hinge loss. In order for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.06456v4-abstract-full').style.display = 'inline'; document.getElementById('1511.06456v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.06456v4-abstract-full" style="display: none;">
        Often, the performance on a supervised machine learning task is evaluated with a emph{task loss} function that cannot be optimized directly. Examples of such loss functions include the classification error, the edit distance and the BLEU score. A common workaround for this problem is to instead optimize a emph{surrogate loss} function, such as for instance cross-entropy or hinge loss. In order for this remedy to be effective, it is important to ensure that minimization of the surrogate loss results in minimization of the task loss, a condition that we call emph{consistency with the task loss}. In this work, we propose another method for deriving differentiable surrogate losses that provably meet this requirement. We focus on the broad class of models that define a score for every input-output pair. Our idea is that this score can be interpreted as an estimate of the task loss, and that the estimation error may be used as a consistent surrogate loss. A distinct feature of such an approach is that it defines the desirable value of the score for every input-output pair. We use this property to design specialized surrogate losses for Encoder-Decoder models often used for sequence prediction tasks. In our experiment, we benchmark on the task of speech recognition. Using a new surrogate loss instead of cross-entropy to train an Encoder-Decoder speech recognizer brings a significant ~13% relative improvement in terms of Character Error Rate (CER) in the case when no extra corpora are used for language modeling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.06456v4-abstract-full').style.display = 'none'; document.getElementById('1511.06456v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 January, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICLR 2016</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1508.04395">arXiv:1508.04395</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1508.04395">pdf</a>, <a href="https://arxiv.org/format/1508.04395">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        End-to-End Attention-based Large Vocabulary Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chorowski%2C+J">Jan Chorowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Serdyuk%2C+D">Dmitriy Serdyuk</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brakel%2C+P">Philemon Brakel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1508.04395v2-abstract-short" style="display: inline;">
        Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1508.04395v2-abstract-full').style.display = 'inline'; document.getElementById('1508.04395v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1508.04395v2-abstract-full" style="display: none;">
        Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1508.04395v2-abstract-full').style.display = 'none'; document.getElementById('1508.04395v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 August, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2015.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1506.07503">arXiv:1506.07503</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1506.07503">pdf</a>, <a href="https://arxiv.org/format/1506.07503">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Attention-Based Models for Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chorowski%2C+J">Jan Chorowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Serdyuk%2C+D">Dmitriy Serdyuk</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1506.07503v1-abstract-short" style="display: inline;">
        Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1506.07503v1-abstract-full').style.display = 'inline'; document.getElementById('1506.07503v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1506.07503v1-abstract-full" style="display: none;">
        Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1506.07503v1-abstract-full').style.display = 'none'; document.getElementById('1506.07503v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2015; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2015.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1506.00619">arXiv:1506.00619</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1506.00619">pdf</a>, <a href="https://arxiv.org/ps/1506.00619">ps</a>, <a href="https://arxiv.org/format/1506.00619">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Blocks and Fuel: Frameworks for deep learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=van+Merri%C3%ABnboer%2C+B">Bart van Merriënboer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dumoulin%2C+V">Vincent Dumoulin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Serdyuk%2C+D">Dmitriy Serdyuk</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Warde-Farley%2C+D">David Warde-Farley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chorowski%2C+J">Jan Chorowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1506.00619v1-abstract-short" style="display: inline;">
        We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano&#39;s symbolic computational graph, and providing an extensive set of utilities to assist training th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1506.00619v1-abstract-full').style.display = 'inline'; document.getElementById('1506.00619v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1506.00619v1-abstract-full" style="display: none;">
        We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano&#39;s symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides a standard format for machine learning datasets. It allows the user to easily iterate over large datasets, performing many types of pre-processing on the fly.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1506.00619v1-abstract-full').style.display = 'none'; document.getElementById('1506.00619v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2015; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2015.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1412.1602">arXiv:1412.1602</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1412.1602">pdf</a>, <a href="https://arxiv.org/format/1412.1602">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chorowski%2C+J">Jan Chorowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1412.1602v1-abstract-short" style="display: inline;">
        We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context creat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1412.1602v1-abstract-full').style.display = 'inline'; document.getElementById('1412.1602v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1412.1602v1-abstract-full" style="display: none;">
        We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1412.1602v1-abstract-full').style.display = 'none'; document.getElementById('1412.1602v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 December, 2014; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2014.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">As accepted to: Deep Learning and Representation Learning Workshop, NIPS 2014</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1409.1259">arXiv:1409.1259</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1409.1259">pdf</a>, <a href="https://arxiv.org/format/1409.1259">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Properties of Neural Machine Translation: Encoder-Decoder Approaches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=van+Merrienboer%2C+B">Bart van Merrienboer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1409.1259v2-abstract-short" style="display: inline;">
        Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on anal&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1409.1259v2-abstract-full').style.display = 'inline'; document.getElementById('1409.1259v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1409.1259v2-abstract-full" style="display: none;">
        Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1409.1259v2-abstract-full').style.display = 'none'; document.getElementById('1409.1259v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2014; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 September, 2014;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2014.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1409.1257">arXiv:1409.1257</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1409.1257">pdf</a>, <a href="https://arxiv.org/format/1409.1257">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pouget-Abadie%2C+J">Jean Pouget-Abadie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=van+Merrienboer%2C+B">Bart van Merrienboer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1409.1257v2-abstract-short" style="display: inline;">
        The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1409.1257v2-abstract-full').style.display = 'inline'; document.getElementById('1409.1257v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1409.1257v2-abstract-full" style="display: none;">
        The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1409.1257v2-abstract-full').style.display = 'none'; document.getElementById('1409.1257v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2014; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 September, 2014;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2014.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1409.0473">arXiv:1409.0473</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1409.0473">pdf</a>, <a href="https://arxiv.org/format/1409.0473">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Neural Machine Translation by Jointly Learning to Align and Translate
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1409.0473v7-abstract-short" style="display: inline;">
        Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1409.0473v7-abstract-full').style.display = 'inline'; document.getElementById('1409.0473v7-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1409.0473v7-abstract-full" style="display: none;">
        Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1409.0473v7-abstract-full').style.display = 'none'; document.getElementById('1409.0473v7-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 May, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 September, 2014;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2014.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICLR 2015 as oral presentation</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1406.1078">arXiv:1406.1078</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1406.1078">pdf</a>, <a href="https://arxiv.org/format/1406.1078">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=van+Merrienboer%2C+B">Bart van Merrienboer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gulcehre%2C+C">Caglar Gulcehre</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bahdanau%2C+D">Dzmitry Bahdanau</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bougares%2C+F">Fethi Bougares</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schwenk%2C+H">Holger Schwenk</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1406.1078v3-abstract-short" style="display: inline;">
        In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1406.1078v3-abstract-full').style.display = 'inline'; document.getElementById('1406.1078v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1406.1078v3-abstract-full" style="display: none;">
        In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1406.1078v3-abstract-full').style.display = 'none'; document.getElementById('1406.1078v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2014; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 June, 2014;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2014.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2014</span>
    </p>
    

    

    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>