<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 119 results for author: <span class="mathjax">Ren, S</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Ren%2C+S">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Ren, S">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Ren%2C+S&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Ren, S">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Ren%2C+S&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Ren%2C+S&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Ren%2C+S&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Ren%2C+S&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13913">arXiv:2204.13913</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13913">pdf</a>, <a href="https://arxiv.org/ps/2204.13913">ps</a>, <a href="https://arxiv.org/format/2204.13913">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Siyu Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13913v1-abstract-short" style="display: inline;">
        Current text-image approaches (e.g., CLIP) typically adopt dual-encoder architecture using pre-trained vision-language representation. However, these models still pose non-trivial memory requirements and substantial incremental indexing time, which makes them less practical on mobile devices. In this paper, we present an effective two-stage framework to compress large pre-trained dual-encoder for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13913v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13913v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13913v1-abstract-full" style="display: none;">
        Current text-image approaches (e.g., CLIP) typically adopt dual-encoder architecture using pre-trained vision-language representation. However, these models still pose non-trivial memory requirements and substantial incremental indexing time, which makes them less practical on mobile devices. In this paper, we present an effective two-stage framework to compress large pre-trained dual-encoder for lightweight text-image retrieval. The resulting model is smaller (39% of the original), faster (1.6x/2.9x for processing image/text respectively), yet performs on par with or better than the original full model on Flickr30K and MSCOCO benchmarks. We also open-source an accompanying realistic mobile image search application.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13913v1-abstract-full').style.display = 'none'; document.getElementById('2204.13913v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NAACL 2022 main conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.08572">arXiv:2204.08572</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.08572">pdf</a>, <a href="https://arxiv.org/format/2204.08572">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3530894">10.1145/3530894 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Expert-Calibrated Learning for Online Optimization with Switching Costs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+P">Pengfei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianyi Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.08572v2-abstract-short" style="display: inline;">
        We study online convex optimization with switching costs, a practically important but also extremely challenging problem due to the lack of complete offline information. By tapping into the power of machine learning (ML) based optimizers, ML-augmented online algorithms (also referred to as expert calibration in this paper) have been emerging as state of the art, with provable worst-case performanc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08572v2-abstract-full').style.display = 'inline'; document.getElementById('2204.08572v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.08572v2-abstract-full" style="display: none;">
        We study online convex optimization with switching costs, a practically important but also extremely challenging problem due to the lack of complete offline information. By tapping into the power of machine learning (ML) based optimizers, ML-augmented online algorithms (also referred to as expert calibration in this paper) have been emerging as state of the art, with provable worst-case performance guarantees. Nonetheless, by using the standard practice of training an ML model as a standalone optimizer and plugging it into an ML-augmented algorithm, the average cost performance can be highly unsatisfactory. In order to address the &#34;how to learn&#34; challenge, we propose EC-L2O (expert-calibrated learning to optimize), which trains an ML-based optimizer by explicitly taking into account the downstream expert calibrator. To accomplish this, we propose a new differentiable expert calibrator that generalizes regularized online balanced descent and offers a provably better competitive ratio than pure ML predictions when the prediction error is large. For training, our loss function is a weighted sum of two different losses -- one minimizing the average ML prediction error for better robustness, and the other one minimizing the post-calibration average cost. We also provide theoretical analysis for EC-L2O, highlighting that expert calibration can be even beneficial for the average cost performance and that the high-percentile tail ratio of the cost achieved by EC-L2O to that of the offline optimal oracle (i.e., tail cost ratio) can be bounded. Finally, we test EC-L2O by running simulations for sustainable datacenter demand response. Our results demonstrate that EC-L2O can empirically achieve a lower average cost as well as a lower competitive ratio than the existing baseline algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.08572v2-abstract-full').style.display = 'none'; document.getElementById('2204.08572v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Pengfei Li and Jianyi Yang contributed equally. This paper has been accepted by and will be presented at the ACM SIGMETRICS/IFIP Performance 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.03240">arXiv:2204.03240</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.03240">pdf</a>, <a href="https://arxiv.org/format/2204.03240">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Speech Pre-training with Acoustic Piece
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuo Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shujie Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+L">Long Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+F">Furu Wei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.03240v1-abstract-short" style="display: inline;">
        Previous speech pre-training methods, such as wav2vec2.0 and HuBERT, pre-train a Transformer encoder to learn deep representations from audio data, with objectives predicting either elements from latent vector quantized space or pre-generated labels (known as target codes) with offline clustering. However, those training signals (quantized elements or codes) are independent across different tokens&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03240v1-abstract-full').style.display = 'inline'; document.getElementById('2204.03240v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.03240v1-abstract-full" style="display: none;">
        Previous speech pre-training methods, such as wav2vec2.0 and HuBERT, pre-train a Transformer encoder to learn deep representations from audio data, with objectives predicting either elements from latent vector quantized space or pre-generated labels (known as target codes) with offline clustering. However, those training signals (quantized elements or codes) are independent across different tokens without considering their relations. According to our observation and analysis, the target codes share obvious patterns aligned with phonemized text data. Based on that, we propose to leverage those patterns to better pre-train the model considering the relations among the codes. The patterns we extracted, called &#34;acoustic piece&#34;s, are from the sentence piece result of HuBERT codes. With the acoustic piece as the training signal, we can implicitly bridge the input audio and natural language, which benefits audio-to-text tasks, such as automatic speech recognition (ASR). Simple but effective, our method &#34;HuBERT-AP&#34; significantly outperforms strong baselines on the LibriSpeech ASR task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.03240v1-abstract-full').style.display = 'none'; document.getElementById('2204.03240v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 4 figures; submitted to Interspeech 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.02485">arXiv:2204.02485</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.02485">pdf</a>, <a href="https://arxiv.org/format/2204.02485">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Training-Free Robust Multimodal Learning via Sample-Wise Jacobian Regularization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Z">Zhengqi Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+Z">Zihui Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+S">Siting Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Hang Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.02485v1-abstract-short" style="display: inline;">
        Multimodal fusion emerges as an appealing technique to improve model performances on many tasks. Nevertheless, the robustness of such fusion methods is rarely involved in the present literature. In this paper, we propose a training-free robust late-fusion method by exploiting conditional independence assumption and Jacobian regularization. Our key is to minimize the Frobenius norm of a Jacobian ma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.02485v1-abstract-full').style.display = 'inline'; document.getElementById('2204.02485v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.02485v1-abstract-full" style="display: none;">
        Multimodal fusion emerges as an appealing technique to improve model performances on many tasks. Nevertheless, the robustness of such fusion methods is rarely involved in the present literature. In this paper, we propose a training-free robust late-fusion method by exploiting conditional independence assumption and Jacobian regularization. Our key is to minimize the Frobenius norm of a Jacobian matrix, where the resulting optimization problem is relaxed to a tractable Sylvester equation. Furthermore, we provide a theoretical error bound of our method and some insights about the function of the extra modality. Several numerical experiments on AV-MNIST, RAVDESS, and VGGsound demonstrate the efficacy of our method under both adversarial attacks and random corruptions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.02485v1-abstract-full').style.display = 'none'; document.getElementById('2204.02485v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13921">arXiv:2203.13921</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13921">pdf</a>, <a href="https://arxiv.org/format/2203.13921">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Semi-Decoupled Approach to Fast and Optimal Hardware-Software Co-Design of Neural Accelerators
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+B">Bingqian Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+Z">Zheyu Yan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+Y">Yiyu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13921v1-abstract-short" style="display: inline;">
        In view of the performance limitations of fully-decoupled designs for neural architectures and accelerators, hardware-software co-design has been emerging to fully reap the benefits of flexible design spaces and optimize neural network performance. Nonetheless, such co-design also enlarges the total search space to practically infinity and presents substantial challenges. While the prior studies h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13921v1-abstract-full').style.display = 'inline'; document.getElementById('2203.13921v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13921v1-abstract-full" style="display: none;">
        In view of the performance limitations of fully-decoupled designs for neural architectures and accelerators, hardware-software co-design has been emerging to fully reap the benefits of flexible design spaces and optimize neural network performance. Nonetheless, such co-design also enlarges the total search space to practically infinity and presents substantial challenges. While the prior studies have been focusing on improving the search efficiency (e.g., via reinforcement learning), they commonly rely on co-searches over the entire architecture-accelerator design space. In this paper, we propose a \emph{semi}-decoupled approach to reduce the size of the total design space by orders of magnitude, yet without losing optimality. We first perform neural architecture search to obtain a small set of optimal architectures for one accelerator candidate. Importantly, this is also the set of (close-to-)optimal architectures for other accelerator designs based on the property that neural architectures&#39; ranking orders in terms of inference latency and energy consumption on different accelerator designs are highly similar. Then, instead of considering all the possible architectures, we optimize the accelerator design only in combination with this small set of architectures, thus significantly reducing the total search cost. We validate our approach by conducting experiments on various architecture spaces for accelerator designs with different dataflows. Our results highlight that we can obtain the optimal design by only navigating over the reduced search space. The source code of this work is at \url{https://github.com/Ren-Research/CoDesign}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13921v1-abstract-full').style.display = 'none'; document.getElementById('2203.13921v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by and presented at the TinyML Research Symposium 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.12595">arXiv:2203.12595</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.12595">pdf</a>, <a href="https://arxiv.org/format/2203.12595">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PhysioMTL: Personalizing Physiological Patterns using Optimal Transport Multi-Task Regression
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+J">Jiacheng Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Darnell%2C+G">Gregory Darnell</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kumar%2C+A">Agni Kumar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+D">Ding Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+X">Xuanlong Nguyen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S+Y">Shirley You Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.12595v1-abstract-short" style="display: inline;">
        Heart rate variability (HRV) is a practical and noninvasive measure of autonomic nervous system activity, which plays an essential role in cardiovascular health. However, using HRV to assess physiology status is challenging. Even in clinical settings, HRV is sensitive to acute stressors such as physical activity, mental stress, hydration, alcohol, and sleep. Wearable devices provide convenient HRV&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12595v1-abstract-full').style.display = 'inline'; document.getElementById('2203.12595v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.12595v1-abstract-full" style="display: none;">
        Heart rate variability (HRV) is a practical and noninvasive measure of autonomic nervous system activity, which plays an essential role in cardiovascular health. However, using HRV to assess physiology status is challenging. Even in clinical settings, HRV is sensitive to acute stressors such as physical activity, mental stress, hydration, alcohol, and sleep. Wearable devices provide convenient HRV measurements, but the irregularity of measurements and uncaptured stressors can bias conventional analytical methods. To better interpret HRV measurements for downstream healthcare applications, we learn a personalized diurnal rhythm as an accurate physiological indicator for each individual. We develop Physiological Multitask-Learning (PhysioMTL) by harnessing Optimal Transport theory within a Multitask-learning (MTL) framework. The proposed method learns an individual-specific predictive model from heterogeneous observations, and enables estimation of an optimal transport map that yields a push forward operation onto the demographic features for each task. Our model outperforms competing MTL methodologies on unobserved predictive tasks for synthetic and two real-world datasets. Specifically, our method provides remarkable prediction results on unseen held-out subjects given only $20\%$ of the subjects in real-world observational studies. Furthermore, our model enables a counterfactual engine that generates the effect of acute stressors and chronic conditions on HRV rhythms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12595v1-abstract-full').style.display = 'none'; document.getElementById('2203.12595v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 9 figures, accepted by CHIL 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.12054">arXiv:2203.12054</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.12054">pdf</a>, <a href="https://arxiv.org/format/2203.12054">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-supervision through Random Segments with Autoregressive Coding (RandSAC)
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hua%2C+T">Tianyu Hua</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+Y">Yonglong Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Hang Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sigal%2C+L">Leonid Sigal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.12054v1-abstract-short" style="display: inline;">
        Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effects various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12054v1-abstract-full').style.display = 'inline'; document.getElementById('2203.12054v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.12054v1-abstract-full" style="display: none;">
        Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effects various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, ImageNet). While our pre-training strategy works with vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder feature layers, which further improves the performance. Our final model, trained on ImageNet, achieves new state-of-the-art linear probing performance 68.3% among comparative predictive self-supervised learning approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.12054v1-abstract-full').style.display = 'none'; document.getElementById('2203.12054v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.09681">arXiv:2203.09681</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.09681">pdf</a>, <a href="https://arxiv.org/format/2203.09681">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HDLock: Exploiting Privileged Encoding to Protect Hyperdimensional Computing Models against IP Stealing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Duan%2C+S">Shijin Duan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+X">Xiaolin Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.09681v1-abstract-short" style="display: inline;">
        Hyperdimensional Computing (HDC) is facing infringement issues due to straightforward computations. This work, for the first time, raises a critical vulnerability of HDC, an attacker can reverse engineer the entire model, only requiring the unindexed hypervector memory. To mitigate this attack, we propose a defense strategy, namely HDLock, which significantly increases the reasoning cost of encodi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09681v1-abstract-full').style.display = 'inline'; document.getElementById('2203.09681v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.09681v1-abstract-full" style="display: none;">
        Hyperdimensional Computing (HDC) is facing infringement issues due to straightforward computations. This work, for the first time, raises a critical vulnerability of HDC, an attacker can reverse engineer the entire model, only requiring the unindexed hypervector memory. To mitigate this attack, we propose a defense strategy, namely HDLock, which significantly increases the reasoning cost of encoding. Specifically, HDLock adds extra feature hypervector combination and permutation in the encoding module. Compared to the standard HDC model, a two-layer-key HDLock can increase the adversarial reasoning complexity by 10 order of magnitudes without inference accuracy loss, with only 21% latency overhead.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09681v1-abstract-full').style.display = 'none'; document.getElementById('2203.09681v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 9 figures, accepted by and to be presented at DAC 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.09680">arXiv:2203.09680</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.09680">pdf</a>, <a href="https://arxiv.org/format/2203.09680">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LeHDC: Learning-Based Hyperdimensional Computing Classifier
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Duan%2C+S">Shijin Duan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yejia Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+X">Xiaolin Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.09680v2-abstract-short" style="display: inline;">
        Thanks to the tiny storage and efficient execution, hyperdimensional Computing (HDC) is emerging as a lightweight learning framework on resource-constrained hardware. Nonetheless, the existing HDC training relies on various heuristic methods, significantly limiting their inference accuracy. In this paper, we propose a new HDC framework, called LeHDC, which leverages a principled learning approach&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09680v2-abstract-full').style.display = 'inline'; document.getElementById('2203.09680v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.09680v2-abstract-full" style="display: none;">
        Thanks to the tiny storage and efficient execution, hyperdimensional Computing (HDC) is emerging as a lightweight learning framework on resource-constrained hardware. Nonetheless, the existing HDC training relies on various heuristic methods, significantly limiting their inference accuracy. In this paper, we propose a new HDC framework, called LeHDC, which leverages a principled learning approach to improve the model accuracy. Concretely, LeHDC maps the existing HDC framework into an equivalent Binary Neural Network architecture, and employs a corresponding training strategy to minimize the training loss. Experimental validation shows that LeHDC outperforms previous HDC training strategies and can improve on average the inference accuracy over 15% compared to the baseline HDC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.09680v2-abstract-full').style.display = 'none'; document.getElementById('2203.09680v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 6 figures, accepted by and to be presented at DAC 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.04894">arXiv:2203.04894</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.04894">pdf</a>, <a href="https://arxiv.org/format/2203.04894">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Brain-Inspired Low-Dimensional Computing Classifier for Inference on Tiny Devices
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Duan%2C+S">Shijin Duan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+X">Xiaolin Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.04894v2-abstract-short" style="display: inline;">
        By mimicking brain-like cognition and exploiting parallelism, hyperdimensional computing (HDC) classifiers have been emerging as a lightweight framework to achieve efficient on-device inference. Nonetheless, they have two fundamental drawbacks, heuristic training process and ultra-high dimension, which result in sub-optimal inference accuracy and large model sizes beyond the capability of tiny dev&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04894v2-abstract-full').style.display = 'inline'; document.getElementById('2203.04894v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.04894v2-abstract-full" style="display: none;">
        By mimicking brain-like cognition and exploiting parallelism, hyperdimensional computing (HDC) classifiers have been emerging as a lightweight framework to achieve efficient on-device inference. Nonetheless, they have two fundamental drawbacks, heuristic training process and ultra-high dimension, which result in sub-optimal inference accuracy and large model sizes beyond the capability of tiny devices with stringent resource constraints. In this paper, we address these fundamental drawbacks and propose a low-dimensional computing (LDC) alternative. Specifically, by mapping our LDC classifier into an equivalent neural network, we optimize our model using a principled training approach. Most importantly, we can improve the inference accuracy while successfully reducing the ultra-high dimension of existing HDC models by orders of magnitude (e.g., 8000 vs. 4/64). We run experiments to evaluate our LDC classifier by considering different datasets for inference on tiny devices, and also implement different models on an FPGA platform for acceleration. The results highlight that our LDC classifier offers an overwhelming advantage over the existing brain-inspired HDC models and is particularly suitable for inference on tiny devices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.04894v2-abstract-full').style.display = 'none'; document.getElementById('2203.04894v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 9 figures, accepted by and presented as a full paper at TinyML Research Symposium 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.12939">arXiv:2202.12939</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.12939">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automated Extraction of Energy Systems Information from Remotely Sensed Data: A Review and Analysis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Simiao Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+W">Wei Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bradbury%2C+K">Kyle Bradbury</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Harrison-Atlas%2C+D">Dylan Harrison-Atlas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Valeri%2C+L+M">Laura Malaguzzi Valeri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Murray%2C+B">Brian Murray</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Malof%2C+J+M">Jordan M. Malof</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.12939v1-abstract-short" style="display: inline;">
        High quality energy systems information is a crucial input to energy systems research, modeling, and decision-making. Unfortunately, precise information about energy systems is often of limited availability, incomplete, or only accessible for a substantial fee or through a non-disclosure agreement. Recently, remotely sensed data (e.g., satellite imagery, aerial photography) have emerged as a poten&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.12939v1-abstract-full').style.display = 'inline'; document.getElementById('2202.12939v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.12939v1-abstract-full" style="display: none;">
        High quality energy systems information is a crucial input to energy systems research, modeling, and decision-making. Unfortunately, precise information about energy systems is often of limited availability, incomplete, or only accessible for a substantial fee or through a non-disclosure agreement. Recently, remotely sensed data (e.g., satellite imagery, aerial photography) have emerged as a potentially rich source of energy systems information. However, the use of these data is frequently challenged by its sheer volume and complexity, precluding manual analysis. Recent breakthroughs in machine learning have enabled automated and rapid extraction of useful information from remotely sensed data, facilitating large-scale acquisition of critical energy system variables. Here we present a systematic review of the literature on this emerging topic, providing an in-depth survey and review of papers published within the past two decades. We first taxonomize the existing literature into ten major areas, spanning the energy value chain. Within each research area, we distill and critically discuss major features that are relevant to energy researchers, including, for example, key challenges regarding the accessibility and reliability of the methods. We then synthesize our findings to identify limitations and trends in the literature as a whole, and discuss opportunities for innovation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.12939v1-abstract-full').style.display = 'none'; document.getElementById('2202.12939v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.12093">arXiv:2202.12093</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.12093">pdf</a>, <a href="https://arxiv.org/format/2202.12093">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        KESA: A Knowledge Enhanced Approach For Sentiment Analysis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Q">Qinghua Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+S">Shuai Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuo Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.12093v1-abstract-short" style="display: inline;">
        Though some recent works focus on injecting sentiment knowledge into pre-trained language models, they usually design mask and reconstruction tasks in the post-training phase. In this paper, we aim to benefit from sentiment knowledge in a lighter way. To achieve this goal, we study sentence-level sentiment analysis and, correspondingly, propose two sentiment-aware auxiliary tasks named sentiment w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.12093v1-abstract-full').style.display = 'inline'; document.getElementById('2202.12093v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.12093v1-abstract-full" style="display: none;">
        Though some recent works focus on injecting sentiment knowledge into pre-trained language models, they usually design mask and reconstruction tasks in the post-training phase. In this paper, we aim to benefit from sentiment knowledge in a lighter way. To achieve this goal, we study sentence-level sentiment analysis and, correspondingly, propose two sentiment-aware auxiliary tasks named sentiment word cloze and conditional sentiment prediction. The first task learns to select the correct sentiment words within the input, given the overall sentiment polarity as prior knowledge. On the contrary, the second task predicts the overall sentiment polarity given the sentiment polarity of the word as prior knowledge. In addition, two kinds of label combination methods are investigated to unify multiple types of labels in each task. We argue that more information can promote the models to learn more profound semantic representation. We implement it in a straightforward way to verify this hypothesis. The experimental results demonstrate that our approach consistently outperforms pre-trained models and is additive to existing knowledge-enhanced post-trained models. The code and data are released at https://github.com/lshowway/KESA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.12093v1-abstract-full').style.display = 'none'; document.getElementById('2202.12093v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.12632">arXiv:2201.12632</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.12632">pdf</a>, <a href="https://arxiv.org/format/2201.12632">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hyperparameter-free deep active learning for regression problems via query synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Simiao Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Y">Yang Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Padilla%2C+W+J">Willie J. Padilla</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Malof%2C+J">Jordan Malof</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.12632v1-abstract-short" style="display: inline;">
        In the past decade, deep active learning (DAL) has heavily focused upon classification problems, or problems that have some &#39;valid&#39; data manifolds, such as natural languages or images. As a result, existing DAL methods are not applicable to a wide variety of important problems -- such as many scientific computing problems -- that involve regression on relatively unstructured input spaces. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.12632v1-abstract-full').style.display = 'inline'; document.getElementById('2201.12632v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.12632v1-abstract-full" style="display: none;">
        In the past decade, deep active learning (DAL) has heavily focused upon classification problems, or problems that have some &#39;valid&#39; data manifolds, such as natural languages or images. As a result, existing DAL methods are not applicable to a wide variety of important problems -- such as many scientific computing problems -- that involve regression on relatively unstructured input spaces. In this work we propose the first DAL query-synthesis approach for regression problems. We frame query synthesis as an inverse problem and use the recently-proposed neural-adjoint (NA) solver to efficiently find points in the continuous input domain that optimize the query-by-committee (QBC) criterion. Crucially, the resulting NA-QBC approach removes the one sensitive hyperparameter of the classical QBC active learning approach - the &#34;pool size&#34;- making NA-QBC effectively hyperparameter free. This is significant because DAL methods can be detrimental, even compared to random sampling, if the wrong hyperparameters are chosen. We evaluate Random, QBC and NA-QBC sampling strategies on four regression problems, including two contemporary scientific computing problems. We find that NA-QBC achieves better average performance than random sampling on every benchmark problem, while QBC can be detrimental if the wrong hyperparameters are chosen.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.12632v1-abstract-full').style.display = 'none'; document.getElementById('2201.12632v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.13610">arXiv:2112.13610</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.13610">pdf</a>, <a href="https://arxiv.org/format/2112.13610">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+Y">Yuan Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+Q">Qingxiu Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guan%2C+J">Jian Guan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+B">Boxi Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhengyan Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+C">Chaojun Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaozhi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qi%2C+F">Fanchao Qi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bao%2C+J">Junwei Bao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nie%2C+J">Jinran Nie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+Z">Zheni Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+Y">Yuxian Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+K">Kun Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+X">Xuancheng Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wenhao Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuhuai Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+J">Jinliang Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+C">Chengqiang Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Huadong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+G">Guoyang Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Z">Zile Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jiajun Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Juanzi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+M">Minlie Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yan%2C+R">Rui Yan</a>
      , et al. (10 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.13610v1-abstract-short" style="display: inline;">
        Realizing general-purpose language intelligence has been a longstanding goal for natural language processing, where standard evaluation benchmarks play a fundamental and guiding role. We argue that for general-purpose language intelligence evaluation, the benchmark itself needs to be comprehensive and systematic. To this end, we propose CUGE, a Chinese Language Understanding and Generation Evaluat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.13610v1-abstract-full').style.display = 'inline'; document.getElementById('2112.13610v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.13610v1-abstract-full" style="display: none;">
        Realizing general-purpose language intelligence has been a longstanding goal for natural language processing, where standard evaluation benchmarks play a fundamental and guiding role. We argue that for general-purpose language intelligence evaluation, the benchmark itself needs to be comprehensive and systematic. To this end, we propose CUGE, a Chinese Language Understanding and Generation Evaluation benchmark with the following features: (1) Hierarchical benchmark framework, where datasets are principally selected and organized with a language capability-task-dataset hierarchy. (2) Multi-level scoring strategy, where different levels of model performance are provided based on the hierarchical framework. To facilitate CUGE, we provide a public leaderboard that can be customized to support flexible model judging criteria. Evaluation results on representative pre-trained language models indicate ample room for improvement towards general-purpose language intelligence. CUGE is publicly available at cuge.baai.ac.cn.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.13610v1-abstract-full').style.display = 'none'; document.getElementById('2112.13610v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.10377">arXiv:2112.10377</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.10377">pdf</a>, <a href="https://arxiv.org/format/2112.10377">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning for Robust Combinatorial Optimization: Algorithm and Application
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Shao%2C+Z">Zhihui Shao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianyi Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+C">Cong Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.10377v1-abstract-short" style="display: inline;">
        Learning to optimize (L2O) has recently emerged as a promising approach to solving optimization problems by exploiting the strong prediction power of neural networks and offering lower runtime complexity than conventional solvers. While L2O has been applied to various problems, a crucial yet challenging class of problems -- robust combinatorial optimization in the form of minimax optimization -- h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.10377v1-abstract-full').style.display = 'inline'; document.getElementById('2112.10377v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.10377v1-abstract-full" style="display: none;">
        Learning to optimize (L2O) has recently emerged as a promising approach to solving optimization problems by exploiting the strong prediction power of neural networks and offering lower runtime complexity than conventional solvers. While L2O has been applied to various problems, a crucial yet challenging class of problems -- robust combinatorial optimization in the form of minimax optimization -- have largely remained under-explored. In addition to the exponentially large decision space, a key challenge for robust combinatorial optimization lies in the inner optimization problem, which is typically non-convex and entangled with outer optimization. In this paper, we study robust combinatorial optimization and propose a novel learning-based optimizer, called LRCO (Learning for Robust Combinatorial Optimization), which quickly outputs a robust solution in the presence of uncertain context. LRCO leverages a pair of learning-based optimizers -- one for the minimizer and the other for the maximizer -- that use their respective objective functions as losses and can be trained without the need of labels for training problem instances. To evaluate the performance of LRCO, we perform simulations for the task offloading problem in vehicular edge computing. Our results highlight that LRCO can greatly reduce the worst-case cost and improve robustness, while having a very low runtime complexity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.10377v1-abstract-full').style.display = 'none'; document.getElementById('2112.10377v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by the IEEE International Conference on Computer Communications (INFOCOM) 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.10254">arXiv:2112.10254</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.10254">pdf</a>, <a href="https://arxiv.org/format/2112.10254">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Inverse deep learning methods and benchmarks for artificial electromagnetic material design
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Simiao Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mahendra%2C+A">Ashwin Mahendra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khatib%2C+O">Omar Khatib</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Y">Yang Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Padilla%2C+W+J">Willie J. Padilla</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Malof%2C+J+M">Jordan M. Malof</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.10254v1-abstract-short" style="display: inline;">
        Deep learning (DL) inverse techniques have increased the speed of artificial electromagnetic material (AEM) design and improved the quality of resulting devices. Many DL inverse techniques have succeeded on a number of AEM design tasks, but to compare, contrast, and evaluate assorted techniques it is critical to clarify the underlying ill-posedness of inverse problems. Here we review state-of-the-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.10254v1-abstract-full').style.display = 'inline'; document.getElementById('2112.10254v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.10254v1-abstract-full" style="display: none;">
        Deep learning (DL) inverse techniques have increased the speed of artificial electromagnetic material (AEM) design and improved the quality of resulting devices. Many DL inverse techniques have succeeded on a number of AEM design tasks, but to compare, contrast, and evaluate assorted techniques it is critical to clarify the underlying ill-posedness of inverse problems. Here we review state-of-the-art approaches and present a comprehensive survey of deep learning inverse methods and invertible and conditional invertible neural networks to AEM design. We produce easily accessible and rapidly implementable AEM design benchmarks, which offers a methodology to efficiently determine the DL technique best suited to solving different design challenges. Our methodology is guided by constraints on repeated simulation and an easily integrated metric, which we propose expresses the relative ill-posedness of any AEM design problem. We show that as the problem becomes increasingly ill-posed, the neural adjoint with boundary loss (NA) generates better solutions faster, regardless of simulation constraints. On simpler AEM design tasks, direct neural networks (NN) fare better when simulations are limited, while geometries predicted by mixture density networks (MDN) and conditional variational auto-encoders (VAE) can improve with continued sampling and re-simulation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.10254v1-abstract-full').style.display = 'none'; document.getElementById('2112.10254v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.07219">arXiv:2112.07219</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.07219">pdf</a>, <a href="https://arxiv.org/format/2112.07219">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A real-time spatiotemporal AI model analyzes skill in open surgical videos
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goodman%2C+E+D">Emmett D. Goodman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Patel%2C+K+K">Krishna K. Patel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yilun Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Locke%2C+W">William Locke</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kennedy%2C+C+J">Chris J. Kennedy</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mehrotra%2C+R">Rohan Mehrotra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Stephen Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guan%2C+M+Y">Melody Y. Guan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Downing%2C+M">Maren Downing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H+W">Hao Wei Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Clark%2C+J+Z">Jevin Z. Clark</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brat%2C+G+A">Gabriel A. Brat</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yeung%2C+S">Serena Yeung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.07219v1-abstract-short" style="display: inline;">
        Open procedures represent the dominant form of surgery worldwide. Artificial intelligence (AI) has the potential to optimize surgical practice and improve patient outcomes, but efforts have focused primarily on minimally invasive techniques. Our work overcomes existing data limitations for training AI models by curating, from YouTube, the largest dataset of open surgical videos to date: 1997 video&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.07219v1-abstract-full').style.display = 'inline'; document.getElementById('2112.07219v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.07219v1-abstract-full" style="display: none;">
        Open procedures represent the dominant form of surgery worldwide. Artificial intelligence (AI) has the potential to optimize surgical practice and improve patient outcomes, but efforts have focused primarily on minimally invasive techniques. Our work overcomes existing data limitations for training AI models by curating, from YouTube, the largest dataset of open surgical videos to date: 1997 videos from 23 surgical procedures uploaded from 50 countries. Using this dataset, we developed a multi-task AI model capable of real-time understanding of surgical behaviors, hands, and tools - the building blocks of procedural flow and surgeon skill. We show that our model generalizes across diverse surgery types and environments. Illustrating this generalizability, we directly applied our YouTube-trained model to analyze open surgeries prospectively collected at an academic medical center and identified kinematic descriptors of surgical skill related to efficiency of hand motion. Our Annotated Videos of Open Surgery (AVOS) dataset and trained model will be made available for further development of surgical AI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.07219v1-abstract-full').style.display = 'none'; document.getElementById('2112.07219v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">22 pages, 4 main text figures, 7 extended data figures, 4 extended data tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.06918">arXiv:2112.06918</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.06918">pdf</a>, <a href="https://arxiv.org/format/2112.06918">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automated Customization of On-Thing Inference for Quality-of-Experience Enhancement
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bai%2C+Y">Yang Bai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+L">Lixing Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+J">Jie Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.06918v1-abstract-short" style="display: inline;">
        The rapid uptake of intelligent applications is pushing deep learning (DL) capabilities to Internet-of-Things (IoT). Despite the emergence of new tools for embedding deep neural networks (DNNs) into IoT devices, providing satisfactory Quality of Experience (QoE) to users is still challenging due to the heterogeneity in DNN architectures, IoT devices, and user preferences. This paper studies automa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.06918v1-abstract-full').style.display = 'inline'; document.getElementById('2112.06918v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.06918v1-abstract-full" style="display: none;">
        The rapid uptake of intelligent applications is pushing deep learning (DL) capabilities to Internet-of-Things (IoT). Despite the emergence of new tools for embedding deep neural networks (DNNs) into IoT devices, providing satisfactory Quality of Experience (QoE) to users is still challenging due to the heterogeneity in DNN architectures, IoT devices, and user preferences. This paper studies automated customization for DL inference on IoT devices (termed as on-thing inference), and our goal is to enhance user QoE by configuring the on-thing inference with an appropriate DNN for users under different usage scenarios. The core of our method is a DNN selection module that learns user QoE patterns on-the-fly and identifies the best-fit DNN for on-thing inference with the learned knowledge. It leverages a novel online learning algorithm, NeuralUCB, that has excellent generalization ability for handling various user QoE patterns. We also embed the knowledge transfer technique in NeuralUCB to expedite the learning process. However, NeuralUCB frequently solicits QoE ratings from users, which incurs non-negligible inconvenience. To address this problem, we design feedback solicitation schemes to reduce the number of QoE solicitations while maintaining the learning efficiency of NeuralUCB. A pragmatic problem, aggregated QoE, is further investigated to improve the practicality of our framework. We conduct experiments on both synthetic and real-world data. The results indicate that our method efficiently learns the user QoE pattern with few solicitations and provides drastic QoE enhancement for IoT devices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.06918v1-abstract-full').style.display = 'none'; document.getElementById('2112.06918v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.02285">arXiv:2112.02285</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.02285">pdf</a>, <a href="https://arxiv.org/ps/2112.02285">ps</a>, <a href="https://arxiv.org/format/2112.02285">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Configuring Intelligent Reflecting Surface with Performance Guarantees: Blind Beamforming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuyi Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+K">Kaiming Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yaowen Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Z">Zhi-Quan Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.02285v1-abstract-short" style="display: inline;">
        This work gives a blind beamforming strategy for intelligent reflecting surface (IRS), aiming to boost the received signal-to-noise ratio (SNR) by coordinating phase shifts across reflective elements in the absence of channel information. While the existing methods of IRS beamforming typically first estimate channels and then optimize phase shifts, we propose a conditional sample mean based statis&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.02285v1-abstract-full').style.display = 'inline'; document.getElementById('2112.02285v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.02285v1-abstract-full" style="display: none;">
        This work gives a blind beamforming strategy for intelligent reflecting surface (IRS), aiming to boost the received signal-to-noise ratio (SNR) by coordinating phase shifts across reflective elements in the absence of channel information. While the existing methods of IRS beamforming typically first estimate channels and then optimize phase shifts, we propose a conditional sample mean based statistical approach that explores the wireless environment via random sampling without performing any channel estimation. Remarkably, the new method just requires a polynomial number of random samples to yield an SNR boost that is quadratic in the number of reflective elements, whereas the standard random-max sampling algorithm can only achieve a linear boost under the same condition. Moreover, we gain additional insight into blind beamforming by interpreting it as a least squares problem. Field tests demonstrate the significant advantages of the proposed blind beamforming algorithm over the benchmark algorithms in enhancing wireless transmission.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.02285v1-abstract-full').style.display = 'none'; document.getElementById('2112.02285v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.02260">arXiv:2112.02260</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.02260">pdf</a>, <a href="https://arxiv.org/ps/2112.02260">ps</a>, <a href="https://arxiv.org/format/2112.02260">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Configuring Intelligent Reflecting Surface with Performance Guarantees: Optimal Beamforming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yaowen Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+K">Kaiming Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuyi Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xin Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Z">Zhi-Quan Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.02260v1-abstract-short" style="display: inline;">
        This work proposes linear time strategies to optimally configure the phase shifts for the reflective elements of an intelligent reflecting surface (IRS). Specifically, we show that the binary phase beamforming can be optimally solved in linear time to maximize the received signal-to-noise ratio (SNR). For the general K-ary phase beamforming, we develop a linear time approximation algorithm that gu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.02260v1-abstract-full').style.display = 'inline'; document.getElementById('2112.02260v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.02260v1-abstract-full" style="display: none;">
        This work proposes linear time strategies to optimally configure the phase shifts for the reflective elements of an intelligent reflecting surface (IRS). Specifically, we show that the binary phase beamforming can be optimally solved in linear time to maximize the received signal-to-noise ratio (SNR). For the general K-ary phase beamforming, we develop a linear time approximation algorithm that guarantees performance within a constant fraction (1+\cos(/K))/2 of the global optimum, e.g., it can attain over 85% of the optimal performance for the quadrature beamforming with K=4. According to the numerical results, the proposed approximation algorithm for discrete IRS beamforming outperforms the existing algorithms significantly in boosting the received SNR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.02260v1-abstract-full').style.display = 'none'; document.getElementById('2112.02260v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.15193">arXiv:2111.15193</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.15193">pdf</a>, <a href="https://arxiv.org/format/2111.15193">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Shunted Self-Attention via Multi-Scale Token Aggregation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+D">Daquan Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+S">Shengfeng He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+J">Jiashi Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xinchao Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.15193v2-abstract-short" style="display: inline;">
        Recent Vision Transformer~(ViT) models have demonstrated encouraging results across various computer vision tasks, thanks to their competence in modeling long-range dependencies of image patches or tokens via self-attention. These models, however, usually designate the similar receptive fields of each token feature within each layer. Such a constraint inevitably limits the ability of each self-att&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.15193v2-abstract-full').style.display = 'inline'; document.getElementById('2111.15193v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.15193v2-abstract-full" style="display: none;">
        Recent Vision Transformer~(ViT) models have demonstrated encouraging results across various computer vision tasks, thanks to their competence in modeling long-range dependencies of image patches or tokens via self-attention. These models, however, usually designate the similar receptive fields of each token feature within each layer. Such a constraint inevitably limits the ability of each self-attention layer in capturing multi-scale features, thereby leading to performance degradation in handling images with multiple objects of different scales. To address this issue, we propose a novel and generic strategy, termed shunted self-attention~(SSA), that allows ViTs to model the attentions at hybrid scales per attention layer. The key idea of SSA is to inject heterogeneous receptive field sizes into tokens: before computing the self-attention matrix, it selectively merges tokens to represent larger object features while keeping certain tokens to preserve fine-grained features. This novel merging scheme enables the self-attention to learn relationships between objects with different sizes and simultaneously reduces the token numbers and the computational cost. Extensive experiments across various tasks demonstrate the superiority of SSA. Specifically, the SSA-based transformer achieves 84.0\% Top-1 accuracy and outperforms the state-of-the-art Focal Transformer on ImageNet with only half of the model size and computation cost, and surpasses Focal Transformer by 1.3 mAP on COCO and 2.9 mIOU on ADE20K under similar parameter and computation cost. Code has been released at https://github.com/OliverRensu/Shunted-Transformer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.15193v2-abstract-full').style.display = 'none'; document.getElementById('2111.15193v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR2022 Oral</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.13311">arXiv:2111.13311</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.13311">pdf</a>, <a href="https://arxiv.org/format/2111.13311">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Blaschke Product Neural Networks (BPNN): A Physics-Infused Neural Network for Phase Retrieval of Meromorphic Functions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+J">Juncheng Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Simiao Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Y">Yang Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khatib%2C+O">Omar Khatib</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Malof%2C+J">Jordan Malof</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Soltani%2C+M">Mohammadreza Soltani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Padilla%2C+W">Willie Padilla</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tarokh%2C+V">Vahid Tarokh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.13311v1-abstract-short" style="display: inline;">
        Numerous physical systems are described by ordinary or partial differential equations whose solutions are given by holomorphic or meromorphic functions in the complex domain. In many cases, only the magnitude of these functions are observed on various points on the purely imaginary jw-axis since coherent measurement of their phases is often expensive. However, it is desirable to retrieve the lost&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.13311v1-abstract-full').style.display = 'inline'; document.getElementById('2111.13311v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.13311v1-abstract-full" style="display: none;">
        Numerous physical systems are described by ordinary or partial differential equations whose solutions are given by holomorphic or meromorphic functions in the complex domain. In many cases, only the magnitude of these functions are observed on various points on the purely imaginary jw-axis since coherent measurement of their phases is often expensive. However, it is desirable to retrieve the lost phases from the magnitudes when possible. To this end, we propose a physics-infused deep neural network based on the Blaschke products for phase retrieval. Inspired by the Helson and Sarason Theorem, we recover coefficients of a rational function of Blaschke products using a Blaschke Product Neural Network (BPNN), based upon the magnitude observations as input. The resulting rational function is then used for phase retrieval. We compare the BPNN to conventional deep neural networks (NNs) on several phase retrieval problems, comprising both synthetic and contemporary real-world problems (e.g., metamaterials for which data collection requires substantial expertise and is time consuming). On each phase retrieval problem, we compare against a population of conventional NNs of varying size and hyperparameter settings. Even without any hyper-parameter search, we find that BPNNs consistently outperform the population of optimized NNs in scarce data scenarios, and do so despite being much smaller models. The results can in turn be applied to calculate the refractive index of metamaterials, which is an important problem in emerging areas of material science.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.13311v1-abstract-full').style.display = 'none'; document.getElementById('2111.13311v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.08227">arXiv:2111.08227</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.08227">pdf</a>, <a href="https://arxiv.org/format/2111.08227">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Medical Physics">physics.med-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1088/1361-6560/ac5b21">10.1088/1361-6560/ac5b21 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Phase function estimation from a diffuse optical image via deep learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+Y">Yuxuan Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niu%2C+C">Chuang Niu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+C">Chen Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shenghan Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cong%2C+W">Wenxiang Cong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Ge Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.08227v1-abstract-short" style="display: inline;">
        The phase function is a key element of a light propagation model for Monte Carlo (MC) simulation, which is usually fitted with an analytic function with associated parameters. In recent years, machine learning methods were reported to estimate the parameters of the phase function of a particular form such as the Henyey-Greenstein phase function but, to our knowledge, no studies have been performed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08227v1-abstract-full').style.display = 'inline'; document.getElementById('2111.08227v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.08227v1-abstract-full" style="display: none;">
        The phase function is a key element of a light propagation model for Monte Carlo (MC) simulation, which is usually fitted with an analytic function with associated parameters. In recent years, machine learning methods were reported to estimate the parameters of the phase function of a particular form such as the Henyey-Greenstein phase function but, to our knowledge, no studies have been performed to determine the form of the phase function. Here we design a convolutional neural network to estimate the phase function from a diffuse optical image without any explicit assumption on the form of the phase function. Specifically, we use a Gaussian mixture model as an example to represent the phase function generally and learn the model parameters accurately. The Gaussian mixture model is selected because it provides the analytic expression of phase function to facilitate deflection angle sampling in MC simulation, and does not significantly increase the number of free parameters. Our proposed method is validated on MC-simulated reflectance images of typical biological tissues using the Henyey-Greenstein phase function with different anisotropy factors. The effects of field of view (FOV) and spatial resolution on the errors are analyzed to optimize the estimation method. The mean squared error of the phase function is 0.01 and the relative error of the anisotropy factor is 3.28%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08227v1-abstract-full').style.display = 'none'; document.getElementById('2111.08227v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.02493">arXiv:2111.02493</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.02493">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Instrumentation and Detectors">physics.ins-det</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1088/1361-6501/ac2dbd">10.1088/1361-6501/ac2dbd <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Roadmap on Signal Processing for Next Generation Measurement Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Iakovidis%2C+D+K">D. K. Iakovidis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ooi%2C+M">M. Ooi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kuang%2C+Y+C">Y. C. Kuang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Demidenko%2C+S">S. Demidenko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shestakov%2C+A">A. Shestakov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sinitsin%2C+V">V. Sinitsin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Henry%2C+M">M. Henry</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sciacchitano%2C+A">A. Sciacchitano</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Discetti%2C+A">A. Discetti</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Donati%2C+S">S. Donati</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Norgia%2C+M">M. Norgia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Menychtas%2C+A">A. Menychtas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Maglogiannis%2C+I">I. Maglogiannis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wriessnegger%2C+S+C">S. C. Wriessnegger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chacon%2C+L+A+B">L. A. Barradas Chacon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dimas%2C+G">G. Dimas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Filos%2C+D">D. Filos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Aletras%2C+A+H">A. H. Aletras</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=T%C3%B6ger%2C+J">J. Tger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+F">F. Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">S. Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Uhl%2C+A">A. Uhl</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Paziewski%2C+J">J. Paziewski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Geng%2C+J">J. Geng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fioranelli%2C+F">F. Fioranelli</a>
      , et al. (9 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.02493v3-abstract-short" style="display: inline;">
        Signal processing is a fundamental component of almost any sensor-enabled system, with a wide range of applications across different scientific disciplines. Time series data, images, and video sequences comprise representative forms of signals that can be enhanced and analysed for information extraction and quantification. The recent advances in artificial intelligence and machine learning are shi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.02493v3-abstract-full').style.display = 'inline'; document.getElementById('2111.02493v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.02493v3-abstract-full" style="display: none;">
        Signal processing is a fundamental component of almost any sensor-enabled system, with a wide range of applications across different scientific disciplines. Time series data, images, and video sequences comprise representative forms of signals that can be enhanced and analysed for information extraction and quantification. The recent advances in artificial intelligence and machine learning are shifting the research attention towards intelligent, data-driven, signal processing. This roadmap presents a critical overview of the state-of-the-art methods and applications aiming to highlight future challenges and research opportunities towards next generation measurement systems. It covers a broad spectrum of topics ranging from basic to industrial research, organized in concise thematic sections that reflect the trends and the impacts of current and future developments per research field. Furthermore, it offers guidance to researchers and funding agencies in identifying new prospects.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.02493v3-abstract-full').style.display = 'none'; document.getElementById('2111.02493v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">48 pages, https://iopscience.iop.org/article/10.1088/1361-6501/ac2dbd</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Measurement Science and Technology 33(1) (2022) 1-48
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.01203">arXiv:2111.01203</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.01203">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3491046">10.1145/3491046 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        One Proxy Device Is Enough for Hardware-Aware Neural Architecture Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+B">Bingqian Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianyi Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+W">Weiwen Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+Y">Yiyu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.01203v2-abstract-short" style="display: inline;">
        Convolutional neural networks (CNNs) are used in numerous real-world applications such as vision-based autonomous driving and video content analysis. To run CNN inference on various target devices, hardware-aware neural architecture search (NAS) is crucial. A key requirement of efficient hardware-aware NAS is the fast evaluation of inference latencies in order to rank different architectures. Whil&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.01203v2-abstract-full').style.display = 'inline'; document.getElementById('2111.01203v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.01203v2-abstract-full" style="display: none;">
        Convolutional neural networks (CNNs) are used in numerous real-world applications such as vision-based autonomous driving and video content analysis. To run CNN inference on various target devices, hardware-aware neural architecture search (NAS) is crucial. A key requirement of efficient hardware-aware NAS is the fast evaluation of inference latencies in order to rank different architectures. While building a latency predictor for each target device has been commonly used in state of the art, this is a very time-consuming process, lacking scalability in the presence of extremely diverse devices. In this work, we address the scalability challenge by exploiting latency monotonicity -- the architecture latency rankings on different devices are often correlated. When strong latency monotonicity exists, we can re-use architectures searched for one proxy device on new target devices, without losing optimality. In the absence of strong latency monotonicity, we propose an efficient proxy adaptation technique to significantly boost the latency monotonicity. Finally, we validate our approach and conduct experiments with devices of different platforms on multiple mainstream search spaces, including MobileNet-V2, MobileNet-V3, NAS-Bench-201, ProxylessNAS and FBNet. Our results highlight that, by using just one proxy device, we can find almost the same Pareto-optimal architectures as the existing per-device NAS, while avoiding the prohibitive cost of building a latency predictor for each device. GitHub: https://github.com/Ren-Research/OneProxy
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.01203v2-abstract-full').style.display = 'none'; document.getElementById('2111.01203v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by the ACM SIGMETRICS 2022. Published in the Proceedings of the ACM on Measurement and Analysis of Computing Systems, vol. 5, no. 3, Article 34, December 2021. GitHub: https://github.com/Ren-Research/OneProxy</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proc. ACM Meas. Anal. Comput. Syst., vol. 5, no. 3, Article 34, December 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.00643">arXiv:2111.00643</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.00643">pdf</a>, <a href="https://arxiv.org/format/2111.00643">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Distilled Collaboration Graph for Multi-Agent Perception
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yiming Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shunli Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+P">Pengxiang Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+S">Siheng Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Feng%2C+C">Chen Feng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+W">Wenjun Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.00643v2-abstract-short" style="display: inline;">
        To promote better performance-bandwidth trade-off for multi-agent perception, we propose a novel distilled collaboration graph (DiscoGraph) to model trainable, pose-aware, and adaptive collaboration among agents. Our key novelties lie in two aspects. First, we propose a teacher-student framework to train DiscoGraph via knowledge distillation. The teacher model employs an early collaboration with h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.00643v2-abstract-full').style.display = 'inline'; document.getElementById('2111.00643v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.00643v2-abstract-full" style="display: none;">
        To promote better performance-bandwidth trade-off for multi-agent perception, we propose a novel distilled collaboration graph (DiscoGraph) to model trainable, pose-aware, and adaptive collaboration among agents. Our key novelties lie in two aspects. First, we propose a teacher-student framework to train DiscoGraph via knowledge distillation. The teacher model employs an early collaboration with holistic-view inputs; the student model is based on intermediate collaboration with single-view inputs. Our framework trains DiscoGraph by constraining post-collaboration feature maps in the student model to match the correspondences in the teacher model. Second, we propose a matrix-valued edge weight in DiscoGraph. In such a matrix, each element reflects the inter-agent attention at a specific spatial region, allowing an agent to adaptively highlight the informative regions. During inference, we only need to use the student model named as the distilled collaboration network (DiscoNet). Attributed to the teacher-student framework, multiple agents with the shared DiscoNet could collaboratively approach the performance of a hypothetical teacher model with a holistic view. Our approach is validated on V2X-Sim 1.0, a large-scale multi-agent perception dataset that we synthesized using CARLA and SUMO co-simulation. Our quantitative and qualitative experiments in multi-agent 3D object detection show that DiscoNet could not only achieve a better performance-bandwidth trade-off than the state-of-the-art collaborative perception methods, but also bring more straightforward design rationale. Our code is available on https://github.com/ai4ce/DiscoNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.00643v2-abstract-full').style.display = 'none'; document.getElementById('2111.00643v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to 35th Conference on Neural Information Processing Systems (NeurIPS 2021)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.13900">arXiv:2110.13900</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.13900">pdf</a>, <a href="https://arxiv.org/format/2110.13900">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+S">Sanyuan Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Chengyi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhengyang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shujie Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Z">Zhuo Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jinyu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kanda%2C+N">Naoyuki Kanda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yoshioka%2C+T">Takuya Yoshioka</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xiao%2C+X">Xiong Xiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+L">Long Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuo Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+Y">Yanmin Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+Y">Yao Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jian Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+M">Michael Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+X">Xiangzhan Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+F">Furu Wei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.13900v4-abstract-short" style="display: inline;">
        Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.13900v4-abstract-full').style.display = 'inline'; document.getElementById('2110.13900v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.13900v4-abstract-full" style="display: none;">
        Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture sequence ordering of input speech, and scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.13900v4-abstract-full').style.display = 'none'; document.getElementById('2110.13900v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.12138">arXiv:2110.12138</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.12138">pdf</a>, <a href="https://arxiv.org/format/2110.12138">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimizing Alignment of Speech and Language Latent Spaces for End-to-End Speech Recognition and Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuo Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+Y">Yao Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shujie Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+Y">Yu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+Y">Yanmin Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+M">Michael Zeng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.12138v1-abstract-short" style="display: inline;">
        The advances in attention-based encoder-decoder (AED) networks have brought great progress to end-to-end (E2E) automatic speech recognition (ASR). One way to further improve the performance of AED-based E2E ASR is to introduce an extra text encoder for leveraging extensive text data and thus capture more context-aware linguistic information. However, this approach brings a mismatch problem between&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.12138v1-abstract-full').style.display = 'inline'; document.getElementById('2110.12138v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.12138v1-abstract-full" style="display: none;">
        The advances in attention-based encoder-decoder (AED) networks have brought great progress to end-to-end (E2E) automatic speech recognition (ASR). One way to further improve the performance of AED-based E2E ASR is to introduce an extra text encoder for leveraging extensive text data and thus capture more context-aware linguistic information. However, this approach brings a mismatch problem between the speech encoder and the text encoder due to the different units used for modeling. In this paper, we propose an embedding aligner and modality switch training to better align the speech and text latent spaces. The embedding aligner is a shared linear projection between text encoder and speech encoder trained by masked language modeling (MLM) loss and connectionist temporal classification (CTC), respectively. The modality switch training randomly swaps speech and text embeddings based on the forced alignment result to learn a joint representation space. Experimental results show that our proposed approach achieves a relative 14% to 19% word error rate (WER) reduction on Librispeech ASR task. We further verify its effectiveness on spoken language understanding (SLU), i.e., an absolute 2.5% to 2.8% F1 score improvement on SNIPS slot filling task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.12138v1-abstract-full').style.display = 'none'; document.getElementById('2110.12138v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to ICASSP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.09726">arXiv:2110.09726</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.09726">pdf</a>, <a href="https://arxiv.org/format/2110.09726">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CGNN: Traffic Classification with Graph Neural Network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pang%2C+B">Bo Pang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+Y">Yongquan Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Siyuan Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Ye Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liao%2C+Q">Qing Liao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+Y">Yan Jia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.09726v1-abstract-short" style="display: inline;">
        Traffic classification associates packet streams with known application labels, which is vital for network security and network management. With the rise of NAT, port dynamics, and encrypted traffic, it is increasingly challenging to obtain unified traffic features for accurate classification. Many state-of-the-art traffic classifiers automatically extract features from the packet stream based on&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09726v1-abstract-full').style.display = 'inline'; document.getElementById('2110.09726v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.09726v1-abstract-full" style="display: none;">
        Traffic classification associates packet streams with known application labels, which is vital for network security and network management. With the rise of NAT, port dynamics, and encrypted traffic, it is increasingly challenging to obtain unified traffic features for accurate classification. Many state-of-the-art traffic classifiers automatically extract features from the packet stream based on deep learning models such as convolution networks. Unfortunately, the compositional and causal relationships between packets are not well extracted in these deep learning models, which affects both prediction accuracy and generalization on different traffic types.
  In this paper, we present a chained graph model on the packet stream to keep the chained compositional sequence. Next, we propose CGNN, a graph neural network based traffic classification method, which builds a graph classifier over automatically extracted features over the chained graph.
  Extensive evaluation over real-world traffic data sets, including normal, encrypted and malicious labels, show that, CGNN improves the prediction accuracy by 23\% to 29\% for application classification, by 2\% to 37\% for malicious traffic classification, and reaches the same accuracy level for encrypted traffic classification. CGNN is quite robust in terms of the recall and precision metrics. We have extensively evaluated the parameter sensitivity of CGNN, which yields optimized parameters that are quite effective for traffic classification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.09726v1-abstract-full').style.display = 'none'; document.getElementById('2110.09726v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.07205">arXiv:2110.07205</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.07205">pdf</a>, <a href="https://arxiv.org/format/2110.07205">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ao%2C+J">Junyi Ao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+R">Rui Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+L">Long Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+C">Chengyi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuo Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shujie Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ko%2C+T">Tom Ko</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Q">Qing Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Z">Zhihua Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+Y">Yao Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jinyu Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+F">Furu Wei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.07205v2-abstract-short" style="display: inline;">
        Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After prepro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.07205v2-abstract-full').style.display = 'inline'; document.getElementById('2110.07205v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.07205v2-abstract-full" style="display: none;">
        Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. We will release our code and model at https://github.com/microsoft/SpeechT5.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.07205v2-abstract-full').style.display = 'none'; document.getElementById('2110.07205v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACL 2022 main conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.11295">arXiv:2109.11295</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.11295">pdf</a>, <a href="https://arxiv.org/format/2109.11295">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Knowledge Distillation for Pre-trained Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L">Lei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yankai Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuhuai Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+P">Peng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Jie Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+X">Xu Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.11295v1-abstract-short" style="display: inline;">
        Knowledge distillation~(KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learn&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.11295v1-abstract-full').style.display = 'inline'; document.getElementById('2109.11295v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.11295v1-abstract-full" style="display: none;">
        Knowledge distillation~(KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10% informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods. Our code is available at https://github.com/lancopku/DynamicKD.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.11295v1-abstract-full').style.display = 'none'; document.getElementById('2109.11295v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Main Conference EMNLP 2021, Camera Ready</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.00523">arXiv:2109.00523</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.00523">pdf</a>, <a href="https://arxiv.org/format/2109.00523">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuhuai Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jinchao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L">Lei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+X">Xu Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Jie Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.00523v1-abstract-short" style="display: inline;">
        Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.00523v1-abstract-full').style.display = 'inline'; document.getElementById('2109.00523v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.00523v1-abstract-full" style="display: none;">
        Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous methods, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a framework named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for data augmentation. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best policy, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.00523v1-abstract-full').style.display = 'none'; document.getElementById('2109.00523v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by EMNLP 2021 main conference (Long Paper)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.13239">arXiv:2108.13239</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.13239">pdf</a>, <a href="https://arxiv.org/ps/2108.13239">ps</a>, <a href="https://arxiv.org/format/2108.13239">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive perturbation adversarial training: based on reinforcement learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nie%2C+Z">Zhishen Nie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Ying Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sp Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+L">Lan Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.13239v1-abstract-short" style="display: inline;">
        Adversarial training has become the primary method to defend against adversarial samples. However, it is hard to practically apply due to many shortcomings. One of the shortcomings of adversarial training is that it will reduce the recognition accuracy of normal samples. Adaptive perturbation adversarial training is proposed to alleviate this problem. It uses marginal adversarial samples that are&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13239v1-abstract-full').style.display = 'inline'; document.getElementById('2108.13239v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.13239v1-abstract-full" style="display: none;">
        Adversarial training has become the primary method to defend against adversarial samples. However, it is hard to practically apply due to many shortcomings. One of the shortcomings of adversarial training is that it will reduce the recognition accuracy of normal samples. Adaptive perturbation adversarial training is proposed to alleviate this problem. It uses marginal adversarial samples that are close to the decision boundary but does not cross the decision boundary for adversarial training, which improves the accuracy of model recognition while maintaining the robustness of the model. However, searching for marginal adversarial samples brings additional computational costs. This paper proposes a method for finding marginal adversarial samples based on reinforcement learning, and combines it with the latest fast adversarial training technology, which effectively speeds up training process and reduces training costs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13239v1-abstract-full').style.display = 'none'; document.getElementById('2108.13239v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.08170">arXiv:2108.08170</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.08170">pdf</a>, <a href="https://arxiv.org/format/2108.08170">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DeepExpress: Heterogeneous and Coupled Sequence Modeling for Express Delivery Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Siyuan Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+B">Bin Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+L">Longbing Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+K">Ke Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiaqi Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Z">Zhiwen Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.08170v1-abstract-short" style="display: inline;">
        The prediction of express delivery sequence, i.e., modeling and estimating the volumes of daily incoming and outgoing parcels for delivery, is critical for online business, logistics, and positive customer experience, and specifically for resource allocation optimization and promotional activity arrangement. A precise estimate of consumer delivery requests has to involve sequential factors such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.08170v1-abstract-full').style.display = 'inline'; document.getElementById('2108.08170v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.08170v1-abstract-full" style="display: none;">
        The prediction of express delivery sequence, i.e., modeling and estimating the volumes of daily incoming and outgoing parcels for delivery, is critical for online business, logistics, and positive customer experience, and specifically for resource allocation optimization and promotional activity arrangement. A precise estimate of consumer delivery requests has to involve sequential factors such as shopping behaviors, weather conditions, events, business campaigns, and their couplings. Besides, conventional sequence prediction assumes a stable sequence evolution, failing to address complex nonlinear sequences and various feature effects in the above multi-source data. Although deep networks and attention mechanisms demonstrate the potential of complex sequence modeling, extant networks ignore the heterogeneous and coupling situation between features and sequences, resulting in weak prediction accuracy. To address these issues, we propose DeepExpress - a deep-learning based express delivery sequence prediction model, which extends the classic seq2seq framework to learning complex coupling between sequence and features. DeepExpress leverages an express delivery seq2seq learning, a carefully-designed heterogeneous feature representation, and a novel joint training attention mechanism to adaptively map heterogeneous data, and capture sequence-feature coupling for precise estimation. Experimental results on real-world data demonstrate that the proposed method outperforms both shallow and deep baseline models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.08170v1-abstract-full').style.display = 'none'; document.getElementById('2108.08170v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.07978">arXiv:2108.07978</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.07978">pdf</a>, <a href="https://arxiv.org/format/2108.07978">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A New Journey from SDRTV to HDRTV
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+X">Xiangyu Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhengwen Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+J+S">Jimmy S. Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+L">Lynhoo Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+Y">Yu Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+C">Chao Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.07978v2-abstract-short" style="display: inline;">
        Nowadays modern displays are capable to render video content with high dynamic range (HDR) and wide color gamut (WCG). However, most available resources are still in standard dynamic range (SDR). Therefore, there is an urgent demand to transform existing SDR-TV contents into their HDR-TV versions. In this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.07978v2-abstract-full').style.display = 'inline'; document.getElementById('2108.07978v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.07978v2-abstract-full" style="display: none;">
        Nowadays modern displays are capable to render video content with high dynamic range (HDR) and wide color gamut (WCG). However, most available resources are still in standard dynamic range (SDR). Therefore, there is an urgent demand to transform existing SDR-TV contents into their HDR-TV versions. In this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step solution pipeline including adaptive global color mapping, local enhancement and highlight generation. Moreover, the above analysis inspires us to present a lightweight network that utilizes global statistics as guidance to conduct image-adaptive color mapping. In addition, we construct a dataset using HDR videos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate the results of SDRTV-to-HDRTV algorithms. Furthermore, our final results achieve state-of-the-art performance in quantitative comparisons and visual quality. The code and dataset are available at https://github.com/chxy95/HDRTVNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.07978v2-abstract-full').style.display = 'none'; document.getElementById('2108.07978v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICCV</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.02980">arXiv:2108.02980</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.02980">pdf</a>, <a href="https://arxiv.org/format/2108.02980">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fine-grained Domain Adaptive Crowd Counting via Point-derived Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yongtuo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+D">Dan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Hanjie Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+H">Hongmin Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+S">Shengfeng He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.02980v1-abstract-short" style="display: inline;">
        Existing domain adaptation methods for crowd counting view each crowd image as a whole and reduce domain discrepancies on crowds and backgrounds simultaneously. However, we argue that these methods are suboptimal, as crowds and backgrounds have quite different characteristics and backgrounds may vary dramatically in different crowd scenes (see Fig.~\ref{teaser}). This makes crowds not well aligned&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02980v1-abstract-full').style.display = 'inline'; document.getElementById('2108.02980v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.02980v1-abstract-full" style="display: none;">
        Existing domain adaptation methods for crowd counting view each crowd image as a whole and reduce domain discrepancies on crowds and backgrounds simultaneously. However, we argue that these methods are suboptimal, as crowds and backgrounds have quite different characteristics and backgrounds may vary dramatically in different crowd scenes (see Fig.~\ref{teaser}). This makes crowds not well aligned across domains together with backgrounds in a holistic manner. To this end, we propose to untangle crowds and backgrounds from crowd images and design fine-grained domain adaption methods for crowd counting. Different from other tasks which possess region-based fine-grained annotations (e.g., segments or bounding boxes), crowd counting only annotates one point on each human head, which impedes the implementation of fine-grained adaptation methods. To tackle this issue, we propose a novel and effective schema to learn crowd segmentation from point-level crowd counting annotations in the context of Multiple Instance Learning. We further leverage the derived segments to propose a crowd-aware fine-grained domain adaptation framework for crowd counting, which consists of two novel adaptation modules, i.e., Crowd Region Transfer (CRT) and Crowd Density Alignment (CDA). Specifically, the CRT module is designed to guide crowd features transfer across domains beyond background distractions, and the CDA module dedicates to constraining the target-domain crowd density distributions. Extensive experiments on multiple cross-domain settings (i.e., Synthetic $\rightarrow$ Real, Fixed $\rightarrow$ Fickle, Normal $\rightarrow$ BadWeather) demonstrate the superiority of the proposed method compared with state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02980v1-abstract-full').style.display = 'none'; document.getElementById('2108.02980v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.02970">arXiv:2108.02970</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.02970">pdf</a>, <a href="https://arxiv.org/format/2108.02970">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reducing Spatial Labeling Redundancy for Semi-supervised Crowd Counting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yongtuo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chai%2C+L">Liangyu Chai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Hanjie Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qin%2C+J">Jing Qin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+D">Dan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+S">Shengfeng He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.02970v1-abstract-short" style="display: inline;">
        Labeling is onerous for crowd counting as it should annotate each individual in crowd images. Recently, several methods have been proposed for semi-supervised crowd counting to reduce the labeling efforts. Given a limited labeling budget, they typically select a few crowd images and densely label all individuals in each of them. Despite the promising results, we argue the None-or-All labeling stra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02970v1-abstract-full').style.display = 'inline'; document.getElementById('2108.02970v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.02970v1-abstract-full" style="display: none;">
        Labeling is onerous for crowd counting as it should annotate each individual in crowd images. Recently, several methods have been proposed for semi-supervised crowd counting to reduce the labeling efforts. Given a limited labeling budget, they typically select a few crowd images and densely label all individuals in each of them. Despite the promising results, we argue the None-or-All labeling strategy is suboptimal as the densely labeled individuals in each crowd image usually appear similar while the massive unlabeled crowd images may contain entirely diverse individuals. To this end, we propose to break the labeling chain of previous methods and make the first attempt to reduce spatial labeling redundancy for semi-supervised crowd counting. First, instead of annotating all the regions in each crowd image, we propose to annotate the representative ones only. We analyze the region representativeness from both vertical and horizontal directions, and formulate them as cluster centers of Gaussian Mixture Models. Additionally, to leverage the rich unlabeled regions, we exploit the similarities among individuals in each crowd image to directly supervise the unlabeled regions via feature propagation instead of the error-prone label propagation employed in the previous methods. In this way, we can transfer the original spatial labeling redundancy caused by individual similarities to effective supervision signals on the unlabeled regions. Extensive experiments on the widely-used benchmarks demonstrate that our method can outperform previous best approaches by a large margin.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02970v1-abstract-full').style.display = 'none'; document.getElementById('2108.02970v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.02759">arXiv:2108.02759</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.02759">pdf</a>, <a href="https://arxiv.org/format/2108.02759">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unifying Global-Local Representations in Salient Object Detection with Transformer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+Q">Qiang Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+N">Nanxuan Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+G">Guoqiang Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+S">Shengfeng He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.02759v1-abstract-short" style="display: inline;">
        The fully convolutional network (FCN) has dominated salient object detection for a long period. However, the locality of CNN requires the model deep enough to have a global receptive field and such a deep model always leads to the loss of local details. In this paper, we introduce a new attention-based encoder, vision transformer, into salient object detection to ensure the globalization of the re&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02759v1-abstract-full').style.display = 'inline'; document.getElementById('2108.02759v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.02759v1-abstract-full" style="display: none;">
        The fully convolutional network (FCN) has dominated salient object detection for a long period. However, the locality of CNN requires the model deep enough to have a global receptive field and such a deep model always leads to the loss of local details. In this paper, we introduce a new attention-based encoder, vision transformer, into salient object detection to ensure the globalization of the representations from shallow to deep layers. With the global view in very shallow layers, the transformer encoder preserves more local representations to recover the spatial details in final saliency maps. Besides, as each layer can capture a global view of its previous layer, adjacent layers can implicitly maximize the representation differences and minimize the redundant features, making that every output feature of transformer layers contributes uniquely for final prediction. To decode features from the transformer, we propose a simple yet effective deeply-transformed decoder. The decoder densely decodes and upsamples the transformer features, generating the final saliency map with less noise injection. Experimental results demonstrate that our method significantly outperforms other FCN-based and transformer-based methods in five benchmarks by a large margin, with an average of 12.17% improvement in terms of Mean Absolute Error (MAE). Code will be available at https://github.com/OliverRensu/GLSTR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.02759v1-abstract-full').style.display = 'none'; document.getElementById('2108.02759v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.12182">arXiv:2107.12182</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.12182">pdf</a>, <a href="https://arxiv.org/format/2107.12182">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Predictive Multiphase Model of Silica Aerogels for Building Envelope Insulations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+J">Jingye Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Maleki%2C+P">Pedram Maleki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=An%2C+L">Lu An</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Di+Luigi%2C+M">Massimigliano Di Luigi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Villa%2C+U">Umberto Villa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+C">Chi Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shenqiang Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Faghihi%2C+D">Danial Faghihi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.12182v2-abstract-short" style="display: inline;">
        This work develops a multiphase thermomechanical model of porous silica aerogel and implements an uncertainty analysis framework consisting of the Sobol methods for global sensitivity analyses and Bayesian inference using a set of experimental data of silica aerogel. A notable feature of this work is implementing a new noise model within the Bayesian inversion to account for data uncertainty and m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.12182v2-abstract-full').style.display = 'inline'; document.getElementById('2107.12182v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.12182v2-abstract-full" style="display: none;">
        This work develops a multiphase thermomechanical model of porous silica aerogel and implements an uncertainty analysis framework consisting of the Sobol methods for global sensitivity analyses and Bayesian inference using a set of experimental data of silica aerogel. A notable feature of this work is implementing a new noise model within the Bayesian inversion to account for data uncertainty and modeling error. The hyper-parameters in the likelihood balance data misfit and prior contribution to the parameter posteriors and prevent their biased estimation. The results indicate that the uncertainty in solid conductivity and elasticity are the most influential parameters affecting the model output variance. Also, the Bayesian inference shows that despite the microstructural randomness in the thermal measurements, the model captures the data with 2% error. However, the model is inadequate in simulating the stress-strain measurements resulting in significant uncertainty in the computational prediction of a building insulation component.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.12182v2-abstract-full').style.display = 'none'; document.getElementById('2107.12182v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Jingye Tan and Pedram Maleki contributed equally to this work</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.07866">arXiv:2107.07866</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.07866">pdf</a>, <a href="https://arxiv.org/format/2107.07866">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MD Simulation of Hundred-Billion-Metal-Atom Cascade Collision on Sunway Taihulight
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+G">Genshen Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+R">Runchu Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuai Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+W">Wen Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+X">Xinfu He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+C">Chungjun Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jue Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.07866v1-abstract-short" style="display: inline;">
        Radiation damage to the steel material of reactor pressure vessels is a major threat to the nuclear reactor safety. It is caused by the metal atom cascade collision, initialized when the atoms are struck by a high-energy neutron. The paper presents MISA-MD, a new implementation of molecular dynamics, to simulate such cascade collision with EAM potential. MISA-MD realizes (1) a hash-based data stru&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.07866v1-abstract-full').style.display = 'inline'; document.getElementById('2107.07866v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.07866v1-abstract-full" style="display: none;">
        Radiation damage to the steel material of reactor pressure vessels is a major threat to the nuclear reactor safety. It is caused by the metal atom cascade collision, initialized when the atoms are struck by a high-energy neutron. The paper presents MISA-MD, a new implementation of molecular dynamics, to simulate such cascade collision with EAM potential. MISA-MD realizes (1) a hash-based data structure to efficiently store an atom and find its neighbors, and (2) several acceleration and optimization strategies based on SW26010 processor of Sunway Taihulight supercomputer, including an efficient potential table storage and interpolation method, a coloring method to avoid write conflicts, and double-buffer and data reuse strategies. The experimental results demonstrated that MISA-MD has good accuracy and scalability, and obtains a parallel efficiency of over 79% in an 655-billion-atom system. Compared with a state-of-the-art MD program LAMMPS, MISA-MD requires less memory usage and achieves better computational performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.07866v1-abstract-full').style.display = 'none'; document.getElementById('2107.07866v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.03610">arXiv:2107.03610</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.03610">pdf</a>, <a href="https://arxiv.org/format/2107.03610">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NccFlow: Unsupervised Learning of Optical Flow With Non-occlusion from Geometry
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+G">Guangming Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuaiqi Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hesheng Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.03610v1-abstract-short" style="display: inline;">
        Optical flow estimation is a fundamental problem of computer vision and has many applications in the fields of robot learning and autonomous driving. This paper reveals novel geometric laws of optical flow based on the insight and detailed definition of non-occlusion. Then, two novel loss functions are proposed for the unsupervised learning of optical flow based on the geometric laws of non-occlus&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03610v1-abstract-full').style.display = 'inline'; document.getElementById('2107.03610v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.03610v1-abstract-full" style="display: none;">
        Optical flow estimation is a fundamental problem of computer vision and has many applications in the fields of robot learning and autonomous driving. This paper reveals novel geometric laws of optical flow based on the insight and detailed definition of non-occlusion. Then, two novel loss functions are proposed for the unsupervised learning of optical flow based on the geometric laws of non-occlusion. Specifically, after the occlusion part of the images are masked, the flowing process of pixels is carefully considered and geometric constraints are conducted based on the geometric laws of optical flow. First, neighboring pixels in the first frame will not intersect during the pixel displacement to the second frame. Secondly, when the cluster containing adjacent four pixels in the first frame moves to the second frame, no other pixels will flow into the quadrilateral formed by them. According to the two geometrical constraints, the optical flow non-intersection loss and the optical flow non-blocking loss in the non-occlusion regions are proposed. Two loss functions punish the irregular and inexact optical flows in the non-occlusion regions. The experiments on datasets demonstrated that the proposed unsupervised losses of optical flow based on the geometric laws in non-occlusion regions make the estimated optical flow more refined in detail, and improve the performance of unsupervised learning of optical flow. In addition, the experiments training on synthetic data and evaluating on real data show that the generalization ability of optical flow network is improved by our proposed unsupervised approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.03610v1-abstract-full').style.display = 'none'; document.getElementById('2107.03610v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 7 figures, under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.12378">arXiv:2106.12378</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.12378">pdf</a>, <a href="https://arxiv.org/format/2106.12378">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Co-advise: Cross Inductive Bias Distillation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Z">Zhengqi Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hua%2C+T">Tianyu Hua</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+Z">Zihui Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+Y">Yonglong Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+S">Shengfeng He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Hang Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.12378v1-abstract-short" style="display: inline;">
        Transformers recently are adapted from the community of natural language processing as a promising substitute of convolution-based neural networks for visual learning tasks. However, its supremacy degenerates given an insufficient amount of training data (e.g., ImageNet). To make it into practical utility, we propose a novel distillation-based method to train vision transformers. Unlike previous w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.12378v1-abstract-full').style.display = 'inline'; document.getElementById('2106.12378v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.12378v1-abstract-full" style="display: none;">
        Transformers recently are adapted from the community of natural language processing as a promising substitute of convolution-based neural networks for visual learning tasks. However, its supremacy degenerates given an insufficient amount of training data (e.g., ImageNet). To make it into practical utility, we propose a novel distillation-based method to train vision transformers. Unlike previous works, where merely heavy convolution-based teachers are provided, we introduce lightweight teachers with different architectural inductive biases (e.g., convolution and involution) to co-advise the student transformer. The key is that teachers with different inductive biases attain different knowledge despite that they are trained on the same dataset, and such different knowledge compounds and boosts the student&#39;s performance during distillation. Equipped with this cross inductive bias distillation method, our vision transformers (termed as CivT) outperform all previous transformers of the same architecture on ImageNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.12378v1-abstract-full').style.display = 'none'; document.getElementById('2106.12378v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.13868">arXiv:2105.13868</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.13868">pdf</a>, <a href="https://arxiv.org/format/2105.13868">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Relation Alignment for Calibrated Cross-modal Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuhuai Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+J">Junyang Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+G">Guangxiang Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Men%2C+R">Rui Men</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+A">An Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Jingren Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+X">Xu Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+H">Hongxia Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.13868v2-abstract-short" style="display: inline;">
        Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. Th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.13868v2-abstract-full').style.display = 'inline'; document.getElementById('2105.13868v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.13868v2-abstract-full" style="display: none;">
        Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.13868v2-abstract-full').style.display = 'none'; document.getElementById('2105.13868v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACL-IJCNLP 2021 main conference (Long Paper)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.08909">arXiv:2105.08909</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.08909">pdf</a>, <a href="https://arxiv.org/format/2105.08909">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3404835.3462879">10.1145/3404835.3462879 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Graph Meta Embeddings for Cold-Start Ads in Click-Through Rate Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ouyang%2C+W">Wentao Ouyang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+X">Xiuwu Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shukui Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L">Li Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+J">Jinmei Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhaojie Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+Y">Yanlong Du</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.08909v1-abstract-short" style="display: inline;">
        Click-through rate (CTR) prediction is one of the most central tasks in online advertising systems. Recent deep learning-based models that exploit feature embedding and high-order data nonlinearity have shown dramatic successes in CTR prediction. However, these models work poorly on cold-start ads with new IDs, whose embeddings are not well learned yet. In this paper, we propose Graph Meta Embeddi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08909v1-abstract-full').style.display = 'inline'; document.getElementById('2105.08909v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.08909v1-abstract-full" style="display: none;">
        Click-through rate (CTR) prediction is one of the most central tasks in online advertising systems. Recent deep learning-based models that exploit feature embedding and high-order data nonlinearity have shown dramatic successes in CTR prediction. However, these models work poorly on cold-start ads with new IDs, whose embeddings are not well learned yet. In this paper, we propose Graph Meta Embedding (GME) models that can rapidly learn how to generate desirable initial embeddings for new ad IDs based on graph neural networks and meta learning. Previous works address this problem from the new ad itself, but ignore possibly useful information contained in existing old ads. In contrast, GMEs simultaneously consider two information sources: the new ad and existing old ads. For the new ad, GMEs exploit its associated attributes. For existing old ads, GMEs first build a graph to connect them with new ads, and then adaptively distill useful information. We propose three specific GMEs from different perspectives to explore what kind of information to use and how to distill information. In particular, GME-P uses Pre-trained neighbor ID embeddings, GME-G uses Generated neighbor ID embeddings and GME-A uses neighbor Attributes. Experimental results on three real-world datasets show that GMEs can significantly improve the prediction performance in both cold-start (i.e., no training data is available) and warm-up (i.e., a small number of training samples are collected) scenarios over five major deep learning-based CTR prediction models. GMEs can be applied to conversion rate (CVR) prediction as well.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08909v1-abstract-full').style.display = 'none'; document.getElementById('2105.08909v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">SIGIR 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.03072">arXiv:2105.03072</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.03072">pdf</a>, <a href="https://arxiv.org/format/2105.03072">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NTIRE 2021 Challenge on Perceptual Image Quality Assessment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+J">Jinjin Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+H">Haoming Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+C">Chao Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+J+S">Jimmy S. Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+Y">Yu Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+S">Shuhang Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cheon%2C+M">Manri Cheon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yoon%2C+S">Sungjun Yoon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+B">Byungyeon Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Junwoo Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Q">Qing Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+H">Haiyang Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bin%2C+Y">Yi Bin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hou%2C+Y">Yuqing Hou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+H">Hengliang Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guo%2C+J">Jingyu Guo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zirui Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hai Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+W">Wenming Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bai%2C+Q">Qingyan Bai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+S">Shuwei Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+W">Weihao Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+M">Mingdeng Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jiahao Wang</a>
      , et al. (25 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.03072v3-abstract-short" style="display: inline;">
        This paper reports on the NTIRE 2021 challenge on perceptual image quality assessment (IQA), held in conjunction with the New Trends in Image Restoration and Enhancement workshop (NTIRE) workshop at CVPR 2021. As a new type of image processing technology, perceptual image processing algorithms based on Generative Adversarial Networks (GAN) have produced images with more realistic textures. These o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03072v3-abstract-full').style.display = 'inline'; document.getElementById('2105.03072v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.03072v3-abstract-full" style="display: none;">
        This paper reports on the NTIRE 2021 challenge on perceptual image quality assessment (IQA), held in conjunction with the New Trends in Image Restoration and Enhancement workshop (NTIRE) workshop at CVPR 2021. As a new type of image processing technology, perceptual image processing algorithms based on Generative Adversarial Networks (GAN) have produced images with more realistic textures. These output images have completely different characteristics from traditional distortions, thus pose a new challenge for IQA methods to evaluate their visual quality. In comparison with previous IQA challenges, the training and testing datasets in this challenge include the outputs of perceptual image processing algorithms and the corresponding subjective scores. Thus they can be used to develop and evaluate IQA methods on GAN-based distortions. The challenge has 270 registered participants in total. In the final testing stage, 13 participating teams submitted their models and fact sheets. Almost all of them have achieved much better results than existing IQA methods, while the winning method can demonstrate state-of-the-art performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.03072v3-abstract-full').style.display = 'none'; document.getElementById('2105.03072v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.00470">arXiv:2105.00470</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.00470">pdf</a>, <a href="https://arxiv.org/format/2105.00470">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Feature Decorrelation in Self-Supervised Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hua%2C+T">Tianyu Hua</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wenxiao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+Z">Zihui Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yue Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Hang Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.00470v2-abstract-short" style="display: inline;">
        In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00470v2-abstract-full').style.display = 'inline'; document.getElementById('2105.00470v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.00470v2-abstract-full" style="display: none;">
        In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00470v2-abstract-full').style.display = 'none'; document.getElementById('2105.00470v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICCV 2021 Oral. The first two authors contribute equally</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.14978">arXiv:2104.14978</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.14978">pdf</a>, <a href="https://arxiv.org/format/2104.14978">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TASE49443.2020.00010">10.1109/TASE49443.2020.00010 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A comparative study of neural network techniques for automatic software vulnerability detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+G">Gaigai Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meng%2C+L">Lianxiao Meng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shuangyin Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+W">Weipeng Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Q">Qiang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+L">Lin Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.14978v1-abstract-short" style="display: inline;">
        Software vulnerabilities are usually caused by design flaws or implementation errors, which could be exploited to cause damage to the security of the system. At present, the most commonly used method for detecting software vulnerabilities is static analysis. Most of the related technologies work based on rules or code similarity (source code level) and rely on manually defined vulnerability featur&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.14978v1-abstract-full').style.display = 'inline'; document.getElementById('2104.14978v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.14978v1-abstract-full" style="display: none;">
        Software vulnerabilities are usually caused by design flaws or implementation errors, which could be exploited to cause damage to the security of the system. At present, the most commonly used method for detecting software vulnerabilities is static analysis. Most of the related technologies work based on rules or code similarity (source code level) and rely on manually defined vulnerability features. However, these rules and vulnerability features are difficult to be defined and designed accurately, which makes static analysis face many challenges in practical applications. To alleviate this problem, some researchers have proposed to use neural networks that have the ability of automatic feature extraction to improve the intelligence of detection. However, there are many types of neural networks, and different data preprocessing methods will have a significant impact on model performance. It is a great challenge for engineers and researchers to choose a proper neural network and data preprocessing method for a given problem. To solve this problem, we have conducted extensive experiments to test the performance of the two most typical neural networks (i.e., Bi-LSTM and RVFL) with the two most classical data preprocessing methods (i.e., the vector representation and the program symbolization methods) on software vulnerability detection problems and obtained a series of interesting research conclusions, which can provide valuable guidelines for researchers and engineers. Specifically, we found that 1) the training speed of RVFL is always faster than BiLSTM, but the prediction accuracy of Bi-LSTM model is higher than RVFL; 2) using doc2vec for vector representation can make the model have faster training speed and generalization ability than using word2vec; and 3) multi-level symbolization is helpful to improve the precision of neural network models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.14978v1-abstract-full').style.display = 'none'; document.getElementById('2104.14978v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been published at April 28,2021. However, there are some experimental data issues in the published manuscript, which are caused by the calculation error of indicators. This paper is a revised version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.14438">arXiv:2103.14438</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.14438">pdf</a>, <a href="https://arxiv.org/format/2103.14438">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gated Transformer Networks for Multivariate Time Series Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+M">Minghao Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shengqi Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+S">Siyuan Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiao%2C+J">Jiahui Jiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yizhou Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhiguang Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+W">Wei Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.14438v1-abstract-short" style="display: inline;">
        Deep learning model (primarily convolutional networks and LSTM) for time series classification has been studied broadly by the community with the wide applications in different domains like healthcare, finance, industrial engineering and IoT. Meanwhile, Transformer Networks recently achieved frontier performance on various natural language processing and computer vision tasks. In this work, we exp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14438v1-abstract-full').style.display = 'inline'; document.getElementById('2103.14438v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.14438v1-abstract-full" style="display: none;">
        Deep learning model (primarily convolutional networks and LSTM) for time series classification has been studied broadly by the community with the wide applications in different domains like healthcare, finance, industrial engineering and IoT. Meanwhile, Transformer Networks recently achieved frontier performance on various natural language processing and computer vision tasks. In this work, we explored a simple extension of the current Transformer Networks with gating, named Gated Transformer Networks (GTN) for the multivariate time series classification problem. With the gating that merges two towers of Transformer which model the channel-wise and step-wise correlations respectively, we show how GTN is naturally and effectively suitable for the multivariate time series classification task. We conduct comprehensive experiments on thirteen dataset with full ablation study. Our results show that GTN is able to achieve competing results with current state-of-the-art deep learning models. We also explored the attention map for the natural interpretability of GTN on time series modeling. Our preliminary results provide a strong baseline for the Transformer Networks on multivariate time series classification task and grounds the foundation for future research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14438v1-abstract-full').style.display = 'none'; document.getElementById('2103.14438v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.14431">arXiv:2103.14431</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.14431">pdf</a>, <a href="https://arxiv.org/format/2103.14431">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Knowledge Expansion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+Z">Zihui Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Sucheng Ren</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Z">Zhengqi Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Hang Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.14431v3-abstract-short" style="display: inline;">
        The popularity of multimodal sensors and the accessibility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modality gap between a unimodal network and unlabeled multimodal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task on unlabeled m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14431v3-abstract-full').style.display = 'inline'; document.getElementById('2103.14431v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.14431v3-abstract-full" style="display: none;">
        The popularity of multimodal sensors and the accessibility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modality gap between a unimodal network and unlabeled multimodal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task on unlabeled multimodal data? In this work, we propose multimodal knowledge expansion (MKE), a knowledge distillation-based framework to effectively utilize multimodal data without requiring labels. Opposite to traditional knowledge distillation, where the student is designed to be lightweight and inferior to the teacher, we observe that a multimodal student model consistently denoises pseudo labels and generalizes better than its teacher. Extensive experiments on four tasks and different modalities verify this finding. Furthermore, we connect the mechanism of MKE to semi-supervised learning and offer both empirical and theoretical explanations to understand the denoising capability of a multimodal student.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14431v3-abstract-full').style.display = 'none'; document.getElementById('2103.14431v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 October, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICCV 2021. Project website: https://tsinghua-mars-lab.github.io/MKE/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.05018">arXiv:2102.05018</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.05018">pdf</a>, <a href="https://arxiv.org/format/2102.05018">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Bandit Learning with Imperfect Context
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jianyi Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ren%2C+S">Shaolei Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.05018v3-abstract-short" style="display: inline;">
        A standard assumption in contextual multi-arm bandit is that the true context is perfectly known before arm selection. Nonetheless, in many practical applications (e.g., cloud resource management), prior to arm selection, the context information can only be acquired by prediction subject to errors or adversarial modification. In this paper, we study a contextual bandit setting in which only imperf&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.05018v3-abstract-full').style.display = 'inline'; document.getElementById('2102.05018v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.05018v3-abstract-full" style="display: none;">
        A standard assumption in contextual multi-arm bandit is that the true context is perfectly known before arm selection. Nonetheless, in many practical applications (e.g., cloud resource management), prior to arm selection, the context information can only be acquired by prediction subject to errors or adversarial modification. In this paper, we study a contextual bandit setting in which only imperfect context is available for arm selection while the true context is revealed at the end of each round. We propose two robust arm selection algorithms: MaxMinUCB (Maximize Minimum UCB) which maximizes the worst-case reward, and MinWD (Minimize Worst-case Degradation) which minimizes the worst-case regret. Importantly, we analyze the robustness of MaxMinUCB and MinWD by deriving both regret and reward bounds compared to an oracle that knows the true context. Our results show that as time goes on, MaxMinUCB and MinWD both perform as asymptotically well as their optimal counterparts that know the reward function. Finally, we apply MaxMinUCB and MinWD to online edge datacenter selection, and run synthetic simulations to validate our theoretical analysis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.05018v3-abstract-full').style.display = 'none'; document.getElementById('2102.05018v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Ren%2C+S&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Ren%2C+S&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Ren%2C+S&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Ren%2C+S&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>