<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 63 results for author: <span class="mathjax">Anderson, A</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Anderson%2C+A">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Anderson, A">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Anderson%2C+A&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Anderson, A">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Anderson%2C+A&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Anderson%2C+A&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Anderson%2C+A&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.02131">arXiv:2205.02131</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.02131">pdf</a>, <a href="https://arxiv.org/format/2205.02131">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Domino Saliency Metrics: Improving Existing Channel Saliency Metrics with Structural Information
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Persand%2C+K">Kaveena Persand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.02131v1-abstract-short" style="display: inline;">
        Channel pruning is used to reduce the number of weights in a Convolutional Neural Network (CNN). Channel pruning removes slices of the weight tensor so that the convolution layer remains dense. The removal of these weight slices from a single layer causes mismatching number of feature maps between layers of the network. A simple solution is to force the number of feature map between layers to matc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.02131v1-abstract-full').style.display = 'inline'; document.getElementById('2205.02131v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.02131v1-abstract-full" style="display: none;">
        Channel pruning is used to reduce the number of weights in a Convolutional Neural Network (CNN). Channel pruning removes slices of the weight tensor so that the convolution layer remains dense. The removal of these weight slices from a single layer causes mismatching number of feature maps between layers of the network. A simple solution is to force the number of feature map between layers to match through the removal of weight slices from subsequent layers. This additional constraint becomes more apparent in DNNs with branches where multiple channels need to be pruned together to keep the network dense. Popular pruning saliency metrics do not factor in the structural dependencies that arise in DNNs with branches. We propose Domino metrics (built on existing channel saliency metrics) to reflect these structural constraints. We test Domino saliency metrics against the baseline channel saliency metrics on multiple networks with branches. Domino saliency metrics improved pruning rates in most tested networks and up to 25% in AlexNet on CIFAR-10.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.02131v1-abstract-full').style.display = 'none'; document.getElementById('2205.02131v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.10369">arXiv:2201.10369</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.10369">pdf</a>, <a href="https://arxiv.org/ps/2201.10369">ps</a>, <a href="https://arxiv.org/format/2201.10369">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Winograd Convolution for Deep Neural Networks: Efficient Point Selection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Alam%2C+S+A">Syed Asad Alam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barabasz%2C+B">Barbara Barabasz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.10369v1-abstract-short" style="display: inline;">
        Convolutional neural networks (CNNs) have dramatically improved the accuracy of tasks such as object recognition, image segmentation and interactive speech systems. CNNs require large amounts of computing resources because ofcomputationally intensive convolution layers. Fast convolution algorithms such as Winograd convolution can greatly reduce the computational cost of these layers at a cost of p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.10369v1-abstract-full').style.display = 'inline'; document.getElementById('2201.10369v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.10369v1-abstract-full" style="display: none;">
        Convolutional neural networks (CNNs) have dramatically improved the accuracy of tasks such as object recognition, image segmentation and interactive speech systems. CNNs require large amounts of computing resources because ofcomputationally intensive convolution layers. Fast convolution algorithms such as Winograd convolution can greatly reduce the computational cost of these layers at a cost of poor numeric properties, such that greater savings in computation exponentially increase floating point errors.
  A defining feature of each Winograd convolution algorithm is a set of real-value points where polynomials are sampled. The choice of points impacts the numeric accuracy of the algorithm, but the optimal set of points for small convolutions remains unknown. Existing work considers only small integers and simple fractions as candidate points. In this work, we propose a novel approach to point selection using points of the form {-1/c , -c, c, 1/c } using the full range of real-valued numbers for c. We show that groups of this form cause cancellations in the Winograd transform matrices that reduce numeric error. We find empirically that the error for different values of c forms a rough curve across the range of real-value numbers helping to localize the values of c that reduce error and that lower errors can be achieved with non-obvious real-valued evaluation points instead of integers or simple fractions. We study a range of sizes for small convolutions and achieve reduction in error ranging from 2% to around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in cases when we select a subset of our proposed points which will always lead to a lower error. Finally we implement a complete Winograd convolution layer and use it to run deep convolution neural networks on real datasets and show that our proposed points reduce error, ranging from 22% to 63%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.10369v1-abstract-full').style.display = 'none'; document.getElementById('2201.10369v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages, 3 figures, 9 tables and 32 equations</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          C.3.2; G.0
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.10436">arXiv:2108.10436</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.10436">pdf</a>, <a href="https://arxiv.org/format/2108.10436">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rewrite Rule Inference Using Equality Saturation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nandi%2C+C">Chandrakana Nandi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Willsey%2C+M">Max Willsey</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+A">Amy Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y+R">Yisu Remy Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saiki%2C+B">Brett Saiki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Adam Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schulz%2C+A">Adriana Schulz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grossman%2C+D">Dan Grossman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tatlock%2C+Z">Zachary Tatlock</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.10436v1-abstract-short" style="display: inline;">
        Many compilers, synthesizers, and theorem provers rely on rewrite rules to simplify expressions or prove equivalences. Developing rewrite rules can be difficult: rules may be subtly incorrect, profitable rules are easy to miss, and rulesets must be rechecked or extended whenever semantics are tweaked. Large rulesets can also be challenging to apply: redundant rules slow down rule-based search and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.10436v1-abstract-full').style.display = 'inline'; document.getElementById('2108.10436v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.10436v1-abstract-full" style="display: none;">
        Many compilers, synthesizers, and theorem provers rely on rewrite rules to simplify expressions or prove equivalences. Developing rewrite rules can be difficult: rules may be subtly incorrect, profitable rules are easy to miss, and rulesets must be rechecked or extended whenever semantics are tweaked. Large rulesets can also be challenging to apply: redundant rules slow down rule-based search and frustrate debugging. This paper explores how equality saturation, a promising technique that uses e-graphs to apply rewrite rules, can also be used to infer rewrite rules. E-graphs can compactly represent the exponentially large sets of enumerated terms and potential rewrite rules. We show that equality saturation efficiently shrinks both sets, leading to faster synthesis of smaller, more general rulesets.
  We prototyped these strategies in a tool dubbed ruler. Compared to a similar tool built on CVC4, ruler synthesizes 5.8X smaller rulesets 25X faster without compromising on proving power. In an end-to-end case study, we show ruler-synthesized rules which perform as well as those crafted by domain experts, and addressed a longstanding issue in a popular open source tool.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.10436v1-abstract-full').style.display = 'none'; document.getElementById('2108.10436v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.00588">arXiv:2108.00588</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.00588">pdf</a>, <a href="https://arxiv.org/format/2108.00588">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding Diversity in Human-AI Data: What Cognitive Style Disaggregation Reveals
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dikkala%2C+R">Rupika Dikkala</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+T">Tianyi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vorvoreanu%2C+M">Mihaela Vorvoreanu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burnett%2C+M">Margaret Burnett</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.00588v2-abstract-short" style="display: inline;">
        Artificial Intelligence (AI) is becoming more pervasive through all levels of society, trying to help us be more productive. Research like Amershi et al.&#39;s 18 guidelines for human-AI interaction aim to provide high-level design advice, yet little remains known about how people react to Applications or Violations of the guidelines. This leaves a gap for designers of human-AI systems applying such g&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.00588v2-abstract-full').style.display = 'inline'; document.getElementById('2108.00588v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.00588v2-abstract-full" style="display: none;">
        Artificial Intelligence (AI) is becoming more pervasive through all levels of society, trying to help us be more productive. Research like Amershi et al.&#39;s 18 guidelines for human-AI interaction aim to provide high-level design advice, yet little remains known about how people react to Applications or Violations of the guidelines. This leaves a gap for designers of human-AI systems applying such guidelines, where AI-powered systems might be working better for certain sets of users than for others, inadvertently introducing inclusiveness issues. To address this, we performed a secondary analysis of 1,016 participants across 16 experiments, disaggregating their data by their 5 cognitive problem-solving styles from the Gender-inclusiveness Magnifier (GenderMag) method and illustrate different situations that participants found themselves in. We found that across all 5 cogniive style spectra, although there were instances where applying the guidelines closed inclusiveness issues, there were also stubborn inclusiveness issues and inadvertent introductions of inclusiveness issues. Lastly, we found that participants&#39; cognitive styles not only clustered by their gender, but they also clustered across different age groups.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.00588v2-abstract-full').style.display = 'none'; document.getElementById('2108.00588v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.00372">arXiv:2107.00372</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.00372">pdf</a>, <a href="https://arxiv.org/format/2107.00372">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Egocentric Image Captioning for Privacy-Preserved Passive Dietary Intake Monitoring
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Qiu%2C+J">Jianing Qiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lo%2C+F+P+-">Frank P. -W. Lo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+X">Xiao Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jobarteh%2C+M+L">Modou L. Jobarteh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jia%2C+W">Wenyan Jia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Baranowski%2C+T">Tom Baranowski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Steiner-Asiedu%2C+M">Matilda Steiner-Asiedu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+K">Alex K. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=McCrory%2C+M+A">Megan A McCrory</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sazonov%2C+E">Edward Sazonov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+M">Mingui Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Frost%2C+G">Gary Frost</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lo%2C+B">Benny Lo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.00372v1-abstract-short" style="display: inline;">
        Camera-based passive dietary intake monitoring is able to continuously capture the eating episodes of a subject, recording rich visual information, such as the type and volume of food being consumed, as well as the eating behaviours of the subject. However, there currently is no method that is able to incorporate these visual clues and provide a comprehensive context of dietary intake from passive&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00372v1-abstract-full').style.display = 'inline'; document.getElementById('2107.00372v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.00372v1-abstract-full" style="display: none;">
        Camera-based passive dietary intake monitoring is able to continuously capture the eating episodes of a subject, recording rich visual information, such as the type and volume of food being consumed, as well as the eating behaviours of the subject. However, there currently is no method that is able to incorporate these visual clues and provide a comprehensive context of dietary intake from passive recording (e.g., is the subject sharing food with others, what food the subject is eating, and how much food is left in the bowl). On the other hand, privacy is a major concern while egocentric wearable cameras are used for capturing. In this paper, we propose a privacy-preserved secure solution (i.e., egocentric image captioning) for dietary assessment with passive monitoring, which unifies food recognition, volume estimation, and scene understanding. By converting images into rich text descriptions, nutritionists can assess individual dietary intake based on the captions instead of the original images, reducing the risk of privacy leakage from images. To this end, an egocentric dietary image captioning dataset has been built, which consists of in-the-wild images captured by head-worn and chest-worn cameras in field studies in Ghana. A novel transformer-based architecture is designed to caption egocentric dietary images. Comprehensive experiments have been conducted to evaluate the effectiveness and to justify the design of the proposed architecture for egocentric dietary image captioning. To the best of our knowledge, this is the first work that applies image captioning to dietary intake assessment in real life settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.00372v1-abstract-full').style.display = 'none'; document.getElementById('2107.00372v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.08146">arXiv:2106.08146</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.08146">pdf</a>, <a href="https://arxiv.org/ps/2106.08146">ps</a>, <a href="https://arxiv.org/format/2106.08146">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Chemical Physics">physics.chem-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1039/D1CP04475C">10.1039/D1CP04475C <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Graphical Gaussian Process Regression Model for Aqueous Solvation Free Energy Prediction of Organic Molecules in Redox Flow Battery
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+P">Peiyuan Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+X">Xiu Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+Y">Yu-Hang Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+M">Muqing Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Amity Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Murugesan%2C+V">Vijayakumar Murugesan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hollas%2C+A">Aaron Hollas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.08146v1-abstract-short" style="display: inline;">
        The solvation free energy of organic molecules is a critical parameter in determining emergent properties such as solubility, liquid-phase equilibrium constants, and pKa and redox potentials in an organic redox flow battery. In this work, we present a machine learning (ML) model that can learn and predict the aqueous solvation free energy of an organic molecule using Gaussian process regression me&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08146v1-abstract-full').style.display = 'inline'; document.getElementById('2106.08146v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.08146v1-abstract-full" style="display: none;">
        The solvation free energy of organic molecules is a critical parameter in determining emergent properties such as solubility, liquid-phase equilibrium constants, and pKa and redox potentials in an organic redox flow battery. In this work, we present a machine learning (ML) model that can learn and predict the aqueous solvation free energy of an organic molecule using Gaussian process regression method based on a new molecular graph kernel. To investigate the performance of the ML model on electrostatic interaction, the nonpolar interaction contribution of solvent and the conformational entropy of solute in solvation free energy, three data sets with implicit or explicit water solvent models, and contribution of conformational entropy of solute are tested. We demonstrate that our ML model can predict the solvation free energy of molecules at chemical accuracy with a mean absolute error of less than 1 kcal/mol for subsets of the QM9 dataset and the Freesolv database. To solve the general data scarcity problem for a graph-based ML model, we propose a dimension reduction algorithm based on the distance between molecular graphs, which can be used to examine the diversity of the molecular data set. It provides a promising way to build a minimum training set to improve prediction for certain test sets where the space of molecular structures is predetermined.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08146v1-abstract-full').style.display = 'none'; document.getElementById('2106.08146v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 June, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.14335">arXiv:2104.14335</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.14335">pdf</a>, <a href="https://arxiv.org/format/2104.14335">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ELF-VC: Efficient Learned Flexible-Rate Video Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Rippel%2C+O">Oren Rippel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+G">Alexander G. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tatwawadi%2C+K">Kedar Tatwawadi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nair%2C+S">Sanjay Nair</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lytle%2C+C">Craig Lytle</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bourdev%2C+L">Lubomir Bourdev</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.14335v1-abstract-short" style="display: inline;">
        While learned video codecs have demonstrated great promise, they have yet to achieve sufficient efficiency for practical deployment. In this work, we propose several novel ideas for learned video compression which allow for improved performance for the low-latency mode (I- and P-frames only) along with a considerable increase in computational efficiency. In this setting, for natural videos our app&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.14335v1-abstract-full').style.display = 'inline'; document.getElementById('2104.14335v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.14335v1-abstract-full" style="display: none;">
        While learned video codecs have demonstrated great promise, they have yet to achieve sufficient efficiency for practical deployment. In this work, we propose several novel ideas for learned video compression which allow for improved performance for the low-latency mode (I- and P-frames only) along with a considerable increase in computational efficiency. In this setting, for natural videos our approach compares favorably across the entire R-D curve under metrics PSNR, MS-SSIM and VMAF against all mainstream video standards (H.264, H.265, AV1) and all ML codecs. At the same time, our approach runs at least 5x faster and has fewer parameters than all ML codecs which report these figures.
  Our contributions include a flexible-rate framework allowing a single model to cover a large and dense range of bitrates, at a negligible increase in computation and parameter count; an efficient backbone optimized for ML-based codecs; and a novel in-loop flow prediction scheme which leverages prior information towards more efficient compression.
  We benchmark our method, which we call ELF-VC (Efficient, Learned and Flexible Video Coding) on popular video test sets UVG and MCL-JCV under metrics PSNR, MS-SSIM and VMAF. For example, on UVG under PSNR, it reduces the BD-rate by 44% against H.264, 26% against H.265, 15% against AV1, and 35% against the current best ML codec. At the same time, on an NVIDIA Titan V GPU our approach encodes/decodes VGA at 49/91 FPS, HD 720 at 19/35 FPS, and HD 1080 at 10/18 FPS.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.14335v1-abstract-full').style.display = 'none'; document.getElementById('2104.14335v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        International Conference on Computer Vision, 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.00874">arXiv:2012.00874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.00874">pdf</a>, <a href="https://arxiv.org/format/2012.00874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        &#34;A cold, technical decision-maker&#34;: Can AI provide explainability, negotiability, and humanity?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Woodruff%2C+A">Allison Woodruff</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+Y+A">Yasmin Asare Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Armstrong%2C+K+J">Katherine Jameson Armstrong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gkiza%2C+M">Marina Gkiza</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jennings%2C+J">Jay Jennings</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moessner%2C+C">Christopher Moessner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Viegas%2C+F">Fernanda Viegas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wattenberg%2C+M">Martin Wattenberg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Webb%2C+a+L">and Lynette Webb</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wrede%2C+F">Fabian Wrede</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kelley%2C+P+G">Patrick Gage Kelley</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.00874v1-abstract-short" style="display: inline;">
        Algorithmic systems are increasingly deployed to make decisions in many areas of people&#39;s lives. The shift from human to algorithmic decision-making has been accompanied by concern about potentially opaque decisions that are not aligned with social values, as well as proposed remedies such as explainability. We present results of a qualitative study of algorithmic decision-making, comprised of fiv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.00874v1-abstract-full').style.display = 'inline'; document.getElementById('2012.00874v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.00874v1-abstract-full" style="display: none;">
        Algorithmic systems are increasingly deployed to make decisions in many areas of people&#39;s lives. The shift from human to algorithmic decision-making has been accompanied by concern about potentially opaque decisions that are not aligned with social values, as well as proposed remedies such as explainability. We present results of a qualitative study of algorithmic decision-making, comprised of five workshops conducted with a total of 60 participants in Finland, Germany, the United Kingdom, and the United States. We invited participants to reason about decision-making qualities such as explainability and accuracy in a variety of domains. Participants viewed AI as a decision-maker that follows rigid criteria and performs mechanical tasks well, but is largely incapable of subjective or morally complex judgments. We discuss participants&#39; consideration of humanity in decision-making, and introduce the concept of &#39;negotiability,&#39; the ability to go beyond formal criteria and work flexibly around the system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.00874v1-abstract-full').style.display = 'none'; document.getElementById('2012.00874v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, 1 appendix, 4 tables</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          K.4; K.3.2; I.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.00590">arXiv:2010.00590</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.00590">pdf</a>, <a href="https://arxiv.org/format/2010.00590">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1038/s41586-021-04167-x">10.1038/s41586-021-04167-x <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantifying social organization and political polarization in online platforms
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Waller%2C+I">Isaac Waller</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.00590v3-abstract-short" style="display: inline;">
        Optimism about the Internet&#39;s potential to bring the world together has been tempered by concerns about its role in inflaming the &#39;culture wars&#39;. Via mass selection into like-minded groups, online society may be becoming more fragmented and polarized, particularly with respect to partisan differences. However, our ability to measure the social makeup of online communities, and in turn understand t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.00590v3-abstract-full').style.display = 'inline'; document.getElementById('2010.00590v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.00590v3-abstract-full" style="display: none;">
        Optimism about the Internet&#39;s potential to bring the world together has been tempered by concerns about its role in inflaming the &#39;culture wars&#39;. Via mass selection into like-minded groups, online society may be becoming more fragmented and polarized, particularly with respect to partisan differences. However, our ability to measure the social makeup of online communities, and in turn understand the social organization of online platforms, is limited by the pseudonymous, unstructured, and large-scale nature of digital discussion. We develop a neural embedding methodology to quantify the positioning of online communities along social dimensions by leveraging large-scale patterns of aggregate behaviour. Applying our methodology to 5.1B Reddit comments made in 10K communities over 14 years, we measure how the macroscale community structure is organized with respect to age, gender, and U.S. political partisanship. Examining political content, we find Reddit underwent a significant polarization event around the 2016 U.S. presidential election, and remained highly polarized for years afterward. Contrary to conventional wisdom, however, individual-level polarization is rare; the system-level shift in 2016 was disproportionately driven by the arrival of new and newly political users. Political polarization on Reddit is unrelated to previous activity on the platform, and is instead temporally aligned with external events. We also observe a stark ideological asymmetry, with the sharp increase in 2016 being entirely attributable to changes in right-wing activity. Our methodology is broadly applicable to the study of online interaction, and our findings have implications for the design of online platforms, understanding the social contexts of online behaviour, and quantifying the dynamics and mechanisms of online polarization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.00590v3-abstract-full').style.display = 'none'; document.getElementById('2010.00590v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">43 pages, 14 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Nature 600 (2021)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.07661">arXiv:2009.07661</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.07661">pdf</a>, <a href="https://arxiv.org/format/2009.07661">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adoption of Twitter&#39;s New Length Limit: Is 280 the New 140?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gligori%C4%87%2C+K">Kristina Gligorić</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=West%2C+R">Robert West</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.07661v1-abstract-short" style="display: inline;">
        In November 2017, Twitter doubled the maximum allowed tweet length from 140 to 280 characters, a drastic switch on one of the world&#39;s most influential social media platforms. In the first long-term study of how the new length limit was adopted by Twitter users, we ask: Does the effect of the new length limit resemble that of the old one? Or did the doubling of the limit fundamentally change how Tw&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.07661v1-abstract-full').style.display = 'inline'; document.getElementById('2009.07661v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.07661v1-abstract-full" style="display: none;">
        In November 2017, Twitter doubled the maximum allowed tweet length from 140 to 280 characters, a drastic switch on one of the world&#39;s most influential social media platforms. In the first long-term study of how the new length limit was adopted by Twitter users, we ask: Does the effect of the new length limit resemble that of the old one? Or did the doubling of the limit fundamentally change how Twitter is shaped by the limited length of posted content? By analyzing Twitter&#39;s publicly available 1% sample over a period of around 3 years, we find that, when the length limit was raised from 140 to 280 characters, the prevalence of tweets around 140 characters dropped immediately, while the prevalence of tweets around 280 characters rose steadily for about 6 months. Despite this rise, tweets approaching the length limit have been far less frequent after than before the switch. We find widely different adoption rates across languages and client-device types. The prevalence of tweets around 140 characters before the switch in a given language is strongly correlated with the prevalence of tweets around 280 characters after the switch in the same language, and very long tweets are vastly more popular on Web clients than on mobile clients. Moreover, tweets of around 280 characters after the switch are syntactically and semantically similar to tweets of around 140 characters before the switch, manifesting patterns of message squeezing in both cases. Taken together, these findings suggest that the new 280-character limit constitutes a new, less intrusive version of the old 140-character limit. The length limit remains an important factor that should be considered in all studies using Twitter data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.07661v1-abstract-full').style.display = 'none'; document.getElementById('2009.07661v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.02878">arXiv:2009.02878</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.02878">pdf</a>, <a href="https://arxiv.org/format/2009.02878">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Benchmarking off-the-shelf statistical shape modeling tools in clinical applications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goparaju%2C+A">Anupama Goparaju</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bone%2C+A">Alexandre Bone</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+N">Nan Hu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Henninger%2C+H+B">Heath B. Henninger</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+E">Andrew E. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Durrleman%2C+S">Stanley Durrleman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jacxsens%2C+M">Matthijs Jacxsens</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Morris%2C+A">Alan Morris</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Csecs%2C+I">Ibolya Csecs</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Marrouche%2C+N">Nassir Marrouche</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Elhabian%2C+S+Y">Shireen Y. Elhabian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.02878v1-abstract-short" style="display: inline;">
        Statistical shape modeling (SSM) is widely used in biology and medicine as a new generation of morphometric approaches for the quantitative analysis of anatomical shapes. Technological advancements of in vivo imaging have led to the development of open-source computational tools that automate the modeling of anatomical shapes and their population-level variability. However, little work has been do&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.02878v1-abstract-full').style.display = 'inline'; document.getElementById('2009.02878v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.02878v1-abstract-full" style="display: none;">
        Statistical shape modeling (SSM) is widely used in biology and medicine as a new generation of morphometric approaches for the quantitative analysis of anatomical shapes. Technological advancements of in vivo imaging have led to the development of open-source computational tools that automate the modeling of anatomical shapes and their population-level variability. However, little work has been done on the evaluation and validation of such tools in clinical applications that rely on morphometric quantifications (e.g., implant design and lesion screening). Here, we systematically assess the outcome of widely used, state-of-the-art SSM tools, namely ShapeWorks, Deformetrica, and SPHARM-PDM. We use both quantitative and qualitative metrics to evaluate shape models from different tools. We propose validation frameworks for anatomical landmark/measurement inference and lesion screening. We also present a lesion screening method to objectively characterize subtle abnormal shape changes with respect to learned population-level statistics of controls. Results demonstrate that SSM tools display different levels of consistencies, where ShapeWorks and Deformetrica models are more consistent compared to models from SPHARM-PDM due to the groupwise approach of estimating surface correspondences. Furthermore, ShapeWorks and Deformetrica shape models are found to capture clinically relevant population-level variability compared to SPHARM-PDM models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.02878v1-abstract-full').style.display = 'none'; document.getElementById('2009.02878v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">22 pages, 22 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.10086">arXiv:2008.10086</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.10086">pdf</a>, <a href="https://arxiv.org/format/2008.10086">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Personalized Models of Human Behavior in Chess
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=McIlroy-Young%2C+R">Reid McIlroy-Young</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+R">Russell Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sen%2C+S">Siddhartha Sen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kleinberg%2C+J">Jon Kleinberg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.10086v2-abstract-short" style="display: inline;">
        Even when machine learning systems surpass human ability in a domain, there are many reasons why AI systems that capture human-like behavior would be desirable: humans may want to learn from them, they may need to collaborate with them, or they may expect them to serve as partners in an extended interaction. Motivated by this goal of human-like AI systems, the problem of predicting human actions -&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.10086v2-abstract-full').style.display = 'inline'; document.getElementById('2008.10086v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.10086v2-abstract-full" style="display: none;">
        Even when machine learning systems surpass human ability in a domain, there are many reasons why AI systems that capture human-like behavior would be desirable: humans may want to learn from them, they may need to collaborate with them, or they may expect them to serve as partners in an extended interaction. Motivated by this goal of human-like AI systems, the problem of predicting human actions -- as opposed to predicting optimal actions -- has become an increasingly useful task. We extend this line of work by developing highly accurate personalized models of human behavior in the context of chess. Chess is a rich domain for exploring these questions, since it combines a set of appealing features: AI systems have achieved superhuman performance but still interact closely with human chess players both as opponents and preparation tools, and there is an enormous amount of recorded data on individual players. Starting with an open-source version of AlphaZero trained on a population of human players, we demonstrate that we can significantly improve prediction of a particular player&#39;s moves by applying a series of fine-tuning adjustments. Furthermore, we can accurately perform stylometry -- predicting who made a given set of actions -- indicating that our personalized models capture human decision-making at an individual level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.10086v2-abstract-full').style.display = 'none'; document.getElementById('2008.10086v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The current version of the paper corrects data processing problems present in the previous version. 21 pages, 13 figures, 7 tables (one very long)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.00177">arXiv:2007.00177</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.00177">pdf</a>, <a href="https://arxiv.org/format/2007.00177">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        De-anonymization of authors through arXiv submissions during double-blind review
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bharadhwaj%2C+H">Homanga Bharadhwaj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Turpin%2C+D">Dylan Turpin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Garg%2C+A">Animesh Garg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.00177v1-abstract-short" style="display: inline;">
        In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? Under two conditions: papers that are released on arXiv befo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.00177v1-abstract-full').style.display = 'inline'; document.getElementById('2007.00177v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.00177v1-abstract-full" style="display: none;">
        In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? Under two conditions: papers that are released on arXiv before the review phase and papers that are not, we examine the correlation between the reputation of their authors with the review scores and acceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv. In order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations. We emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims. A blog post accompanying the paper and our scraping code will be linked in the project website https://sites.google.com/view/deanon-arxiv/home
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.00177v1-abstract-full').style.display = 'none'; document.getElementById('2007.00177v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.01855">arXiv:2006.01855</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.01855">pdf</a>, <a href="https://arxiv.org/format/2006.01855">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3394486.3403219">10.1145/3394486.3403219 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Aligning Superhuman AI with Human Behavior: Chess as a Model System
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=McIlroy-Young%2C+R">Reid McIlroy-Young</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sen%2C+S">Siddhartha Sen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kleinberg%2C+J">Jon Kleinberg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.01855v3-abstract-short" style="display: inline;">
        As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between hu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.01855v3-abstract-full').style.display = 'inline'; document.getElementById('2006.01855v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.01855v3-abstract-full" style="display: none;">
        As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance.
  We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well.
  We develop and introduce Maia, a customized version of Alpha-Zero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.01855v3-abstract-full').style.display = 'none'; document.getElementById('2006.01855v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 June, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 11 figure, Proceedings of the 25th ACM SIGKDD international conference on Knowledge discovery and data mining, Virtual 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.10709">arXiv:2005.10709</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.10709">pdf</a>, <a href="https://arxiv.org/format/2005.10709">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TASO: Time and Space Optimization for Memory-Constrained DNN Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+Y">Yuan Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Radu%2C+V">Valentin Radu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Boyle%2C+M+F+P">Michael F. P. O&#39;Boyle</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.10709v1-abstract-short" style="display: inline;">
        Convolutional neural networks (CNNs) are used in many embedded applications, from industrial robotics and automation systems to biometric identification on mobile devices. State-of-the-art classification is typically achieved by large networks, which are prohibitively expensive to run on mobile and embedded devices with tightly constrained memory and energy budgets. We propose an approach for ahea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.10709v1-abstract-full').style.display = 'inline'; document.getElementById('2005.10709v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.10709v1-abstract-full" style="display: none;">
        Convolutional neural networks (CNNs) are used in many embedded applications, from industrial robotics and automation systems to biometric identification on mobile devices. State-of-the-art classification is typically achieved by large networks, which are prohibitively expensive to run on mobile and embedded devices with tightly constrained memory and energy budgets. We propose an approach for ahead-of-time domain specific optimization of CNN models, based on an integer linear programming (ILP) for selecting primitive operations to implement convolutional layers. We optimize the trade-off between execution time and memory consumption by: 1) attempting to minimize execution time across the whole network by selecting data layouts and primitive operations to implement each layer; and 2) allocating an appropriate workspace that reflects the upper bound of memory footprint per layer. These two optimization strategies can be used to run any CNN on any platform with a C compiler. Our evaluation with a range of popular ImageNet neural architectures (GoogleNet, AlexNet, VGG, ResNet and SqueezeNet) on the ARM Cortex-A15 yields speedups of 8x compared to a greedy algorithm based primitive selection, reduces memory requirement by 2.2x while sacrificing only 15% of inference time compared to a solver that considers inference time only. In addition, our optimization approach exposes a range of optimal points for different configurations across the Pareto frontier of memory and latency trade-off, which can be used under arbitrary system constraints.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.10709v1-abstract-full').style.display = 'none'; document.getElementById('2005.10709v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.03376">arXiv:2004.03376</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.03376">pdf</a>, <a href="https://arxiv.org/format/2004.03376">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/SSCI47803.2020.9308157">10.1109/SSCI47803.2020.9308157 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Composition of Saliency Metrics for Channel Pruning with a Myopic Oracle
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Persand%2C+K">Kaveena Persand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.03376v2-abstract-short" style="display: inline;">
        The computation and memory needed for Convolutional Neural Network (CNN) inference can be reduced by pruning weights from the trained network. Pruning is guided by a pruning saliency, which heuristically approximates the change in the loss function associated with the removal of specific weights. Many pruning signals have been proposed, but the performance of each heuristic depends on the particul&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.03376v2-abstract-full').style.display = 'inline'; document.getElementById('2004.03376v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.03376v2-abstract-full" style="display: none;">
        The computation and memory needed for Convolutional Neural Network (CNN) inference can be reduced by pruning weights from the trained network. Pruning is guided by a pruning saliency, which heuristically approximates the change in the loss function associated with the removal of specific weights. Many pruning signals have been proposed, but the performance of each heuristic depends on the particular trained network. This leaves the data scientist with a difficult choice. When using any one saliency metric for the entire pruning process, we run the risk of the metric assumptions being invalidated, leading to poor decisions being made by the metric. Ideally we could combine the best aspects of different saliency metrics. However, despite an extensive literature review, we are unable to find any prior work on composing different saliency metrics. The chief difficulty lies in combining the numerical output of different saliency metrics, which are not directly comparable.
  We propose a method to compose several primitive pruning saliencies, to exploit the cases where each saliency measure does well. Our experiments show that the composition of saliencies avoids many poor pruning choices identified by individual saliencies. In most cases our method finds better selections than even the best individual pruning saliency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.03376v2-abstract-full').style.display = 'none'; document.getElementById('2004.03376v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2020 IEEE Symposium Series on Computational Intelligence (SSCI)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.02976">arXiv:2001.02976</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.02976">pdf</a>, <a href="https://arxiv.org/format/2001.02976">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Performance-Oriented Neural Architecture Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+J">Jing Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dahyot%2C+R">Rozenn Dahyot</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.02976v1-abstract-short" style="display: inline;">
        Hardware-Software Co-Design is a highly successful strategy for improving performance of domain-specific computing systems. We argue for the application of the same methodology to deep learning; specifically, we propose to extend neural architecture search with information about the hardware to ensure that the model designs produced are highly efficient in addition to the typical criteria around a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.02976v1-abstract-full').style.display = 'inline'; document.getElementById('2001.02976v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.02976v1-abstract-full" style="display: none;">
        Hardware-Software Co-Design is a highly successful strategy for improving performance of domain-specific computing systems. We argue for the application of the same methodology to deep learning; specifically, we propose to extend neural architecture search with information about the hardware to ensure that the model designs produced are highly efficient in addition to the typical criteria around accuracy. Using the task of keyword spotting in audio on edge computing devices, we demonstrate that our approach results in neural architecture that is not only highly accurate, but also efficiently mapped to the computing platform which will perform the inference. Using our modified neural architecture search, we demonstrate $0.88\%$ increase in TOP-1 accuracy with $1.85\times$ reduction in latency for keyword spotting in audio on an embedded SoC, and $1.59\times$ on a high-end GPU.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.02976v1-abstract-full').style.display = 'none'; document.getElementById('2001.02976v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 January, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The 2019 International Conference on High Performance Computing &amp; Simulation</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.11278">arXiv:1912.11278</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.11278">pdf</a>, <a href="https://arxiv.org/format/1912.11278">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Welcome New Gabbers to the Pittsburgh Synagogue Shooting: The Evolution of Gab
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=McIlroy-Young%2C+R">Reid McIlroy-Young</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.11278v1-abstract-short" style="display: inline;">
        Gab, an online social media platform with very little content moderation, has recently come to prominence as an alt-right community and a haven for hate speech. We document the evolution of Gab since its inception until a Gab user carried out the most deadly attack on the Jewish community in US history. We investigate Gab language use, study how topics evolved over time, and find that the shooters&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.11278v1-abstract-full').style.display = 'inline'; document.getElementById('1912.11278v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.11278v1-abstract-full" style="display: none;">
        Gab, an online social media platform with very little content moderation, has recently come to prominence as an alt-right community and a haven for hate speech. We document the evolution of Gab since its inception until a Gab user carried out the most deadly attack on the Jewish community in US history. We investigate Gab language use, study how topics evolved over time, and find that the shooters&#39; posts were among the most consistently anti-Semitic on Gab, but that hundreds of other users were even more extreme.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.11278v1-abstract-full').style.display = 'none'; document.getElementById('1912.11278v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 December, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the International AAAI Conference on Web and Social Media, 13(01), 651-654 2019
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.04116">arXiv:1912.04116</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.04116">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MRI correlates of chronic symptoms in mild traumatic brain injury
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kerley%2C+C+I">Cailey I. Kerley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schilling%2C+K+G">Kurt G. Schilling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Blaber%2C+J">Justin Blaber</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Miller%2C+B">Beth Miller</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Newton%2C+A">Allen Newton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+W">Adam W. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Landman%2C+B+A">Bennett A. Landman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rex%2C+T+S">Tonia S. Rex</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.04116v2-abstract-short" style="display: inline;">
        Veterans with mild traumatic brain injury (mTBI) have reported auditory and visual dysfunction that persists beyond the acute incident. The etiology behind these symptoms is difficult to characterize with current clinical imaging. These functional deficits may be caused by shear injury or micro-bleeds, which can be detected with special imaging modalities. We explore these hypotheses in a pilot st&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.04116v2-abstract-full').style.display = 'inline'; document.getElementById('1912.04116v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.04116v2-abstract-full" style="display: none;">
        Veterans with mild traumatic brain injury (mTBI) have reported auditory and visual dysfunction that persists beyond the acute incident. The etiology behind these symptoms is difficult to characterize with current clinical imaging. These functional deficits may be caused by shear injury or micro-bleeds, which can be detected with special imaging modalities. We explore these hypotheses in a pilot study of multi-parametric MRI. We extract over 1,000 imaging and clinical metrics and project them to a low-dimensional space, where we can discriminate between healthy controls and patients with mTBI. We also show correlations between the metric representations and patient symptoms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.04116v2-abstract-full').style.display = 'none'; document.getElementById('1912.04116v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 June, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 December, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">SPIE Medical Imaging 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.07927">arXiv:1911.07927</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.07927">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning Captures More Accurate Diffusion Fiber Orientations Distributions than Constrained Spherical Deconvolution
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nath%2C+V">Vishwesh Nath</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schilling%2C+K+G">Kurt G. Schilling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hansen%2C+C+B">Colin B. Hansen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Parvathaneni%2C+P">Prasanna Parvathaneni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hainline%2C+A+E">Allison E. Hainline</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bermudez%2C+C">Camilo Bermudez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Plassard%2C+A+J">Andrew J. Plassard</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Janve%2C+V">Vaibhav Janve</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Y">Yurui Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Blaber%2C+J+A">Justin A. Blaber</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=St%C4%99pniewska%2C+I">Iwona Stępniewska</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+W">Adam W. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Landman%2C+B+A">Bennett A. Landman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.07927v1-abstract-short" style="display: inline;">
        Confocal histology provides an opportunity to establish intra-voxel fiber orientation distributions that can be used to quantitatively assess the biological relevance of diffusion weighted MRI models, e.g., constrained spherical deconvolution (CSD). Here, we apply deep learning to investigate the potential of single shell diffusion weighted MRI to explain histologically observed fiber orientation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.07927v1-abstract-full').style.display = 'inline'; document.getElementById('1911.07927v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.07927v1-abstract-full" style="display: none;">
        Confocal histology provides an opportunity to establish intra-voxel fiber orientation distributions that can be used to quantitatively assess the biological relevance of diffusion weighted MRI models, e.g., constrained spherical deconvolution (CSD). Here, we apply deep learning to investigate the potential of single shell diffusion weighted MRI to explain histologically observed fiber orientation distributions (FOD) and compare the derived deep learning model with a leading CSD approach. This study (1) demonstrates that there exists additional information in the diffusion signal that is not currently exploited by CSD, and (2) provides an illustrative data-driven model that makes use of this information.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.07927v1-abstract-full').style.display = 'none'; document.getElementById('1911.07927v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">2 pages, 4 figures. This work was accepted and published as an abstract at ISMRM 2018 held in Paris, France</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.12252">arXiv:1909.12252</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.12252">pdf</a>, <a href="https://arxiv.org/format/1909.12252">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Geometry">cs.CG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3385412.3386012">10.1145/3385412.3386012 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Synthesizing Structured CAD Models with Equality Saturation and Inverse Transformations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nandi%2C+C">Chandrakana Nandi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Willsey%2C+M">Max Willsey</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Adam Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wilcox%2C+J+R">James R. Wilcox</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Darulova%2C+E">Eva Darulova</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grossman%2C+D">Dan Grossman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tatlock%2C+Z">Zachary Tatlock</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.12252v3-abstract-short" style="display: inline;">
        Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure.
  This paper proposes a second decompilation stage to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.12252v3-abstract-full').style.display = 'inline'; document.getElementById('1909.12252v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.12252v3-abstract-full" style="display: none;">
        Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure.
  This paper proposes a second decompilation stage to robustly &#34;shrink&#34; unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.12252v3-abstract-full').style.display = 'none'; document.getElementById('1909.12252v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 September, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        PLDI 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.02565">arXiv:1909.02565</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.02565">pdf</a>, <a href="https://arxiv.org/format/1909.02565">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Causal Effects of Brevity on Style and Success in Social Media
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gligoric%2C+K">Kristina Gligoric</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=West%2C+R">Robert West</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.02565v1-abstract-short" style="display: inline;">
        In online communities, where billions of people strive to propagate their messages, understanding how wording affects success is of primary importance. In this work, we are interested in one particularly salient aspect of wording: brevity. What is the causal effect of brevity on message success? What are the linguistic traits of brevity? When is brevity beneficial, and when is it not? Whereas most&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.02565v1-abstract-full').style.display = 'inline'; document.getElementById('1909.02565v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.02565v1-abstract-full" style="display: none;">
        In online communities, where billions of people strive to propagate their messages, understanding how wording affects success is of primary importance. In this work, we are interested in one particularly salient aspect of wording: brevity. What is the causal effect of brevity on message success? What are the linguistic traits of brevity? When is brevity beneficial, and when is it not? Whereas most prior work has studied the effect of wording on style and success in observational setups, we conduct a controlled experiment, in which crowd workers shorten social media posts to prescribed target lengths and other crowd workers subsequently rate the original and shortened versions. This allows us to isolate the causal effect of brevity on the success of a message. We find that concise messages are on average more successful than the original messages up to a length reduction of 30-40%. The optimal reduction is on average between 10% and 20%. The observed effect is robust across different subpopulations of raters and is the strongest for raters who visit social media on a daily basis. Finally, we discover unique linguistic and content traits of brevity and correlate them with the measured probability of success in order to distinguish effective from ineffective shortening strategies. Overall, our findings are important for developing a better understanding of the effect of brevity on the success of messages in online social media.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.02565v1-abstract-full').style.display = 'none'; document.getElementById('1909.02565v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1908.08783">arXiv:1908.08783</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1908.08783">pdf</a>, <a href="https://arxiv.org/format/1908.08783">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Fitness Functions for Machine Programming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mandal%2C+S">Shantanu Mandal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+T+A">Todd A. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Turek%2C+J+S">Javier S. Turek</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gottschlich%2C+J">Justin Gottschlich</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+S">Shengtian Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Muzahid%2C+A">Abdullah Muzahid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1908.08783v5-abstract-short" style="display: inline;">
        The problem of automatic software generation is known as Machine Programming. In this work, we propose a framework based on genetic algorithms to solve this problem. Although genetic algorithms have been used successfully for many problems, one criticism is that hand-crafting its fitness function, the test that aims to effectively guide its evolution, can be notably challenging. Our framework pres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.08783v5-abstract-full').style.display = 'inline'; document.getElementById('1908.08783v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1908.08783v5-abstract-full" style="display: none;">
        The problem of automatic software generation is known as Machine Programming. In this work, we propose a framework based on genetic algorithms to solve this problem. Although genetic algorithms have been used successfully for many problems, one criticism is that hand-crafting its fitness function, the test that aims to effectively guide its evolution, can be notably challenging. Our framework presents a novel approach to learn the fitness function using neural networks to predict values of ideal fitness functions. We also augment the evolutionary process with a minimally intrusive search heuristic. This heuristic improves the framework&#39;s ability to discover correct programs from ones that are approximately correct and does so with negligible computational overhead. We compare our approach with several state-of-the-art program synthesis methods and demonstrate that it finds more correct programs with fewer candidate program generations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1908.08783v5-abstract-full').style.display = 'none'; document.getElementById('1908.08783v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 August, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.06319">arXiv:1907.06319</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.06319">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enabling Multi-Shell b-Value Generalizability of Data-Driven Diffusion Models with Deep SHORE
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nath%2C+V">Vishwesh Nath</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+I">Ilwoo Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schilling%2C+K+G">Kurt G. Schilling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Parvathaneni%2C+P">Prasanna Parvathaneni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hansen%2C+C+B">Colin B. Hansen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tang%2C+Y">Yucheng Tang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huo%2C+Y">Yuankai Huo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Janve%2C+V+A">Vaibhav A. Janve</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Y">Yurui Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stepniewska%2C+I">Iwona Stepniewska</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+W">Adam W. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Landman%2C+B+A">Bennett A. Landman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.06319v3-abstract-short" style="display: inline;">
        Intra-voxel models of the diffusion signal are essential for interpreting organization of the tissue environment at micrometer level with data at millimeter resolution. Recent advances in data driven methods have enabled direct compari-son and optimization of methods for in-vivo data with externally validated histological sections with both 2-D and 3-D histology. Yet, all existing methods make lim&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.06319v3-abstract-full').style.display = 'inline'; document.getElementById('1907.06319v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.06319v3-abstract-full" style="display: none;">
        Intra-voxel models of the diffusion signal are essential for interpreting organization of the tissue environment at micrometer level with data at millimeter resolution. Recent advances in data driven methods have enabled direct compari-son and optimization of methods for in-vivo data with externally validated histological sections with both 2-D and 3-D histology. Yet, all existing methods make limiting assumptions of either (1) model-based linkages between b-values or (2) limited associations with single shell data. We generalize prior deep learning models that used single shell spherical harmonic transforms to integrate the re-cently developed simple harmonic oscillator reconstruction (SHORE) basis. To enable learning on the SHORE manifold, we present an alternative formulation of the fiber orientation distribution (FOD) object using the SHORE basis while rep-resenting the observed diffusion weighted data in the SHORE basis. To ensure consistency of hyper-parameter optimization for SHORE, we present our Deep SHORE approach to learn on a data-optimized manifold. Deep SHORE is evalu-ated with eight-fold cross-validation of a preclinical MRI-histology data with four b-values. Generalizability of in-vivo human data is evaluated on two separate 3T MRI scanners. Specificity in terms of angular correlation (ACC) with the preclinical data improved on single shell: 0.78 relative to 0.73 and 0.73, multi-shell: 0.80 relative to 0.74 (p &lt; 0.001). In the in-vivo human data, Deep SHORE was more consistent across scanners with 0.63 relative to other multi-shell methods 0.39, 0.52 and 0.57 in terms of ACC. In conclusion, Deep SHORE is a promising method to enable data driven learning with DW-MRI under conditions with varying b-values, number of diffusion shells, and gradient directions per shell.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.06319v3-abstract-full').style.display = 'none'; document.getElementById('1907.06319v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 July, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.09666">arXiv:1906.09666</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.09666">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Aerial hyperspectral imagery and deep neural networks for high-throughput yield phenotyping in wheat
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Moghimi%2C+A">Ali Moghimi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+C">Ce Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+J+A">James A. Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.09666v1-abstract-short" style="display: inline;">
        Crop production needs to increase in a sustainable manner to meet the growing global demand for food. To identify crop varieties with high yield potential, plant scientists and breeders evaluate the performance of hundreds of lines in multiple locations over several years. To facilitate the process of selecting advanced varieties, an automated framework was developed in this study. A hyperspectral&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.09666v1-abstract-full').style.display = 'inline'; document.getElementById('1906.09666v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.09666v1-abstract-full" style="display: none;">
        Crop production needs to increase in a sustainable manner to meet the growing global demand for food. To identify crop varieties with high yield potential, plant scientists and breeders evaluate the performance of hundreds of lines in multiple locations over several years. To facilitate the process of selecting advanced varieties, an automated framework was developed in this study. A hyperspectral camera was mounted on an unmanned aerial vehicle to collect aerial imagery with high spatial and spectral resolution. Aerial images were captured in two consecutive growing seasons from three experimental yield fields composed of hundreds experimental plots (1x2.4 meter), each contained a single wheat line. The grain of more than thousand wheat plots was harvested by a combine, weighed, and recorded as the ground truth data. To leverage the high spatial resolution and investigate the yield variation within the plots, images of plots were divided into sub-plots by integrating image processing techniques and spectral mixture analysis with the expert domain knowledge. Afterwards, the sub-plot dataset was divided into train, validation, and test sets using stratified sampling. Subsequent to extracting features from each sub-plot, deep neural networks were trained for yield estimation. The coefficient of determination for predicting the yield of the test dataset at sub-plot scale was 0.79 with root mean square error of 5.90 grams. In addition to providing insights into yield variation at sub-plot scale, the proposed framework can facilitate the process of high-throughput yield phenotyping as a valuable decision support tool. It offers the possibility of (i) remote visual inspection of the plots, (ii) studying the effect of crop density on yield, and (iii) optimizing plot size to investigate more lines in a dedicated field each year.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.09666v1-abstract-full').style.display = 'none'; document.getElementById('1906.09666v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 June, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.06317">arXiv:1906.06317</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.06317">pdf</a>, <a href="https://arxiv.org/format/1906.06317">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computational Physics">physics.comp-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.cpc.2020.107275">10.1016/j.cpc.2020.107275 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        freud: A Software Suite for High Throughput Analysis of Particle Simulation Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ramasubramani%2C+V">Vyas Ramasubramani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dice%2C+B+D">Bradley D. Dice</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Harper%2C+E+S">Eric S. Harper</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Spellings%2C+M+P">Matthew P. Spellings</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+J+A">Joshua A. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Glotzer%2C+S+C">Sharon C. Glotzer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.06317v3-abstract-short" style="display: inline;">
        The freud Python package is a powerful library for analyzing simulation data. Written with modern simulation and data analysis workflows in mind, freud provides a Python interface to fast, parallelized C++ routines that run efficiently on laptops, workstations, and supercomputing clusters. The package provides the core tools for finding particle neighbors in periodic systems, and offers a uniform&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.06317v3-abstract-full').style.display = 'inline'; document.getElementById('1906.06317v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.06317v3-abstract-full" style="display: none;">
        The freud Python package is a powerful library for analyzing simulation data. Written with modern simulation and data analysis workflows in mind, freud provides a Python interface to fast, parallelized C++ routines that run efficiently on laptops, workstations, and supercomputing clusters. The package provides the core tools for finding particle neighbors in periodic systems, and offers a uniform API to a wide variety of methods implemented using these tools. As such, freud users can access standard methods such as the radial distribution function as well as newer, more specialized methods such as the potential of mean force and torque and local crystal environment analysis with equal ease. While many comparable tools place a heavy emphasis on reading and operating on trajectory file formats, freud instead accepts numerical arrays of data directly as inputs. By remaining agnostic to its data source, freud is suitable for analyzing any coarse-grained particle simulation, regardless of the original data representation or simulation method. When used for on-the-fly analysis in conjunction with scriptable simulation software such as HOOMD-blue, freud enables smart simulations that adapt to the current state of the system, allowing users to study phenomena such as nucleation and growth.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.06317v3-abstract-full').style.display = 'none'; document.getElementById('1906.06317v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 June, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper is accepted for publication in Computer Physics Communications. For the source code on Github, see https://github.com/glotzerlab/freud . For the documentation on ReadTheDocs, see https://freud.readthedocs.io/en/stable</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.04675">arXiv:1906.04675</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.04675">pdf</a>, <a href="https://arxiv.org/format/1906.04675">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Taxonomy of Saliency Metrics for Channel Pruning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Persand%2C+K">Kaveena Persand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.04675v2-abstract-short" style="display: inline;">
        Pruning unimportant parameters can allow deep neural networks (DNNs) to reduce their heavy computation and memory requirements. A saliency metric estimates which parameters can be safely pruned with little impact on the classification performance of the DNN. Many saliency metrics have been proposed, each within the context of a wider pruning algorithm. The result is that it is difficult to separat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.04675v2-abstract-full').style.display = 'inline'; document.getElementById('1906.04675v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.04675v2-abstract-full" style="display: none;">
        Pruning unimportant parameters can allow deep neural networks (DNNs) to reduce their heavy computation and memory requirements. A saliency metric estimates which parameters can be safely pruned with little impact on the classification performance of the DNN. Many saliency metrics have been proposed, each within the context of a wider pruning algorithm. The result is that it is difficult to separate the effectiveness of the saliency metric from the wider pruning algorithm that surrounds it. Similar-looking saliency metrics can yield very different results because of apparently minor design choices. We propose a taxonomy of saliency metrics based on four mostly-orthogonal principal components. We show that a broad range of metrics from the pruning literature can be grouped according to these components. Our taxonomy not only serves as a guide to prior work, but allows us to construct new saliency metrics by exploring novel combinations of our taxonomic components. We perform an in-depth experimental investigation of more than 300 saliency metrics. Our results provide decisive answers to open research questions, and demonstrate the importance of reduction and scaling when pruning groups of weights. We find that some of our constructed metrics can outperform the best existing state-of-the-art metrics for convolutional neural network channel pruning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.04675v2-abstract-full').style.display = 'none'; document.getElementById('1906.04675v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 June, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.09708">arXiv:1903.09708</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.09708">pdf</a>, <a href="https://arxiv.org/format/1903.09708">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Explaining Reinforcement Learning to Mere Mortals: An Empirical Study
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dodge%2C+J">Jonathan Dodge</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sadarangani%2C+A">Amrita Sadarangani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Juozapaitis%2C+Z">Zoe Juozapaitis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Newman%2C+E">Evan Newman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Irvine%2C+J">Jed Irvine</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chattopadhyay%2C+S">Souti Chattopadhyay</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fern%2C+A">Alan Fern</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burnett%2C+M">Margaret Burnett</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.09708v2-abstract-short" style="display: inline;">
        We present a user study to investigate the impact of explanations on non-experts&#39; understanding of reinforcement learning (RL) agents. We investigate both a common RL visualization, saliency maps (the focus of attention), and a more recent explanation type, reward-decomposition bars (predictions of future types of rewards). We designed a 124 participant, four-treatment experiment to compare partic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.09708v2-abstract-full').style.display = 'inline'; document.getElementById('1903.09708v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.09708v2-abstract-full" style="display: none;">
        We present a user study to investigate the impact of explanations on non-experts&#39; understanding of reinforcement learning (RL) agents. We investigate both a common RL visualization, saliency maps (the focus of attention), and a more recent explanation type, reward-decomposition bars (predictions of future types of rewards). We designed a 124 participant, four-treatment experiment to compare participants&#39; mental models of an RL agent in a simple Real-Time Strategy (RTS) game. Our results show that the combination of both saliency and reward bars were needed to achieve a statistically significant improvement in mental model score over the control. In addition, our qualitative analysis of the data reveals a number of effects for further study.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.09708v2-abstract-full').style.display = 'none'; document.getElementById('1903.09708v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 March, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.03308">arXiv:1903.03308</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.03308">pdf</a>, <a href="https://arxiv.org/format/1903.03308">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Thanks for Stopping By: A Study of &#34;Thanks&#34; Usage on Wikimedia
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+S">Swati Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zia%2C+L">Leila Zia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.03308v1-abstract-short" style="display: inline;">
        The Thanks feature on Wikipedia, also known as &#34;Thanks&#34;, is a tool with which editors can quickly and easily send one other positive feedback. The aim of this project is to better understand this feature: its scope, the characteristics of a typical &#34;Thanks&#34; interaction, and the effects of receiving a thank on individual editors. We study the motivational impacts of &#34;Thanks&#34; because maintaining edi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.03308v1-abstract-full').style.display = 'inline'; document.getElementById('1903.03308v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.03308v1-abstract-full" style="display: none;">
        The Thanks feature on Wikipedia, also known as &#34;Thanks&#34;, is a tool with which editors can quickly and easily send one other positive feedback. The aim of this project is to better understand this feature: its scope, the characteristics of a typical &#34;Thanks&#34; interaction, and the effects of receiving a thank on individual editors. We study the motivational impacts of &#34;Thanks&#34; because maintaining editor engagement is a central problem for crowdsourced repositories of knowledge such as Wikimedia. Our main findings are that most editors have not been exposed to the Thanks feature (meaning they have never given nor received a thank), thanks are typically sent upwards (from less experienced to more experienced editors), and receiving a thank is correlated with having high levels of editor engagement. Though the prevalence of &#34;Thanks&#34; usage varies by editor experience, the impact of receiving a thank seems mostly consistent for all users. We empirically demonstrate that receiving a thank has a strong positive effect on short-term editor activity across the board and provide preliminary evidence that thanks could compound to have long-term effects as well.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.03308v1-abstract-full').style.display = 'none'; document.getElementById('1903.03308v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1901.05049">arXiv:1901.05049</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1901.05049">pdf</a>, <a href="https://arxiv.org/format/1901.05049">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3403572">10.1145/3403572 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bonseyes AI Pipeline -- bringing AI to you. End-to-end integration of data, algorithms and deployment tools
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=de+Prado%2C+M">Miguel de Prado</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Su%2C+J">Jing Su</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saeed%2C+R">Rabia Saeed</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Keller%2C+L">Lorenzo Keller</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vallez%2C+N">Noelia Vallez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Benini%2C+L">Luca Benini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Llewellynn%2C+T">Tim Llewellynn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ouerhani%2C+N">Nabil Ouerhani</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=and%2C+R+D">Rozenn Dahyot and</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pazos%2C+N">Nuria Pazos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1901.05049v3-abstract-short" style="display: inline;">
        Next generation of embedded Information and Communication Technology (ICT) systems are collaborative systems able to perform autonomous tasks. The remarkable expansion of the embedded ICT market, together with the rise and breakthroughs of Artificial Intelligence (AI), have put the focus on the Edge as it stands as one of the keys for the next technological revolution: the seamless integration of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.05049v3-abstract-full').style.display = 'inline'; document.getElementById('1901.05049v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1901.05049v3-abstract-full" style="display: none;">
        Next generation of embedded Information and Communication Technology (ICT) systems are collaborative systems able to perform autonomous tasks. The remarkable expansion of the embedded ICT market, together with the rise and breakthroughs of Artificial Intelligence (AI), have put the focus on the Edge as it stands as one of the keys for the next technological revolution: the seamless integration of AI in our daily life. However, training and deployment of custom AI solutions on embedded devices require a fine-grained integration of data, algorithms, and tools to achieve high accuracy. Such integration requires a high level of expertise that becomes a real bottleneck for small and medium enterprises wanting to deploy AI solutions on the Edge which, ultimately, slows down the adoption of AI on daily-life applications. In this work, we present a modular AI pipeline as an integrating framework to bring data, algorithms, and deployment tools together. By removing the integration barriers and lowering the required expertise, we can interconnect the different stages of tools and provide a modular end-to-end development of AI products for embedded devices. Our AI pipeline consists of four modular main steps: i) data ingestion, ii) model training, iii) deployment optimization and, iv) the IoT hub integration. To show the effectiveness of our pipeline, we provide examples of different AI applications during each of the steps. Besides, we integrate our deployment framework, LPDNN, into the AI pipeline and present its lightweight architecture and deployment capabilities for embedded devices. Finally, we demonstrate the results of the AI pipeline by showing the deployment of several AI applications such as keyword spotting, image classification and object detection on a set of well-known embedded platforms, where LPDNN consistently outperforms all other popular deployment frameworks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1901.05049v3-abstract-full').style.display = 'none'; document.getElementById('1901.05049v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 January, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.06981">arXiv:1811.06981</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.06981">pdf</a>, <a href="https://arxiv.org/format/1811.06981">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learned Video Compression
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Rippel%2C+O">Oren Rippel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nair%2C+S">Sanjay Nair</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lew%2C+C">Carissa Lew</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Branson%2C+S">Steve Branson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+G">Alexander G. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bourdev%2C+L">Lubomir Bourdev</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.06981v1-abstract-short" style="display: inline;">
        We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the first ML-based method to do so.
  We evaluate our approach on standard video compression test sets of varying resolutions, and benchmark against all mainstream commercial c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.06981v1-abstract-full').style.display = 'inline'; document.getElementById('1811.06981v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.06981v1-abstract-full" style="display: none;">
        We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the first ML-based method to do so.
  We evaluate our approach on standard video compression test sets of varying resolutions, and benchmark against all mainstream commercial codecs, in the low-latency mode. On standard-definition videos, relative to our algorithm, HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On high-definition 1080p videos, H.265 and VP9 typically produce codes up to 20% larger, and H.264 up to 35% larger. Furthermore, our approach does not suffer from blocking artifacts and pixelation, and thus produces videos that are more visually pleasing.
  We propose two main contributions. The first is a novel architecture for video compression, which (1) generalizes motion estimation to perform any learned compensation beyond simple translations, (2) rather than strictly relying on previously transmitted reference frames, maintains a state of arbitrary information learned by the model, and (3) enables jointly compressing all transmitted signals (such as optical flow and residual).
  Secondly, we present a framework for ML-based spatial rate control: namely, a mechanism for assigning variable bitrates across space for each frame. This is a critical component for video coding, which to our knowledge had not been developed within a machine learning setting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.06981v1-abstract-full').style.display = 'none'; document.getElementById('1811.06981v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.04260">arXiv:1810.04260</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.04260">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Inter-Scanner Harmonization of High Angular Resolution DW-MRI using Null Space Deep Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nath%2C+V">Vishwesh Nath</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Parvathaneni%2C+P">Prasanna Parvathaneni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hansen%2C+C+B">Colin B. Hansen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hainline%2C+A+E">Allison E. Hainline</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bermudez%2C+C">Camilo Bermudez</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Remedios%2C+S">Samuel Remedios</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Blaber%2C+J+A">Justin A. Blaber</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Schilling%2C+K+G">Kurt G. Schilling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lyu%2C+I">Ilwoo Lyu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Janve%2C+V">Vaibhav Janve</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Y">Yurui Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Stepniewska%2C+I">Iwona Stepniewska</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rogers%2C+B+P">Baxter P. Rogers</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Newton%2C+A+T">Allen T. Newton</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Davis%2C+L+T">L. Taylor Davis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luci%2C+J">Jeff Luci</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+W">Adam W. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Landman%2C+B+A">Bennett A. Landman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.04260v1-abstract-short" style="display: inline;">
        Diffusion-weighted magnetic resonance imaging (DW-MRI) allows for non-invasive imaging of the local fiber architecture of the human brain at a millimetric scale. Multiple classical approaches have been proposed to detect both single (e.g., tensors) and multiple (e.g., constrained spherical deconvolution, CSD) fiber population orientations per voxel. However, existing techniques generally exhibit l&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.04260v1-abstract-full').style.display = 'inline'; document.getElementById('1810.04260v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.04260v1-abstract-full" style="display: none;">
        Diffusion-weighted magnetic resonance imaging (DW-MRI) allows for non-invasive imaging of the local fiber architecture of the human brain at a millimetric scale. Multiple classical approaches have been proposed to detect both single (e.g., tensors) and multiple (e.g., constrained spherical deconvolution, CSD) fiber population orientations per voxel. However, existing techniques generally exhibit low reproducibility across MRI scanners. Herein, we propose a data-driven tech-nique using a neural network design which exploits two categories of data. First, training data were acquired on three squirrel monkey brains using ex-vivo DW-MRI and histology of the brain. Second, repeated scans of human subjects were acquired on two different scanners to augment the learning of the network pro-posed. To use these data, we propose a new network architecture, the null space deep network (NSDN), to simultaneously learn on traditional observed/truth pairs (e.g., MRI-histology voxels) along with repeated observations without a known truth (e.g., scan-rescan MRI). The NSDN was tested on twenty percent of the histology voxels that were kept completely blind to the network. NSDN significantly improved absolute performance relative to histology by 3.87% over CSD and 1.42% over a recently proposed deep neural network approach. More-over, it improved reproducibility on the paired data by 21.19% over CSD and 10.09% over a recently proposed deep approach. Finally, NSDN improved gen-eralizability of the model to a third in vivo human scanner (which was not used in training) by 16.08% over CSD and 10.41% over a recently proposed deep learn-ing approach. This work suggests that data-driven approaches for local fiber re-construction are more reproducible, informative and precise and offers a novel, practical method for determining these models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.04260v1-abstract-full').style.display = 'none'; document.getElementById('1810.04260v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03611">arXiv:1810.03611</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03611">pdf</a>, <a href="https://arxiv.org/format/1810.03611">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding the Origins of Bias in Word Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Brunet%2C+M">Marc-Etienne Brunet</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alkalay-Houlihan%2C+C">Colleen Alkalay-Houlihan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zemel%2C+R">Richard Zemel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03611v2-abstract-short" style="display: inline;">
        The power of machine learning systems not only promises great technical progress, but risks societal harm. As a recent example, researchers have shown that popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems, from automated translation services to curriculum vitae scanners, can amplify stereotypes i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03611v2-abstract-full').style.display = 'inline'; document.getElementById('1810.03611v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03611v2-abstract-full" style="display: none;">
        The power of machine learning systems not only promises great technical progress, but risks societal harm. As a recent example, researchers have shown that popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems, from automated translation services to curriculum vitae scanners, can amplify stereotypes in important contexts. Although methods have been developed to measure these biases and alter word embeddings to mitigate their biased representations, there is a lack of understanding in how word embedding bias depends on the training data. In this work, we develop a technique for understanding the origins of bias in word embeddings. Given a word embedding trained on a corpus, our method identifies how perturbing the corpus will affect the bias of the resulting embedding. This can be used to trace the origins of word embedding bias back to the original training documents. Using our method, one can investigate trends in the bias of the underlying corpus and identify subsets of documents whose removal would most reduce bias. We demonstrate our techniques on both a New York Times and Wikipedia corpus and find that our influence function-based approximations are very accurate.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03611v2-abstract-full').style.display = 'none'; document.getElementById('1810.03611v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.10572">arXiv:1809.10572</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.10572">pdf</a>, <a href="https://arxiv.org/format/1809.10572">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mathematical Software">cs.MS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ARITH.2019.00018">10.1109/ARITH.2019.00018 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.10572v2-abstract-short" style="display: inline;">
        Quantization of weights and activations in Deep Neural Networks (DNNs) is a powerful technique for network compression, and has enjoyed significant attention and success. However, much of the inference-time benefit of quantization is accessible only through the use of customized hardware accelerators or by providing an FPGA implementation of quantized arithmetic.
  Building on prior work, we show&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.10572v2-abstract-full').style.display = 'inline'; document.getElementById('1809.10572v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.10572v2-abstract-full" style="display: none;">
        Quantization of weights and activations in Deep Neural Networks (DNNs) is a powerful technique for network compression, and has enjoyed significant attention and success. However, much of the inference-time benefit of quantization is accessible only through the use of customized hardware accelerators or by providing an FPGA implementation of quantized arithmetic.
  Building on prior work, we show how to construct arbitrary bit-precise signed and unsigned integer operations using a software technique which logically \emph{embeds} a vector architecture with custom bit-width lanes in universally available fixed-width scalar arithmetic.
  We evaluate our approach on a high-end Intel Haswell processor, and an embedded ARM processor. Our approach yields very fast implementations of bit-precise custom DNN operations, which often match or exceed the performance of operations quantized to the sizes supported in native arithmetic. At the strongest level of quantization, our approach yields a maximum speedup of $\thicksim6\times$ on the Intel platform, and $\thicksim10\times$ on the ARM platform versus quantization to native 8-bit integers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.10572v2-abstract-full').style.display = 'none'; document.getElementById('1809.10572v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 December, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 September, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.02318">arXiv:1804.02318</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.02318">pdf</a>, <a href="https://arxiv.org/format/1804.02318">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How Constraints Affect Content: The Case of Twitter&#39;s Switch from 140 to 280 Characters
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gligori%C4%87%2C+K">Kristina Gligorić</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=West%2C+R">Robert West</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.02318v2-abstract-short" style="display: inline;">
        It is often said that constraints affect creative production, both in terms of form and quality. Online social media platforms frequently impose constraints on the content that users can produce, limiting the range of possible contributions. Do these restrictions tend to push creators towards producing more or less successful content? How do creators adapt their contributions to fit the limits imp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.02318v2-abstract-full').style.display = 'inline'; document.getElementById('1804.02318v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.02318v2-abstract-full" style="display: none;">
        It is often said that constraints affect creative production, both in terms of form and quality. Online social media platforms frequently impose constraints on the content that users can produce, limiting the range of possible contributions. Do these restrictions tend to push creators towards producing more or less successful content? How do creators adapt their contributions to fit the limits imposed by social media platforms? To answer these questions, we conduct an observational study of a recent event: on November 7, 2017, Twitter changed the maximum allowable length of a tweet from 140 to 280 characters, thereby significantly altering its signature constraint. In the first study of this switch, we compare tweets with nearly or exactly 140 characters before the change to tweets of the same length posted after the change. This setup enables us to characterize how users alter their tweets to fit the constraint and how this affects their tweets&#39; success. We find that in response to a length constraint, users write more tersely, use more abbreviations and contracted forms, and use fewer definite articles. Also, although in general tweet success increases with length, we find initial evidence that tweets made to fit the 140-character constraint tend to be more successful than similar-length tweets written when the constraint was removed, suggesting that the length constraint improved tweet quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.02318v2-abstract-full').style.display = 'none'; document.getElementById('1804.02318v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 April, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in the Proceedings of AAAI ICWSM 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.10986">arXiv:1803.10986</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.10986">pdf</a>, <a href="https://arxiv.org/format/1803.10986">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Error Analysis and Improving the Accuracy of Winograd Convolution for Deep Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Barabasz%2C+B">Barbara Barabasz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Soodhalter%2C+K+M">Kirk M. Soodhalter</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.10986v3-abstract-short" style="display: inline;">
        Popular deep neural networks (DNNs) spend the majority of their execution time computing convolutions. The Winograd family of algorithms can greatly reduce the number of arithmetic operations required and is present in many DNN software frameworks. However, the performance gain is at the expense of a reduction in floating point (FP) numerical accuracy. In this paper, we analyse the worst case FP e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.10986v3-abstract-full').style.display = 'inline'; document.getElementById('1803.10986v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.10986v3-abstract-full" style="display: none;">
        Popular deep neural networks (DNNs) spend the majority of their execution time computing convolutions. The Winograd family of algorithms can greatly reduce the number of arithmetic operations required and is present in many DNN software frameworks. However, the performance gain is at the expense of a reduction in floating point (FP) numerical accuracy. In this paper, we analyse the worst case FP error and prove the estimation of norm and conditioning of the algorithm. We show that the bound grows exponentially with the size of the convolution, but the error bound of the \textit{modified} algorithm is smaller than the original one. We propose several methods for reducing FP error. We propose a canonical evaluation ordering based on Huffman coding that reduces summation error. We study the selection of sampling &#34;points&#34; experimentally and find empirically good points for the most important sizes. We identify the main factors associated with good points. In addition, we explore other methods to reduce FP error, including mixed-precision convolution, and pairwise summation across DNN channels. Using our methods we can significantly reduce FP error for a given block size, which allows larger block sizes and reduced computation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.10986v3-abstract-full').style.display = 'none'; document.getElementById('1803.10986v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 May, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 March, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.09597">arXiv:1802.09597</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.09597">pdf</a>, <a href="https://arxiv.org/format/1802.09597">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3178876.3186129">10.1145/3178876.3186129 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mapping the Invocation Structure of Online Political Interaction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Raghavan%2C+M">Manish Raghavan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kleinberg%2C+J">Jon Kleinberg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.09597v1-abstract-short" style="display: inline;">
        The surge in political information, discourse, and interaction has been one of the most important developments in social media over the past several years. There is rich structure in the interaction among different viewpoints on the ideological spectrum. However, we still have only a limited analytical vocabulary for expressing the ways in which these viewpoints interact.
  In this paper, we devel&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.09597v1-abstract-full').style.display = 'inline'; document.getElementById('1802.09597v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.09597v1-abstract-full" style="display: none;">
        The surge in political information, discourse, and interaction has been one of the most important developments in social media over the past several years. There is rich structure in the interaction among different viewpoints on the ideological spectrum. However, we still have only a limited analytical vocabulary for expressing the ways in which these viewpoints interact.
  In this paper, we develop network-based methods that operate on the ways in which users share content; we construct \emph{invocation graphs} on Web domains showing the extent to which pages from one domain are invoked by users to reply to posts containing pages from other domains. When we locate the domains on a political spectrum induced from the data, we obtain an embedded graph showing how these interaction links span different distances on the spectrum. The structure of this embedded network, and its evolution over time, helps us derive macro-level insights about how political interaction unfolded through 2016, leading up to the US Presidential election. In particular, we find that the domains invoked in replies spanned increasing distances on the spectrum over the months approaching the election, and that there was clear asymmetry between the left-to-right and right-to-left patterns of linkage.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.09597v1-abstract-full').style.display = 'none'; document.getElementById('1802.09597v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The Web Conference 2018 (WWW 2018)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1711.08019">arXiv:1711.08019</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1711.08019">pdf</a>, <a href="https://arxiv.org/format/1711.08019">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Toward Foraging for Understanding of StarCraft Agents: An Empirical Study
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Penney%2C+S">Sean Penney</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dodge%2C+J">Jonathan Dodge</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hilderbrand%2C+C">Claudia Hilderbrand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Simpson%2C+L">Logan Simpson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burnett%2C+M">Margaret Burnett</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1711.08019v3-abstract-short" style="display: inline;">
        Assessing and understanding intelligent agents is a difficult task for users that lack an AI background. A relatively new area, called &#34;Explainable AI,&#34; is emerging to help address this problem, but little is known about how users would forage through information an explanation system might offer. To inform the development of Explainable AI systems, we conducted a formative study, using the lens o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1711.08019v3-abstract-full').style.display = 'inline'; document.getElementById('1711.08019v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1711.08019v3-abstract-full" style="display: none;">
        Assessing and understanding intelligent agents is a difficult task for users that lack an AI background. A relatively new area, called &#34;Explainable AI,&#34; is emerging to help address this problem, but little is known about how users would forage through information an explanation system might offer. To inform the development of Explainable AI systems, we conducted a formative study, using the lens of Information Foraging Theory, into how experienced users foraged in the domain of StarCraft to assess an agent. Our results showed that participants faced difficult foraging problems. These foraging problems caused participants to entirely miss events that were important to them, reluctantly choose to ignore actions they did not want to ignore, and bear high cognitive, navigation, and information costs to access the information they needed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1711.08019v3-abstract-full').style.display = 'none'; document.getElementById('1711.08019v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 December, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 November, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 10 figures, to appear in ACM IUI 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1711.06953">arXiv:1711.06953</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1711.06953">pdf</a>, <a href="https://arxiv.org/format/1711.06953">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How the Experts Do It: Assessing and Explaining Agent Behaviors in Real-Time Strategy Games
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dodge%2C+J">Jonathan Dodge</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Penney%2C+S">Sean Penney</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hilderbrand%2C+C">Claudia Hilderbrand</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Burnett%2C+M">Margaret Burnett</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1711.06953v1-abstract-short" style="display: inline;">
        How should an AI-based explanation system explain an agent&#39;s complex behavior to ordinary end users who have no background in AI? Answering this question is an active research area, for if an AI-based explanation system could effectively explain intelligent agents&#39; behavior, it could enable the end users to understand, assess, and appropriately trust (or distrust) the agents attempting to help the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1711.06953v1-abstract-full').style.display = 'inline'; document.getElementById('1711.06953v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1711.06953v1-abstract-full" style="display: none;">
        How should an AI-based explanation system explain an agent&#39;s complex behavior to ordinary end users who have no background in AI? Answering this question is an active research area, for if an AI-based explanation system could effectively explain intelligent agents&#39; behavior, it could enable the end users to understand, assess, and appropriately trust (or distrust) the agents attempting to help them. To provide insights into this question, we turned to human expert explainers in the real-time strategy domain, &#34;shoutcaster&#34;, to understand (1) how they foraged in an evolving strategy game in real time, (2) how they assessed the players&#39; behaviors, and (3) how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters&#39; foraging strategies for gleaning information necessary to assess and explain the players; a characterization of the types of implicit questions shoutcasters answered; and implications for creating explanations by using the patterns
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1711.06953v1-abstract-full').style.display = 'none'; document.getElementById('1711.06953v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 11 figures, submitted to CHI 2017</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1710.01079">arXiv:1710.01079</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1710.01079">pdf</a>, <a href="https://arxiv.org/format/1710.01079">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimal DNN Primitive Selection with Partitioned Boolean Quadratic Programming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1710.01079v2-abstract-short" style="display: inline;">
        Deep Neural Networks (DNNs) require very large amounts of computation both for training and for inference when deployed in the field. Many different algorithms have been proposed to implement the most computationally expensive layers of DNNs. Further, each of these algorithms has a large number of variants, which offer different trade-offs of parallelism, data locality, memory footprint, and execu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.01079v2-abstract-full').style.display = 'inline'; document.getElementById('1710.01079v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1710.01079v2-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) require very large amounts of computation both for training and for inference when deployed in the field. Many different algorithms have been proposed to implement the most computationally expensive layers of DNNs. Further, each of these algorithms has a large number of variants, which offer different trade-offs of parallelism, data locality, memory footprint, and execution time. In addition, specific algorithms operate much more efficiently on specialized data layouts and formats.
  We state the problem of optimal primitive selection in the presence of data format transformations, and show that it is NP-hard by demonstrating an embedding in the Partitioned Boolean Quadratic Assignment problem (PBQP).
  We propose an analytic solution via a PBQP solver, and evaluate our approach experimentally by optimizing several popular DNNs using a library of more than 70 DNN primitives, on an embedded platform and a general purpose platform. We show experimentally that significant gains are possible versus the state of the art vendor libraries by using a principled analytic solution to the problem of layout selection in the presence of data format transformations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.01079v2-abstract-full').style.display = 'none'; document.getElementById('1710.01079v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 November, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 October, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1709.03395">arXiv:1709.03395</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1709.03395">pdf</a>, <a href="https://arxiv.org/format/1709.03395">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low-memory GEMM-based convolution algorithms for deep neural networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vasudevan%2C+A">Aravind Vasudevan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Keane%2C+C">Cormac Keane</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1709.03395v1-abstract-short" style="display: inline;">
        Deep neural networks (DNNs) require very large amounts of computation both for training and for inference when deployed in the field. A common approach to implementing DNNs is to recast the most computationally expensive operations as general matrix multiplication (GEMM). However, as we demonstrate in this paper, there are a great many different ways to express DNN convolution operations using GEM&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1709.03395v1-abstract-full').style.display = 'inline'; document.getElementById('1709.03395v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1709.03395v1-abstract-full" style="display: none;">
        Deep neural networks (DNNs) require very large amounts of computation both for training and for inference when deployed in the field. A common approach to implementing DNNs is to recast the most computationally expensive operations as general matrix multiplication (GEMM). However, as we demonstrate in this paper, there are a great many different ways to express DNN convolution operations using GEMM. Although different approaches all perform the same number of operations, the size of temporary data structures differs significantly. Convolution of an input matrix with dimensions $C \times H \times W$, requires $O(K^2CHW)$ additional space using the classical im2col approach. More recently memory-efficient approaches requiring just $O(KCHW)$ auxiliary space have been proposed.
  We present two novel GEMM-based algorithms that require just $O(MHW)$ and $O(KW)$ additional space respectively, where $M$ is the number of channels in the result of the convolution. These algorithms dramatically reduce the space overhead of DNN convolution, making it much more suitable for memory-limited embedded systems. Experimental evaluation shows that our low-memory algorithms are just as fast as the best patch-building approaches despite requiring just a fraction of the amount of additional memory. Our low-memory algorithms have excellent data locality which gives them a further edge over patch-building algorithms when multiple cores are used. As a result, our low memory algorithms often outperform the best patch-building algorithms using multiple threads.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1709.03395v1-abstract-full').style.display = 'none'; document.getElementById('1709.03395v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 16 figures and 3 tables. arXiv admin note: text overlap with arXiv:1704.04428</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1705.10689">arXiv:1705.10689</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1705.10689">pdf</a>, <a href="https://arxiv.org/format/1705.10689">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3041021.3054197">10.1145/3041021.3054197 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Auditing Search Engines for Differential Satisfaction Across Demographics
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Mehrotra%2C+R">Rishabh Mehrotra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ashton Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Diaz%2C+F">Fernando Diaz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sharma%2C+A">Amit Sharma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wallach%2C+H">Hanna Wallach</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yilmaz%2C+E">Emine Yilmaz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1705.10689v1-abstract-short" style="display: inline;">
        Many online services, such as search engines, social media platforms, and digital marketplaces, are advertised as being available to any user, regardless of their age, gender, or other demographic factors. However, there are growing concerns that these services may systematically underserve some groups of users. In this paper, we present a framework for internally auditing such services for differ&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.10689v1-abstract-full').style.display = 'inline'; document.getElementById('1705.10689v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1705.10689v1-abstract-full" style="display: none;">
        Many online services, such as search engines, social media platforms, and digital marketplaces, are advertised as being available to any user, regardless of their age, gender, or other demographic factors. However, there are growing concerns that these services may systematically underserve some groups of users. In this paper, we present a framework for internally auditing such services for differences in user satisfaction across demographic groups, using search engines as a case study. We first explain the pitfalls of naïvely comparing the behavioral metrics that are commonly used to evaluate search engines. We then propose three methods for measuring latent differences in user satisfaction from observed differences in evaluation metrics. To develop these methods, we drew on ideas from the causal inference literature and the multilevel modeling literature. Our framework is broadly applicable to other online services, and provides general insight into interpreting their evaluation metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.10689v1-abstract-full').style.display = 'none'; document.getElementById('1705.10689v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages Accepted at WWW 2017</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1705.07199">arXiv:1705.07199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1705.07199">pdf</a>, <a href="https://arxiv.org/format/1705.07199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The High-Dimensional Geometry of Binary Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+G">Alexander G. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Berg%2C+C+P">Cory P. Berg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1705.07199v1-abstract-short" style="display: inline;">
        Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07199v1-abstract-full').style.display = 'inline'; document.getElementById('1705.07199v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1705.07199v1-abstract-full" style="display: none;">
        Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07199v1-abstract-full').style.display = 'none'; document.getElementById('1705.07199v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 May, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 4 Figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1704.04428">arXiv:1704.04428</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1704.04428">pdf</a>, <a href="https://arxiv.org/format/1704.04428">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Performance">cs.PF</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Parallel Multi Channel Convolution using General Matrix Multiplication
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Vasudevan%2C+A">Aravind Vasudevan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Andrew Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gregg%2C+D">David Gregg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1704.04428v2-abstract-short" style="display: inline;">
        Convolutional neural networks (CNNs) have emerged as one of the most successful machine learning technologies for image and video processing. The most computationally intensive parts of CNNs are the convolutional layers, which convolve multi-channel images with multiple kernels. A common approach to implementing convolutional layers is to expand the image into a column matrix (im2col) and perform&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.04428v2-abstract-full').style.display = 'inline'; document.getElementById('1704.04428v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1704.04428v2-abstract-full" style="display: none;">
        Convolutional neural networks (CNNs) have emerged as one of the most successful machine learning technologies for image and video processing. The most computationally intensive parts of CNNs are the convolutional layers, which convolve multi-channel images with multiple kernels. A common approach to implementing convolutional layers is to expand the image into a column matrix (im2col) and perform Multiple Channel Multiple Kernel (MCMK) convolution using an existing parallel General Matrix Multiplication (GEMM) library. This im2col conversion greatly increases the memory footprint of the input matrix and reduces data locality.
  In this paper we propose a new approach to MCMK convolution that is based on General Matrix Multiplication (GEMM), but not on im2col. Our algorithm eliminates the need for data replication on the input thereby enabling us to apply the convolution kernels on the input images directly. We have implemented several variants of our algorithm on a CPU processor and an embedded ARM processor. On the CPU, our algorithm is faster than im2col in most cases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.04428v2-abstract-full').style.display = 'none'; document.getElementById('1704.04428v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 April, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Camera ready version to be published at ASAP 2017 - The 28th Annual IEEE International Conference on Application-specific Systems, Architectures and Processors. 6 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1704.02341">arXiv:1704.02341</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1704.02341">pdf</a>, <a href="https://arxiv.org/format/1704.02341">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HiFrames: High Performance Data Frames in a Scripting Language
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Totoni%2C+E">Ehsan Totoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hassan%2C+W+U">Wajih Ul Hassan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+T+A">Todd A. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shpeisman%2C+T">Tatiana Shpeisman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1704.02341v1-abstract-short" style="display: inline;">
        Data frames in scripting languages are essential abstractions for processing structured data. However, existing data frame solutions are either not distributed (e.g., Pandas in Python) and therefore have limited scalability, or they are not tightly integrated with array computations (e.g., Spark SQL). This paper proposes a novel compiler-based approach where we integrate data frames into the High&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.02341v1-abstract-full').style.display = 'inline'; document.getElementById('1704.02341v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1704.02341v1-abstract-full" style="display: none;">
        Data frames in scripting languages are essential abstractions for processing structured data. However, existing data frame solutions are either not distributed (e.g., Pandas in Python) and therefore have limited scalability, or they are not tightly integrated with array computations (e.g., Spark SQL). This paper proposes a novel compiler-based approach where we integrate data frames into the High Performance Analytics Toolkit (HPAT) to build HiFrames. It provides expressive and flexible data frame APIs which are tightly integrated with array operations. HiFrames then automatically parallelizes and compiles relational operations along with other array computations in end-to-end data analytics programs, and generates efficient MPI/C++ code. We demonstrate that HiFrames is significantly faster than alternatives such as Spark SQL on clusters, without forcing the programmer to switch to embedded SQL for part of the program. HiFrames is 3.6x to 70x faster than Spark SQL for basic relational operations, and can be up to 20,000x faster for advanced analytics operations, such as weighted moving averages (WMA), that the map-reduce paradigm cannot handle effectively. HiFrames is also 5x faster than Spark SQL for TPCx-BB Q26 on 64 nodes of Cori supercomputer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.02341v1-abstract-full').style.display = 'none'; document.getElementById('1704.02341v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1702.02092">arXiv:1702.02092</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1702.02092">pdf</a>, <a href="https://arxiv.org/ps/1702.02092">ps</a>, <a href="https://arxiv.org/format/1702.02092">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Characterisation of speech diversity using self-organising maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+T+A+F">Tom A. F. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Powers%2C+D+M+W">David M. W. Powers</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1702.02092v1-abstract-short" style="display: inline;">
        We report investigations into speaker classification of larger quantities of unlabelled speech data using small sets of manually phonemically annotated speech. The Kohonen speech typewriter is a semi-supervised method comprised of self-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a 2D array of cells that learn vector representations of the data based on neighbourhoods. In&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02092v1-abstract-full').style.display = 'inline'; document.getElementById('1702.02092v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1702.02092v1-abstract-full" style="display: none;">
        We report investigations into speaker classification of larger quantities of unlabelled speech data using small sets of manually phonemically annotated speech. The Kohonen speech typewriter is a semi-supervised method comprised of self-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a 2D array of cells that learn vector representations of the data based on neighbourhoods. In this paper, we report a method to evaluate pronunciation using multilevel SOMs with /hVd/ single syllable utterances for the study of vowels, for Australian pronunciation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02092v1-abstract-full').style.display = 'none'; document.getElementById('1702.02092v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 January, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16th Speech Science and Technology Conference (SST2016)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1611.04934">arXiv:1611.04934</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1611.04934">pdf</a>, <a href="https://arxiv.org/format/1611.04934">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HPAT: High Performance Analytics with Scripting Ease-of-Use
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Totoni%2C+E">Ehsan Totoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+T+A">Todd A. Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shpeisman%2C+T">Tatiana Shpeisman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1611.04934v2-abstract-short" style="display: inline;">
        Big data analytics requires high programmer productivity and high performance simultaneously on large-scale clusters. However, current big data analytics frameworks (e.g. Apache Spark) have prohibitive runtime overheads since they are library-based. We introduce a novel auto-parallelizing compiler approach that exploits the characteristics of the data analytics domain such as the map/reduce parall&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.04934v2-abstract-full').style.display = 'inline'; document.getElementById('1611.04934v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1611.04934v2-abstract-full" style="display: none;">
        Big data analytics requires high programmer productivity and high performance simultaneously on large-scale clusters. However, current big data analytics frameworks (e.g. Apache Spark) have prohibitive runtime overheads since they are library-based. We introduce a novel auto-parallelizing compiler approach that exploits the characteristics of the data analytics domain such as the map/reduce parallel pattern and is robust, unlike previous auto-parallelization methods. Using this approach, we build High Performance Analytics Toolkit (HPAT), which parallelizes high-level scripting (Julia) programs automatically, generates efficient MPI/C++ code, and provides resiliency. Furthermore, it provides automatic optimizations for scripting programs, such as fusion of array operations. Thus, HPAT is 369x to 2033x faster than Spark on the Cori supercomputer and 20x to 256x times on Amazon AWS.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.04934v2-abstract-full').style.display = 'none'; document.getElementById('1611.04934v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 November, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2016.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1611.01714">arXiv:1611.01714</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1611.01714">pdf</a>, <a href="https://arxiv.org/format/1611.01714">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Fine Tuning: A Modular Approach to Learning on Small Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A">Ark Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shaffer%2C+K">Kyle Shaffer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yankov%2C+A">Artem Yankov</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Corley%2C+C+D">Court D. Corley</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hodas%2C+N+O">Nathan O. Hodas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1611.01714v1-abstract-short" style="display: inline;">
        In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01714v1-abstract-full').style.display = 'inline'; document.getElementById('1611.01714v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1611.01714v1-abstract-full" style="display: none;">
        In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01714v1-abstract-full').style.display = 'none'; document.getElementById('1611.01714v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 November, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2016.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1609.09339">arXiv:1609.09339</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1609.09339">pdf</a>, <a href="https://arxiv.org/format/1609.09339">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Digital Libraries">cs.DL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Physics Education">physics.ed-ph</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1103/PhysRevPhysEducRes.13.010121">10.1103/PhysRevPhysEducRes.13.010121 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Linking Behavior in the PER Coauthorship Network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+K+A">Katharine A Anderson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Crespi%2C+M">Matthew Crespi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sayre%2C+E+C">Eleanor C Sayre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1609.09339v4-abstract-short" style="display: inline;">
        There is considerable long-term interest in understanding the dynamics of collaboration networks, and how these networks form and evolve over time. Most of the work done on the dynamics of social networks focuses on well-established communities. Work examining emerging social networks is rarer, simply because data is difficult to obtain in real time. In this paper, we use thirty years of data from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1609.09339v4-abstract-full').style.display = 'inline'; document.getElementById('1609.09339v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1609.09339v4-abstract-full" style="display: none;">
        There is considerable long-term interest in understanding the dynamics of collaboration networks, and how these networks form and evolve over time. Most of the work done on the dynamics of social networks focuses on well-established communities. Work examining emerging social networks is rarer, simply because data is difficult to obtain in real time. In this paper, we use thirty years of data from an emerging scientific community to look at that crucial early stage in the development of a social network. We show that when the field was very young, islands of individual researchers labored in relative isolation, and the coauthorship network was disconnected. Thirty years later, rather than a cluster of individuals, we find a true collaborative community, bound together by a robust collaboration network. However, this change did not take place gradually\textemdash the network remained a loose assortment of isolated individuals until the mid-2000s, when those smaller parts suddenly knit themselves together into a single whole. In the rest of this paper, we consider the role of three factors in these observed structural changes: growth, changes in social norms, and the introduction of institutions such as field-specific conferences and journals. We have data from the very earliest years of the field, a period which includes the introduction of two different institutions: the first field-specific conference, and the first field-specific journals. We also identify two relevant behavioral shifts: a discrete increase in coauthorship coincident with the first conference, and a shift among established authors away from collaborating with outsiders, towards collaborating with each other. The interaction of these factors gives us insight into the formation of collaboration networks more broadly.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1609.09339v4-abstract-full').style.display = 'none'; document.getElementById('1609.09339v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 September, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to PhysRevPER</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Phys. Rev. Phys. Educ. Res. 13, 010121 (2017)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.00435">arXiv:1607.00435</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.00435">pdf</a>, <a href="https://arxiv.org/format/1607.00435">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decoding the Encoding of Functional Brain Networks: an fMRI Classification Comparison of Non-negative Matrix Factorization (NMF), Independent Component Analysis (ICA), and Sparse Coding Algorithms
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+J">Jianwen Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Douglas%2C+P+K">Pamela K. Douglas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y+N">Ying Nian Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brody%2C+A+L">Arthur L. Brody</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Anderson%2C+A+E">Ariana E. Anderson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.00435v1-abstract-short" style="display: inline;">
        Brain networks in fMRI are typically identified using spatial independent component analysis (ICA), yet mathematical constraints such as sparse coding and positivity both provide alternate biologically-plausible frameworks for generating brain networks. Non-negative Matrix Factorization (NMF) would suppress negative BOLD signal by enforcing positivity. Spatial sparse coding algorithms ($L1$ Regula&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00435v1-abstract-full').style.display = 'inline'; document.getElementById('1607.00435v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.00435v1-abstract-full" style="display: none;">
        Brain networks in fMRI are typically identified using spatial independent component analysis (ICA), yet mathematical constraints such as sparse coding and positivity both provide alternate biologically-plausible frameworks for generating brain networks. Non-negative Matrix Factorization (NMF) would suppress negative BOLD signal by enforcing positivity. Spatial sparse coding algorithms ($L1$ Regularized Learning and K-SVD) would impose local specialization and a discouragement of multitasking, where the total observed activity in a single voxel originates from a restricted number of possible brain networks.
  The assumptions of independence, positivity, and sparsity to encode task-related brain networks are compared; the resulting brain networks for different constraints are used as basis functions to encode the observed functional activity at a given time point. These encodings are decoded using machine learning to compare both the algorithms and their assumptions, using the time series weights to predict whether a subject is viewing a video, listening to an audio cue, or at rest, in 304 fMRI scans from 51 subjects.
  For classifying cognitive activity, the sparse coding algorithm of $L1$ Regularized Learning consistently outperformed 4 variations of ICA across different numbers of networks and noise levels (p$&lt;$0.001). The NMF algorithms, which suppressed negative BOLD signal, had the poorest accuracy. Within each algorithm, encodings using sparser spatial networks (containing more zero-valued voxels) had higher classification accuracy (p$&lt;$0.001). The success of sparse coding algorithms may suggest that algorithms which enforce sparse coding, discourage multitasking, and promote local specialization may capture better the underlying source processes than those which allow inexhaustible local processes such as ICA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00435v1-abstract-full').style.display = 'none'; document.getElementById('1607.00435v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Anderson%2C+A&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Anderson%2C+A&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Anderson%2C+A&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>