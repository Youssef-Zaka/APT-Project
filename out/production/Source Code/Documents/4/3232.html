<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 75 results for author: <span class="mathjax">Khan, S</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/eess"  aria-role="search">
    
      Searching in archive <strong>eess</strong>. <a href="/search/?searchtype=author&amp;query=Khan%2C+S">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Khan, S">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Khan%2C+S&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Khan, S">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Khan%2C+S&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Khan%2C+S&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Khan%2C+S&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01649">arXiv:2205.01649</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01649">pdf</a>, <a href="https://arxiv.org/format/2205.01649">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Enriched Features for Fast Image Restoration and Enhancement
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01649v1-abstract-short" style="display: inline;">
        Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01649v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01649v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01649v1-abstract-full" style="display: none;">
        Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatial details are preserved but the contextual information cannot be precisely encoded. In the latter case, generated outputs are semantically reliable but spatially less accurate. This paper presents a new architecture with a holistic goal of maintaining spatially-precise high-resolution representations through the entire network, and receiving complementary contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing the following key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) non-local attention mechanism for capturing contextual information, and (d) attention based multi-scale feature aggregation. Our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on six real image benchmark datasets demonstrate that our method, named as MIRNet-v2 , achieves state-of-the-art results for a variety of image processing tasks, including defocus deblurring, image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNetv2
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01649v1-abstract-full').style.display = 'none'; document.getElementById('2205.01649v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This article supersedes arXiv:2003.06792. Accepted for publication in TPAMI</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.10846">arXiv:2204.10846</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.10846">pdf</a>, <a href="https://arxiv.org/format/2204.10846">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Video Object Segmentation via Cutout Prediction and Tagging
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Kini%2C+J">Jyoti Kini</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shah%2C+M">Mubarak Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.10846v1-abstract-short" style="display: inline;">
        We propose a novel self-supervised Video Object Segmentation (VOS) approach that strives to achieve better object-background discriminability for accurate object segmentation. Distinct from previous self-supervised VOS methods, our approach is based on a discriminative learning loss formulation that takes into account both object and background information to ensure object-background discriminabil&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10846v1-abstract-full').style.display = 'inline'; document.getElementById('2204.10846v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.10846v1-abstract-full" style="display: none;">
        We propose a novel self-supervised Video Object Segmentation (VOS) approach that strives to achieve better object-background discriminability for accurate object segmentation. Distinct from previous self-supervised VOS methods, our approach is based on a discriminative learning loss formulation that takes into account both object and background information to ensure object-background discriminability, rather than using only object appearance. The discriminative learning loss comprises cutout-based reconstruction (cutout region represents part of a frame, whose pixels are replaced with some constant values) and tag prediction loss terms. The cutout-based reconstruction term utilizes a simple cutout scheme to learn the pixel-wise correspondence between the current and previous frames in order to reconstruct the original current frame with added cutout region in it. The introduced cutout patch guides the model to focus as much on the significant features of the object of interest as the less significant ones, thereby implicitly equipping the model to address occlusion-based scenarios. Next, the tag prediction term encourages object-background separability by grouping tags of all pixels in the cutout region that are similar, while separating them from the tags of the rest of the reconstructed frame pixels. Additionally, we introduce a zoom-in scheme that addresses the problem of small object segmentation by capturing fine structural information at multiple scales. Our proposed approach, termed CT-VOS, achieves state-of-the-art results on two challenging benchmarks: DAVIS-2017 and Youtube-VOS. A detailed ablation showcases the importance of the proposed loss formulation to effectively capture object-background discriminability and the impact of our zoom-in scheme to accurately segment small-sized objects.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.10846v1-abstract-full').style.display = 'none'; document.getElementById('2204.10846v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.07756">arXiv:2204.07756</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.07756">pdf</a>, <a href="https://arxiv.org/format/2204.07756">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual Attention Methods in Deep Learning: An In-Depth Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Hassanin%2C+M">Mohammed Hassanin</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Anwar%2C+S">Saeed Anwar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Radwan%2C+I">Ibrahim Radwan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad S Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mian%2C+A">Ajmal Mian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.07756v2-abstract-short" style="display: inline;">
        Inspired by the human cognitive system, attention is a mechanism that imitates the human cognitive awareness about specific information, amplifying critical details to focus more on the essential aspects of data. Deep learning has employed attention to boost performance for many applications. Interestingly, the same attention design can suit processing different data modalities and can easily be i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07756v2-abstract-full').style.display = 'inline'; document.getElementById('2204.07756v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.07756v2-abstract-full" style="display: none;">
        Inspired by the human cognitive system, attention is a mechanism that imitates the human cognitive awareness about specific information, amplifying critical details to focus more on the essential aspects of data. Deep learning has employed attention to boost performance for many applications. Interestingly, the same attention design can suit processing different data modalities and can easily be incorporated into large networks. Furthermore, multiple complementary attention mechanisms can be incorporated in one network. Hence, attention techniques have become extremely attractive. However, the literature lacks a comprehensive survey specific to attention techniques to guide researchers in employing attention in their deep models. Note that, besides being demanding in terms of training data and computational resources, transformers only cover a single category in self-attention out of the many categories available. We fill this gap and provide an in-depth survey of 50 attention techniques categorizing them by their most prominent features. We initiate our discussion by introducing the fundamental concepts behind the success of attention mechanism. Next, we furnish some essentials such as the strengths and limitations of each attention category, describe their fundamental building blocks, basic formulations with primary usage, and applications specifically for computer vision. We also discuss the challenges and open questions related to attention mechanism in general. Finally, we recommend possible future research directions for deep attention.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.07756v2-abstract-full').style.display = 'none'; document.getElementById('2204.07756v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04218">arXiv:2204.04218</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04218">pdf</a>, <a href="https://arxiv.org/format/2204.04218">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Georgescu%2C+M">Mariana-Iuliana Georgescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Miron%2C+A">Andreea-Iuliana Miron</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Savencu%2C+O">Olivian Savencu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ristea%2C+N">Nicolae-Catalin Ristea</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Verga%2C+N">Nicolae Verga</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04218v2-abstract-short" style="display: inline;">
        Super-resolving medical images can help physicians in providing more accurate diagnostics. In many situations, computed tomography (CT) or magnetic resonance imaging (MRI) techniques output several scans (modes) during a single investigation, which can jointly be used (in a multimodal fashion) to further boost the quality of super-resolution results. To this end, we propose a novel multimodal mult&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04218v2-abstract-full').style.display = 'inline'; document.getElementById('2204.04218v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04218v2-abstract-full" style="display: none;">
        Super-resolving medical images can help physicians in providing more accurate diagnostics. In many situations, computed tomography (CT) or magnetic resonance imaging (MRI) techniques output several scans (modes) during a single investigation, which can jointly be used (in a multimodal fashion) to further boost the quality of super-resolution results. To this end, we propose a novel multimodal multi-head convolutional attention module to super-resolve CT and MRI scans. Our attention module uses the convolution operation to perform joint spatial-channel attention on multiple concatenated input tensors, where the kernel (receptive field) size controls the reduction rate of the spatial attention and the number of convolutional filters controls the reduction rate of the channel attention, respectively. We introduce multiple attention heads, each head having a distinct receptive field size corresponding to a particular reduction rate for the spatial attention. We integrate our multimodal multi-head convolutional attention (MMHCA) into two deep neural architectures for super-resolution and conduct experiments on three data sets. Our empirical results show the superiority of our attention module over the state-of-the-art attention mechanisms used in super-resolution. Moreover, we conduct an ablation study to assess the impact of the components involved in our attention module, e.g. the number of inputs or the number of heads.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04218v2-abstract-full').style.display = 'none'; document.getElementById('2204.04218v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.01152">arXiv:2204.01152</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.01152">pdf</a>, <a href="https://arxiv.org/format/2204.01152">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intelligent Reflective Surface Deployment in 6G: A Comprehensive Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Naeem%2C+F">Faisal Naeem</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kaddoum%2C+G">Georges Kaddoum</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Saud Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+K+S">Komal S. Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.01152v1-abstract-short" style="display: inline;">
        Intelligent reflecting surfaces (IRSs) are considered a promising technology that can smartly reconfigure the wireless environment to enhance the performance of future wireless networks. However, the deployment of IRSs still faces challenges due to highly dynamic and mobile unmanned aerial vehicle (UAV) enabled wireless environments to achieve higher capacity. This paper sheds light on the differe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01152v1-abstract-full').style.display = 'inline'; document.getElementById('2204.01152v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.01152v1-abstract-full" style="display: none;">
        Intelligent reflecting surfaces (IRSs) are considered a promising technology that can smartly reconfigure the wireless environment to enhance the performance of future wireless networks. However, the deployment of IRSs still faces challenges due to highly dynamic and mobile unmanned aerial vehicle (UAV) enabled wireless environments to achieve higher capacity. This paper sheds light on the different deployment strategies for IRSs in future terrestrial and non-terrestrial networks. Specifically, in this paper, we introduce key theoretical concepts underlying the IRS paradigm and discuss the design aspects related to the deployment of IRSs in 6G networks. We also explore optimization-based IRS deployment techniques to improve system performance in terrestrial and aerial IRSs. Furthermore, we survey model-free reinforcement learning (RL) techniques from the deployment aspect to address the challenges of achieving higher capacity in complex and mobile IRS-assisted UAV wireless systems. Finally, we highlight challenges and future research directions from the deployment aspect of IRSs for improving system performance for the future 6G network.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01152v1-abstract-full').style.display = 'none'; document.getElementById('2204.01152v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 3 Figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.00112">arXiv:2204.00112</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.00112">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gallium Oxide Heterojunction Diodes for Improved High-Temperature Performance
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Sohel%2C+S+H">Shahadat H. Sohel</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kotecha%2C+R">Ramchandra Kotecha</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+I+S">Imran S Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Heinselman%2C+K+N">Karen N. Heinselman</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Narumanchi%2C+S">Sreekant Narumanchi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tellekamp%2C+M+B">M Brooks Tellekamp</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zakutayev%2C+A">Andriy Zakutayev</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.00112v1-abstract-short" style="display: inline;">
        $β$-Ga${_2}$O${_3}&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00112v1-abstract-full').style.display = 'inline'; document.getElementById('2204.00112v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.00112v1-abstract-full" style="display: none;">
        $β$-Ga${_2}$O${_3}$ based semiconductor devices are expected to have significantly improved high-power and high-temperature performance due to its ultra-wide bandgap of close to 5 eV. However, the high-temperature operation of these ultra-wide-bandgap devices is usually limited by the relatively low 1-2 eV built-in potential at the Schottky barrier with most high-work-function metals. Here, we report heterojunction p-NiO/n-$β$-Ga${_2}$O${_3}$ diodes fabrication and optimization for high-temperature device applications, demonstrating a current rectification ratio of more than 10${^6}$ at 410°C. The NiO heterojunction diode can achieve higher turn-on voltage and lower reverse leakage current compared to the Ni-based Schottky diode fabricated on the same single crystal $β$-Ga${_2}$O${_3}$ substrate, despite charge transport dominated by interfacial recombination. Electrical characterization and device modeling show that these advantages are due to a higher built-in potential and additional band offset. These results suggest that heterojunction p-n diodes based on $β$-Ga${_2}$O${_3}$ can significantly improve high-temperature electronic device and sensor performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00112v1-abstract-full').style.display = 'none'; document.getElementById('2204.00112v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.08262">arXiv:2202.08262</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.08262">pdf</a>, <a href="https://arxiv.org/format/2202.08262">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Phase Aberration Robust Beamformer for Planewave US Using Self-Supervised Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Huh%2C+J">Jaeyoung Huh</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+J+C">Jong Chul Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.08262v1-abstract-short" style="display: inline;">
        Ultrasound (US) is widely used for clinical imaging applications thanks to its real-time and non-invasive nature. However, its lesion detectability is often limited in many applications due to the phase aberration artefact caused by variations in the speed of sound (SoS) within body parts. To address this, here we propose a novel self-supervised 3D CNN that enables phase aberration robust plane-wa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.08262v1-abstract-full').style.display = 'inline'; document.getElementById('2202.08262v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.08262v1-abstract-full" style="display: none;">
        Ultrasound (US) is widely used for clinical imaging applications thanks to its real-time and non-invasive nature. However, its lesion detectability is often limited in many applications due to the phase aberration artefact caused by variations in the speed of sound (SoS) within body parts. To address this, here we propose a novel self-supervised 3D CNN that enables phase aberration robust plane-wave imaging. Instead of aiming at estimating the SoS distribution as in conventional methods, our approach is unique in that the network is trained in a self-supervised manner to robustly generate a high-quality image from various phase aberrated images by modeling the variation in the speed of sound as stochastic. Experimental results using real measurements from tissue-mimicking phantom and \textit{in vivo} scans confirmed that the proposed method can significantly reduce the phase aberration artifacts and improve the visual quality of deep scans.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.08262v1-abstract-full').style.display = 'none'; document.getElementById('2202.08262v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 12 figures, submitted to IEEE-TMI</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.06372">arXiv:2202.06372</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.06372">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+A">Asifullah Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+H">Saddam Hussain Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Saif%2C+M">Mahrukh Saif</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Batool%2C+A">Asiya Batool</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sohail%2C+A">Anabia Sohail</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+M+W">Muhammad Waleed Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.06372v2-abstract-short" style="display: inline;">
        The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing threat to humans worldwide, creating a health crisis that infected millions of lives, as well as devastating the global economy. Deep learning (DL) techniques have proved helpful in analysis and delineation of infectious regions in radiological images in a timely manner. This paper makes an in-depth survey of DL techniques&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.06372v2-abstract-full').style.display = 'inline'; document.getElementById('2202.06372v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.06372v2-abstract-full" style="display: none;">
        The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing threat to humans worldwide, creating a health crisis that infected millions of lives, as well as devastating the global economy. Deep learning (DL) techniques have proved helpful in analysis and delineation of infectious regions in radiological images in a timely manner. This paper makes an in-depth survey of DL techniques and draws a taxonomy based on diagnostic strategies and learning approaches. DL techniques are systematically categorized into classification, segmentation, and multi-stage approaches for COVID-19 diagnosis at image and region level analysis. Each category includes pre-trained and custom-made Convolutional Neural Network architectures for detecting COVID-19 infection in radiographic imaging modalities; X-Ray, and Computer Tomography (CT). Furthermore, a discussion is made on challenges in developing diagnostic techniques such as cross-platform interoperability and examining imaging modality. Similarly, a review of the various methodologies and performance measures used in these techniques is also presented. This survey provides an insight into the promising areas of research in DL for analyzing radiographic images, and further accelerates the research in designing customized DL based diagnostic tools for effectively dealing with new variants of COVID-19 and emerging challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.06372v2-abstract-full').style.display = 'none'; document.getElementById('2202.06372v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Pages: 44, Figures: 7, Tables: 14</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.06039">arXiv:2202.06039</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.06039">pdf</a>, <a href="https://arxiv.org/format/2202.06039">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Application of Modular Vehicle Technology to Mitigate Bus Bunching
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+Z+S">Zaid Saeed Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=He%2C+W">Weili He</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Menendez%2C+M">Monica Menendez</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.06039v1-abstract-short" style="display: inline;">
        The stochastic nature of public transport systems leads to headway variability and bus bunching, causing both operator and passenger cost to increase significantly. Traditional strategies to counter bus bunching, including bus-holding, stop-skipping, and bus substitution/insertion, suffer from trade-offs and shortcomings. Autonomous modular vehicle (AMV) technology provides an additional level of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.06039v1-abstract-full').style.display = 'inline'; document.getElementById('2202.06039v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.06039v1-abstract-full" style="display: none;">
        The stochastic nature of public transport systems leads to headway variability and bus bunching, causing both operator and passenger cost to increase significantly. Traditional strategies to counter bus bunching, including bus-holding, stop-skipping, and bus substitution/insertion, suffer from trade-offs and shortcomings. Autonomous modular vehicle (AMV) technology provides an additional level of flexibility in bus dispatching and operations, which can offer significant benefits in mitigating bus bunching compared to strategies available with conventional buses. This paper introduces a novel alternative to stop-skipping by leveraging the new capabilities offered by AMVs (in particular, en-route coupling and decoupling of modular units). We develop a simple bus-splitting strategy that directs a modular bus to decouple into individual units when it experiences a headway longer than a given threshold. We then use a macroscopic simulation to present a proof-of-concept evaluation of the proposed modular strategy compared to a benchmark traditional stop-skipping strategy and the base (no control) case. We find that the proposed strategy outperforms the benchmark in decreasing each of the three travel time components: waiting time, in-vehicle time, and walking time (which it eliminates completely). It therefore reduces the overhead of bus bunching and thus the travel cost by more than twice as much as the benchmark for busy bus lines. Simultaneously, it also reduces headway variability to a comparable degree. Furthermore, we analyze different control thresholds for applying the proposed strategy, and show that it is most effective when applied proactively, i.e. with the control action being triggered even by small headway deviations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.06039v1-abstract-full').style.display = 'none'; document.getElementById('2202.06039v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 February, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages, 10 figures, 3 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.09873">arXiv:2201.09873</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.09873">pdf</a>, <a href="https://arxiv.org/format/2201.09873">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transformers in Medical Imaging: A Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Shamshad%2C+F">Fahad Shamshad</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+M+H">Muhammad Haris Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Fu%2C+H">Huazhu Fu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.09873v1-abstract-short" style="display: inline;">
        Following unprecedented success on the natural language tasks, Transformers have been successfully applied to several computer vision problems, achieving state-of-the-art results and prompting researchers to reconsider the supremacy of convolutional neural networks (CNNs) as {de facto} operators. Capitalizing on these advances in computer vision, the medical imaging field has also witnessed growin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.09873v1-abstract-full').style.display = 'inline'; document.getElementById('2201.09873v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.09873v1-abstract-full" style="display: none;">
        Following unprecedented success on the natural language tasks, Transformers have been successfully applied to several computer vision problems, achieving state-of-the-art results and prompting researchers to reconsider the supremacy of convolutional neural networks (CNNs) as {de facto} operators. Capitalizing on these advances in computer vision, the medical imaging field has also witnessed growing interest for Transformers that can capture global context compared to CNNs with local receptive fields. Inspired from this transition, in this survey, we attempt to provide a comprehensive review of the applications of Transformers in medical imaging covering various aspects, ranging from recently proposed architectural designs to unsolved issues. Specifically, we survey the use of Transformers in medical image segmentation, detection, classification, reconstruction, synthesis, registration, clinical report generation, and other tasks. In particular, for each of these applications, we develop taxonomy, identify application-specific challenges as well as provide insights to solve them, and highlight recent trends. Further, we provide a critical discussion of the field&#39;s current state as a whole, including the identification of key challenges, open problems, and outlining promising future directions. We hope this survey will ignite further interest in the community and provide researchers with an up-to-date reference regarding applications of Transformer models in medical imaging. Finally, to cope with the rapid development in this field, we intend to regularly update the relevant latest papers and their open-source implementations at \url{https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.09873v1-abstract-full').style.display = 'none'; document.getElementById('2201.09873v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">41 pages, \url{https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging}</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.05373">arXiv:2201.05373</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.05373">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A New Deep Hybrid Boosted and Ensemble Learning-based Brain Tumor Analysis using MRI
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Zahoor%2C+M+M">Mirza Mumtaz Zahoor</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Qureshi%2C+S+A">Shahzad Ahmad Qureshi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+H">Saddam Hussain Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+A">Asifullah Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.05373v2-abstract-short" style="display: inline;">
        Brain tumors analysis is important in timely diagnosis and effective treatment to cure patients. Tumor analysis is challenging because of tumor morphology like size, location, texture, and heteromorphic appearance in the medical images. In this regard, a novel two-phase deep learning-based framework is proposed to detect and categorize brain tumors in magnetic resonance images (MRIs). In the first&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.05373v2-abstract-full').style.display = 'inline'; document.getElementById('2201.05373v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.05373v2-abstract-full" style="display: none;">
        Brain tumors analysis is important in timely diagnosis and effective treatment to cure patients. Tumor analysis is challenging because of tumor morphology like size, location, texture, and heteromorphic appearance in the medical images. In this regard, a novel two-phase deep learning-based framework is proposed to detect and categorize brain tumors in magnetic resonance images (MRIs). In the first phase, a novel deep boosted features and ensemble classifiers (DBF-EC) scheme is proposed to detect tumor MRI images from healthy individuals effectively. The deep boosted feature space is achieved through the customized and well-performing deep convolutional neural networks (CNNs), and consequently, fed into the ensemble of machine learning (ML) classifiers. While in the second phase, a new hybrid features fusion-based brain tumor classification approach is proposed, comprised of dynamic-static feature and ML classifier to categorize different tumor types. The dynamic features are extracted from the proposed BRAIN-RENet CNN, which carefully learns heteromorphic and inconsistent behavior of various tumors, while the static features are extracted using HOG. The effectiveness of the proposed two-phase brain tumor analysis framework is validated on two standard benchmark datasets; collected from Kaggle and Figshare containing different types of tumor, including glioma, meningioma, pituitary, and normal images. Experimental results proved that the proposed DBF-EC detection scheme outperforms and achieved accuracy (99.56%), precision (0.9991), recall (0.9899), F1-Score (0.9945), MCC (0.9892), and AUC-PR (0.9990). While the classification scheme, the joint employment of the deep features fusion of proposed BRAIN-RENet and HOG features improves performance significantly in terms of recall (0.9913), precision (0.9906), F1-Score (0.9909), and accuracy (99.20%) on diverse datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.05373v2-abstract-full').style.display = 'none'; document.getElementById('2201.05373v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 January, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">26 pages, 9 figures, 8 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.02574">arXiv:2201.02574</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.02574">pdf</a>, <a href="https://arxiv.org/format/2201.02574">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Incremental Learning Approach to Automatically Recognize Pulmonary Diseases from the Multi-vendor Chest Radiographs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Sirshar%2C+M">Mehreen Sirshar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hassan%2C+T">Taimur Hassan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Akram%2C+M+U">Muhammad Usman Akram</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+A">Shoab Ahmed Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.02574v2-abstract-short" style="display: inline;">
        Pulmonary diseases can cause severe respiratory problems, leading to sudden death if not treated timely. Many researchers have utilized deep learning systems to diagnose pulmonary disorders using chest X-rays (CXRs). However, such systems require exhaustive training efforts on large-scale data to effectively diagnose chest abnormalities. Furthermore, procuring such large-scale data is often infeas&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.02574v2-abstract-full').style.display = 'inline'; document.getElementById('2201.02574v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.02574v2-abstract-full" style="display: none;">
        Pulmonary diseases can cause severe respiratory problems, leading to sudden death if not treated timely. Many researchers have utilized deep learning systems to diagnose pulmonary disorders using chest X-rays (CXRs). However, such systems require exhaustive training efforts on large-scale data to effectively diagnose chest abnormalities. Furthermore, procuring such large-scale data is often infeasible and impractical, especially for rare diseases. With the recent advances in incremental learning, researchers have periodically tuned deep neural networks to learn different classification tasks with few training examples. Although, such systems can resist catastrophic forgetting, they treat the knowledge representations independently of each other, and this limits their classification performance. Also, to the best of our knowledge, there is no incremental learning-driven image diagnostic framework that is specifically designed to screen pulmonary disorders from the CXRs. To address this, we present a novel framework that can learn to screen different chest abnormalities incrementally. In addition to this, the proposed framework is penalized through an incremental learning loss function that infers Bayesian theory to recognize structural and semantic inter-dependencies between incrementally learned knowledge representations to diagnose the pulmonary diseases effectively, regardless of the scanner specifications. We tested the proposed framework on five public CXR datasets containing different chest abnormalities, where it outperformed various state-of-the-art system through various metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.02574v2-abstract-full').style.display = 'none'; document.getElementById('2201.02574v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 January, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Computers in Biology and Medicine</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Computers in Biology and Medicine, 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.02896">arXiv:2112.02896</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.02896">pdf</a>, <a href="https://arxiv.org/format/2112.02896">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tunable Image Quality Control of 3-D Ultrasound using Switchable CycleGAN
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Huh%2C+J">Jaeyoung Huh</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Choi%2C+S">Sungjin Choi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shin%2C+D">Dongkuk Shin</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lee%2C+E+S">Eun Sun Lee</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+J+C">Jong Chul Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.02896v1-abstract-short" style="display: inline;">
        In contrast to 2-D ultrasound (US) for uniaxial plane imaging, a 3-D US imaging system can visualize a volume along three axial planes. This allows for a full view of the anatomy, which is useful for gynecological (GYN) and obstetrical (OB) applications. Unfortunately, the 3-D US has an inherent limitation in resolution compared to the 2-D US. In the case of 3-D US with a 3-D mechanical probe, for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.02896v1-abstract-full').style.display = 'inline'; document.getElementById('2112.02896v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.02896v1-abstract-full" style="display: none;">
        In contrast to 2-D ultrasound (US) for uniaxial plane imaging, a 3-D US imaging system can visualize a volume along three axial planes. This allows for a full view of the anatomy, which is useful for gynecological (GYN) and obstetrical (OB) applications. Unfortunately, the 3-D US has an inherent limitation in resolution compared to the 2-D US. In the case of 3-D US with a 3-D mechanical probe, for example, the image quality is comparable along the beam direction, but significant deterioration in image quality is often observed in the other two axial image planes. To address this, here we propose a novel unsupervised deep learning approach to improve 3-D US image quality. In particular, using {\em unmatched} high-quality 2-D US images as a reference, we trained a recently proposed switchable CycleGAN architecture so that every mapping plane in 3-D US can learn the image quality of 2-D US images. Thanks to the switchable architecture, our network can also provide real-time control of image enhancement level based on user preference, which is ideal for a user-centric scanner setup. Extensive experiments with clinical evaluation confirm that our method offers significantly improved image quality as well user-friendly flexibility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.02896v1-abstract-full').style.display = 'none'; document.getElementById('2112.02896v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 December, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.09416">arXiv:2111.09416</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.09416">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Highly Accurate and Reliable Wireless Network Slicing in 5th Generation Networks: A Hybrid Deep Learning Approach
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Sulaiman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Suleman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ali%2C+Y">Yasir Ali</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khalid%2C+M">Muhammad Khalid</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ullah%2C+Z">Zahid Ullah</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mumtaz%2C+S">Shahid Mumtaz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.09416v1-abstract-short" style="display: inline;">
        In the current era, the next-generation networks like 5th generation (5G) and 6th generation (6G) networks require high security, low latency with a high reliable standards and capacity. In these networks, reconfigurable wireless network slicing is considered as one of the key elements for 5G and 6G networks. A reconfigurable slicing allows the operators to run various instances of the network usi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.09416v1-abstract-full').style.display = 'inline'; document.getElementById('2111.09416v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.09416v1-abstract-full" style="display: none;">
        In the current era, the next-generation networks like 5th generation (5G) and 6th generation (6G) networks require high security, low latency with a high reliable standards and capacity. In these networks, reconfigurable wireless network slicing is considered as one of the key elements for 5G and 6G networks. A reconfigurable slicing allows the operators to run various instances of the network using a single infrastructure for a better quality of services (QoS). The QoS can be achieved by reconfiguring and optimizing these networks using Artificial intelligence and machine learning algorithms. To develop a smart decision-making mechanism for network management and restricting network slice failures, machine learning-enabled reconfigurable wireless network solutions are required. In this paper, we propose a hybrid deep learning model that consists of a convolution neural network (CNN) and long short term memory (LSTM). The CNN performs resource allocation, network reconfiguration, and slice selection while the LSTM is used for statistical information (load balancing, error rate etc.) regarding network slices. The applicability of the proposed model is validated by using multiple unknown devices, slice failure, and overloading conditions. The overall accuracy of 95.17% is achieved by the proposed model that reflects its applicability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.09416v1-abstract-full').style.display = 'none'; document.getElementById('2111.09416v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.03207">arXiv:2111.03207</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.03207">pdf</a>, <a href="https://arxiv.org/format/2111.03207">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Artificial Neural Network-Based Voltage Control of DC/DC Converter for DC Microgrid Applications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+H+S">Hussain Sarwar Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mohamed%2C+I+S">Ihab S. Mohamed</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kauhaniemi%2C+K">Kimmo Kauhaniemi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+L">Lantao Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.03207v1-abstract-short" style="display: inline;">
        The rapid growth of renewable energy technology enables the concept of microgrid (MG) to be widely accepted in the power systems. Due to the advantages of the DC distribution system such as easy integration of energy storage and less system loss, DC MG attracts significant attention nowadays. The linear controller such as PI or PID is matured and extensively used by the power electronics industry,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03207v1-abstract-full').style.display = 'inline'; document.getElementById('2111.03207v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.03207v1-abstract-full" style="display: none;">
        The rapid growth of renewable energy technology enables the concept of microgrid (MG) to be widely accepted in the power systems. Due to the advantages of the DC distribution system such as easy integration of energy storage and less system loss, DC MG attracts significant attention nowadays. The linear controller such as PI or PID is matured and extensively used by the power electronics industry, but their performance is not optimal as system parameters are changed. In this study, an artificial neural network (ANN) based voltage control strategy is proposed for the DC-DC boost converter. In this paper, the model predictive control (MPC) is used as an expert, which provides the data to train the proposed ANN. As ANN is tuned finely, then it is utilized directly to control the step-up DC converter. The main advantage of the ANN is that the neural network system identification decreases the inaccuracy of the system model even with inaccurate parameters and has less computational burden compared to MPC due to its parallel structure. To validate the performance of the proposed ANN, extensive MATLAB/Simulink simulations are carried out. The simulation results show that the ANN-based control strategy has better performance under different loading conditions comparison to the PI controller. The accuracy of the trained ANN model is about 97%, which makes it suitable to be used for DC microgrid applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03207v1-abstract-full').style.display = 'none'; document.getElementById('2111.03207v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted for publication at the 6th IEEE Workshop on the Electronic Grid (eGrid 2021). It has 6 pages, 9 figures, 2 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.06400">arXiv:2110.06400</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.06400">pdf</a>, <a href="https://arxiv.org/format/2110.06400">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CyTran: Cycle-Consistent Transformers for Non-Contrast to Contrast CT Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Ristea%2C+N">Nicolae-Catalin Ristea</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Miron%2C+A">Andreea-Iuliana Miron</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Savencu%2C+O">Olivian Savencu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Georgescu%2C+M">Mariana-Iuliana Georgescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Verga%2C+N">Nicolae Verga</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.06400v2-abstract-short" style="display: inline;">
        We propose a novel approach to translate unpaired contrast computed tomography (CT) scans to non-contrast CT scans and the other way around. Solving this task has two important applications: (i) to automatically generate contrast CT scans for patients for whom injecting contrast substance is not an option, and (ii) to enhance alignment between contrast and non-contrast CT by reducing the differenc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.06400v2-abstract-full').style.display = 'inline'; document.getElementById('2110.06400v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.06400v2-abstract-full" style="display: none;">
        We propose a novel approach to translate unpaired contrast computed tomography (CT) scans to non-contrast CT scans and the other way around. Solving this task has two important applications: (i) to automatically generate contrast CT scans for patients for whom injecting contrast substance is not an option, and (ii) to enhance alignment between contrast and non-contrast CT by reducing the differences induced by the contrast substance before registration. Our approach is based on cycle-consistent generative adversarial convolutional transformers, for short, CyTran. Our neural model can be trained on unpaired images, due to the integration of a cycle-consistency loss. To deal with high-resolution images, we design a hybrid architecture based on convolutional and multi-head attention layers. In addition, we introduce a novel data set, Coltea-Lung-CT-100W, containing 3D triphasic lung CT scans (with a total of 37,290 images) collected from 100 female patients. Each scan contains three phases (non-contrast, early portal venous, and late arterial), allowing us to perform experiments to compare our novel approach with state-of-the-art methods for image style transfer. Our empirical results show that CyTran outperforms all competing methods. Moreover, we show that CyTran can be employed as a preliminary step to improve a state-of-the-art medical image alignment method. We release our novel model and data set as open source at: https://github.com/ristea/cycle-transformer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.06400v2-abstract-full').style.display = 'none'; document.getElementById('2110.06400v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 October, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.05201">arXiv:2110.05201</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.05201">pdf</a>, <a href="https://arxiv.org/format/2110.05201">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Performance Analysis of Fractional Learning Algorithms
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Wahab%2C+A">Abdul Wahab</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Naseem%2C+I">Imran Naseem</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+J+C">Jong Chul Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.05201v1-abstract-short" style="display: inline;">
        Fractional learning algorithms are trending in signal processing and adaptive filtering recently. However, it is unclear whether the proclaimed superiority over conventional algorithms is well-grounded or is a myth as their performance has never been extensively analyzed. In this article, a rigorous analysis of fractional variants of the least mean squares and steepest descent algorithms is perfor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.05201v1-abstract-full').style.display = 'inline'; document.getElementById('2110.05201v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.05201v1-abstract-full" style="display: none;">
        Fractional learning algorithms are trending in signal processing and adaptive filtering recently. However, it is unclear whether the proclaimed superiority over conventional algorithms is well-grounded or is a myth as their performance has never been extensively analyzed. In this article, a rigorous analysis of fractional variants of the least mean squares and steepest descent algorithms is performed. Some critical schematic kinks in fractional learning algorithms are identified. Their origins and consequences on the performance of the learning algorithms are discussed and swift ready-witted remedies are proposed. Apposite numerical experiments are conducted to discuss the convergence and efficiency of the fractional learning algorithms in stochastic environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.05201v1-abstract-full').style.display = 'none'; document.getElementById('2110.05201v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">29 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.14249">arXiv:2109.14249</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.14249">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multipath CNN with alpha matte inference for knee tissue segmentation from MRI
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Sheheryar Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Azam%2C+B">Basim Azam</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yao%2C+Y">Yongcheng Yao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chen%2C+W">Weitian Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.14249v1-abstract-short" style="display: inline;">
        Precise segmentation of knee tissues from magnetic resonance imaging (MRI) is critical in quantitative imaging and diagnosis. Convolutional neural networks (CNNs), which are state of the art, have limitations owing to the lack of image-specific adaptation, such as low tissue contrasts and structural inhomogeneities, thereby leading to incomplete segmentation results. This paper presents a deep lea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.14249v1-abstract-full').style.display = 'inline'; document.getElementById('2109.14249v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.14249v1-abstract-full" style="display: none;">
        Precise segmentation of knee tissues from magnetic resonance imaging (MRI) is critical in quantitative imaging and diagnosis. Convolutional neural networks (CNNs), which are state of the art, have limitations owing to the lack of image-specific adaptation, such as low tissue contrasts and structural inhomogeneities, thereby leading to incomplete segmentation results. This paper presents a deep learning based automatic segmentation framework for knee tissue segmentation. A novel multipath CNN-based method is proposed, which consists of an encoder decoder-based segmentation network in combination with a low rank tensor-reconstructed segmentation network. Low rank reconstruction in MRI tensor sub-blocks is introduced to exploit the structural and morphological variations in knee tissues. To further improve the segmentation from CNNs, trimap generation, which effectively utilizes superimposed regions, is proposed for defining high, medium and low confidence regions from the multipath CNNs. The secondary path with low rank reconstructed input mitigates the conditions in which the primary segmentation network can potentially fail and overlook the boundary regions. The outcome of the segmentation is solved as an alpha matting problem by blending the trimap with the source input. Experiments on Osteoarthritis Initiative (OAI) datasets and a self prepared scan validate the effectiveness of the proposed method. We specifically demonstrate the application of the proposed method in a cartilage segmentation based thickness map for diagnosis purposes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.14249v1-abstract-full').style.display = 'none'; document.getElementById('2109.14249v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.11720">arXiv:2108.11720</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.11720">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Segmentation of Shoulder Muscle MRI Using a New Region and Edge based Deep Auto-Encoder
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+H">Saddam Hussain Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+A">Asifullah Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lee%2C+Y+S">Yeon Soo Lee</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hassan%2C+M">Mehdi Hassan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=jeong%2C+W+K">Woong Kyo jeong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.11720v1-abstract-short" style="display: inline;">
        Automatic segmentation of shoulder muscle MRI is challenging due to the high variation in muscle size, shape, texture, and spatial position of tears. Manual segmentation of tear and muscle portion is hard, time-consuming, and subjective to pathological expertise. This work proposes a new Region and Edge-based Deep Auto-Encoder (RE-DAE) for shoulder muscle MRI segmentation. The proposed RE-DAE harm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.11720v1-abstract-full').style.display = 'inline'; document.getElementById('2108.11720v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.11720v1-abstract-full" style="display: none;">
        Automatic segmentation of shoulder muscle MRI is challenging due to the high variation in muscle size, shape, texture, and spatial position of tears. Manual segmentation of tear and muscle portion is hard, time-consuming, and subjective to pathological expertise. This work proposes a new Region and Edge-based Deep Auto-Encoder (RE-DAE) for shoulder muscle MRI segmentation. The proposed RE-DAE harmoniously employs average and max-pooling operation in the encoder and decoder blocks of the Convolutional Neural Network (CNN). Region-based segmentation incorporated in the Deep Auto-Encoder (DAE) encourages the network to extract smooth and homogenous regions. In contrast, edge-based segmentation tries to learn the boundary and anatomical information. These two concepts, systematically combined in a DAE, generate a discriminative and sparse hybrid feature space (exploiting both region homogeneity and boundaries). Moreover, the concept of static attention is exploited in the proposed RE-DAE that helps in effectively learning the tear region. The performances of the proposed MRI segmentation based DAE architectures have been tested using a 3D MRI shoulder muscle dataset using the hold-out cross-validation technique. The MRI data has been collected from the Korea University Anam Hospital, Seoul, South Korea. Experimental comparisons have been conducted by employing innovative custom-made and existing pre-trained CNN architectures both using transfer learning and fine-tuning. Objective evaluation on the muscle datasets using the proposed SA-RE-DAE showed a dice similarity of 85.58% and 87.07%, an accuracy of 81.57% and 95.58% for tear and muscle regions, respectively. The high visual quality and the objective result suggest that the proposed SA-RE-DAE is able to correctly segment tear and muscle regions in shoulder muscle MRI for better clinical decisions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.11720v1-abstract-full').style.display = 'none'; document.getElementById('2108.11720v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Pages: 23, 8 Figures, 2 Tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.01127">arXiv:2108.01127</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.01127">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hybrid Quantum-Classical Neural Network for Incident Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+Z">Zadid Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+M">Sakib Mahmud Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tine%2C+J+M">Jean Michel Tine</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Comert%2C+A+T">Ayse Turhan Comert</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Rice%2C+D">Diamon Rice</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Comert%2C+G">Gurcan Comert</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Michalaka%2C+D">Dimitra Michalaka</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mwakalonge%2C+J">Judith Mwakalonge</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Majumdar%2C+R">Reek Majumdar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chowdhury%2C+M">Mashrur Chowdhury</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.01127v1-abstract-short" style="display: inline;">
        The efficiency and reliability of real-time incident detection models directly impact the affected corridors&#39; traffic safety and operational conditions. The recent emergence of cloud-based quantum computing infrastructure and innovations in noisy intermediate-scale quantum devices have revealed a new era of quantum-enhanced algorithms that can be leveraged to improve real-time incident detection a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.01127v1-abstract-full').style.display = 'inline'; document.getElementById('2108.01127v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.01127v1-abstract-full" style="display: none;">
        The efficiency and reliability of real-time incident detection models directly impact the affected corridors&#39; traffic safety and operational conditions. The recent emergence of cloud-based quantum computing infrastructure and innovations in noisy intermediate-scale quantum devices have revealed a new era of quantum-enhanced algorithms that can be leveraged to improve real-time incident detection accuracy. In this research, a hybrid machine learning model, which includes classical and quantum machine learning (ML) models, is developed to identify incidents using the connected vehicle (CV) data. The incident detection performance of the hybrid model is evaluated against baseline classical ML models. The framework is evaluated using data from a microsimulation tool for different incident scenarios. The results indicate that a hybrid neural network containing a 4-qubit quantum layer outperforms all other baseline models when there is a lack of training data. We have created three datasets; DS-1 with sufficient training data, and DS-2 and DS-3 with insufficient training data. The hybrid model achieves a recall of 98.9%, 98.3%, and 96.6% for DS-1, DS-2, and DS-3, respectively. For DS-2 and DS-3, the average improvement in F2-score (measures model&#39;s performance to correctly identify incidents) achieved by the hybrid model is 1.9% and 7.8%, respectively, compared to the classical models. It shows that with insufficient data, which may be common for CVs, the hybrid ML model will perform better than the classical models. With the continuing improvements of quantum computing infrastructure, the quantum ML models could be a promising alternative for CV-related applications when the available data is insufficient.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.01127v1-abstract-full').style.display = 'none'; document.getElementById('2108.01127v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.07333">arXiv:2107.07333</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.07333">pdf</a>, <a href="https://arxiv.org/format/2107.07333">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Anomaly Instance Segmentation for Baggage Threat Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Hassan%2C+T">Taimur Hassan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Akcay%2C+S">Samet Akcay</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Bennamoun%2C+M">Mohammed Bennamoun</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Werghi%2C+N">Naoufel Werghi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.07333v2-abstract-short" style="display: inline;">
        Identifying potential threats concealed within the baggage is of prime concern for the security staff. Many researchers have developed frameworks that can detect baggage threats from X-ray scans. However, to the best of our knowledge, all of these frameworks require extensive training on large-scale and well-annotated datasets, which are hard to procure in the real world. This paper presents a nov&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.07333v2-abstract-full').style.display = 'inline'; document.getElementById('2107.07333v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.07333v2-abstract-full" style="display: none;">
        Identifying potential threats concealed within the baggage is of prime concern for the security staff. Many researchers have developed frameworks that can detect baggage threats from X-ray scans. However, to the best of our knowledge, all of these frameworks require extensive training on large-scale and well-annotated datasets, which are hard to procure in the real world. This paper presents a novel unsupervised anomaly instance segmentation framework that recognizes baggage threats, in X-ray scans, as anomalies without requiring any ground truth labels. Furthermore, thanks to its stylization capacity, the framework is trained only once, and at the inference stage, it detects and extracts contraband items regardless of their scanner specifications. Our one-staged approach initially learns to reconstruct normal baggage content via an encoder-decoder network utilizing a proposed stylization loss function. The model subsequently identifies the abnormal regions by analyzing the disparities within the original and the reconstructed scans. The anomalous regions are then clustered and post-processed to fit a bounding box for their localization. In addition, an optional classifier can also be appended with the proposed framework to recognize the categories of these extracted anomalies. A thorough evaluation of the proposed system on four public baggage X-ray datasets, without any re-training, demonstrates that it achieves competitive performance as compared to the conventional fully supervised methods (i.e., the mean average precision score of 0.7941 on SIXray, 0.8591 on GDXray, 0.7483 on OPIXray, and 0.5439 on COMPASS-XP dataset) while outperforming state-of-the-art semi-supervised and unsupervised baggage threat detection frameworks by 67.37%, 32.32%, 47.19%, and 45.81% in terms of F1 score across SIXray, GDXray, OPIXray, and COMPASS-XP datasets, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.07333v2-abstract-full').style.display = 'none'; document.getElementById('2107.07333v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in J-AIHC, Source Code is available at https://github.com/taimurhassan/anomaly</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.08957">arXiv:2105.08957</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.08957">pdf</a>, <a href="https://arxiv.org/format/2105.08957">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3411764.3445171">10.1145/3411764.3445171 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SEMOUR: A Scripted Emotional Speech Repository for Urdu
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Zaheer%2C+N">Nimra Zaheer</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ahmad%2C+O+U">Obaid Ullah Ahmad</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ahmed%2C+A">Ammar Ahmed</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+M+S">Muhammad Shehryar Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shabbir%2C+M">Mudassir Shabbir</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.08957v1-abstract-short" style="display: inline;">
        Designing reliable Speech Emotion Recognition systems is a complex task that inevitably requires sufficient data for training purposes. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. In this paper, we present SEMOUR, the first scripted database of emotion-tagged speech in the Urdu language, to design an Urdu Speech Recognition Syste&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08957v1-abstract-full').style.display = 'inline'; document.getElementById('2105.08957v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.08957v1-abstract-full" style="display: none;">
        Designing reliable Speech Emotion Recognition systems is a complex task that inevitably requires sufficient data for training purposes. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. In this paper, we present SEMOUR, the first scripted database of emotion-tagged speech in the Urdu language, to design an Urdu Speech Recognition System. Our gender-balanced dataset contains 15,040 unique instances recorded by eight professional actors eliciting a syntactically complex script. The dataset is phonetically balanced, and reliably exhibits a varied set of emotions as marked by the high agreement scores among human raters in experiments. We also provide various baseline speech emotion prediction scores on the database, which could be used for various applications like personalized robot assistants, diagnosis of psychological disorders, and getting feedback from a low-tech-enabled population, etc. On a random test sample, our model correctly predicts an emotion with a state-of-the-art 92% accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08957v1-abstract-full').style.display = 'none'; document.getElementById('2105.08957v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted in CHI 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.09305">arXiv:2104.09305</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.09305">pdf</a>, <a href="https://arxiv.org/format/2104.09305">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tracking agitation in people living with dementia in a care environment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+S">Shehroz S. Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sooriyakumaran%2C+T">Thaejaesh Sooriyakumaran</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Rich%2C+K">Katherine Rich</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Spasojevic%2C+S">Sofija Spasojevic</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+B">Bing Ye</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Newman%2C+K">Kristine Newman</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Iaboni%2C+A">Andrea Iaboni</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mihailidis%2C+A">Alex Mihailidis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.09305v2-abstract-short" style="display: inline;">
        Agitation is a symptom that communicates distress in people living with dementia (PwD), and that can place them and others at risk. In a long term care (LTC) environment, care staff track and document these symptoms as a way to detect when there has been a change in resident status to assess risk, and to monitor for response to interventions. However, this documentation can be time-consuming, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09305v2-abstract-full').style.display = 'inline'; document.getElementById('2104.09305v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.09305v2-abstract-full" style="display: none;">
        Agitation is a symptom that communicates distress in people living with dementia (PwD), and that can place them and others at risk. In a long term care (LTC) environment, care staff track and document these symptoms as a way to detect when there has been a change in resident status to assess risk, and to monitor for response to interventions. However, this documentation can be time-consuming, and due to staffing constraints, episodes of agitation may go unobserved. This brings into question the reliability of these assessments, and presents an opportunity for technology to help track and monitor behavioural symptoms in dementia. In this paper, we present the outcomes of a 2 year real-world study performed in a dementia unit, where a multi-modal wearable device was worn by $20$ PwD. In line with a commonly used clinical documentation tool, this large multi-modal time-series data was analyzed to track the presence of episodes of agitation in 8-hour nursing shifts. The development of a baseline classification model (AUC=0.717) on this dataset and subsequent improvement (AUC= 0.779) lays the groundwork for automating the process of annotating agitation events in nursing charts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09305v2-abstract-full').style.display = 'none'; document.getElementById('2104.09305v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 9 figures, 2 Tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.02471">arXiv:2104.02471</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.02471">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Facial Feature Discovery Framework for Race Classification Using Deep Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+K">Khalil Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ali%2C+J">Jehad Ali</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Uddin%2C+I">Irfan Uddin</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Sahib Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Roh%2C+B">Byeong-hee Roh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.02471v1-abstract-short" style="display: inline;">
        Race classification is a long-standing challenge in the field of face image analysis. The investigation of salient facial features is an important task to avoid processing all face parts. Face segmentation strongly benefits several face analysis tasks, including ethnicity and race classification. We propose a raceclassification algorithm using a prior face segmentation framework. A deep convolutio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02471v1-abstract-full').style.display = 'inline'; document.getElementById('2104.02471v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.02471v1-abstract-full" style="display: none;">
        Race classification is a long-standing challenge in the field of face image analysis. The investigation of salient facial features is an important task to avoid processing all face parts. Face segmentation strongly benefits several face analysis tasks, including ethnicity and race classification. We propose a raceclassification algorithm using a prior face segmentation framework. A deep convolutional neural network (DCNN) was used to construct a face segmentation model. For training the DCNN, we label face images according to seven different classes, that is, nose, skin, hair, eyes, brows, back, and mouth. The DCNN model developed in the first phase was used to create segmentation results. The probabilistic classification method is used, and probability maps (PMs) are created for each semantic class. We investigated five salient facial features from among seven that help in race classification. Features are extracted from the PMs of five classes, and a new model is trained based on the DCNN. We assessed the performance of the proposed race classification method on four standard face datasets, reporting superior results compared with previous studies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.02471v1-abstract-full').style.display = 'none'; document.getElementById('2104.02471v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Number of pages in the paper are 15</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Under review in Computer, Material, and Continua, 2021
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.14301">arXiv:2103.14301</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.14301">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluation of Preprocessing Techniques for U-Net Based Automated Liver Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Islam%2C+M">Muhammad Islam</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+K+N">Kaleem Nawaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+M+S">Muhammad Salman Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.14301v1-abstract-short" style="display: inline;">
        To extract liver from medical images is a challenging task due to similar intensity values of liver with adjacent organs, various contrast levels, various noise associated with medical images and irregular shape of liver. To address these issues, it is important to preprocess the medical images, i.e., computerized tomography (CT) and magnetic resonance imaging (MRI) data prior to liver analysis an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14301v1-abstract-full').style.display = 'inline'; document.getElementById('2103.14301v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.14301v1-abstract-full" style="display: none;">
        To extract liver from medical images is a challenging task due to similar intensity values of liver with adjacent organs, various contrast levels, various noise associated with medical images and irregular shape of liver. To address these issues, it is important to preprocess the medical images, i.e., computerized tomography (CT) and magnetic resonance imaging (MRI) data prior to liver analysis and quantification. This paper investigates the impact of permutation of various preprocessing techniques for CT images, on the automated liver segmentation using deep learning, i.e., U-Net architecture. The study focuses on Hounsfield Unit (HU) windowing, contrast limited adaptive histogram equalization (CLAHE), z-score normalization, median filtering and Block-Matching and 3D (BM3D) filtering. The segmented results show that combination of three techniques; HU-windowing, median filtering and z-score normalization achieve optimal performance with Dice coefficient of 96.93%, 90.77% and 90.84% for training, validation and testing respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.14301v1-abstract-full').style.display = 'none'; document.getElementById('2103.14301v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.13334">arXiv:2102.13334</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.13334">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Integration of deep learning with expectation maximization for spatial cue based speech separation in reverberant conditions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Gul%2C+S">Sania Gul</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+M+S">Muhammad Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shah%2C+S+W">Syed Waqar Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.13334v1-abstract-short" style="display: inline;">
        In this paper, we formulate a blind source separation (BSS) framework, which allows integrating U-Net based deep learning source separation network with probabilistic spatial machine learning expectation maximization (EM) algorithm for separating speech in reverberant conditions. Our proposed model uses a pre-trained deep learning convolutional neural network, U-Net, for clustering the interaural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.13334v1-abstract-full').style.display = 'inline'; document.getElementById('2102.13334v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.13334v1-abstract-full" style="display: none;">
        In this paper, we formulate a blind source separation (BSS) framework, which allows integrating U-Net based deep learning source separation network with probabilistic spatial machine learning expectation maximization (EM) algorithm for separating speech in reverberant conditions. Our proposed model uses a pre-trained deep learning convolutional neural network, U-Net, for clustering the interaural level difference (ILD) cues and machine learning expectation maximization (EM) algorithm for clustering the interaural phase difference (IPD) cues. The integrated model exploits the complementary strengths of the two approaches to BSS: the strong modeling power of supervised neural networks and the ease of unsupervised machine learning algorithms, whose few parameters can be estimated on as little as a single segment of an audio mixture. The results show an average improvement of 4.3 dB in signal to distortion ratio (SDR) and 4.3% in short time speech intelligibility (STOI) over the EM based source separation algorithm MESSL-GS (model-based expectation-maximization source separation and localization with garbage source) and 4.5 dB in SDR and 8% in STOI over deep learning convolutional neural network (U-Net) based speech separation algorithm SONET under the reverberant conditions ranging from anechoic to those mostly encountered in the real world.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.13334v1-abstract-full').style.display = 'none'; document.getElementById('2102.13334v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.09673">arXiv:2102.09673</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.09673">pdf</a>, <a href="https://arxiv.org/format/2102.09673">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Effective Cache Apportioning for Performance Isolation Under Compiler Guidance
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Chatterjee%2C+B">Bodhisatwa Chatterjee</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Sharjeel Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Pande%2C+S">Santosh Pande</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.09673v2-abstract-short" style="display: inline;">
        With a growing number of cores per socket in modern data-centers where multi-tenancy of a diverse set of applications must be efficiently supported, effective sharing of the last level cache is a very important problem. This is challenging because modern workloads exhibit dynamic phase behavior - their cache requirements &amp; sensitivity vary across different execution points. To tackle this problem,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.09673v2-abstract-full').style.display = 'inline'; document.getElementById('2102.09673v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.09673v2-abstract-full" style="display: none;">
        With a growing number of cores per socket in modern data-centers where multi-tenancy of a diverse set of applications must be efficiently supported, effective sharing of the last level cache is a very important problem. This is challenging because modern workloads exhibit dynamic phase behavior - their cache requirements &amp; sensitivity vary across different execution points. To tackle this problem, we propose Com-CAS, a compiler-guided cache apportioning system that provides smart cache allocation to co-executing applications in a system. The front-end of Com-CAS is primarily a compiler-framework equipped with learning mechanisms to predict cache requirements, while the backend consists of an allocation framework with a pro-active scheduler that apportions cache dynamically to co-executing applications. Our system improved average throughput by 21%, with a maximum of 54% while maintaining the worst individual application execution time degradation within 15% to meet SLA requirements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.09673v2-abstract-full').style.display = 'none'; document.getElementById('2102.09673v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.01143">arXiv:2102.01143</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.01143">pdf</a>, <a href="https://arxiv.org/format/2102.01143">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICTAI50040.2020.00178">10.1109/ICTAI50040.2020.00178 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        toon2real: Translating Cartoon Images to Realistic Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Sultan%2C+K+M+A">K. M. Arefeen Sultan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Jubair%2C+M+I">Mohammad Imrul Jubair</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Islam%2C+M+N">MD. Nahidul Islam</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+H">Sayed Hossain Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.01143v1-abstract-short" style="display: inline;">
        In terms of Image-to-image translation, Generative Adversarial Networks (GANs) has achieved great success even when it is used in the unsupervised dataset. In this work, we aim to translate cartoon images to photo-realistic images using GAN. We apply several state-of-the-art models to perform this task; however, they fail to perform good quality translations. We observe that the shallow difference&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.01143v1-abstract-full').style.display = 'inline'; document.getElementById('2102.01143v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.01143v1-abstract-full" style="display: none;">
        In terms of Image-to-image translation, Generative Adversarial Networks (GANs) has achieved great success even when it is used in the unsupervised dataset. In this work, we aim to translate cartoon images to photo-realistic images using GAN. We apply several state-of-the-art models to perform this task; however, they fail to perform good quality translations. We observe that the shallow difference between these two domains causes this issue. Based on this idea, we propose a method based on CycleGAN model for image translation from cartoon domain to photo-realistic domain. To make our model efficient, we implemented Spectral Normalization which added stability in our model. We demonstrate our experimental results and show that our proposed model has achieved the lowest Frechet Inception Distance score and better results compared to another state-of-the-art technique, UNIT.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.01143v1-abstract-full').style.display = 'none'; document.getElementById('2102.01143v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted as a short paper at ICTAI 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.01381">arXiv:2101.01381</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.01381">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analysis of the satellite navigational data in the Baseband signal processing of Galileo E5 AltBOC signal
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Subhan Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhu%2C+Y">Yiqun Zhu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Jawad%2C+M">Muhammad Jawad</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Safder%2C+M+U">Muhammad Umair Safder</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Jaffery%2C+M+H">Mujtaba Hussain Jaffery</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Javid%2C+S">Salman Javid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.01381v2-abstract-short" style="display: inline;">
        The advancement in the world of global satellite systems has enhanced the accuracy and reliability between various constellations. However, these enhancements have made numerous challenges for the receivers designer. For instance, comparing the acquisition and tracking of the Galileo signals turn into relatively complex processes after the utilization of Alternate Binary Offset Carrier AltBOC modu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01381v2-abstract-full').style.display = 'inline'; document.getElementById('2101.01381v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.01381v2-abstract-full" style="display: none;">
        The advancement in the world of global satellite systems has enhanced the accuracy and reliability between various constellations. However, these enhancements have made numerous challenges for the receivers designer. For instance, comparing the acquisition and tracking of the Galileo signals turn into relatively complex processes after the utilization of Alternate Binary Offset Carrier AltBOC modulation scheme. This paper presents an efficient and unique method for comparing baseband signal processing of the complex receiver structure of the Galileo E5 AltBOC signal. More specifically, the data demodulation has attained after comparing the noisy satellite data sets with the clean data sets. Moreover, the paper presents the implementation of signal acquisition, code tracking, multipath noise characteristics, and carrier tracking for various datasets of the satellite after the eradication of noise. The results obtained in the paper are promising and provide the through treatment to the problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.01381v2-abstract-full').style.display = 'none'; document.getElementById('2101.01381v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Arctic Journal 2018
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.05073">arXiv:2012.05073</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.05073">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        COVID-19 Detection in Chest X-Ray Images using a New Channel Boosted CNN
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+H">Saddam Hussain Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sohail%2C+A">Anabia Sohail</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+A">Asifullah Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.05073v2-abstract-short" style="display: inline;">
        COVID-19 is a highly contagious respiratory infection that has affected a large population across the world and continues with its devastating consequences. It is imperative to detect COVID-19 at the earliest to limit the span of infection. In this work, a new classification technique CB-STM-RENet based on deep Convolutional Neural Network (CNN) and Channel Boosting is proposed for the screening o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.05073v2-abstract-full').style.display = 'inline'; document.getElementById('2012.05073v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.05073v2-abstract-full" style="display: none;">
        COVID-19 is a highly contagious respiratory infection that has affected a large population across the world and continues with its devastating consequences. It is imperative to detect COVID-19 at the earliest to limit the span of infection. In this work, a new classification technique CB-STM-RENet based on deep Convolutional Neural Network (CNN) and Channel Boosting is proposed for the screening of COVID-19 in chest X-Rays. In this connection, to learn the COVID-19 specific radiographic patterns, a new convolution block based on split-transform-merge (STM) is developed. This new block systematically incorporates region and edge-based operations at each branch to capture the diverse set of features at various levels, especially those related to region homogeneity, textural variations, and boundaries of the infected region. The learning and discrimination capability of the proposed CNN architecture is enhanced by exploiting the Channel Boosting idea that concatenates the auxiliary channels along with the original channels. The auxiliary channels are generated from the pre-trained CNNs using Transfer Learning. The effectiveness of the proposed technique CB-STM-RENet is evaluated on three different datasets of chest X-Rays namely CoV-Healthy-6k, CoV-NonCoV-10k, and CoV-NonCoV-15k. The performance comparison of the proposed CB-STM-RENet with the existing techniques exhibits high performance both in discriminating COVID-19 chest infections from Healthy, as well as, other types of chest infections. CB-STM-RENet provides the highest performance on all these three datasets; especially on the stringent CoV-NonCoV-15k dataset. The good detection rate (97%), and high precision (93%) of the proposed technique suggest that it can be adapted for the diagnosis of COVID-19 infected patients. The test code is available at https://github.com/PRLAB21/COVID-19-Detection-System-using-Chest-X-Ray-Images.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.05073v2-abstract-full').style.display = 'none'; document.getElementById('2012.05073v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 December, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Pages: 26 Tables: 3 Figures: 10 Equations: 8</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.03124">arXiv:2012.03124</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.03124">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Development and Characterization of a Chest CT Atlas
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Xu%2C+K">Kaiwen Xu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Gao%2C+R">Riqiang Gao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+M+S">Mirza S. Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Bao%2C+S">Shunxing Bao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tang%2C+Y">Yucheng Tang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Deppen%2C+S+A">Steve A. Deppen</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Huo%2C+Y">Yuankai Huo</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sandler%2C+K+L">Kim L. Sandler</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Massion%2C+P+P">Pierre P. Massion</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Heinrich%2C+M+P">Mattias P. Heinrich</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Landman%2C+B+A">Bennett A. Landman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.03124v1-abstract-short" style="display: inline;">
        A major goal of lung cancer screening is to identify individuals with particular phenotypes that are associated with high risk of cancer. Identifying relevant phenotypes is complicated by the variation in body position and body composition. In the brain, standardized coordinate systems (e.g., atlases) have enabled separate consideration of local features from gross/global structure. To date, no an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.03124v1-abstract-full').style.display = 'inline'; document.getElementById('2012.03124v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.03124v1-abstract-full" style="display: none;">
        A major goal of lung cancer screening is to identify individuals with particular phenotypes that are associated with high risk of cancer. Identifying relevant phenotypes is complicated by the variation in body position and body composition. In the brain, standardized coordinate systems (e.g., atlases) have enabled separate consideration of local features from gross/global structure. To date, no analogous standard atlas has been presented to enable spatial mapping and harmonization in chest computational tomography (CT). In this paper, we propose a thoracic atlas built upon a large low dose CT (LDCT) database of lung cancer screening program. The study cohort includes 466 male and 387 female subjects with no screening detected malignancy (age 46-79 years, mean 64.9 years). To provide spatial mapping, we optimize a multi-stage inter-subject non-rigid registration pipeline for the entire thoracic space. We evaluate the optimized pipeline relative to two baselines with alternative non-rigid registration module: the same software with default parameters and an alternative software. We achieve a significant improvement in terms of registration success rate based on manual QA. For the entire study cohort, the optimized pipeline achieves a registration success rate of 91.7%. The application validity of the developed atlas is evaluated in terms of discriminative capability for different anatomic phenotypes, including body mass index (BMI), chronic obstructive pulmonary disease (COPD), and coronary artery calcification (CAC).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.03124v1-abstract-full').style.display = 'none'; document.getElementById('2012.03124v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by SPIE2021 Medical Imaging (oral)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.02776">arXiv:2012.02776</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.02776">pdf</a>, <a href="https://arxiv.org/format/2012.02776">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to Fuse Asymmetric Feature Maps in Siamese Trackers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Han%2C+W">Wencheng Han</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Dong%2C+X">Xingping Dong</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shen%2C+J">Jianbing Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.02776v2-abstract-short" style="display: inline;">
        Recently, Siamese-based trackers have achieved promising performance in visual tracking. Most recent Siamese-based trackers typically employ a depth-wise cross-correlation (DW-XCorr) to obtain multi-channel correlation information from the two feature maps (target and search region). However, DW-XCorr has several limitations within Siamese-based tracking: it can easily be fooled by distractors, ha&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.02776v2-abstract-full').style.display = 'inline'; document.getElementById('2012.02776v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.02776v2-abstract-full" style="display: none;">
        Recently, Siamese-based trackers have achieved promising performance in visual tracking. Most recent Siamese-based trackers typically employ a depth-wise cross-correlation (DW-XCorr) to obtain multi-channel correlation information from the two feature maps (target and search region). However, DW-XCorr has several limitations within Siamese-based tracking: it can easily be fooled by distractors, has fewer activated channels, and provides weak discrimination of object boundaries. Further, DW-XCorr is a handcrafted parameter-free module and cannot fully benefit from offline learning on large-scale data. We propose a learnable module, called the asymmetric convolution (ACM), which learns to better capture the semantic correlation information in offline training on large-scale data. Different from DW-XCorr and its predecessor(XCorr), which regard a single feature map as the convolution kernel, our ACM decomposes the convolution operation on a concatenated feature map into two mathematically equivalent operations, thereby avoiding the need for the feature maps to be of the same size (width and height)during concatenation. Our ACM can incorporate useful prior information, such as bounding-box size, with standard visual features. Furthermore, ACM can easily be integrated into existing Siamese trackers based on DW-XCorror XCorr. To demonstrate its generalization ability, we integrate ACM into three representative trackers: SiamFC, SiamRPN++, and SiamBAN. Our experiments reveal the benefits of the proposed ACM, which outperforms existing methods on six tracking benchmarks. On the LaSOT test set, our ACM-based tracker obtains a significant improvement of 5.8% in terms of success (AUC), over the baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.02776v2-abstract-full').style.display = 'none'; document.getElementById('2012.02776v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.02238">arXiv:2012.02238</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.02238">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-rays Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Rahman%2C+T">Tawsifur Rahman</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khandakar%2C+A">Amith Khandakar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Qiblawey%2C+Y">Yazan Qiblawey</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tahir%2C+A">Anas Tahir</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kiranyaz%2C+S">Serkan Kiranyaz</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kashem%2C+S+B+A">Saad Bin Abul Kashem</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Islam%2C+M+T">Mohammad Tariqul Islam</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Maadeed%2C+S+A">Somaya Al Maadeed</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zughaier%2C+S+M">Susu M Zughaier</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+M+S">Muhammad Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chowdhury%2C+M+E+H">Muhammad E. H. Chowdhury</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.02238v1-abstract-short" style="display: inline;">
        The use of computer-aided diagnosis in the reliable and fast detection of coronavirus disease (COVID-19) has become a necessity to prevent the spread of the virus during the pandemic to ease the burden on the medical infrastructure. Chest X-ray (CXR) imaging has several advantages over other imaging techniques as it is cheap, easily accessible, fast and portable. This paper explores the effect of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.02238v1-abstract-full').style.display = 'inline'; document.getElementById('2012.02238v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.02238v1-abstract-full" style="display: none;">
        The use of computer-aided diagnosis in the reliable and fast detection of coronavirus disease (COVID-19) has become a necessity to prevent the spread of the virus during the pandemic to ease the burden on the medical infrastructure. Chest X-ray (CXR) imaging has several advantages over other imaging techniques as it is cheap, easily accessible, fast and portable. This paper explores the effect of various popular image enhancement techniques and states the effect of each of them on the detection performance. We have compiled the largest X-ray dataset called COVQU-20, consisting of 18,479 normal, non-COVID lung opacity and COVID-19 CXR images. To the best of our knowledge, this is the largest public COVID positive database. Ground glass opacity is the common symptom reported in COVID-19 pneumonia patients and so a mixture of 3616 COVID-19, 6012 non-COVID lung opacity, and 8851 normal chest X-ray images were used to create this dataset. Five different image enhancement techniques: histogram equalization, contrast limited adaptive histogram equalization, image complement, gamma correction, and Balance Contrast Enhancement Technique were used to improve COVID-19 detection accuracy. Six different Convolutional Neural Networks (CNNs) were investigated in this study. Gamma correction technique outperforms other enhancement techniques in detecting COVID-19 from standard and segmented lung CXR images. The accuracy, precision, sensitivity, f1-score, and specificity in the detection of COVID-19 with gamma correction on CXR images were 96.29%, 96.28%, 96.29%, 96.28% and 96.27% respectively. The accuracy, precision, sensitivity, F1-score, and specificity were 95.11 %, 94.55 %, 94.56 %, 94.53 % and 95.59 % respectively for segmented lung images. The proposed approach with very high and comparable performance will boost the fast and robust COVID-19 detection using chest X-ray images.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.02238v1-abstract-full').style.display = 'none'; document.getElementById('2012.02238v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">34 pages, 6 Tables, 11 Figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.07491">arXiv:2011.07491</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.07491">pdf</a>, <a href="https://arxiv.org/format/2011.07491">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Anomaly Detection in Video via Self-Supervised and Multi-Task Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Georgescu%2C+M">Mariana-Iuliana Georgescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Barbalau%2C+A">Antonio Barbalau</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Popescu%2C+M">Marius Popescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shah%2C+M">Mubarak Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.07491v3-abstract-short" style="display: inline;">
        Anomaly detection in video is a challenging computer vision problem. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without full supervision. In this paper, we approach anomalous event detection in video through self-supervised and multi-task learning at the object level. We first utilize a pre-trained detector to detect objects. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.07491v3-abstract-full').style.display = 'inline'; document.getElementById('2011.07491v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.07491v3-abstract-full" style="display: none;">
        Anomaly detection in video is a challenging computer vision problem. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without full supervision. In this paper, we approach anomalous event detection in video through self-supervised and multi-task learning at the object level. We first utilize a pre-trained detector to detect objects. Then, we train a 3D convolutional neural network to produce discriminative anomaly-specific information by jointly learning multiple proxy tasks: three self-supervised and one based on knowledge distillation. The self-supervised tasks are: (i) discrimination of forward/backward moving objects (arrow of time), (ii) discrimination of objects in consecutive/intermittent frames (motion irregularity) and (iii) reconstruction of object-specific appearance information. The knowledge distillation task takes into account both classification and detection information, generating large prediction discrepancies between teacher and student models when anomalies occur. To the best of our knowledge, we are the first to approach anomalous event detection in video as a multi-task learning problem, integrating multiple self-supervised and knowledge distillation proxy tasks in a single architecture. Our lightweight architecture outperforms the state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. Additionally, we perform an ablation study demonstrating the importance of integrating self-supervised learning and normality-specific distillation in a multi-task learning setting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.07491v3-abstract-full').style.display = 'none'; document.getElementById('2011.07491v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CVPR 2021. Main paper and supplementary are both included</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.15440">arXiv:2010.15440</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.15440">pdf</a>, <a href="https://arxiv.org/format/2010.15440">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TPAMI.2020.3033882">10.1109/TPAMI.2020.3033882 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+S">Salman S. Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sundar%2C+V">Varun Sundar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Boominathan%2C+V">Vivek Boominathan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Veeraraghavan%2C+A">Ashok Veeraraghavan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mitra%2C+K">Kaushik Mitra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.15440v1-abstract-short" style="display: inline;">
        Lensless imaging has emerged as a potential solution towards realizing ultra-miniature cameras by eschewing the bulky lens in a traditional camera. Without a focusing lens, the lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, the current iterative-optimization-based reconstruction algorithms produce noisier and perceptually poorer imag&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.15440v1-abstract-full').style.display = 'inline'; document.getElementById('2010.15440v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.15440v1-abstract-full" style="display: none;">
        Lensless imaging has emerged as a potential solution towards realizing ultra-miniature cameras by eschewing the bulky lens in a traditional camera. Without a focusing lens, the lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, the current iterative-optimization-based reconstruction algorithms produce noisier and perceptually poorer images. In this work, we propose a non-iterative deep learning based reconstruction approach that results in orders of magnitude improvement in image quality for lensless reconstructions. Our approach, called $\textit{FlatNet}$, lays down a framework for reconstructing high-quality photorealistic images from mask-based lensless cameras, where the camera&#39;s forward model formulation is known. FlatNet consists of two stages: (1) an inversion stage that maps the measurement into a space of intermediate reconstruction by learning parameters within the forward model formulation, and (2) a perceptual enhancement stage that improves the perceptual quality of this intermediate reconstruction. These stages are trained together in an end-to-end manner. We show high-quality reconstructions by performing extensive experiments on real and challenging scenes using two different types of lensless prototypes: one which uses a separable forward model and another, which uses a more general non-separable cropped-convolution model. Our end-to-end approach is fast, produces photorealistic reconstructions, and is easy to adopt for other mask-based lensless cameras.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.15440v1-abstract-full').style.display = 'none'; document.getElementById('2010.15440v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2020. Supplementary material attached. For project website, see https://siddiquesalman.github.io/flatnet/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.12065">arXiv:2010.12065</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.12065">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A generalized deep learning model for multi-disease Chest X-Ray diagnostics
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Bajwa%2C+N">Nabit Bajwa</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Bajwa%2C+K">Kedar Bajwa</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Rana%2C+A">Atif Rana</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shakeel%2C+M+F">M. Faique Shakeel</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Haqqi%2C+K">Kashif Haqqi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+A">Suleiman Ali Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.12065v1-abstract-short" style="display: inline;">
        We investigate the generalizability of deep convolutional neural network (CNN) on the task of disease classification from chest x-rays collected over multiple sites. We systematically train the model using datasets from three independent sites with different patient populations: National Institute of Health (NIH), Stanford University Medical Centre (CheXpert), and Shifa International Hospital (SIH&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.12065v1-abstract-full').style.display = 'inline'; document.getElementById('2010.12065v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.12065v1-abstract-full" style="display: none;">
        We investigate the generalizability of deep convolutional neural network (CNN) on the task of disease classification from chest x-rays collected over multiple sites. We systematically train the model using datasets from three independent sites with different patient populations: National Institute of Health (NIH), Stanford University Medical Centre (CheXpert), and Shifa International Hospital (SIH). We formulate a sequential training approach and demonstrate that the model produces generalized prediction performance using held out test sets from the three sites. Our model generalizes better when trained on multiple datasets, with the CheXpert-Shifa-NET model performing significantly better (p-values &lt; 0.05) than the models trained on individual datasets for 3 out of the 4 distinct disease classes. The code for training the model will be made available open source at: www.github.com/link-to-code at the time of publication.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.12065v1-abstract-full').style.display = 'none'; document.getElementById('2010.12065v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.02814">arXiv:2010.02814</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.02814">pdf</a>, <a href="https://arxiv.org/format/2010.02814">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Anomaly Detection Approach to Identify Early Cases in a Pandemic using Chest X-rays
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+S">Shehroz S. Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khoshbakhtian%2C+F">Faraz Khoshbakhtian</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ashraf%2C+A+B">Ahmed Bilal Ashraf</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.02814v2-abstract-short" style="display: inline;">
        The current COVID-19 pandemic is now getting contained, albeit at the cost of morethan2.3million human lives. A critical phase in any pandemic is the early detection of cases to develop preventive treatments and strategies. In the case of COVID-19,several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a give&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02814v2-abstract-full').style.display = 'inline'; document.getElementById('2010.02814v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.02814v2-abstract-full" style="display: none;">
        The current COVID-19 pandemic is now getting contained, albeit at the cost of morethan2.3million human lives. A critical phase in any pandemic is the early detection of cases to develop preventive treatments and strategies. In the case of COVID-19,several studies have indicated that chest radiography images of the infected patients show characteristic abnormalities. However, at the onset of a given pandemic, such asCOVID-19, there may not be sufficient data for the affected cases to train models for their robust detection. Hence, supervised classification is ill-posed for this problem because the time spent in collecting large amounts of data from infected persons could lead to the loss of human lives and delays in preventive interventions. Therefore, we formulate the problem of identifying early cases in a pandemic as an anomaly detection problem, in which the data for healthy patients is abundantly available, whereas no training data is present for the class of interest (COVID-19 in our case). To solve this problem, we present several unsupervised deep learning approaches, including convolutional and adversarially trained autoencoder. We tested two settings on a publicly available dataset (COVIDx)by training the model on chest X-rays from (i) only healthy adults, and (ii) healthy and other non-COVID-19 pneumonia, and detected COVID-19 as an anomaly. Afterperforming3-fold cross validation, we obtain a ROC-AUC of0.765. These results are very encouraging and pave the way towards research for ensuring emergency preparedness in future pandemics, especially the ones that could be detected from chest X-rays
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02814v2-abstract-full').style.display = 'none'; document.getElementById('2010.02814v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 3 tables, 3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.13158">arXiv:2009.13158</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.13158">pdf</a>, <a href="https://arxiv.org/format/2009.13158">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Trainable Structure Tensors for Autonomous Baggage Threat Detection Under Extreme Occlusion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Hassan%2C+T">Taimur Hassan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Akcay%2C+S">Samet Akcay</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Bennamoun%2C+M">Mohammed Bennamoun</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Werghi%2C+N">Naoufel Werghi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.13158v2-abstract-short" style="display: inline;">
        Detecting baggage threats is one of the most difficult tasks, even for expert officers. Many researchers have developed computer-aided screening systems to recognize these threats from the baggage X-ray scans. However, all of these frameworks are limited in identifying the contraband items under extreme occlusion. This paper presents a novel instance segmentation framework that utilizes trainable&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.13158v2-abstract-full').style.display = 'inline'; document.getElementById('2009.13158v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.13158v2-abstract-full" style="display: none;">
        Detecting baggage threats is one of the most difficult tasks, even for expert officers. Many researchers have developed computer-aided screening systems to recognize these threats from the baggage X-ray scans. However, all of these frameworks are limited in identifying the contraband items under extreme occlusion. This paper presents a novel instance segmentation framework that utilizes trainable structure tensors to highlight the contours of the occluded and cluttered contraband items (by scanning multiple predominant orientations), while simultaneously suppressing the irrelevant baggage content. The proposed framework has been extensively tested on four publicly available X-ray datasets where it outperforms the state-of-the-art frameworks in terms of mean average precision scores. Furthermore, to the best of our knowledge, it is the only framework that has been validated on combined grayscale and colored scans obtained from four different types of X-ray scanners.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.13158v2-abstract-full').style.display = 'none'; document.getElementById('2009.13158v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 September, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACCV-2020 Camera Ready, Source Code: https://github.com/taimurhassan/TST</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.08864">arXiv:2009.08864</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.08864">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Classification and Region Analysis of COVID-19 Infection using Lung CT Images and Deep Convolutional Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+H">Saddam Hussain Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sohail%2C+A">Anabia Sohail</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+A">Asifullah Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lee%2C+Y+S">Yeon Soo Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.08864v1-abstract-short" style="display: inline;">
        COVID-19 is a global health problem. Consequently, early detection and analysis of the infection patterns are crucial for controlling infection spread as well as devising a treatment plan. This work proposes a two-stage deep Convolutional Neural Networks (CNNs) based framework for delineation of COVID-19 infected regions in Lung CT images. In the first stage, initially, COVID-19 specific CT image&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.08864v1-abstract-full').style.display = 'inline'; document.getElementById('2009.08864v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.08864v1-abstract-full" style="display: none;">
        COVID-19 is a global health problem. Consequently, early detection and analysis of the infection patterns are crucial for controlling infection spread as well as devising a treatment plan. This work proposes a two-stage deep Convolutional Neural Networks (CNNs) based framework for delineation of COVID-19 infected regions in Lung CT images. In the first stage, initially, COVID-19 specific CT image features are enhanced using a two-level discrete wavelet transformation. These enhanced CT images are then classified using the proposed custom-made deep CoV-CTNet. In the second stage, the CT images classified as infectious images are provided to the segmentation models for the identification and analysis of COVID-19 infectious regions. In this regard, we propose a novel semantic segmentation model CoV-RASeg, which systematically uses average and max pooling operations in the encoder and decoder blocks. This systematic utilization of max and average pooling operations helps the proposed CoV-RASeg in simultaneously learning both the boundaries and region homogeneity. Moreover, the idea of attention is incorporated to deal with mildly infected regions. The proposed two-stage framework is evaluated on a standard Lung CT image dataset, and its performance is compared with the existing deep CNN models. The performance of the proposed CoV-CTNet is evaluated using Mathew Correlation Coefficient (MCC) measure (0.98) and that of proposed CoV-RASeg using Dice Similarity (DS) score (0.95). The promising results on an unseen test set suggest that the proposed framework has the potential to help the radiologists in the identification and analysis of COVID-19 infected regions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.08864v1-abstract-full').style.display = 'none'; document.getElementById('2009.08864v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Pages: 32, Tables: 6, Figures: 14</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.07076">arXiv:2009.07076</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.07076">pdf</a>, <a href="https://arxiv.org/ps/2009.07076">ps</a>, <a href="https://arxiv.org/format/2009.07076">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comments on &#34;Design of momentum fractional LMS for Hammerstein nonlinear system identification with application to electrically stimulated muscle model&#34;
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Wahab%2C+A">Abdul Wahab</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+Z">Farrukh Zeeshan Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.07076v2-abstract-short" style="display: inline;">
        The purpose of this article is to discuss some aspects of the convergence analysis performed in the paper [Design of momentum fractional LMS for Hammerstein nonlinear system identification with application to electrically stimulated muscle model, Eur. Phys. J. Plus (2019) \textbf{134}: 407]. It is highlighted that the way the authors prove convergence suffers a lack of correct and valid mathematic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.07076v2-abstract-full').style.display = 'inline'; document.getElementById('2009.07076v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.07076v2-abstract-full" style="display: none;">
        The purpose of this article is to discuss some aspects of the convergence analysis performed in the paper [Design of momentum fractional LMS for Hammerstein nonlinear system identification with application to electrically stimulated muscle model, Eur. Phys. J. Plus (2019) \textbf{134}: 407]. It is highlighted that the way the authors prove convergence suffers a lack of correct and valid mathematical justifications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.07076v2-abstract-full').style.display = 'none'; document.getElementById('2009.07076v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 September, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: substantial text overlap with arXiv:2003.09597</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.05277">arXiv:2009.05277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.05277">pdf</a>, <a href="https://arxiv.org/format/2009.05277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Biomolecules">q-bio.BM</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s00521-021-06558-7">10.1007/s00521-021-06558-7 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AFP-SRC: Identification of Antifreeze Proteins Using Sparse Representation Classifier
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Usman%2C+M">Muhammad Usman</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wahab%2C+A">Abdul Wahab</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.05277v3-abstract-short" style="display: inline;">
        Species living in the extreme cold environment fight against the harsh conditions using antifreeze proteins (AFPs), that manipulates the freezing mechanism of water in more than one way. This amazing nature of AFP turns out to be extremely useful in several industrial and medical applications. The lack of similarity in their structure and sequence makes their prediction an arduous task and identif&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.05277v3-abstract-full').style.display = 'inline'; document.getElementById('2009.05277v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.05277v3-abstract-full" style="display: none;">
        Species living in the extreme cold environment fight against the harsh conditions using antifreeze proteins (AFPs), that manipulates the freezing mechanism of water in more than one way. This amazing nature of AFP turns out to be extremely useful in several industrial and medical applications. The lack of similarity in their structure and sequence makes their prediction an arduous task and identifying them experimentally in the wet-lab is time-consuming and expensive. In this research, we propose a computational framework for the prediction of AFPs which is essentially based on a sample-specific classification method using the sparse reconstruction. A linear model and an over-complete dictionary matrix of known AFPs are used to predict a sparse class-label vector that provides a sample-association score. Delta-rule is applied for the reconstruction of two pseudo-samples using lower and upper parts of the sample-association vector and based on the minimum recovery score, class labels are assigned. We compare our approach with contemporary methods on a standard dataset and the proposed method is found to outperform in terms of Balanced accuracy and Youden&#39;s index. The MATLAB implementation of the proposed method is available at the author&#39;s GitHub page (\{https://github.com/Shujaat123/AFP-SRC}{https://github.com/Shujaat123/AFP-SRC}).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.05277v3-abstract-full').style.display = 'none'; document.getElementById('2009.05277v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 September, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This preprint has not undergone any post-submission improvements or corrections. The Version of Record of this article is published in Neural Computing and Applications, and is available online at https://doi.org/10.1007/s00521-021-06558-7</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.05221">arXiv:2009.05221</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.05221">pdf</a>, <a href="https://arxiv.org/ps/2009.05221">ps</a>, <a href="https://arxiv.org/format/2009.05221">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comments on &#34;Generalization of the gradient method with fractional order gradient direction&#34;
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Wahab%2C+A">Abdul Wahab</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.05221v1-abstract-short" style="display: inline;">
        In this paper, a detrimental mathematical mistake is pointed out in the proof of \textit{Theorem 1} presented in the paper\textit{ [Generalization of the gradient method with fractional order gradient direction, J. Franklin Inst., 357 (2020) 2514-2532]}. It is highlighted that the way the authors prove the convergence of the fractional extreme points of a real valued function to its integer order&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.05221v1-abstract-full').style.display = 'inline'; document.getElementById('2009.05221v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.05221v1-abstract-full" style="display: none;">
        In this paper, a detrimental mathematical mistake is pointed out in the proof of \textit{Theorem 1} presented in the paper\textit{ [Generalization of the gradient method with fractional order gradient direction, J. Franklin Inst., 357 (2020) 2514-2532]}. It is highlighted that the way the authors prove the convergence of the fractional extreme points of a real valued function to its integer order extreme points lacks correct and valid mathematical argument. Rest of the theorems contained in the paper are mostly announced without any proof relaying on that of Theorem 1.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.05221v1-abstract-full').style.display = 'none'; document.getElementById('2009.05221v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.04401">arXiv:2009.04401</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.04401">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Is Third-Party Provided Travel Time Helpful to Estimate Freeway Performance Measures?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+M">Sakib Mahmud Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Patire%2C+A+D">Anthony David Patire</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.04401v1-abstract-short" style="display: inline;">
        Transportation agencies monitor freeway performance using various measures such as VMT (Vehicle Miles Traveled), VHD (Vehicle Hours of Delay), and VHT (Vehicle Hours Traveled). Public transportation agencies typically rely on point detector data to estimate these freeway performance measures. Point detectors, such as inductive loops cannot capture the travel time for a corridor, which can lead to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.04401v1-abstract-full').style.display = 'inline'; document.getElementById('2009.04401v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.04401v1-abstract-full" style="display: none;">
        Transportation agencies monitor freeway performance using various measures such as VMT (Vehicle Miles Traveled), VHD (Vehicle Hours of Delay), and VHT (Vehicle Hours Traveled). Public transportation agencies typically rely on point detector data to estimate these freeway performance measures. Point detectors, such as inductive loops cannot capture the travel time for a corridor, which can lead to inaccurate performance measure estimation. This research develops a hybrid method, which estimates of freeway performance measures using a mix of probe data from third-parties and data from traditional point detectors. Using a simulated I-210 model, the overall framework using multiple data sources is evaluated, and compared with the traditional point detector-based estimation method. In the traditional method, point speeds are estimated with the flow and occupancy values using the g-factors. Data from 5% of the total vehicle are used to generate the third-party vendor provided travel time data. The analysis is conducted for multiple scenarios, including peak and off-peak periods. Findings suggest that fusing data from both third-party vendors and point detectors can help estimate performance measures better, compared to the traditional method, in scenarios that have noticeable traffic demand on freeways.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.04401v1-abstract-full').style.display = 'none'; document.getElementById('2009.04401v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.13646">arXiv:2008.13646</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.13646">pdf</a>, <a href="https://arxiv.org/format/2008.13646">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Switchable Deep Beamformer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Huh%2C+J">Jaeyoung Huh</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+J+C">Jong Chul Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.13646v2-abstract-short" style="display: inline;">
        Recent proposals of deep beamformers using deep neural networks have attracted significant attention as computational efficient alternatives to adaptive and compressive beamformers. Moreover, deep beamformers are versatile in that image post-processing algorithms can be combined with the beamforming. Unfortunately, in the current technology, a separate beamformer should be trained and stored for e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.13646v2-abstract-full').style.display = 'inline'; document.getElementById('2008.13646v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.13646v2-abstract-full" style="display: none;">
        Recent proposals of deep beamformers using deep neural networks have attracted significant attention as computational efficient alternatives to adaptive and compressive beamformers. Moreover, deep beamformers are versatile in that image post-processing algorithms can be combined with the beamforming. Unfortunately, in the current technology, a separate beamformer should be trained and stored for each application, demanding significant scanner resources. To address this problem, here we propose a {\em switchable} deep beamformer that can produce various types of output such as DAS, speckle removal, deconvolution, etc., using a single network with a simple switch. In particular, the switch is implemented through Adaptive Instance Normalization (AdaIN) layers, so that various output can be generated by merely changing the AdaIN code. Experimental results using B-mode focused ultrasound confirm the flexibility and efficacy of the proposed methods for various applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.13646v2-abstract-full').style.display = 'none'; document.getElementById('2008.13646v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.12328">arXiv:2008.12328</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.12328">pdf</a>, <a href="https://arxiv.org/format/2008.12328">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TPAMI.2021.3074805">10.1109/TPAMI.2021.3074805 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Background-Agnostic Framework with Adversarial Training for Abnormal Event Detection in Video
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Georgescu%2C+M">Mariana-Iuliana Georgescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ionescu%2C+R+T">Radu Tudor Ionescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Popescu%2C+M">Marius Popescu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shah%2C+M">Mubarak Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.12328v4-abstract-short" style="display: inline;">
        Abnormal event detection in video is a complex computer vision problem that has attracted significant attention in recent years. The complexity of the task arises from the commonly-adopted definition of an abnormal event, that is, a rarely occurring event that typically depends on the surrounding context. Following the standard formulation of abnormal event detection as outlier detection, we propo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.12328v4-abstract-full').style.display = 'inline'; document.getElementById('2008.12328v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.12328v4-abstract-full" style="display: none;">
        Abnormal event detection in video is a complex computer vision problem that has attracted significant attention in recent years. The complexity of the task arises from the commonly-adopted definition of an abnormal event, that is, a rarely occurring event that typically depends on the surrounding context. Following the standard formulation of abnormal event detection as outlier detection, we propose a background-agnostic framework that learns from training videos containing only normal events. Our framework is composed of an object detector, a set of appearance and motion auto-encoders, and a set of classifiers. Since our framework only looks at object detections, it can be applied to different scenes, provided that normal events are defined identically across scenes and that the single main factor of variation is the background. To overcome the lack of abnormal data during training, we propose an adversarial learning strategy for the auto-encoders. We create a scene-agnostic set of out-of-domain pseudo-abnormal examples, which are correctly reconstructed by the auto-encoders before applying gradient ascent on the pseudo-abnormal examples. We further utilize the pseudo-abnormal examples to serve as abnormal examples when training appearance-based and motion-based binary classifiers to discriminate between normal and abnormal latent features and reconstructions. We compare our framework with the state-of-the-art methods on four benchmark data sets, using various evaluation metrics. Compared to existing methods, the empirical results indicate that our approach achieves favorable performance on all data sets. In addition, we provide region-based and track-based annotations for two large-scale abnormal event detection data sets from the literature, namely ShanghaiTech and Subway.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.12328v4-abstract-full').style.display = 'none'; document.getElementById('2008.12328v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.10774">arXiv:2008.10774</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.10774">pdf</a>, <a href="https://arxiv.org/format/2008.10774">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Image Colorization: A Survey and Dataset
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Anwar%2C+S">Saeed Anwar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tahir%2C+M">Muhammad Tahir</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+C">Chongyi Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mian%2C+A">Ajmal Mian</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Muzaffar%2C+A+W">Abdul Wahab Muzaffar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.10774v3-abstract-short" style="display: inline;">
        Image colorization is the process of estimating RGB colors for grayscale images or video frames to improve their aesthetic and perceptual quality. Deep learning techniques for image colorization have progressed notably over the last decade, calling the need for a systematic survey and benchmarking of these techniques. This article presents a comprehensive survey of recent state-of-the-art deep lea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.10774v3-abstract-full').style.display = 'inline'; document.getElementById('2008.10774v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.10774v3-abstract-full" style="display: none;">
        Image colorization is the process of estimating RGB colors for grayscale images or video frames to improve their aesthetic and perceptual quality. Deep learning techniques for image colorization have progressed notably over the last decade, calling the need for a systematic survey and benchmarking of these techniques. This article presents a comprehensive survey of recent state-of-the-art deep learning-based image colorization techniques, describing their fundamental block architectures, inputs, optimizers, loss functions, training protocols, and training data \textit{etc.} It categorizes the existing colorization techniques into seven classes and discusses important factors governing their performance, such as benchmark datasets and evaluation metrics. We highlight the limitations of existing datasets and introduce a new dataset specific to colorization. Using the existing datasets and our new one, we perform an extensive experimental evaluation of existing image colorization methods. Finally, we discuss the limitations of existing methods and recommend possible solutions as well as future research directions for this rapidly evolving topic of deep image colorization. Dataset and codes for evaluation are publicly available at https://github.com/saeed-anwar/ColorSurvey
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.10774v3-abstract-full').style.display = 'none'; document.getElementById('2008.10774v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.06568">arXiv:2008.06568</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.06568">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Evaluation Framework of End-to-End 5G Millimeter Wave Communication for Connected Vehicle Applications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+Z">Zadid Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S+M">Sakib Mahmud Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chowdhury%2C+M">Mashrur Chowdhury</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Rahman%2C+M">Mizanur Rahman</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Islam%2C+M">Mhafuzul Islam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.06568v1-abstract-short" style="display: inline;">
        The internet-of-things (IoT) environment connects different intelligent devices together and enables seamless data communication between the connected devices. Connected vehicles (CVs) are one of the primary example of the IoT, and the efficient, reliable, and safe operation of CVs demands a reliable wireless communication system, which can ensure high throughput and low communication latency. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.06568v1-abstract-full').style.display = 'inline'; document.getElementById('2008.06568v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.06568v1-abstract-full" style="display: none;">
        The internet-of-things (IoT) environment connects different intelligent devices together and enables seamless data communication between the connected devices. Connected vehicles (CVs) are one of the primary example of the IoT, and the efficient, reliable, and safe operation of CVs demands a reliable wireless communication system, which can ensure high throughput and low communication latency. The 5G millimeter wave (5G mmWave) wireless communication network offers such benefits, which can be the enabler of CV applications, especially for dense urban areas with high number of CVs. In this study, we present a simulation-based evaluation framework of end-to-end 5G mmWave communication for CV applications. In addition, we compare the 5G mmWave with the Dedicated Short Range Communication (DSRC) technology for a CV application. The simulation framework is developed using two simulators, a network simulator and a traffic simulator. In order to develop the framework in this study, we have used Network Simulator 3 (ns-3) and SUMO, an open-source microscopic roadway traffic simulator. We have used end-to-end latency, packet loss and throughput as the performance evaluation metrics. We have found that for dense urban areas, 5G mmWave can achieve higher throughput, lower latency and lower data loss compared to DSRC. 5G mmWave can support CV applications with high throughput requirement on the downlink data flow. Through further investigation, we have found that the performance of 5G mmWave is significantly impacted by the penetration level of CVs, maximum CV speed, and CV application requirements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.06568v1-abstract-full').style.display = 'none'; document.getElementById('2008.06568v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 9 figures, 1 table, submitted for publication as presentation or journal paper at transportation research board 2021 (TRB 2021). arXiv admin note: text overlap with arXiv:1808.04517</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.05205">arXiv:2007.05205</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.05205">pdf</a>, <a href="https://arxiv.org/format/2007.05205">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OT-driven Multi-Domain Unsupervised Ultrasound Image Artifact Removal using a Single CNN
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Huh%2C+J">Jaeyoung Huh</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+J+C">Jong Chul Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.05205v1-abstract-short" style="display: inline;">
        Ultrasound imaging (US) often suffers from distinct image artifacts from various sources. Classic approaches for solving these problems are usually model-based iterative approaches that have been developed specifically for each type of artifact, which are often computationally intensive. Recently, deep learning approaches have been proposed as computationally efficient and high performance alterna&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.05205v1-abstract-full').style.display = 'inline'; document.getElementById('2007.05205v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.05205v1-abstract-full" style="display: none;">
        Ultrasound imaging (US) often suffers from distinct image artifacts from various sources. Classic approaches for solving these problems are usually model-based iterative approaches that have been developed specifically for each type of artifact, which are often computationally intensive. Recently, deep learning approaches have been proposed as computationally efficient and high performance alternatives. Unfortunately, in the current deep learning approaches, a dedicated neural network should be trained with matched training data for each specific artifact type. This poses a fundamental limitation in the practical use of deep learning for US, since large number of models should be stored to deal with various US image artifacts. Inspired by the recent success of multi-domain image transfer, here we propose a novel, unsupervised, deep learning approach in which a single neural network can be used to deal with different types of US artifacts simply by changing a mask vector that switches between different target domains. Our algorithm is rigorously derived using an optimal transport (OT) theory for cascaded probability measures. Experimental results using phantom and in vivo data demonstrate that the proposed method can generate high quality image by removing distinct artifacts, which are comparable to those obtained by separately trained multiple neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.05205v1-abstract-full').style.display = 'none'; document.getElementById('2007.05205v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.02294">arXiv:2007.02294</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.02294">pdf</a>, <a href="https://arxiv.org/format/2007.02294">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applied Physics">physics.app-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/OJAP.2020.2991522">10.1109/OJAP.2020.2991522 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ultra-Wideband Antenna with MIMO Diversity for 5G Wireless Communication
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+M+S">M. Saeed Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Iftikhar%2C+A">Adnan Iftikhar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shubair%2C+R+M">Raed M. Shubair</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Capobianco%2C+A">Antonio-D. Capobianco</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Braaten%2C+B+D">Benjamin D. Braaten</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Anagnostou%2C+D+E">Dimitris E. Anagnostou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.02294v1-abstract-short" style="display: inline;">
        An eight element, compact Ultra Wideband-Multiple Input Multiple Output (UWB-MIMO) antenna capable of providing high data rates for future Fifth Generation (5G) terminal equipments along with the provision of necessary bandwidth for Third Generation (3G) and Fourth Generation (4G) communications that accomplishes band rejection from 4.85 to 6.35 GHz by deploying a Inductor Capacitor (LC) stub on t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.02294v1-abstract-full').style.display = 'inline'; document.getElementById('2007.02294v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.02294v1-abstract-full" style="display: none;">
        An eight element, compact Ultra Wideband-Multiple Input Multiple Output (UWB-MIMO) antenna capable of providing high data rates for future Fifth Generation (5G) terminal equipments along with the provision of necessary bandwidth for Third Generation (3G) and Fourth Generation (4G) communications that accomplishes band rejection from 4.85 to 6.35 GHz by deploying a Inductor Capacitor (LC) stub on the ground plane is presented. The incorporated stub also provides flexibility to reject any selected band as well as bandwidth control. The orthogonal placement of the printed monopoles permits polarization diversity and provides high isolation. In the proposed eight element UWB-MIMO/diversity antenna, monopole pair 3-4 are 180o mirrored transform of monopole pair 1-2 which lie on the opposite corners of a planar 50 x 50 mm2 substrate. Four additional monopoles are then placed perpendicularly to the same board leading to a total size of 50 x 50 x 25 mm3 only. The simulated results are validated by comparing the measurements of a fabricated prototype. It was concluded that the design meets the target specifications over the entire bandwidth of 2 to 12 GHz with a reflection coefficient better than -10 dB (except the rejected band), isolation more than 17 dB, low envelope correlation, low gain variation, stable radiation pattern, and strong rejection of the signals in the Wireless Local Area Network (WLAN) band. Overall, compact and reduced complexity of the proposed eight element architecture, strengthens its practical viability for the diversity applications in future 5G terminal equipments amongst other MIMO antennas designs present in the literature.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.02294v1-abstract-full').style.display = 'none'; document.getElementById('2007.02294v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.14773">arXiv:2006.14773</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.14773">pdf</a>, <a href="https://arxiv.org/format/2006.14773">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pushing the Limit of Unsupervised Learning for Ultrasound Image Artifact Removal
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Shujaat Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Huh%2C+J">Jaeyoung Huh</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+J+C">Jong Chul Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.14773v1-abstract-short" style="display: inline;">
        Ultrasound (US) imaging is a fast and non-invasive imaging modality which is widely used for real-time clinical imaging applications without concerning about radiation hazard. Unfortunately, it often suffers from poor visual quality from various origins, such as speckle noises, blurring, multi-line acquisition (MLA), limited RF channels, small number of view angles for the case of plane wave imagi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.14773v1-abstract-full').style.display = 'inline'; document.getElementById('2006.14773v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.14773v1-abstract-full" style="display: none;">
        Ultrasound (US) imaging is a fast and non-invasive imaging modality which is widely used for real-time clinical imaging applications without concerning about radiation hazard. Unfortunately, it often suffers from poor visual quality from various origins, such as speckle noises, blurring, multi-line acquisition (MLA), limited RF channels, small number of view angles for the case of plane wave imaging, etc. Classical methods to deal with these problems include image-domain signal processing approaches using various adaptive filtering and model-based approaches. Recently, deep learning approaches have been successfully used for ultrasound imaging field. However, one of the limitations of these approaches is that paired high quality images for supervised training are difficult to obtain in many practical applications. In this paper, inspired by the recent theory of unsupervised learning using optimal transport driven cycleGAN (OT-cycleGAN), we investigate applicability of unsupervised deep learning for US artifact removal problems without matched reference data. Experimental results for various tasks such as deconvolution, speckle removal, limited data artifact removal, etc. confirmed that our unsupervised learning method provides comparable results to supervised learning for many practical applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.14773v1-abstract-full').style.display = 'none'; document.getElementById('2006.14773v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Khan%2C+S&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Khan%2C+S&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Khan%2C+S&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>