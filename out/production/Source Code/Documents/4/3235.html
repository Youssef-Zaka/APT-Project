<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 58 results for author: <span class="mathjax">Yang, M</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/eess"  aria-role="search">
    
      Searching in archive <strong>eess</strong>. <a href="/search/?searchtype=author&amp;query=Yang%2C+M">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Yang, M">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Yang%2C+M&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Yang, M">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Yang%2C+M&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Yang%2C+M&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Yang%2C+M&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01649">arXiv:2205.01649</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01649">pdf</a>, <a href="https://arxiv.org/format/2205.01649">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Enriched Features for Fast Image Restoration and Enhancement
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01649v1-abstract-short" style="display: inline;">
        Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01649v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01649v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01649v1-abstract-full" style="display: none;">
        Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatial details are preserved but the contextual information cannot be precisely encoded. In the latter case, generated outputs are semantically reliable but spatially less accurate. This paper presents a new architecture with a holistic goal of maintaining spatially-precise high-resolution representations through the entire network, and receiving complementary contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing the following key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) non-local attention mechanism for capturing contextual information, and (d) attention based multi-scale feature aggregation. Our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on six real image benchmark datasets demonstrate that our method, named as MIRNet-v2 , achieves state-of-the-art results for a variety of image processing tasks, including defocus deblurring, image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNetv2
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01649v1-abstract-full').style.display = 'none'; document.getElementById('2205.01649v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This article supersedes arXiv:2003.06792. Accepted for publication in TPAMI</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.01593">arXiv:2204.01593</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.01593">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimize Deep Learning Models for Prediction of Gene Mutations Using Unsupervised Clustering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Chen%2C+Z">Zihan Chen</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+X">Xingyu Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Miaomiao Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhang%2C+H">Hong Zhang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Xu%2C+X+S">Xu Steven Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.01593v2-abstract-short" style="display: inline;">
        Deep learning has become the mainstream methodological choice for analyzing and interpreting whole-slide digital pathology images (WSIs). It is commonly assumed that tumor regions carry most predictive information. In this paper, we proposed an unsupervised clustering-based multiple-instance learning, and apply our method to develop deep-learning models for prediction of gene mutations using WSIs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01593v2-abstract-full').style.display = 'inline'; document.getElementById('2204.01593v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.01593v2-abstract-full" style="display: none;">
        Deep learning has become the mainstream methodological choice for analyzing and interpreting whole-slide digital pathology images (WSIs). It is commonly assumed that tumor regions carry most predictive information. In this paper, we proposed an unsupervised clustering-based multiple-instance learning, and apply our method to develop deep-learning models for prediction of gene mutations using WSIs from three cancer types in The Cancer Genome Atlas (TCGA) studies (CRC, LUAD, and HNSCC). We showed that unsupervised clustering of image patches could help identify predictive patches, exclude patches lack of predictive information, and therefore improve prediction on gene mutations in all three different cancer types, compared with the WSI based method without selection of image patches and models based on only tumor regions. Additionally, our proposed algorithm outperformed two recently published baseline algorithms leveraging unsupervised clustering to assist model prediction. The unsupervised-clustering-based approach for mutation prediction allows identification of the spatial regions related to mutation of a specific gene via the resolved probability scores, highlighting the heterogeneity of a predicted genotype in the tumor microenvironment. Finally, our study also demonstrated that selection of tumor regions of WSIs is not always the best way to identify patches for prediction of gene mutations, and other tissue types in the tumor micro-environment may provide better prediction ability for gene mutations than tumor tissues.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.01593v2-abstract-full').style.display = 'none'; document.getElementById('2204.01593v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.15937">arXiv:2203.15937</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.15937">pdf</a>, <a href="https://arxiv.org/format/2203.15937">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mu Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hirschi%2C+K">Kevin Hirschi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Looney%2C+S+D">Stephen D. Looney</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kang%2C+O">Okim Kang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hansen%2C+J+H+L">John H. L. Hansen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.15937v2-abstract-short" style="display: inline;">
        Current leading mispronunciation detection and diagnosis (MDD) systems achieve promising performance via end-to-end phoneme recognition. One challenge of such end-to-end solutions is the scarcity of human-annotated phonemes on natural L2 speech. In this work, we leverage unlabeled L2 speech via a pseudo-labeling (PL) procedure and extend the fine-tuning approach based on pre-trained self-supervise&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15937v2-abstract-full').style.display = 'inline'; document.getElementById('2203.15937v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.15937v2-abstract-full" style="display: none;">
        Current leading mispronunciation detection and diagnosis (MDD) systems achieve promising performance via end-to-end phoneme recognition. One challenge of such end-to-end solutions is the scarcity of human-annotated phonemes on natural L2 speech. In this work, we leverage unlabeled L2 speech via a pseudo-labeling (PL) procedure and extend the fine-tuning approach based on pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec 2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples plus the created pseudo-labeled L2 speech samples. Our pseudo labels are dynamic and are produced by an ensemble of the online model on-the-fly, which ensures that our model is robust to pseudo label noise. We show that fine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and 2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning baseline. The proposed PL method is also shown to outperform conventional offline PL methods. Compared to the state-of-the-art MDD systems, our MDD solution achieves a more accurate and consistent phonetic error diagnosis. In addition, we conduct an open test on a separate UTD-4Accents dataset, where our system recognition outputs show a strong correlation with human perception, based on accentedness and intelligibility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.15937v2-abstract-full').style.display = 'none'; document.getElementById('2203.15937v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to Interspeech 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11279">arXiv:2203.11279</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11279">pdf</a>, <a href="https://arxiv.org/ps/2203.11279">ps</a>, <a href="https://arxiv.org/format/2203.11279">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3524499">10.1145/3524499 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EEG based Emotion Recognition: A Tutorial and Review
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+X">Xiang Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhang%2C+Y">Yazhou Zhang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tiwari%2C+P">Prayag Tiwari</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Song%2C+D">Dawei Song</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hu%2C+B">Bin Hu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Meihong Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhao%2C+Z">Zhigang Zhao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kumar%2C+N">Neeraj Kumar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Marttinen%2C+P">Pekka Marttinen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11279v1-abstract-short" style="display: inline;">
        Emotion recognition technology through analyzing the EEG signal is currently an essential concept in Artificial Intelligence and holds great potential in emotional health care, human-computer interaction, multimedia content recommendation, etc. Though there have been several works devoted to reviewing EEG-based emotion recognition, the content of these reviews needs to be updated. In addition, tho&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11279v1-abstract-full').style.display = 'inline'; document.getElementById('2203.11279v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11279v1-abstract-full" style="display: none;">
        Emotion recognition technology through analyzing the EEG signal is currently an essential concept in Artificial Intelligence and holds great potential in emotional health care, human-computer interaction, multimedia content recommendation, etc. Though there have been several works devoted to reviewing EEG-based emotion recognition, the content of these reviews needs to be updated. In addition, those works are either fragmented in content or only focus on specific techniques adopted in this area but neglect the holistic perspective of the entire technical routes. Hence, in this paper, we review from the perspective of researchers who try to take the first step on this topic. We review the recent representative works in the EEG-based emotion recognition research and provide a tutorial to guide the researchers to start from the beginning. The scientific basis of EEG-based emotion recognition in the psychological and physiological levels is introduced. Further, we categorize these reviewed works into different technical routes and illustrate the theoretical basis and the research motivation, which will help the readers better understand why those techniques are studied and employed. At last, existing challenges and future investigations are also discussed in this paper, which guides the researchers to decide potential future research directions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11279v1-abstract-full').style.display = 'none'; document.getElementById('2203.11279v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.12584">arXiv:2201.12584</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.12584">pdf</a>, <a href="https://arxiv.org/format/2201.12584">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Convolutional Filtering in Simplicial Complexes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Isufi%2C+E">Elvin Isufi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Maosheng Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.12584v1-abstract-short" style="display: inline;">
        This paper proposes convolutional filtering for data whose structure can be modeled by a simplicial complex (SC). SCs are mathematical tools that not only capture pairwise relationships as graphs but account also for higher-order network structures. These filters are built by following the shift-and-sum principle of the convolution operation and rely on the Hodge-Laplacians to shift the signal wit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.12584v1-abstract-full').style.display = 'inline'; document.getElementById('2201.12584v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.12584v1-abstract-full" style="display: none;">
        This paper proposes convolutional filtering for data whose structure can be modeled by a simplicial complex (SC). SCs are mathematical tools that not only capture pairwise relationships as graphs but account also for higher-order network structures. These filters are built by following the shift-and-sum principle of the convolution operation and rely on the Hodge-Laplacians to shift the signal within the simplex. But since in SCs we have also inter-simplex coupling, we use the incidence matrices to transfer the signal in adjacent simplices and build a filter bank to jointly filter signals from different levels. We prove some interesting properties for the proposed filter bank, including permutation and orientation equivariance, a computational complexity that is linear in the SC dimension, and a spectral interpretation using the simplicial Fourier transform. We illustrate the proposed approach with numerical experiments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.12584v1-abstract-full').style.display = 'none'; document.getElementById('2201.12584v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, accepted in ICASSP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.11720">arXiv:2201.11720</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.11720">pdf</a>, <a href="https://arxiv.org/format/2201.11720">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Algebraic Topology">math.AT</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Spectral Theory">math.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Simplicial Convolutional Filters
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Maosheng Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Isufi%2C+E">Elvin Isufi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Schaub%2C+M+T">Michael T. Schaub</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Leus%2C+G">Geert Leus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.11720v1-abstract-short" style="display: inline;">
        We study linear filters for processing signals supported on abstract topological spaces modeled as simplicial complexes, which may be interpreted as generalizations of graphs that account for nodes, edges, triangular faces etc. To process such signals, we develop simplicial convolutional filters defined as matrix polynomials of the lower and upper Hodge Laplacians. First, we study the properties o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11720v1-abstract-full').style.display = 'inline'; document.getElementById('2201.11720v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.11720v1-abstract-full" style="display: none;">
        We study linear filters for processing signals supported on abstract topological spaces modeled as simplicial complexes, which may be interpreted as generalizations of graphs that account for nodes, edges, triangular faces etc. To process such signals, we develop simplicial convolutional filters defined as matrix polynomials of the lower and upper Hodge Laplacians. First, we study the properties of these filters and show that they are linear and shift-invariant, as well as permutation and orientation equivariant. These filters can also be implemented in a distributed fashion with a low computational complexity, as they involve only (multiple rounds of) simplicial shifting between upper and lower adjacent simplices. Second, focusing on edge-flows, we study the frequency responses of these filters and examine how we can use the Hodge-decomposition to delineate gradient, curl and harmonic frequencies. We discuss how these frequencies correspond to the lower- and the upper-adjacent couplings and the kernel of the Hodge Laplacian, respectively, and can be tuned independently by our filter designs. Third, we study different procedures for designing simplicial convolutional filters and discuss their relative advantages. Finally, we corroborate our simplicial filters in several applications: to extract different frequency components of a simplicial signal, to denoise edge flows, and to analyze financial markets and traffic networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11720v1-abstract-full').style.display = 'none'; document.getElementById('2201.11720v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 13 figures, 2 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.03186">arXiv:2201.03186</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.03186">pdf</a>, <a href="https://arxiv.org/format/2201.03186">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MyoPS: A Benchmark of Myocardial Pathology Segmentation Combining Three-Sequence Cardiac Magnetic Resonance Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+L">Lei Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wu%2C+F">Fuping Wu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+S">Sihan Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Luo%2C+X">Xinzhe Luo</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Martin-Isla%2C+C">Carlos Martin-Isla</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhai%2C+S">Shuwei Zhai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhang%2C+J">Jianpeng Zhang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu7%2C+Y">Yanfei Liu7</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhang%2C+Z">Zhen Zhang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ankenbrand%2C+M+J">Markus J. Ankenbrand</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Jiang%2C+H">Haochuan Jiang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhang%2C+X">Xiaoran Zhang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+L">Linhong Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Arega%2C+T+W">Tewodros Weldebirhan Arega</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Altunok%2C+E">Elif Altunok</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhao%2C+Z">Zhou Zhao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+F">Feiyan Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ma%2C+J">Jun Ma</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+X">Xiaoping Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Puybareau%2C+E">Elodie Puybareau</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Oksuz%2C+I">Ilkay Oksuz</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Bricq%2C+S">Stephanie Bricq</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+W">Weisheng Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Punithakumar%2C+K">Kumaradevan Punithakumar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tsaftaris%2C+S+A">Sotirios A. Tsaftaris</a>
      , et al. (7 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.03186v1-abstract-short" style="display: inline;">
        Assessment of myocardial viability is essential in diagnosis and treatment management of patients suffering from myocardial infarction, and classification of pathology on myocardium is the key to this assessment. This work defines a new task of medical image analysis, i.e., to perform myocardial pathology segmentation (MyoPS) combining three-sequence cardiac magnetic resonance (CMR) images, which&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.03186v1-abstract-full').style.display = 'inline'; document.getElementById('2201.03186v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.03186v1-abstract-full" style="display: none;">
        Assessment of myocardial viability is essential in diagnosis and treatment management of patients suffering from myocardial infarction, and classification of pathology on myocardium is the key to this assessment. This work defines a new task of medical image analysis, i.e., to perform myocardial pathology segmentation (MyoPS) combining three-sequence cardiac magnetic resonance (CMR) images, which was first proposed in the MyoPS challenge, in conjunction with MICCAI 2020. The challenge provided 45 paired and pre-aligned CMR images, allowing algorithms to combine the complementary information from the three CMR sequences for pathology segmentation. In this article, we provide details of the challenge, survey the works from fifteen participants and interpret their methods according to five aspects, i.e., preprocessing, data augmentation, learning strategy, model architecture and post-processing. In addition, we analyze the results with respect to different factors, in order to examine the key obstacles and explore potential of solutions, as well as to provide a benchmark for future research. We conclude that while promising results have been reported, the research is still in the early stage, and more in-depth exploration is needed before a successful application to the clinics. Note that MyoPS data and evaluation tool continue to be publicly available upon registration via its homepage (www.sdspeople.fudan.edu.cn/zhuangxiahai/0/myops20/).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.03186v1-abstract-full').style.display = 'none'; document.getElementById('2201.03186v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.02972">arXiv:2201.02972</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.02972">pdf</a>, <a href="https://arxiv.org/ps/2201.02972">ps</a>, <a href="https://arxiv.org/format/2201.02972">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Performance Analysis and Power Allocation of Joint Communication and Sensing Towards Future Communication Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+M">Meng Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Minglei Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+H">Huifang Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zeng%2C+K">Kun Zeng</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhang%2C+Z">Zhaoming Zhang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Cheng%2C+X">Xiancheng Cheng</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Nallanathan%2C+A">Arumugam Nallanathan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ng%2C+D+W+K">Derrick Wing Kwan Ng</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+G">Guangjian Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.02972v1-abstract-short" style="display: inline;">
        To mitigate the radar and communication frequency overlapping caused by massive devices access, we propose a novel joint communication and sensing (JCS) system in this paper, where a micro base station (MiBS) can realize target sensing and cooperative communication simultaneously. Concretely, the MiBS, as the sensing equipment, can also serve as a full-duplex (FD) decode-and-forward (DF) relay to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.02972v1-abstract-full').style.display = 'inline'; document.getElementById('2201.02972v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.02972v1-abstract-full" style="display: none;">
        To mitigate the radar and communication frequency overlapping caused by massive devices access, we propose a novel joint communication and sensing (JCS) system in this paper, where a micro base station (MiBS) can realize target sensing and cooperative communication simultaneously. Concretely, the MiBS, as the sensing equipment, can also serve as a full-duplex (FD) decode-and-forward (DF) relay to assist the end-to-end communication. To further improve the spectrum utilization, non-orthogonal multiple access (NOMA) is adopted such that the communication between the macro base station (MaBS) and the Internet-of-Things (IoT) devices. To facilitate the performance evaluation, the exact and asymptotic outage probabilities, ergodic rates, sensing probability of the system are characterized. Subsequently, two optimal power allocation (OPA) problems of maximizing the received signal-to-interference-plus-noise ratio of sensing signal and maximizing the sum rate for communication are designed that are solved by means of the Lagrangian method and function monotonicity. The simulation results demonstrate that: 1) the proposed JCS NOMA system can accomplish both communication enhancement and sensing function under the premise of the same power consumption as non-cooperative NOMA; 2) the proposed OPA schemes manifest superiorities over a random power allocation scheme.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.02972v1-abstract-full').style.display = 'none'; document.getElementById('2201.02972v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.12322">arXiv:2111.12322</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.12322">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.jclepro.2021.129840">10.1016/j.jclepro.2021.129840 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stochastic optimal scheduling of demand response-enabled microgrids with renewable generations: An analytical-heuristic approach
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+Y">Yang Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+K">Kang Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+Z">Zhen Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yu%2C+Y">Yang Yu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Xu%2C+R">Runnan Xu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Miaosen Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.12322v1-abstract-short" style="display: inline;">
        In the context of transition towards cleaner and sustainable energy production, microgrids have become an effective way for tackling environmental pollution and energy crisis issues. With the increasing penetration of renewables, how to coordinate demand response and renewable generations is a critical and challenging issue in the field of microgrid scheduling. To this end, a bi-level scheduling m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12322v1-abstract-full').style.display = 'inline'; document.getElementById('2111.12322v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.12322v1-abstract-full" style="display: none;">
        In the context of transition towards cleaner and sustainable energy production, microgrids have become an effective way for tackling environmental pollution and energy crisis issues. With the increasing penetration of renewables, how to coordinate demand response and renewable generations is a critical and challenging issue in the field of microgrid scheduling. To this end, a bi-level scheduling model is put forward for isolated microgrids with consideration of multi-stakeholders in this paper, where the lower- and upper-level models respectively aim to the minimization of user cost and microgrid operational cost under real-time electricity pricing environments. In order to solve this model, this research combines Jaya algorithm and interior point method (IPM) to develop a hybrid analysis-heuristic solution method called Jaya-IPM, where the lower- and upper- levels are respectively addressed by the IPM and the Jaya, and the scheduling scheme is obtained via iterations between the two levels. After that, the real-time prices updated by the upper-level model and the electricity plans determined by the lower-level model will be alternately iterated between the upper- and lower- levels through the real-time pricing mechanism to obtain an optimal scheduling plan. The test results show that the proposed method can coordinate the uncertainty of renewable generations with demand response strategies, thereby achieving a balance between the interests of microgrid and users; and that by leveraging demand response, the flexibility of the load side can be fully exploited to achieve peak load shaving while maintaining the balance of supply and demand. In addition, the Jaya-IPM algorithm is proven to be superior to the traditional hybrid intelligent algorithm (HIA) and the CPLEX solver in terms of optimization results and calculation efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12322v1-abstract-full').style.display = 'none'; document.getElementById('2111.12322v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Journal of Cleaner Production</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Journal of Cleaner Production 330 (2022) 129840
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.12228">arXiv:2111.12228</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.12228">pdf</a>, <a href="https://arxiv.org/format/2111.12228">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Artificial intelligence enabled radio propagation for communications-Part II: Scenario identification and channel modeling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Huang%2C+C">Chen Huang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=He%2C+R">Ruisi He</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ai%2C+B">Bo Ai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Molisch%2C+A+F">Andreas F. Molisch</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lau%2C+B+K">Buon Kiong Lau</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Haneda%2C+K">Katsuyuki Haneda</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+B">Bo Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+C">Cheng-Xiang Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mi Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Oestges%2C+C">Claude Oestges</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhong%2C+Z">Zhangdui Zhong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.12228v1-abstract-short" style="display: inline;">
        This two-part paper investigates the application of artificial intelligence (AI) and in particular machine learning (ML) to the study of wireless propagation channels. In Part I, we introduced AI and ML as well as provided a comprehensive survey on ML enabled channel characterization and antenna-channel optimization, and in this part (Part II) we review state-of-the-art literature on scenario iden&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12228v1-abstract-full').style.display = 'inline'; document.getElementById('2111.12228v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.12228v1-abstract-full" style="display: none;">
        This two-part paper investigates the application of artificial intelligence (AI) and in particular machine learning (ML) to the study of wireless propagation channels. In Part I, we introduced AI and ML as well as provided a comprehensive survey on ML enabled channel characterization and antenna-channel optimization, and in this part (Part II) we review state-of-the-art literature on scenario identification and channel modeling here. In particular, the key ideas of ML for scenario identification and channel modeling/prediction are presented, and the widely used ML methods for propagation scenario identification and channel modeling and prediction are analyzed and compared. Based on the state-of-art, the future challenges of AI/ML-based channel data processing techniques are given as well.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12228v1-abstract-full').style.display = 'none'; document.getElementById('2111.12228v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.12227">arXiv:2111.12227</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.12227">pdf</a>, <a href="https://arxiv.org/format/2111.12227">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Artificial intelligence enabled radio propagation for communications-Part I: Channel characterization and antenna-channel optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Huang%2C+C">Chen Huang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=He%2C+R">Ruisi He</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ai%2C+B">Bo Ai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Molisch%2C+A+F">Andreas F. Molisch</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lau%2C+B+K">Buon Kiong Lau</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Haneda%2C+K">Katsuyuki Haneda</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+B">Bo Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+C">Cheng-Xiang Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mi Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Oestges%2C+C">Claude Oestges</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhong%2C+Z">Zhangdui Zhong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.12227v1-abstract-short" style="display: inline;">
        To provide higher data rates, as well as better coverage, cost efficiency, security, adaptability, and scalability, the 5G and beyond 5G networks are developed with various artificial intelligence techniques. In this two-part paper, we investigate the application of artificial intelligence (AI) and in particular machine learning (ML) to the study of wireless propagation channels. It firstly provid&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12227v1-abstract-full').style.display = 'inline'; document.getElementById('2111.12227v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.12227v1-abstract-full" style="display: none;">
        To provide higher data rates, as well as better coverage, cost efficiency, security, adaptability, and scalability, the 5G and beyond 5G networks are developed with various artificial intelligence techniques. In this two-part paper, we investigate the application of artificial intelligence (AI) and in particular machine learning (ML) to the study of wireless propagation channels. It firstly provides a comprehensive overview of ML for channel characterization and ML-based antenna-channel optimization in this first part, and then it gives a state-of-the-art literature review of channel scenario identification and channel modeling in Part II. Fundamental results and key concepts of ML for communication networks are presented, and widely used ML methods for channel data processing, propagation channel estimation, and characterization are analyzed and compared. A discussion of challenges and future research directions for ML-enabled next generation networks of the topics covered in this part rounds off the paper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.12227v1-abstract-full').style.display = 'none'; document.getElementById('2111.12227v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.03263">arXiv:2111.03263</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.03263">pdf</a>, <a href="https://arxiv.org/format/2111.03263">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning Based Near-Orthogonal Superposition Code for Short Message Transmission
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Bian%2C+C">Chenghong Bian</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingyu Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hsu%2C+C">Chin-Wei Hsu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kim%2C+H">Hun-Seok Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.03263v2-abstract-short" style="display: inline;">
        Massive machine type communication (mMTC) has attracted new coding schemes optimized for reliable short message transmission. In this paper, a novel deep learning based near-orthogonal superposition (NOS) coding scheme is proposed for reliable transmission of short messages in the additive white Gaussian noise (AWGN) channel for mMTC applications. Similar to recent hyper-dimensional modulation (HD&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03263v2-abstract-full').style.display = 'inline'; document.getElementById('2111.03263v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.03263v2-abstract-full" style="display: none;">
        Massive machine type communication (mMTC) has attracted new coding schemes optimized for reliable short message transmission. In this paper, a novel deep learning based near-orthogonal superposition (NOS) coding scheme is proposed for reliable transmission of short messages in the additive white Gaussian noise (AWGN) channel for mMTC applications. Similar to recent hyper-dimensional modulation (HDM), the NOS encoder spreads the information bits to multiple near-orthogonal high dimensional vectors to be combined (superimposed) into a single vector for transmission. The NOS decoder first estimates the information vectors and then performs a cyclic redundancy check (CRC)-assisted K-best tree-search algorithm to further reduce the packet error rate. The proposed NOS encoder and decoder are deep neural networks (DNNs) jointly trained as an auto encoder and decoder pair to learn a new NOS coding scheme with near-orthogonal codewords. Simulation results show the proposed deep learning-based NOS scheme outperforms HDM and Polar code with CRC-aided list decoding for short(32-bit) message transmission.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03263v2-abstract-full').style.display = 'none'; document.getElementById('2111.03263v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 November, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.04482">arXiv:2110.04482</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.04482">pdf</a>, <a href="https://arxiv.org/format/2110.04482">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mu Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ding%2C+S">Shaojin Ding</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chen%2C+T">Tianlong Chen</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+T">Tong Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+Z">Zhangyang Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.04482v1-abstract-short" style="display: inline;">
        This work presents a lifelong learning approach to train a multilingual Text-To-Speech (TTS) system, where each language was seen as an individual task and was learned sequentially and continually. It does not require pooled data from all languages altogether, and thus alleviates the storage and computation burden. One of the challenges of lifelong learning methods is &#34;catastrophic forgetting&#34;: in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.04482v1-abstract-full').style.display = 'inline'; document.getElementById('2110.04482v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.04482v1-abstract-full" style="display: none;">
        This work presents a lifelong learning approach to train a multilingual Text-To-Speech (TTS) system, where each language was seen as an individual task and was learned sequentially and continually. It does not require pooled data from all languages altogether, and thus alleviates the storage and computation burden. One of the challenges of lifelong learning methods is &#34;catastrophic forgetting&#34;: in TTS scenario it means that model performance quickly degrades on previous languages when adapted to a new language. We approach this problem via a data-replay-based lifelong learning method. We formulate the replay process as a supervised learning problem, and propose a simple yet effective dual-sampler framework to tackle the heavily language-imbalanced training samples. Through objective and subjective evaluations, we show that this supervised learning formulation outperforms other gradient-based and regularization-based lifelong learning methods, achieving 43% Mel-Cepstral Distortion reduction compared to a fine-tuning baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.04482v1-abstract-full').style.display = 'none'; document.getElementById('2110.04482v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.04456">arXiv:2110.04456</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.04456">pdf</a>, <a href="https://arxiv.org/format/2110.04456">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Joint Source-Channel Coding for Wireless Image Transmission with Adaptive Rate Control
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingyu Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kim%2C+H">Hun-Seok Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.04456v1-abstract-short" style="display: inline;">
        We present a novel adaptive deep joint source-channel coding (JSCC) scheme for wireless image transmission. The proposed scheme supports multiple rates using a single deep neural network (DNN) model and learns to dynamically control the rate based on the channel condition and image contents. Specifically, a policy network is introduced to exploit the tradeoff space between the rate and signal qual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.04456v1-abstract-full').style.display = 'inline'; document.getElementById('2110.04456v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.04456v1-abstract-full" style="display: none;">
        We present a novel adaptive deep joint source-channel coding (JSCC) scheme for wireless image transmission. The proposed scheme supports multiple rates using a single deep neural network (DNN) model and learns to dynamically control the rate based on the channel condition and image contents. Specifically, a policy network is introduced to exploit the tradeoff space between the rate and signal quality. To train the policy network, the Gumbel-Softmax trick is adopted to make the policy network differentiable and hence the whole JSCC scheme can be trained end-to-end. To the best of our knowledge, this is the first deep JSCC scheme that can automatically adjust its rate using a single network model. Experiments show that our scheme successfully learns a reasonable policy that decreases channel bandwidth utilization for high SNR scenarios or simple image contents. For an arbitrary target rate, our rate-adaptive scheme using a single model achieves similar performance compared to an optimized model specifically trained for that fixed target rate. To reproduce our results, we make the source code publicly available at https://github.com/mingyuyng/Dynamic_JSCC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.04456v1-abstract-full').style.display = 'none'; document.getElementById('2110.04456v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.03715">arXiv:2110.03715</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.03715">pdf</a>, <a href="https://arxiv.org/format/2110.03715">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PEAF: Learnable Power Efficient Analog Acoustic Features for Audio Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Bergsma%2C+B">Boris Bergsma</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Minhao Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Cernak%2C+M">Milos Cernak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.03715v2-abstract-short" style="display: inline;">
        At the end of Moore&#39;s law, new computing paradigms are required to prolong the battery life of wearable and IoT smart audio devices. Theoretical analysis and physical validation have shown that analog signal processing (ASP) can be more power-efficient than its digital counterpart in the realm of low-to-medium signal-to-noise ratio applications. In addition, ASP allows a direct interface with an a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.03715v2-abstract-full').style.display = 'inline'; document.getElementById('2110.03715v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.03715v2-abstract-full" style="display: none;">
        At the end of Moore&#39;s law, new computing paradigms are required to prolong the battery life of wearable and IoT smart audio devices. Theoretical analysis and physical validation have shown that analog signal processing (ASP) can be more power-efficient than its digital counterpart in the realm of low-to-medium signal-to-noise ratio applications. In addition, ASP allows a direct interface with an analog microphone without a power-hungry analog-to-digital converter. Here, we present power-efficient analog acoustic features (PEAF) that are validated by fabricated CMOS chips for running audio recognition. Linear, non-linear, and learnable PEAF variants are evaluated on two speech processing tasks that are demanded in many battery-operated devices: wake word detection (WWD) and keyword spotting (KWS). Compared to digital acoustic features, higher power efficiency with competitive classification accuracy can be obtained. A novel theoretical framework based on information theory is established to analyze the information flow in each individual stage of the feature extraction pipeline. The analysis identifies the information bottleneck and helps improve the KWS accuracy by up to 7%. This work may pave the way to building more power-efficient smart audio devices with best-in-class inference performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.03715v2-abstract-full').style.display = 'none'; document.getElementById('2110.03715v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to Interspeech 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.02585">arXiv:2110.02585</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.02585">pdf</a>, <a href="https://arxiv.org/format/2110.02585">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Simplicial Convolutional Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Maosheng Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Isufi%2C+E">Elvin Isufi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Leus%2C+G">Geert Leus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.02585v1-abstract-short" style="display: inline;">
        Graphs can model networked data by representing them as nodes and their pairwise relationships as edges. Recently, signal processing and neural networks have been extended to process and learn from data on graphs, with achievements in tasks like graph signal reconstruction, graph or node classifications, and link prediction. However, these methods are only suitable for data defined on the nodes of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.02585v1-abstract-full').style.display = 'inline'; document.getElementById('2110.02585v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.02585v1-abstract-full" style="display: none;">
        Graphs can model networked data by representing them as nodes and their pairwise relationships as edges. Recently, signal processing and neural networks have been extended to process and learn from data on graphs, with achievements in tasks like graph signal reconstruction, graph or node classifications, and link prediction. However, these methods are only suitable for data defined on the nodes of a graph. In this paper, we propose a simplicial convolutional neural network (SCNN) architecture to learn from data defined on simplices, e.g., nodes, edges, triangles, etc. We study the SCNN permutation and orientation equivariance, complexity, and spectral analysis. Finally, we test the SCNN performance for imputing citations on a coauthorship complex.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.02585v1-abstract-full').style.display = 'none'; document.getElementById('2110.02585v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 Pages, 2 figures, 1 table, submitted to ICASSP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.05194">arXiv:2109.05194</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.05194">pdf</a>, <a href="https://arxiv.org/format/2109.05194">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OFDM-guided Deep Joint Source Channel Coding for Wireless Multipath Fading Channels
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingyu Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Bian%2C+C">Chenghong Bian</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kim%2C+H">Hun-Seok Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.05194v1-abstract-short" style="display: inline;">
        We investigate joint source channel coding (JSCC) for wireless image transmission over multipath fading channels. Inspired by recent works on deep learning based JSCC and model-based learning methods, we combine an autoencoder with orthogonal frequency division multiplexing (OFDM) to cope with multipath fading. The proposed encoder and decoder use convolutional neural networks (CNNs) and directly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.05194v1-abstract-full').style.display = 'inline'; document.getElementById('2109.05194v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.05194v1-abstract-full" style="display: none;">
        We investigate joint source channel coding (JSCC) for wireless image transmission over multipath fading channels. Inspired by recent works on deep learning based JSCC and model-based learning methods, we combine an autoencoder with orthogonal frequency division multiplexing (OFDM) to cope with multipath fading. The proposed encoder and decoder use convolutional neural networks (CNNs) and directly map the source images to complex-valued baseband samples for OFDM transmission. The multipath channel and OFDM are represented by non-trainable (deterministic) but differentiable layers so that the system can be trained end-to-end. Furthermore, our JSCC decoder further incorporates explicit channel estimation, equalization, and additional subnets to enhance the performance. The proposed method exhibits 2.5 -- 4 dB SNR gain for the equivalent image quality compared to conventional schemes that employ state-of-the-art but separate source and channel coding such as BPG and LDPC. The performance further improves when the system incorporates the channel state information (CSI) feedback. The proposed scheme is robust against OFDM signal clipping and parameter mismatch for the channel model used in training and evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.05194v1-abstract-full').style.display = 'none'; document.getElementById('2109.05194v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 17 figures. arXiv admin note: text overlap with arXiv:2101.03909</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.03444">arXiv:2109.03444</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.03444">pdf</a>, <a href="https://arxiv.org/format/2109.03444">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TPAMI.2021.3106790">10.1109/TPAMI.2021.3106790 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Toward Real-World Super-Resolution via Adaptive Downsampling Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Son%2C+S">Sanghyun Son</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kim%2C+J">Jaeha Kim</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lai%2C+W">Wei-Sheng Lai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Husan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lee%2C+K+M">Kyoung Mu Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.03444v1-abstract-short" style="display: inline;">
        Most image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs that are constructed by a predetermined operation, e.g., bicubic downsampling. As existing methods typically learn an inverse mapping of the specific function, they produce blurry results when applied to real-world images whose exact formulation is different and unknown. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.03444v1-abstract-full').style.display = 'inline'; document.getElementById('2109.03444v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.03444v1-abstract-full" style="display: none;">
        Most image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs that are constructed by a predetermined operation, e.g., bicubic downsampling. As existing methods typically learn an inverse mapping of the specific function, they produce blurry results when applied to real-world images whose exact formulation is different and unknown. Therefore, several methods attempt to synthesize much more diverse LR samples or learn a realistic downsampling model. However, due to restrictive assumptions on the downsampling process, they are still biased and less generalizable. This study proposes a novel method to simulate an unknown downsampling process without imposing restrictive prior knowledge. We propose a generalizable low-frequency loss (LFL) in the adversarial training framework to imitate the distribution of target LR images without using any paired examples. Furthermore, we design an adaptive data loss (ADL) for the downsampler, which can be adaptively learned and updated from the data during the training loops. Extensive experiments validate that our downsampling model can facilitate existing SR methods to perform more accurate reconstructions on various synthetic and real-world examples than the conventional approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.03444v1-abstract-full').style.display = 'none'; document.getElementById('2109.03444v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at TPAMI</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.04856">arXiv:2107.04856</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.04856">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Graphene-based Distributed 3D Sensing Electrodes for Mapping Spatiotemporal Auricular Physiological Signals
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Huang%2C+Q">Q. Huang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wu%2C+C">C. Wu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hou%2C+S">S. Hou</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sun%2C+H">H. Sun</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yao%2C+K">K. Yao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Law%2C+J">J. Law</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">M. Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Vellaisamy%2C+A+L+R">A. L. R. Vellaisamy</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yu%2C+X">X. Yu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chan%2C+H+Y">H. Y. Chan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lao%2C+L">L. Lao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sun%2C+Y">Y. Sun</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+W+J">W. J. Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.04856v1-abstract-short" style="display: inline;">
        Underneath the ear skin there are richly branching vascular and neural networks that ultimately connecting to our heart and brain. Hence, the three-dimensional (3D) mapping of auricular electrophysiological signals could provide a new perspective for biomedical studies such as diagnosis of cardiovascular diseases and neurological disorders. However, it is still extremely challenging for current se&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.04856v1-abstract-full').style.display = 'inline'; document.getElementById('2107.04856v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.04856v1-abstract-full" style="display: none;">
        Underneath the ear skin there are richly branching vascular and neural networks that ultimately connecting to our heart and brain. Hence, the three-dimensional (3D) mapping of auricular electrophysiological signals could provide a new perspective for biomedical studies such as diagnosis of cardiovascular diseases and neurological disorders. However, it is still extremely challenging for current sensing techniques to cover the entire ultra-curved auricle. Here, we report a graphene-based ear-conformable sensing device with embedded and distributed 3D electrodes which enable full-auricle physiological monitoring. The sensing device, which incorporates programable 3D electrode thread array and personalized auricular mold, has 3D-conformable sensing interfaces with curved auricular skin, and was developed using one-step multi-material 3D-printing process. As a proof-of-concept, spatiotemporal auricular electrical skin resistance (AESR) mapping was demonstrated. For the first time, 3D AESR contours were generated and human subject-specific AESR distributions among a population were observed. From the data of 17 volunteers, the auricular region-specific AESR changes after cycling exercise were observed in 98% of the tests and were validated via machine learning techniques. Correlations of AESR with heart rate and blood pressure were also studied using statistical analysis. This 3D electronic platform and AESR-based new biometrical findings show promising biomedical applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.04856v1-abstract-full').style.display = 'none'; document.getElementById('2107.04856v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.02494">arXiv:2107.02494</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.02494">pdf</a>, <a href="https://arxiv.org/format/2107.02494">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Independent Encoder for Deep Hierarchical Unsupervised Image-to-Image Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+K">Kai Ye</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ye%2C+Y">Yinru Ye</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Minqiang Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hu%2C+B">Bin Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.02494v1-abstract-short" style="display: inline;">
        The main challenges of image-to-image (I2I) translation are to make the translated image realistic and retain as much information from the source domain as possible. To address this issue, we propose a novel architecture, termed as IEGAN, which removes the encoder of each network and introduces an encoder that is independent of other networks. Compared with previous models, it embodies three advan&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.02494v1-abstract-full').style.display = 'inline'; document.getElementById('2107.02494v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.02494v1-abstract-full" style="display: none;">
        The main challenges of image-to-image (I2I) translation are to make the translated image realistic and retain as much information from the source domain as possible. To address this issue, we propose a novel architecture, termed as IEGAN, which removes the encoder of each network and introduces an encoder that is independent of other networks. Compared with previous models, it embodies three advantages of our model: Firstly, it is more directly and comprehensively to grasp image information since the encoder no longer receives loss from generator and discriminator. Secondly, the independent encoder allows each network to focus more on its own goal which makes the translated image more realistic. Thirdly, the reduction in the number of encoders performs more unified image representation. However, when the independent encoder applies two down-sampling blocks, it&#39;s hard to extract semantic information. To tackle this problem, we propose deep and shallow information space containing characteristic and semantic information, which can guide the model to translate high-quality images under the task with significant shape or texture change. We compare IEGAN with other previous models, and conduct researches on semantic information consistency and component ablation at the same time. These experiments show the superiority and effectiveness of our architecture. Our code is published on: https://github.com/Elvinky/IEGAN.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.02494v1-abstract-full').style.display = 'none'; document.getElementById('2107.02494v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.04392">arXiv:2106.04392</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.04392">pdf</a>, <a href="https://arxiv.org/format/2106.04392">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Signal Transformer: Complex-valued Attention and Meta-Learning for Signal Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Dong%2C+Y">Yihong Dong</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Peng%2C+Y">Ying Peng</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Muqiao Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lu%2C+S">Songtao Lu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shi%2C+Q">Qingjiang Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.04392v2-abstract-short" style="display: inline;">
        Deep neural networks have been shown as a class of useful tools for addressing signal recognition issues in recent years, especially for identifying the nonlinear feature structures of signals. However, this power of most deep learning techniques heavily relies on an abundant amount of training data, so the performance of classic neural nets decreases sharply when the number of training data sampl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04392v2-abstract-full').style.display = 'inline'; document.getElementById('2106.04392v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.04392v2-abstract-full" style="display: none;">
        Deep neural networks have been shown as a class of useful tools for addressing signal recognition issues in recent years, especially for identifying the nonlinear feature structures of signals. However, this power of most deep learning techniques heavily relies on an abundant amount of training data, so the performance of classic neural nets decreases sharply when the number of training data samples is small or unseen data are presented in the testing phase. This calls for an advanced strategy, i.e., model-agnostic meta-learning (MAML), which is able to capture the invariant representation of the data samples or signals. In this paper, inspired by the special structure of the signal, i.e., real and imaginary parts consisted in practical time-series signals, we propose a Complex-valued Attentional MEta Learner (CAMEL) for the problem of few-shot signal recognition by leveraging attention and meta-learning in the complex domain. To the best of our knowledge, this is also the first complex-valued MAML that can find the first-order stationary points of general nonconvex problems with theoretical convergence guarantees. Extensive experiments results showcase the superiority of the proposed CAMEL compared with the state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.04392v2-abstract-full').style.display = 'none'; document.getElementById('2106.04392v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.01511">arXiv:2105.01511</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.01511">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Radio Communication Scenarios in 5G-Railways
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=He%2C+R">Ruisi He</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ai%2C+B">Bo Ai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhong%2C+Z">Zhangdui Zhong</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mi Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Huang%2C+C">Chen Huang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chen%2C+R">Ruifeng Chen</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ding%2C+J">Jianwen Ding</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mi%2C+H">Hang Mi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ma%2C+Z">Zhangfeng Ma</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Sun%2C+G">Guiqi Sun</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+C">Changzhu Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.01511v1-abstract-short" style="display: inline;">
        With the rapid development of railways, especially high-speed railways, there is an increasingly urgent demand for new wireless communication system for railways. Taking the mature 5G technology as an opportunity, 5G-railways (5G-R) have been widely regarded as a solution to meet the diversified demands of railway wireless communications. For the design, deployment and improvement of 5G-R networks&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.01511v1-abstract-full').style.display = 'inline'; document.getElementById('2105.01511v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.01511v1-abstract-full" style="display: none;">
        With the rapid development of railways, especially high-speed railways, there is an increasingly urgent demand for new wireless communication system for railways. Taking the mature 5G technology as an opportunity, 5G-railways (5G-R) have been widely regarded as a solution to meet the diversified demands of railway wireless communications. For the design, deployment and improvement of 5G-R networks, radio communication scenario classification plays an important role, affecting channel modeling and system performance evaluation. In this paper, a standardized radio communication scenario classification, including 18 scenarios, is proposed for 5G-R. This paper analyzes the differences of 5G-R scenarios compared with the traditional cellular networks and GSM-railways, according to 5G-R requirements and the unique physical environment and propagation characteristics. The proposed standardized scenario classification helps deepen the research of 5G-R and promote the development and application of the existing advanced technologies in railways.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.01511v1-abstract-full').style.display = 'none'; document.getElementById('2105.01511v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.00315">arXiv:2104.00315</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.00315">pdf</a>, <a href="https://arxiv.org/format/2104.00315">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Sound Localization via Iterative Contrastive Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Lin%2C+Y">Yan-Bo Lin</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tseng%2C+H">Hung-Yu Tseng</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lee%2C+H">Hsin-Ying Lee</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lin%2C+Y">Yen-Yu Lin</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.00315v1-abstract-short" style="display: inline;">
        Sound localization aims to find the source of the audio signal in the visual scene. However, it is labor-intensive to annotate the correlations between the signals sampled from the audio and visual modalities, thus making it difficult to supervise the learning of a machine for this task. In this work, we propose an iterative contrastive learning framework that requires no data annotations. At each&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.00315v1-abstract-full').style.display = 'inline'; document.getElementById('2104.00315v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.00315v1-abstract-full" style="display: none;">
        Sound localization aims to find the source of the audio signal in the visual scene. However, it is labor-intensive to annotate the correlations between the signals sampled from the audio and visual modalities, thus making it difficult to supervise the learning of a machine for this task. In this work, we propose an iterative contrastive learning framework that requires no data annotations. At each iteration, the proposed method takes the 1) localization results in images predicted in the previous iteration, and 2) semantic relationships inferred from the audio signals as the pseudo-labels. We then use the pseudo-labels to learn the correlation between the visual and audio signals sampled from the same video (intra-frame sampling) as well as the association between those extracted across videos (inter-frame relation). Our iterative strategy gradually encourages the localization of the sounding objects and reduces the correlation between the non-sounding regions and the reference audio. Quantitative and qualitative experimental results demonstrate that the proposed framework performs favorably against existing unsupervised and weakly-supervised methods on the sound localization task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.00315v1-abstract-full').style.display = 'none'; document.getElementById('2104.00315v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.12587">arXiv:2103.12587</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.12587">pdf</a>, <a href="https://arxiv.org/format/2103.12587">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.23919/EUSIPCO54536.2021.9616185">10.23919/EUSIPCO54536.2021.9616185 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Finite Impulse Response Filters for Simplicial Complexes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Maosheng Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Isufi%2C+E">Elvin Isufi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Schaub%2C+M+T">Michael T. Schaub</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Leus%2C+G">Geert Leus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.12587v1-abstract-short" style="display: inline;">
        In this paper, we study linear filters to process signals defined on simplicial complexes, i.e., signals defined on nodes, edges, triangles, etc. of a simplicial complex, thereby generalizing filtering operations for graph signals. We propose a finite impulse response filter based on the Hodge Laplacian, and demonstrate how this filter can be designed to amplify or attenuate certain spectral compo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12587v1-abstract-full').style.display = 'inline'; document.getElementById('2103.12587v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.12587v1-abstract-full" style="display: none;">
        In this paper, we study linear filters to process signals defined on simplicial complexes, i.e., signals defined on nodes, edges, triangles, etc. of a simplicial complex, thereby generalizing filtering operations for graph signals. We propose a finite impulse response filter based on the Hodge Laplacian, and demonstrate how this filter can be designed to amplify or attenuate certain spectral components of simplicial signals. Specifically, we discuss how, unlike in the case of node signals, the Fourier transform in the context of edge signals can be understood in terms of two orthogonal subspaces corresponding to the gradient-flow signals and curl-flow signals arising from the Hodge decomposition. By assigning different filter coefficients to the associated terms of the Hodge Laplacian, we develop a subspace-varying filter which enables more nuanced control over these signal types. Numerical experiments are conducted to show the potential of simplicial filters for sub-component extraction, denoising and model approximation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.12587v1-abstract-full').style.display = 'none'; document.getElementById('2103.12587v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages; 3 figures; 1 table</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        29th European Signal Processing Conference (EUSIPCO 2021)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.10199">arXiv:2103.10199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.10199">pdf</a>, <a href="https://arxiv.org/ps/2103.10199">ps</a>, <a href="https://arxiv.org/format/2103.10199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Characterizations of OTFS Modulation over multipath Rapid Fading Channel
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+H">Haoyan Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+Y">Yanming Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Min Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhang%2C+Q">Qiongjie Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.10199v2-abstract-short" style="display: inline;">
        Orthogonal time frequency space (OTFS) modulation has been confirmed to provide significant performance advantages against Doppler in high-mobility scenarios. The core feature of OTFS is that the time-variant channel is converted into a non-fading 2D channel in the delay-Doppler (DD) domain so that all symbols experience the same channel gain. In now available literature, the channel is assumed to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10199v2-abstract-full').style.display = 'inline'; document.getElementById('2103.10199v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.10199v2-abstract-full" style="display: none;">
        Orthogonal time frequency space (OTFS) modulation has been confirmed to provide significant performance advantages against Doppler in high-mobility scenarios. The core feature of OTFS is that the time-variant channel is converted into a non-fading 2D channel in the delay-Doppler (DD) domain so that all symbols experience the same channel gain. In now available literature, the channel is assumed to be quasi-static over an OTFS frame. As for more practical channels, the input-output relation will be time-variant as the environment or medium changes. In this paper, we analyze the characterizations of OTFS modulation over a more general multipath channel, where the signal of each path has experienced a unique rapid fading. First, we derive the explicit input-output relationship of OTFS in the DD domain for the case of ideal pulse and rectangular pulse. It is shown that the rapid fading will produce extra Doppler dispersion without impacting on delay domain. We next demonstrate that OTFS can be interpreted as an efficient time diversity technology that combines space-time encoding and interleaving. Simulation results reveal that OTFS is insensitive to rapid fading and still outperforms orthogonal frequency-division multiplexing (OFDM) in these types of channels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.10199v2-abstract-full').style.display = 'none'; document.getElementById('2103.10199v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.03909">arXiv:2101.03909</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.03909">pdf</a>, <a href="https://arxiv.org/ps/2101.03909">ps</a>, <a href="https://arxiv.org/format/2101.03909">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Joint Source Channel Coding for WirelessImage Transmission with OFDM
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingyu Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Bian%2C+C">Chenghong Bian</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kim%2C+H">Hun-Seok Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.03909v2-abstract-short" style="display: inline;">
        We present a deep learning based joint source channel coding (JSCC) scheme for wireless image transmission over multipath fading channels with non-linear signal clipping. The proposed encoder and decoder use convolutional neural networks (CNN) and directly map the source images to complex-valued baseband samples for orthogonal frequency division multiplexing (OFDM) transmission. The proposed model&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.03909v2-abstract-full').style.display = 'inline'; document.getElementById('2101.03909v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.03909v2-abstract-full" style="display: none;">
        We present a deep learning based joint source channel coding (JSCC) scheme for wireless image transmission over multipath fading channels with non-linear signal clipping. The proposed encoder and decoder use convolutional neural networks (CNN) and directly map the source images to complex-valued baseband samples for orthogonal frequency division multiplexing (OFDM) transmission. The proposed model-driven machine learning approach eliminates the need for separate source and channel coding while integrating an OFDM datapath to cope with multipath fading channels. The end-to-end JSCC communication system combines trainable CNN layers with non-trainable but differentiable layers representing the multipath channel model and OFDM signal processing blocks. Our results show that injecting domain expert knowledge by incorporating OFDM baseband processing blocks into the machine learning framework significantly enhances the overall performance compared to an unstructured CNN. Our method outperforms conventional schemes that employ state-of-the-art but separate source and channel coding such as BPG and LDPC with OFDM. Moreover, our method is shown to be robust against non-linear signal clipping in OFDM for various channel conditions that do not match the model parameter used during the training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.03909v2-abstract-full').style.display = 'none'; document.getElementById('2101.03909v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICC 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.11835">arXiv:2011.11835</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.11835">pdf</a>, <a href="https://arxiv.org/ps/2011.11835">ps</a>, <a href="https://arxiv.org/format/2011.11835">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Peer Offloading with Delayed Feedback in Fog Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Miao Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhu%2C+H">Hongbin Zhu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Qian%2C+H">Hua Qian</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Koucheryavy%2C+Y">Yevgeni Koucheryavy</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Samouylov%2C+K">Konstantin Samouylov</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+H">Haifeng Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.11835v2-abstract-short" style="display: inline;">
        Comparing to cloud computing, fog computing performs computation and services at the edge of networks, thus relieving the computation burden of the data center and reducing the task latency of end devices. Computation latency is a crucial performance metric in fog computing, especially for real-time applications. In this paper, we study a peer computation offloading problem for a fog network with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.11835v2-abstract-full').style.display = 'inline'; document.getElementById('2011.11835v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.11835v2-abstract-full" style="display: none;">
        Comparing to cloud computing, fog computing performs computation and services at the edge of networks, thus relieving the computation burden of the data center and reducing the task latency of end devices. Computation latency is a crucial performance metric in fog computing, especially for real-time applications. In this paper, we study a peer computation offloading problem for a fog network with unknown dynamics. In this scenario, each fog node (FN) can offload their computation tasks to neighboring FNs in a time slot manner. The offloading latency, however, could not be fed back to the task dispatcher instantaneously due to the uncertainty of the processing time in peer FNs. Besides, peer competition occurs when different FNs offload tasks to one FN at the same time. To tackle the above difficulties, we model the computation offloading problem as a sequential FN selection problem with delayed information feedback. Using adversarial multi-arm bandit framework, we construct an online learning policy to deal with delayed information feedback. Different contention resolution approaches are considered to resolve peer competition. Performance analysis shows that the regret of the proposed algorithm, or the performance loss with suboptimal FN selections, achieves a sub-linear order, suggesting an optimal FN selection policy. In addition, we prove that the proposed strategy can result in a Nash equilibrium (NE) with all FNs playing the same policy. Simulation results validate the effectiveness of the proposed policy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.11835v2-abstract-full').style.display = 'none'; document.getElementById('2011.11835v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 11 figures, accepted by IEEE Internet of Things Journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.13413">arXiv:2010.13413</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.13413">pdf</a>, <a href="https://arxiv.org/ps/2010.13413">ps</a>, <a href="https://arxiv.org/format/2010.13413">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/OJSP.2021.3056897">10.1109/OJSP.2021.3056897 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Node-Adaptive Regularization for Graph Signal Reconstruction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Maosheng Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Coutino%2C+M">Mario Coutino</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Leus%2C+G">Geert Leus</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Isufi%2C+E">Elvin Isufi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.13413v2-abstract-short" style="display: inline;">
        A critical task in graph signal processing is to estimate the true signal from noisy observations over a subset of nodes, also known as the reconstruction problem. In this paper, we propose a node-adaptive regularization for graph signal reconstruction, which surmounts the conventional Tikhonov regularization, giving rise to more degrees of freedom; hence, an improved performance. We formulate the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.13413v2-abstract-full').style.display = 'inline'; document.getElementById('2010.13413v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.13413v2-abstract-full" style="display: none;">
        A critical task in graph signal processing is to estimate the true signal from noisy observations over a subset of nodes, also known as the reconstruction problem. In this paper, we propose a node-adaptive regularization for graph signal reconstruction, which surmounts the conventional Tikhonov regularization, giving rise to more degrees of freedom; hence, an improved performance. We formulate the node-adaptive graph signal denoising problem, study its bias-variance trade-off, and identify conditions under which a lower mean squared error and variance can be obtained with respect to Tikhonov regularization. Compared with existing approaches, the node-adaptive regularization enjoys more general priors on the local signal variation, which can be obtained by optimally designing the regularization weights based on Prony&#39;s method or semidefinite programming. As these approaches require additional prior knowledge, we also propose a minimax (worst-case) strategy to address instances where this extra information is unavailable. Numerical experiments with synthetic and real data corroborate the proposed regularization strategy for graph signal denoising and interpolation, and show its improved performance compared with competing alternatives.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.13413v2-abstract-full').style.display = 'none'; document.getElementById('2010.13413v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.05210">arXiv:2009.05210</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.05210">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A 6.3-Nanowatt-per-Channel 96-Channel Neural Spike Processor for a Movement-Intention-Decoding Brain-Computer-Interface Implant
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Jiang%2C+Z">Zhewei Jiang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+J">Jiangyi Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chundi%2C+P+K">Pavan K. Chundi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kim%2C+S+J">Sung Justin Kim</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Minhao Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kang%2C+J">Joonseong Kang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Jung%2C+S">Seungchul Jung</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kim%2C+S+J">Sang Joon Kim</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Seok%2C+M">Mingoo Seok</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.05210v1-abstract-short" style="display: inline;">
        This paper presents microwatt end-to-end neural signal processing hardware for deployment-stage real-time upper-limb movement intent decoding. This module features intercellular spike detection, sorting, and decoding operations for a 96-channel prosthetic implant. We design the algorithms for those operations to achieve minimal computation complexity while matching or advancing the accuracy of sta&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.05210v1-abstract-full').style.display = 'inline'; document.getElementById('2009.05210v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.05210v1-abstract-full" style="display: none;">
        This paper presents microwatt end-to-end neural signal processing hardware for deployment-stage real-time upper-limb movement intent decoding. This module features intercellular spike detection, sorting, and decoding operations for a 96-channel prosthetic implant. We design the algorithms for those operations to achieve minimal computation complexity while matching or advancing the accuracy of state-of-art Brain-Computer-Interface sorting and movement decoding. Based on those algorithms, we devise the architect of the neural signal processing hardware with the focus on hardware reuse and event-driven operation. The design achieves among the highest levels of integration, reducing wireless data rate by more than four orders of magnitude. The chip prototype in a 180-nm high-VTH, achieving the lowest power dissipation of 0.61 uW for 96 channels, 21X lower than the prior art at a comparable/better accuracy even with integration of kinematic state estimation computation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.05210v1-abstract-full').style.display = 'none'; document.getElementById('2009.05210v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.05780">arXiv:2008.05780</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.05780">pdf</a>, <a href="https://arxiv.org/format/2008.05780">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Modality Pathology Segmentation Framework: Application to Cardiac Magnetic Resonance Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Zhang%2C+Z">Zhen Zhang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+C">Chenyu Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ding%2C+W">Wangbin Ding</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+S">Sihan Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Pei%2C+C">Chenhao Pei</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingjing Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Huang%2C+L">Liqin Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.05780v1-abstract-short" style="display: inline;">
        Multi-sequence of cardiac magnetic resonance (CMR) images can provide complementary information for myocardial pathology (scar and edema). However, it is still challenging to fuse these underlying information for pathology segmentation effectively. This work presents an automatic cascade pathology segmentation framework based on multi-modality CMR images. It mainly consists of two neural networks:&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.05780v1-abstract-full').style.display = 'inline'; document.getElementById('2008.05780v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.05780v1-abstract-full" style="display: none;">
        Multi-sequence of cardiac magnetic resonance (CMR) images can provide complementary information for myocardial pathology (scar and edema). However, it is still challenging to fuse these underlying information for pathology segmentation effectively. This work presents an automatic cascade pathology segmentation framework based on multi-modality CMR images. It mainly consists of two neural networks: an anatomical structure segmentation network (ASSN) and a pathological region segmentation network (PRSN). Specifically, the ASSN aims to segment the anatomical structure where the pathology may exist, and it can provide a spatial prior for the pathological region segmentation. In addition, we integrate a denoising auto-encoder (DAE) into the ASSN to generate segmentation results with plausible shapes. The PRSN is designed to segment pathological region based on the result of ASSN, in which a fusion block based on channel attention is proposed to better aggregate multi-modality information from multi-modality CMR images. Experiments from the MyoPS2020 challenge dataset show that our framework can achieve promising performance for myocardial scar and edema segmentation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.05780v1-abstract-full').style.display = 'none'; document.getElementById('2008.05780v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages,MyoPS 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.04902">arXiv:2008.04902</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.04902">pdf</a>, <a href="https://arxiv.org/format/2008.04902">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to See Through Obstructions with Layered Decomposition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+Y">Yu-Lun Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lai%2C+W">Wei-Sheng Lai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chuang%2C+Y">Yung-Yu Chuang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Huang%2C+J">Jia-Bin Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.04902v3-abstract-short" style="display: inline;">
        We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera. Our method leverages motion differences between the background and obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two la&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.04902v3-abstract-full').style.display = 'inline'; document.getElementById('2008.04902v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.04902v3-abstract-full" style="display: none;">
        We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera. Our method leverages motion differences between the background and obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. This learning-based layer reconstruction module facilitates accommodating potential errors in the flow estimation and brittle assumptions, such as brightness consistency. We show that the proposed approach learned from synthetically generated data performs well to real images. Experimental results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.04902v3-abstract-full').style.display = 'none'; document.getElementById('2008.04902v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://alex04072000.github.io/SOLD/ Code: https://github.com/alex04072000/SOLD Extension of the CVPR 2020 paper: arXiv:2004.01180</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.03765">arXiv:2008.03765</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.03765">pdf</a>, <a href="https://arxiv.org/format/2008.03765">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low-Light Maritime Image Enhancement with Regularized Illumination Optimization and Deep Noise Suppression
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Guo%2C+Y">Yu Guo</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lu%2C+Y">Yuxu Lu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+R+W">Ryan Wen Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Meifang Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chui%2C+K+T">Kwok Tai Chui</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.03765v1-abstract-short" style="display: inline;">
        Maritime images captured under low-light imaging condition easily suffer from low visibility and unexpected noise, leading to negative effects on maritime traffic supervision and management. To promote imaging performance, it is necessary to restore the important visual information from degraded low-light images. In this paper, we propose to enhance the low-light images through regularized illumin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.03765v1-abstract-full').style.display = 'inline'; document.getElementById('2008.03765v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.03765v1-abstract-full" style="display: none;">
        Maritime images captured under low-light imaging condition easily suffer from low visibility and unexpected noise, leading to negative effects on maritime traffic supervision and management. To promote imaging performance, it is necessary to restore the important visual information from degraded low-light images. In this paper, we propose to enhance the low-light images through regularized illumination optimization and deep noise suppression. In particular, a hybrid regularized variational model, which combines L0-norm gradient sparsity prior with structure-aware regularization, is presented to refine the coarse illumination map originally estimated using Max-RGB. The adaptive gamma correction method is then introduced to adjust the refined illumination map. Based on the assumption of Retinex theory, a guided filter-based detail boosting method is introduced to optimize the reflection map. The adjusted illumination and optimized reflection maps are finally combined to generate the enhanced maritime images. To suppress the effect of unwanted noise on imaging performance, a deep learning-based blind denoising framework is further introduced to promote the visual quality of enhanced image. In particular, this framework is composed of two sub-networks, i.e., E-Net and D-Net adopted for noise level estimation and non-blind noise reduction, respectively. The main benefit of our image enhancement method is that it takes full advantage of the regularized illumination optimization and deep blind denoising. Comprehensive experiments have been conducted on both synthetic and realistic maritime images to compare our proposed method with several state-of-the-art imaging methods. Experimental results have illustrated its superior performance in terms of both quantitative and qualitative evaluations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.03765v1-abstract-full').style.display = 'none'; document.getElementById('2008.03765v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 14 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.01201">arXiv:2008.01201</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.01201">pdf</a>, <a href="https://arxiv.org/format/2008.01201">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mixup-CAM: Weakly-supervised Semantic Segmentation via Uncertainty Regularization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Chang%2C+Y">Yu-Ting Chang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+Q">Qiaosong Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hung%2C+W">Wei-Chih Hung</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Piramuthu%2C+R">Robinson Piramuthu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tsai%2C+Y">Yi-Hsuan Tsai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.01201v1-abstract-short" style="display: inline;">
        Obtaining object response maps is one important step to achieve weakly-supervised semantic segmentation using image-level labels. However, existing methods rely on the classification task, which could result in a response map only attending on discriminative object regions as the network does not need to see the entire object for optimizing the classification loss. To tackle this issue, we propose&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.01201v1-abstract-full').style.display = 'inline'; document.getElementById('2008.01201v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.01201v1-abstract-full" style="display: none;">
        Obtaining object response maps is one important step to achieve weakly-supervised semantic segmentation using image-level labels. However, existing methods rely on the classification task, which could result in a response map only attending on discriminative object regions as the network does not need to see the entire object for optimizing the classification loss. To tackle this issue, we propose a principled and end-to-end train-able framework to allow the network to pay attention to other parts of the object, while producing a more complete and uniform response map. Specifically, we introduce the mixup data augmentation scheme into the classification network and design two uncertainty regularization terms to better interact with the mixup strategy. In experiments, we conduct extensive analysis to demonstrate the proposed method and show favorable performance against state-of-the-art approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.01201v1-abstract-full').style.display = 'none'; document.getElementById('2008.01201v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at BMVC 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.01183">arXiv:2008.01183</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.01183">pdf</a>, <a href="https://arxiv.org/format/2008.01183">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Weakly-Supervised Semantic Segmentation via Sub-category Exploration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Chang%2C+Y">Yu-Ting Chang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+Q">Qiaosong Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hung%2C+W">Wei-Chih Hung</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Piramuthu%2C+R">Robinson Piramuthu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tsai%2C+Y">Yi-Hsuan Tsai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.01183v1-abstract-short" style="display: inline;">
        Existing weakly-supervised semantic segmentation methods using image-level annotations typically rely on initial responses to locate object regions. However, such response maps generated by the classification network usually focus on discriminative object parts, due to the fact that the network does not need the entire object for optimizing the objective function. To enforce the network to pay att&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.01183v1-abstract-full').style.display = 'inline'; document.getElementById('2008.01183v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.01183v1-abstract-full" style="display: none;">
        Existing weakly-supervised semantic segmentation methods using image-level annotations typically rely on initial responses to locate object regions. However, such response maps generated by the classification network usually focus on discriminative object parts, due to the fact that the network does not need the entire object for optimizing the objective function. To enforce the network to pay attention to other parts of an object, we propose a simple yet effective approach that introduces a self-supervised task by exploiting the sub-category information. Specifically, we perform clustering on image features to generate pseudo sub-categories labels within each annotated parent class, and construct a sub-category objective to assign the network to a more challenging task. By iteratively clustering image features, the training process does not limit itself to the most discriminative object parts, hence improving the quality of the response maps. We conduct extensive analysis to validate the proposed method and show that our approach performs favorably against the state-of-the-art approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.01183v1-abstract-full').style.display = 'none'; document.getElementById('2008.01183v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.13854">arXiv:2007.13854</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.13854">pdf</a>, <a href="https://arxiv.org/format/2007.13854">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-030-27272-2_29">10.1007/978-3-030-27272-2_29 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Lesion Segmentation for Diabetic Retinopathy using Adversarial Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Xiao%2C+Q">Qiqi Xiao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zou%2C+J">Jiaxu Zou</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Muqiao Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Gaudio%2C+A">Alex Gaudio</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kitani%2C+K">Kris Kitani</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Smailagic%2C+A">Asim Smailagic</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Costa%2C+P">Pedro Costa</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Xu%2C+M">Min Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.13854v1-abstract-short" style="display: inline;">
        Diabetic Retinopathy (DR) is a leading cause of blindness in working age adults. DR lesions can be challenging to identify in fundus images, and automatic DR detection systems can offer strong clinical value. Of the publicly available labeled datasets for DR, the Indian Diabetic Retinopathy Image Dataset (IDRiD) presents retinal fundus images with pixel-level annotations of four distinct lesions:&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.13854v1-abstract-full').style.display = 'inline'; document.getElementById('2007.13854v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.13854v1-abstract-full" style="display: none;">
        Diabetic Retinopathy (DR) is a leading cause of blindness in working age adults. DR lesions can be challenging to identify in fundus images, and automatic DR detection systems can offer strong clinical value. Of the publicly available labeled datasets for DR, the Indian Diabetic Retinopathy Image Dataset (IDRiD) presents retinal fundus images with pixel-level annotations of four distinct lesions: microaneurysms, hemorrhages, soft exudates and hard exudates. We utilize the HEDNet edge detector to solve a semantic segmentation task on this dataset, and then propose an end-to-end system for pixel-level segmentation of DR lesions by incorporating HEDNet into a Conditional Generative Adversarial Network (cGAN). We design a loss function that adds adversarial loss to segmentation loss. Our experiments show that the addition of the adversarial loss improves the lesion segmentation performance over the baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.13854v1-abstract-full').style.display = 'none'; document.getElementById('2007.13854v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to International Conference on Image Analysis and Recognition, ICIAR 2019. Published at https://doi.org/10.1007/978-3-030-27272-2_29 Code: https://github.com/zoujx96/DR-segmentation</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.14563">arXiv:2006.14563</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.14563">pdf</a>, <a href="https://arxiv.org/format/2006.14563">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamically Mitigating Data Discrepancy with Balanced Focal Loss for Replay Attack Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Dou%2C+Y">Yongqiang Dou</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+H">Haocheng Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Maolin Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Xu%2C+Y">Yanyan Xu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ke%2C+D">Dengfeng Ke</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.14563v1-abstract-short" style="display: inline;">
        It becomes urgent to design effective anti-spoofing algorithms for vulnerable automatic speaker verification systems due to the advancement of high-quality playback devices. Current studies mainly treat anti-spoofing as a binary classification problem between bonafide and spoofed utterances, while lack of indistinguishable samples makes it difficult to train a robust spoofing detector. In this pap&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.14563v1-abstract-full').style.display = 'inline'; document.getElementById('2006.14563v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.14563v1-abstract-full" style="display: none;">
        It becomes urgent to design effective anti-spoofing algorithms for vulnerable automatic speaker verification systems due to the advancement of high-quality playback devices. Current studies mainly treat anti-spoofing as a binary classification problem between bonafide and spoofed utterances, while lack of indistinguishable samples makes it difficult to train a robust spoofing detector. In this paper, we argue that for anti-spoofing, it needs more attention for indistinguishable samples over easily-classified ones in the modeling process, to make correct discrimination a top priority. Therefore, to mitigate the data discrepancy between training and inference, we propose to leverage a balanced focal loss function as the training objective to dynamically scale the loss based on the traits of the sample itself. Besides, in the experiments, we select three kinds of features that contain both magnitude-based and phase-based information to form complementary and informative features. Experimental results on the ASVspoof2019 dataset demonstrate the superiority of the proposed methods by comparison between our systems and top-performing ones. Systems trained with the balanced focal loss perform significantly better than conventional cross-entropy loss. With complementary features, our fusion system with only three kinds of features outperforms other systems containing five or more complex single models by 22.5% for min-tDCF and 7% for EER, achieving a min-tDCF and an EER of 0.0124 and 0.55% respectively. Furthermore, we present and discuss the evaluation results on real replay data apart from the simulated ASVspoof2019 data, indicating that research for anti-spoofing still has a long way to go.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.14563v1-abstract-full').style.display = 'none'; document.getElementById('2006.14563v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been accepted by the 25th International Conference on Pattern Recognition (ICPR2020)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.09727">arXiv:2005.09727</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.09727">pdf</a>, <a href="https://arxiv.org/format/2005.09727">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ventral-Dorsal Neural Networks: Object Detection via Selective Attention
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Ebrahimpour%2C+M+K">Mohammad K. Ebrahimpour</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+J">Jiayun Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yu%2C+Y">Yen-Yun Yu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Reese%2C+J+L">Jackson L. Reese</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Moghtaderi%2C+A">Azadeh Moghtaderi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Noelle%2C+D+C">David C. Noelle</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.09727v1-abstract-short" style="display: inline;">
        Deep Convolutional Neural Networks (CNNs) have been repeatedly proven to perform well on image classification tasks. Object detection methods, however, are still in need of significant improvements. In this paper, we propose a new framework called Ventral-Dorsal Networks (VDNets) which is inspired by the structure of the human visual system. Roughly, the visual input signal is analyzed along two s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.09727v1-abstract-full').style.display = 'inline'; document.getElementById('2005.09727v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.09727v1-abstract-full" style="display: none;">
        Deep Convolutional Neural Networks (CNNs) have been repeatedly proven to perform well on image classification tasks. Object detection methods, however, are still in need of significant improvements. In this paper, we propose a new framework called Ventral-Dorsal Networks (VDNets) which is inspired by the structure of the human visual system. Roughly, the visual input signal is analyzed along two separate neural streams, one in the temporal lobe and the other in the parietal lobe. The coarse functional distinction between these streams is between object recognition -- the &#34;what&#34; of the signal -- and extracting location related information -- the &#34;where&#34; of the signal. The ventral pathway from primary visual cortex, entering the temporal lobe, is dominated by &#34;what&#34; information, while the dorsal pathway, into the parietal lobe, is dominated by &#34;where&#34; information. Inspired by this structure, we propose the integration of a &#34;Ventral Network&#34; and a &#34;Dorsal Network&#34;, which are complementary. Information about object identity can guide localization, and location information can guide attention to relevant image regions, improving object recognition. This new dual network framework sharpens the focus of object detection. Our experimental results reveal that the proposed method outperforms state-of-the-art object detection approaches on PASCAL VOC 2007 by 8% (mAP) and PASCAL VOC 2012 by 3% (mAP). Moreover, a comparison of techniques on Yearbook images displays substantial qualitative and quantitative benefits of VDNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.09727v1-abstract-full').style.display = 'none'; document.getElementById('2005.09727v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">in Proceedings of WACV. arXiv admin note: substantial text overlap with arXiv:2005.07787</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.07787">arXiv:2005.07787</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.07787">pdf</a>, <a href="https://arxiv.org/format/2005.07787">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WW-Nets: Dual Neural Networks for Object Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Ebrahimpour%2C+M+K">Mohammad K. Ebrahimpour</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Falandays%2C+J+B">J. Ben Falandays</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Spevack%2C+S">Samuel Spevack</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Noelle%2C+D+C">David C. Noelle</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.07787v1-abstract-short" style="display: inline;">
        We propose a new deep convolutional neural network framework that uses object location knowledge implicit in network connection weights to guide selective attention in object detection tasks. Our approach is called What-Where Nets (WW-Nets), and it is inspired by the structure of human visual pathways. In the brain, vision incorporates two separate streams, one in the temporal lobe and the other i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.07787v1-abstract-full').style.display = 'inline'; document.getElementById('2005.07787v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.07787v1-abstract-full" style="display: none;">
        We propose a new deep convolutional neural network framework that uses object location knowledge implicit in network connection weights to guide selective attention in object detection tasks. Our approach is called What-Where Nets (WW-Nets), and it is inspired by the structure of human visual pathways. In the brain, vision incorporates two separate streams, one in the temporal lobe and the other in the parietal lobe, called the ventral stream and the dorsal stream, respectively. The ventral pathway from primary visual cortex is dominated by &#34;what&#34; information, while the dorsal pathway is dominated by &#34;where&#34; information. Inspired by this structure, we have proposed an object detection framework involving the integration of a &#34;What Network&#34; and a &#34;Where Network&#34;. The aim of the What Network is to provide selective attention to the relevant parts of the input image. The Where Network uses this information to locate and classify objects of interest. In this paper, we compare this approach to state-of-the-art algorithms on the PASCAL VOC 2007 and 2012 and COCO object detection challenge datasets. Also, we compare out approach to human &#34;ground-truth&#34; attention. We report the results of an eye-tracking experiment on human subjects using images from PASCAL VOC 2007, and we demonstrate interesting relationships between human overt attention and information processing in our WW-Nets. Finally, we provide evidence that our proposed method performs favorably in comparison to other object detection approaches, often by a large margin. The code and the eye-tracking ground-truth dataset can be found at: https://github.com/mkebrahimpour.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.07787v1-abstract-full').style.display = 'none'; document.getElementById('2005.07787v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.14003">arXiv:2004.14003</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.14003">pdf</a>, <a href="https://arxiv.org/format/2004.14003">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The International Workshop on Osteoarthritis Imaging Knee MRI Segmentation Challenge: A Multi-Institute Evaluation and Analysis Framework on a Standardized Dataset
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Desai%2C+A+D">Arjun D. Desai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Caliva%2C+F">Francesco Caliva</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Iriondo%2C+C">Claudia Iriondo</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khosravan%2C+N">Naji Khosravan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mortazi%2C+A">Aliasghar Mortazi</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Jambawalikar%2C+S">Sachin Jambawalikar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Torigian%2C+D">Drew Torigian</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ellermann%2C+J">Jutta Ellermann</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Akcakaya%2C+M">Mehmet Akcakaya</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Bagci%2C+U">Ulas Bagci</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tibrewala%2C+R">Radhika Tibrewala</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Flament%2C+I">Io Flament</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=O%60Brien%2C+M">Matthew O`Brien</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Majumdar%2C+S">Sharmila Majumdar</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Perslev%2C+M">Mathias Perslev</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Pai%2C+A">Akshay Pai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Igel%2C+C">Christian Igel</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Dam%2C+E+B">Erik B. Dam</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Gaj%2C+S">Sibaji Gaj</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingrui Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Nakamura%2C+K">Kunio Nakamura</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+X">Xiaojuan Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Deniz%2C+C+M">Cem M. Deniz</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Juras%2C+V">Vladimir Juras</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Regatte%2C+R">Ravinder Regatte</a>
      , et al. (4 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.14003v2-abstract-short" style="display: inline;">
        Purpose: To organize a knee MRI segmentation challenge for characterizing the semantic and clinical efficacy of automatic segmentation methods relevant for monitoring osteoarthritis progression.
  Methods: A dataset partition consisting of 3D knee MRI from 88 subjects at two timepoints with ground-truth articular (femoral, tibial, patellar) cartilage and meniscus segmentations was standardized. Ch&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.14003v2-abstract-full').style.display = 'inline'; document.getElementById('2004.14003v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.14003v2-abstract-full" style="display: none;">
        Purpose: To organize a knee MRI segmentation challenge for characterizing the semantic and clinical efficacy of automatic segmentation methods relevant for monitoring osteoarthritis progression.
  Methods: A dataset partition consisting of 3D knee MRI from 88 subjects at two timepoints with ground-truth articular (femoral, tibial, patellar) cartilage and meniscus segmentations was standardized. Challenge submissions and a majority-vote ensemble were evaluated using Dice score, average symmetric surface distance, volumetric overlap error, and coefficient of variation on a hold-out test set. Similarities in network segmentations were evaluated using pairwise Dice correlations. Articular cartilage thickness was computed per-scan and longitudinally. Correlation between thickness error and segmentation metrics was measured using Pearson&#39;s coefficient. Two empirical upper bounds for ensemble performance were computed using combinations of model outputs that consolidated true positives and true negatives.
  Results: Six teams (T1-T6) submitted entries for the challenge. No significant differences were observed across all segmentation metrics for all tissues (p=1.0) among the four top-performing networks (T2, T3, T4, T6). Dice correlations between network pairs were high (&gt;0.85). Per-scan thickness errors were negligible among T1-T4 (p=0.99) and longitudinal changes showed minimal bias (&lt;0.03mm). Low correlations (&lt;0.41) were observed between segmentation metrics and thickness error. The majority-vote ensemble was comparable to top performing networks (p=1.0). Empirical upper bound performances were similar for both combinations (p=1.0).
  Conclusion: Diverse networks learned to segment the knee similarly where high segmentation accuracy did not correlate to cartilage thickness accuracy. Voting ensembles did not outperform individual networks but may help regularize individual models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.14003v2-abstract-full').style.display = 'none'; document.getElementById('2004.14003v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to Radiology: Artificial Intelligence; Fixed typos</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.02270">arXiv:2004.02270</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.02270">pdf</a>, <a href="https://arxiv.org/ps/2004.02270">ps</a>, <a href="https://arxiv.org/format/2004.02270">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Medical Physics">physics.med-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Game of Learning Bloch Equation Simulations for MR Fingerprinting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingrui Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Jiang%2C+Y">Yun Jiang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ma%2C+D">Dan Ma</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Mehta%2C+B+B">Bhairav B. Mehta</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Griswold%2C+M+A">Mark A. Griswold</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.02270v1-abstract-short" style="display: inline;">
        Purpose: This work proposes a novel approach to efficiently generate MR fingerprints for MR fingerprinting (MRF) problems based on the unsupervised deep learning model generative adversarial networks (GAN). Methods: The GAN model is adopted and modified for better convergence and performance, resulting in an MRF specific model named GAN-MRF. The GAN-MRF model is trained, validated, and tested usin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.02270v1-abstract-full').style.display = 'inline'; document.getElementById('2004.02270v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.02270v1-abstract-full" style="display: none;">
        Purpose: This work proposes a novel approach to efficiently generate MR fingerprints for MR fingerprinting (MRF) problems based on the unsupervised deep learning model generative adversarial networks (GAN). Methods: The GAN model is adopted and modified for better convergence and performance, resulting in an MRF specific model named GAN-MRF. The GAN-MRF model is trained, validated, and tested using different MRF fingerprints simulated from the Bloch equations with certain MRF sequence. The performance and robustness of the model are further tested by using in vivo data collected on a 3 Tesla scanner from a healthy volunteer together with MRF dictionaries with different sizes. T1, T2 maps are generated and compared quantitatively. Results: The validation and testing curves for the GAN-MRF model show no evidence of high bias or high variance problems. The sample MRF fingerprints generated from the trained GAN-MRF model agree well with the benchmark fingerprints simulated from the Bloch equations. The in vivo T1, T2 maps generated from the GAN-MRF fingerprints are in good agreement with those generated from the Bloch simulated fingerprints, showing good performance and robustness of the proposed GAN-MRF model. Moreover, the MRF dictionary generation time is reduced from hours to sub-second for the testing dictionary. Conclusion: The GAN-MRF model enables a fast and accurate generation of the MRF fingerprints. It significantly reduces the MRF dictionary generation process and opens the door for real-time applications and sequence optimization problems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.02270v1-abstract-full').style.display = 'none'; document.getElementById('2004.02270v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.01179">arXiv:2004.01179</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.01179">pdf</a>, <a href="https://arxiv.org/format/2004.01179">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+Y">Yu-Lun Liu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lai%2C+W">Wei-Sheng Lai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chen%2C+Y">Yu-Sheng Chen</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kao%2C+Y">Yi-Lung Kao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Chuang%2C+Y">Yung-Yu Chuang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Huang%2C+J">Jia-Bin Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.01179v1-abstract-short" style="display: inline;">
        Recovering a high dynamic range (HDR) image from a single low dynamic range (LDR) input image is challenging due to missing details in under-/over-exposed regions caused by quantization and saturation of camera sensors. In contrast to existing learning-based methods, our core idea is to incorporate the domain knowledge of the LDR image formation pipeline into our model. We model the HDRto-LDR imag&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.01179v1-abstract-full').style.display = 'inline'; document.getElementById('2004.01179v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.01179v1-abstract-full" style="display: none;">
        Recovering a high dynamic range (HDR) image from a single low dynamic range (LDR) input image is challenging due to missing details in under-/over-exposed regions caused by quantization and saturation of camera sensors. In contrast to existing learning-based methods, our core idea is to incorporate the domain knowledge of the LDR image formation pipeline into our model. We model the HDRto-LDR image formation pipeline as the (1) dynamic range clipping, (2) non-linear mapping from a camera response function, and (3) quantization. We then propose to learn three specialized CNNs to reverse these steps. By decomposing the problem into specific sub-tasks, we impose effective physical constraints to facilitate the training of individual sub-networks. Finally, we jointly fine-tune the entire model end-to-end to reduce error accumulation. With extensive quantitative and qualitative experiments on diverse image datasets, we demonstrate that the proposed method performs favorably against state-of-the-art single-image HDR reconstruction algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.01179v1-abstract-full').style.display = 'none'; document.getElementById('2004.01179v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2020. Project page: https://www.cmlab.csie.ntu.edu.tw/~yulunliu/SingleHDR Code: https://github.com/alex04072000/SingleHDR</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.08436">arXiv:2003.08436</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.08436">pdf</a>, <a href="https://arxiv.org/format/2003.08436">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Collaborative Distillation for Ultra-Resolution Universal Style Transfer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+H">Huan Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+Y">Yijun Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+Y">Yuehai Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hu%2C+H">Haoji Hu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.08436v2-abstract-short" style="display: inline;">
        Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Co&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.08436v2-abstract-full').style.display = 'inline'; document.getElementById('2003.08436v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.08436v2-abstract-full" style="display: none;">
        Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher&#39;s features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.08436v2-abstract-full').style.display = 'none'; document.getElementById('2003.08436v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2020, higher-resolution images than the camera-ready version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.07761">arXiv:2003.07761</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.07761">pdf</a>, <a href="https://arxiv.org/format/2003.07761">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CycleISP: Real Image Restoration via Improved Data Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Zamir%2C+S+W">Syed Waqas Zamir</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Arora%2C+A">Aditya Arora</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+S">Salman Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hayat%2C+M">Munawar Hayat</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Ming-Hsuan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shao%2C+L">Ling Shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.07761v1-abstract-short" style="display: inline;">
        The availability of large-scale datasets has helped unleash the true potential of deep convolutional neural networks (CNNs). However, for the single-image denoising problem, capturing a real dataset is an unacceptably expensive and cumbersome procedure. Consequently, image denoising algorithms are mostly developed and evaluated on synthetic data that is usually generated with a widespread assumpti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.07761v1-abstract-full').style.display = 'inline'; document.getElementById('2003.07761v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.07761v1-abstract-full" style="display: none;">
        The availability of large-scale datasets has helped unleash the true potential of deep convolutional neural networks (CNNs). However, for the single-image denoising problem, capturing a real dataset is an unacceptably expensive and cumbersome procedure. Consequently, image denoising algorithms are mostly developed and evaluated on synthetic data that is usually generated with a widespread assumption of additive white Gaussian noise (AWGN). While the CNNs achieve impressive results on these synthetic datasets, they do not perform well when applied on real camera images, as reported in recent benchmark datasets. This is mainly because the AWGN is not adequate for modeling the real camera noise which is signal-dependent and heavily transformed by the camera imaging pipeline. In this paper, we present a framework that models camera imaging pipeline in forward and reverse directions. It allows us to produce any number of realistic image pairs for denoising both in RAW and sRGB spaces. By training a new image denoising network on realistic synthetic data, we achieve the state-of-the-art performance on real camera benchmark datasets. The parameters in our model are ~5 times lesser than the previous best method for RAW denoising. Furthermore, we demonstrate that the proposed framework generalizes beyond image denoising problem e.g., for color matching in stereoscopic cinema. The source code and pre-trained models are available at https://github.com/swz30/CycleISP.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.07761v1-abstract-full').style.display = 'none'; document.getElementById('2003.07761v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2020 (Oral)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.01159">arXiv:2003.01159</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.01159">pdf</a>, <a href="https://arxiv.org/ps/2003.01159">ps</a>, <a href="https://arxiv.org/format/2003.01159">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Geophysics">physics.geo-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Time-domain sparsity promoting least-squares reverse time migration with source estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mengmeng Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Fang%2C+Z">Zhilong Fang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Witte%2C+P">Philipp Witte</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Herrmann%2C+F+J">Felix J. Herrmann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.01159v1-abstract-short" style="display: inline;">
        Least-squares reverse time migration is well-known for its capability to generate artifact-free true-amplitude subsurface images through fitting observed data in the least-squares sense. However, when applied to realistic imaging problems, this approach is faced with issues related to overfitting and excessive computational costs induced by many wave-equation solves. The fact that the source funct&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.01159v1-abstract-full').style.display = 'inline'; document.getElementById('2003.01159v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.01159v1-abstract-full" style="display: none;">
        Least-squares reverse time migration is well-known for its capability to generate artifact-free true-amplitude subsurface images through fitting observed data in the least-squares sense. However, when applied to realistic imaging problems, this approach is faced with issues related to overfitting and excessive computational costs induced by many wave-equation solves. The fact that the source function is unknown complicates this situation even further. Motivated by recent results in stochastic optimization and transform-domain sparsity-promotion, we demonstrate that the computational costs of inversion can be reduced significantly while avoiding imaging artifacts and restoring amplitudes. While powerful, these new approaches do require accurate information on the source-time function, which is often lacking. Without this information, the imaging quality deteriorates rapidly. We address this issue by presenting an approach where the source-time function is estimated on the fly through a technique known as variable projection. Aside from introducing negligible computational overhead, the proposed method is shown to perform well on imaging problems with noisy data and problems that involve complex settings such as salt. In either case, the presented method produces high resolution high-amplitude fidelity images including an estimates for the source-time function. In addition, due to its use of stochastic optimization, we arrive at these images at roughly one to two times the cost of conventional reverse time migration involving all data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.01159v1-abstract-full').style.display = 'none'; document.getElementById('2003.01159v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.00459">arXiv:2003.00459</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.00459">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.3390/s20051309">10.3390/s20051309 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A BeiDou Signal Acquisition Approach Using Variable Length Data Accumulation based on Signal Delay and Multiplication
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Menghuan Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wu%2C+H">Hong Wu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wang%2C+Q">Qiqi Wang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhao%2C+Y">Yingxin Zhao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Liu%2C+Z">Zhiyang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.00459v1-abstract-short" style="display: inline;">
        The secondary modulation with the NeumannHoffman code increases the possibility of bit sign transition. Unlike other GNSS signals, there is no pilot component for synchronization in BeiDou B1/B3 signals, which increases the complexity in acquisition. A previous study has shown that the delay and multiplication (DAM) method is able to eliminate the bit sign transition problem, but it only applies t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.00459v1-abstract-full').style.display = 'inline'; document.getElementById('2003.00459v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.00459v1-abstract-full" style="display: none;">
        The secondary modulation with the NeumannHoffman code increases the possibility of bit sign transition. Unlike other GNSS signals, there is no pilot component for synchronization in BeiDou B1/B3 signals, which increases the complexity in acquisition. A previous study has shown that the delay and multiplication (DAM) method is able to eliminate the bit sign transition problem, but it only applies to pretty strong signals. In this paper, a DAM-based BeiDou signal acquisition approach, called variable length data accumulation (VLDA), is proposed to acquire weak satellite signals. Firstly, the performance of DAM method versus the different delays is analyzed. The DAM operation not only eliminates bit sign transition, but it also increases noise power. Secondly, long-term signal is periodically accumulated to improve signal intensity in order to acquire weak signals. While considering the Doppler frequency shift of ranging codes, the signal length must be compensated before accumulating long-term signal. Finally, the fast-Fourier-transform based parallel code phase algorithm are used for acquisition. The simulation results indicate that the proposed VLDA method has better acquisition sensitivity than traditional non-coherent integration method under the same calculation amount. The VLDA method only requires approximately 27.5% of calculations to achieve the same acquisition sensitivity (35 dBHz). What is more, the actual experimental results verify the feasibility of the VLDA method. It can be concluded that the proposed approach is an effective and feasible method for solving the bit sign transition problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.00459v1-abstract-full').style.display = 'none'; document.getElementById('2003.00459v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been accepted by Sensors(17 pages,9 figures)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.02880">arXiv:2002.02880</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.02880">pdf</a>, <a href="https://arxiv.org/format/2002.02880">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multilayer Routing and Resource Assignment in Spatial Channel Networks (SCNs): Oriented Toward the Massive SDM Era
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingcong Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Wu%2C+Q">Qian Wu</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Shigeno%2C+M">Maiko Shigeno</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=YongbingZhang"> YongbingZhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.02880v1-abstract-short" style="display: inline;">
        In the past few decades, optical transport networks (OTNs) have undergone significant evolution, from the earliest wavelength-division multiplexing (WDM) OTNs to elastic optical networks (EONs) and later to space-division multiplexing (SDM) OTNs, to address the continuous growth of Internet traffic. By 2024, Pbps-level OTNs are expected, far exceeding the capacity limit of single-mode fibers. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.02880v1-abstract-full').style.display = 'inline'; document.getElementById('2002.02880v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.02880v1-abstract-full" style="display: none;">
        In the past few decades, optical transport networks (OTNs) have undergone significant evolution, from the earliest wavelength-division multiplexing (WDM) OTNs to elastic optical networks (EONs) and later to space-division multiplexing (SDM) OTNs, to address the continuous growth of Internet traffic. By 2024, Pbps-level OTNs are expected, far exceeding the capacity limit of single-mode fibers. The massive SDM era is on the horizon. In this context, newly designed OTNs called spatial channel networks (SCNs), which achieve high cost efficiency by means of practical hierarchical optical cross-connects, have recently been proposed. However, the evolution of OTNs will simultaneously present challenges related to resource allocation in networking. For instance, with the evolution from WDM-OTNs to EONs, the resource allocation problem was transformed from the routing and wavelength assignment (RWA) problem to the routing and spectrum assignment (RSA) problem due to the additionally introduced constraint of spectrum contiguity. Similarly, specially designed algorithms are also expected to be essential for addressing the resource allocation problem in SCNs. In this paper, we define this new problem as the routing, spatial channel, and spectrum assignment (RSCSA) problem. We propose an integer linear programming (ILP) model and a heuristic algorithm to solve the RSCSA problem. We examine the performance of the proposed approaches via simulation experiments. The results show that both proposed approaches are effective in finding the optimal solutions or solutions close to the lower bounds. To the best of our knowledge, this is the first work to focus on the problem of resource allocation in SCNs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.02880v1-abstract-full').style.display = 'none'; document.getElementById('2002.02880v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.06907">arXiv:1912.06907</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.06907">pdf</a>, <a href="https://arxiv.org/format/1912.06907">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Migrating Monarch Butterfly Localization Using Multi-Sensor Fusion Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mingyu Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Hsiao%2C+R">Roger Hsiao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Carichner%2C+G">Gordy Carichner</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ernst%2C+K">Katherine Ernst</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lim%2C+J">Jaechan Lim</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Green%2C+D+A">Delbert A. Green II</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Lee%2C+I">Inhee Lee</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Blaauw%2C+D">David Blaauw</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Kim%2C+H">Hun-Seok Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.06907v1-abstract-short" style="display: inline;">
        Details of Monarch butterfly migration from the U.S. to Mexico remain a mystery due to lack of a proper localization technology to accurately localize and track butterfly migration. In this paper, we propose a deep learning based butterfly localization algorithm that can estimate a butterfly&#39;s daily location by analyzing a light and temperature sensor data log continuously obtained from an ultra-l&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.06907v1-abstract-full').style.display = 'inline'; document.getElementById('1912.06907v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.06907v1-abstract-full" style="display: none;">
        Details of Monarch butterfly migration from the U.S. to Mexico remain a mystery due to lack of a proper localization technology to accurately localize and track butterfly migration. In this paper, we propose a deep learning based butterfly localization algorithm that can estimate a butterfly&#39;s daily location by analyzing a light and temperature sensor data log continuously obtained from an ultra-low power, mm-scale sensor attached to the butterfly. To train and test the proposed neural network based multi-sensor fusion localization algorithm, we collected over 1500 days of real world sensor measurement data with 82 volunteers all over the U.S. The proposed algorithm exhibits a mean absolute error of &lt;1.5 degree in latitude and &lt;0.5 degree in longitude Earth coordinate, satisfying our target goal for the Monarch butterfly migration study.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.06907v1-abstract-full').style.display = 'none'; document.getElementById('1912.06907v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 December, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">under review for ICASSP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.04016">arXiv:1912.04016</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.04016">pdf</a>, <a href="https://arxiv.org/format/1912.04016">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Neural Network for Fast and Accurate Single Image Super-Resolution via Channel-Attention-based Fusion of Orientation-aware Features
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Chen%2C+D">Du Chen</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=He%2C+Z">Zewei He</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Cao%2C+Y">Yanpeng Cao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+J">Jiangxin Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Cao%2C+Y">Yanlong Cao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M+Y">Michael Ying Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tang%2C+S">Siliang Tang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Zhuang%2C+Y">Yueting Zhuang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.04016v1-abstract-short" style="display: inline;">
        Recently, Convolutional Neural Networks (CNNs) have been successfully adopted to solve the ill-posed single image super-resolution (SISR) problem. A commonly used strategy to boost the performance of CNN-based SISR models is deploying very deep networks, which inevitably incurs many obvious drawbacks (e.g., a large number of network parameters, heavy computational loads, and difficult model traini&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.04016v1-abstract-full').style.display = 'inline'; document.getElementById('1912.04016v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.04016v1-abstract-full" style="display: none;">
        Recently, Convolutional Neural Networks (CNNs) have been successfully adopted to solve the ill-posed single image super-resolution (SISR) problem. A commonly used strategy to boost the performance of CNN-based SISR models is deploying very deep networks, which inevitably incurs many obvious drawbacks (e.g., a large number of network parameters, heavy computational loads, and difficult model training). In this paper, we aim to build more accurate and faster SISR models via developing better-performing feature extraction and fusion techniques. Firstly, we proposed a novel Orientation-Aware feature extraction and fusion Module (OAM), which contains a mixture of 1D and 2D convolutional kernels (i.e., 5 x 1, 1 x 5, and 3 x 3) for extracting orientation-aware features. Secondly, we adopt the channel attention mechanism as an effective technique to adaptively fuse features extracted in different directions and in hierarchically stacked convolutional stages. Based on these two important improvements, we present a compact but powerful CNN-based model for high-quality SISR via Channel Attention-based fusion of Orientation-Aware features (SISR-CA-OA). Extensive experimental results verify the superiority of the proposed SISR-CA-OA model, performing favorably against the state-of-the-art SISR models in terms of both restoration accuracy and computational efficiency. The source codes will be made publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.04016v1-abstract-full').style.display = 'none'; document.getElementById('1912.04016v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 December, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 11 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.11808">arXiv:1911.11808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.11808">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Object Portion Tracking in 4D Fluorescence Microscopy Imagery with Deep Feature Maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Jiao%2C+Y">Yang Jiao</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Weng%2C+M">Mo Weng</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Mei Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.11808v1-abstract-short" style="display: inline;">
        3D fluorescence microscopy of living organisms has increasingly become an essential and powerful tool in biomedical research and diagnosis. An exploding amount of imaging data has been collected, whereas efficient and effective computational tools to extract information from them are still lagging behind. This is largely due to the challenges in analyzing biological data. Interesting biological st&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11808v1-abstract-full').style.display = 'inline'; document.getElementById('1911.11808v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.11808v1-abstract-full" style="display: none;">
        3D fluorescence microscopy of living organisms has increasingly become an essential and powerful tool in biomedical research and diagnosis. An exploding amount of imaging data has been collected, whereas efficient and effective computational tools to extract information from them are still lagging behind. This is largely due to the challenges in analyzing biological data. Interesting biological structures are not only small, but are often morphologically irregular and highly dynamic. Although tracking cells in live organisms has been studied for years, existing tracking methods for cells are not effective in tracking subcellular structures, such as protein complexes, which feature in continuous morphological changes including split and merge, in addition to fast migration and complex motion. In this paper, we first define the problem of multi-object portion tracking to model the protein object tracking process. A multi-object tracking method with portion matching is proposed based on 3D segmentation results. The proposed method distills deep feature maps from deep networks, then recognizes and matches object portions using an extended search. Experimental results confirm that the proposed method achieves 2.96% higher on consistent tracking accuracy and 35.48% higher on event identification accuracy than the state-of-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11808v1-abstract-full').style.display = 'none'; document.getElementById('1911.11808v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 8 figures, CVPR 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.10202">arXiv:1910.10202</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.10202">pdf</a>, <a href="https://arxiv.org/format/1910.10202">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Complex Transformer: A Framework for Modeling Complex-Valued Sequence
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/eess?searchtype=author&amp;query=Yang%2C+M">Muqiao Yang</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Ma%2C+M+Q">Martin Q. Ma</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Li%2C+D">Dongyu Li</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Tsai%2C+Y+H">Yao-Hung Hubert Tsai</a>, 
      
      <a href="/search/eess?searchtype=author&amp;query=Salakhutdinov%2C+R">Ruslan Salakhutdinov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.10202v2-abstract-short" style="display: inline;">
        While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.10202v2-abstract-full').style.display = 'inline'; document.getElementById('1910.10202v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.10202v2-abstract-full" style="display: none;">
        While deep learning has received a surge of interest in a variety of fields in recent years, major deep learning models barely use complex numbers. However, speech, signal and audio data are naturally complex-valued after Fourier Transform, and studies have shown a potentially richer representation of complex nets. In this paper, we propose a Complex Transformer, which incorporates the transformer model as a backbone for sequence modeling; we also develop attention and encoder-decoder network operating for complex input. The model achieves state-of-the-art performance on the MusicNet dataset and an In-phase Quadrature (IQ) signal dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.10202v2-abstract-full').style.display = 'none'; document.getElementById('1910.10202v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 October, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?searchtype=author&amp;query=Yang%2C+M&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?searchtype=author&amp;query=Yang%2C+M&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?searchtype=author&amp;query=Yang%2C+M&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>