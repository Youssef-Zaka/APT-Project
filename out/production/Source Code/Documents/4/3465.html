<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/0.17.4.post2/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/0.17.4.post2/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/0.17.4.post2/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/0.17.4.post2/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>
  <script type="text/javascript" src="https://arxiv-org.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-tqqyqk/b/20/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=3b3dcb4c"></script>

    <script type="text/javascript">
    window.ATL_JQ_PAGE_PROPS =  {
    	"triggerFunction": function(showCollectorDialog) {
    		//Requires that jQuery is available!
    		$("#feedback-button").click(function(e) {
    			e.preventDefault();
    			showCollectorDialog();
    		});
    	},
      fieldValues: {
        "components": ["16000"],  // Search component.
        "versions": ["14260"],  // Release search-0.5.6
        "customfield_11401": window.location.href
      }
    };
    </script>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/0.17.4.post2/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/0.17.4.post2/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;43 of 43 results for author: <span class="mathjax">Dao, T</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      <button class="button is-small" id="feedback-button">Feedback?</button>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?searchtype=author&amp;query=Dao%2C+T">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Dao, T">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Dao%2C+T&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Dao, T">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.13831">arXiv:2204.13831</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.13831">pdf</a>, <a href="https://arxiv.org/ps/2204.13831">ps</a>, <a href="https://arxiv.org/format/2204.13831">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Combinatorics">math.CO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Average Redundancy of Variable-Length Balancing Schemes à la Knuth
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+D+T">Duc Tu Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kiah%2C+H+M">Han Mao Kiah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+T+T">Tuan Thanh Nguyen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.13831v1-abstract-short" style="display: inline;">
        We study and propose schemes that map messages onto constant-weight codewords using variable-length prefixes. We provide polynomial-time computable formulas that estimate the average number of redundant bits incurred by our schemes. In addition to the exact formulas, we also perform an asymptotic analysis and demonstrate that our scheme uses $\frac12 \log n+O(1)$ redundant bits to encode messages&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13831v1-abstract-full').style.display = 'inline'; document.getElementById('2204.13831v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.13831v1-abstract-full" style="display: none;">
        We study and propose schemes that map messages onto constant-weight codewords using variable-length prefixes. We provide polynomial-time computable formulas that estimate the average number of redundant bits incurred by our schemes. In addition to the exact formulas, we also perform an asymptotic analysis and demonstrate that our scheme uses $\frac12 \log n+O(1)$ redundant bits to encode messages into length-$n$ words with weight $(n/2)+{\sf q}$ for constant ${\sf q}$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.13831v1-abstract-full').style.display = 'none'; document.getElementById('2204.13831v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06735">arXiv:2204.06735</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06735">pdf</a>, <a href="https://arxiv.org/ps/2204.06735">ps</a>, <a href="https://arxiv.org/format/2204.06735">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.4204/EPTCS.358.18">10.4204/EPTCS.358.18 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Algebraizability of the Logic of Quasi-N4-Lattices
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Neto%2C+C+S+L">Clodomir Silva Lima Neto</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+T+N">Thiago Nascimento da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rivieccio%2C+U">Umberto Rivieccio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06735v1-abstract-short" style="display: inline;">
        The class of quasi-N4-lattices (QN4-lattices) was introduced as a common generalization of quasi-Nelson algebras and N4-lattices, in such a way that N4-lattices are precisely the QN4-lattices satisfying the double negation law (~~x = x) and quasi-Nelson algebras are the QN4-lattices satisfying the explosive law (x ^ ~x) -&gt; y = ((x ^ ~x) -&gt; y) -&gt; ((x ^ ~x) -&gt; y). In this paper we introduce, via a H&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06735v1-abstract-full').style.display = 'inline'; document.getElementById('2204.06735v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06735v1-abstract-full" style="display: none;">
        The class of quasi-N4-lattices (QN4-lattices) was introduced as a common generalization of quasi-Nelson algebras and N4-lattices, in such a way that N4-lattices are precisely the QN4-lattices satisfying the double negation law (~~x = x) and quasi-Nelson algebras are the QN4-lattices satisfying the explosive law (x ^ ~x) -&gt; y = ((x ^ ~x) -&gt; y) -&gt; ((x ^ ~x) -&gt; y). In this paper we introduce, via a Hilbert-style presentation, a logic (L_QN4) whose algebraic semantics is a class of algebras that we show to be term-equivalent to QN4-lattices. The result is obtained by showing that the calculus introduced by us is algebraizable in the sense of Blok and Pigozzi, and its equivalent algebraic semantics is term-equivalent to  the class of QN4-lattices. As a prospect for future investigation, we consider the question of how one could place L_QN4 within the family of relevance logics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06735v1-abstract-full').style.display = 'none'; document.getElementById('2204.06735v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In Proceedings NCL 2022, arXiv:2204.06359</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        EPTCS 358, 2022, pp. 240-253
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.00595">arXiv:2204.00595</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.00595">pdf</a>, <a href="https://arxiv.org/format/2204.00595">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Monarch: Expressive Structured Matrices for Efficient and Accurate Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Beidi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N">Nimit Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Desai%2C+A">Arjun Desai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Poli%2C+M">Michael Poli</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Grogan%2C+J">Jessica Grogan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+A">Alexander Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rao%2C+A">Aniruddh Rao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.00595v1-abstract-short" style="display: inline;">
        Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dens&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00595v1-abstract-full').style.display = 'inline'; document.getElementById('2204.00595v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.00595v1-abstract-full" style="display: none;">
        Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called &#34;reverse sparsification,&#34; Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.00595v1-abstract-full').style.display = 'none'; document.getElementById('2204.00595v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.11206">arXiv:2203.11206</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.11206">pdf</a>, <a href="https://arxiv.org/ps/2203.11206">ps</a>, <a href="https://arxiv.org/format/2203.11206">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Phase Recognition in Contrast-Enhanced CT Scans based on Deep Learning and Random Sampling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+B+T">Binh T. Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+T+V">Thang V. Nguyen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pham%2C+H+H">Hieu H. Pham</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nguyen%2C+H+Q">Ha Q. Nguyen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.11206v1-abstract-short" style="display: inline;">
        A fully automated system for interpreting abdominal computed tomography (CT) scans with multiple phases of contrast enhancement requires an accurate classification of the phases. This work aims at developing and validating a precise, fast multi-phase classifier to recognize three main types of contrast phases in abdominal CT scans. We propose in this study a novel method that uses a random samplin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11206v1-abstract-full').style.display = 'inline'; document.getElementById('2203.11206v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.11206v1-abstract-full" style="display: none;">
        A fully automated system for interpreting abdominal computed tomography (CT) scans with multiple phases of contrast enhancement requires an accurate classification of the phases. This work aims at developing and validating a precise, fast multi-phase classifier to recognize three main types of contrast phases in abdominal CT scans. We propose in this study a novel method that uses a random sampling mechanism on top of deep CNNs for the phase recognition of abdominal CT scans of four different phases: non-contrast, arterial, venous, and others. The CNNs work as a slice-wise phase prediction, while the random sampling selects input slices for the CNN models. Afterward, majority voting synthesizes the slice-wise results of the CNNs, to provide the final prediction at scan level. Our classifier was trained on 271,426 slices from 830 phase-annotated CT scans, and when combined with majority voting on 30% of slices randomly chosen from each scan, achieved a mean F1-score of 92.09% on our internal test set of 358 scans. The proposed method was also evaluated on 2 external test sets: CTPAC-CCRCC (N = 242) and LiTS (N = 131), which were annotated by our experts. Although a drop in performance has been observed, the model performance remained at a high level of accuracy with a mean F1-score of 76.79% and 86.94% on CTPAC-CCRCC and LiTS datasets, respectively. Our experimental results also showed that the proposed method significantly outperformed the state-of-the-art 3D approaches while requiring less computation time for inference.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.11206v1-abstract-full').style.display = 'none'; document.getElementById('2203.11206v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication by Medical Physics</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.04215">arXiv:2112.04215</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.04215">pdf</a>, <a href="https://arxiv.org/format/2112.04215">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Models are Continual Learners
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Fini%2C+E">Enrico Fini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Costa%2C+V+G+T">Victor G. Turrisi da Costa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alameda-Pineda%2C+X">Xavier Alameda-Pineda</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ricci%2C+E">Elisa Ricci</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alahari%2C+K">Karteek Alahari</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mairal%2C+J">Julien Mairal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.04215v2-abstract-short" style="display: inline;">
        Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly conv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.04215v2-abstract-full').style.display = 'inline'; document.getElementById('2112.04215v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.04215v2-abstract-full" style="display: none;">
        Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.04215v2-abstract-full').style.display = 'none'; document.getElementById('2112.04215v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.00029">arXiv:2112.00029</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.00029">pdf</a>, <a href="https://arxiv.org/format/2112.00029">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Beidi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liang%2C+K">Kaizhao Liang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+J">Jiaming Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Z">Zhao Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Christopher Re</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.00029v1-abstract-short" style="display: inline;">
        Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model com&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.00029v1-abstract-full').style.display = 'inline'; document.getElementById('2112.00029v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.00029v1-abstract-full" style="display: none;">
        Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.00029v1-abstract-full').style.display = 'none'; document.getElementById('2112.00029v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.14237">arXiv:2111.14237</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.14237">pdf</a>, <a href="https://arxiv.org/format/2111.14237">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation">stat.CO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data-independent Low-complexity KLT Approximations for Image and Video Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Rad%C3%BCnz%2C+A+P">A. P. Radünz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silveira%2C+T+L+T">T. L. T. da Silveira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bayer%2C+F+M">F. M. Bayer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cintra%2C+R+J">R. J. Cintra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.14237v1-abstract-short" style="display: inline;">
        The Karhunen-Loève transform (KLT) is often used for data decorrelation and dimensionality reduction. The KLT is able to optimally retain the signal energy in only few transform components, being mathematically suitable for image and video compression. However, in practice, because of its high computational cost and dependence on the input signal, its application in real-time scenarios is preclude&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.14237v1-abstract-full').style.display = 'inline'; document.getElementById('2111.14237v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.14237v1-abstract-full" style="display: none;">
        The Karhunen-Loève transform (KLT) is often used for data decorrelation and dimensionality reduction. The KLT is able to optimally retain the signal energy in only few transform components, being mathematically suitable for image and video compression. However, in practice, because of its high computational cost and dependence on the input signal, its application in real-time scenarios is precluded. This work proposes low-computational cost approximations for the KLT. We focus on the blocklengths $N \in \{4, 8, 16, 32 \}$ because they are widely employed in image and video coding standards such as JPEG and high efficiency video coding (HEVC). Extensive computational experiments demonstrate the suitability of the proposed low-complexity transforms for image and video compression.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.14237v1-abstract-full').style.display = 'none'; document.getElementById('2111.14237v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 15 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.15343">arXiv:2110.15343</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.15343">pdf</a>, <a href="https://arxiv.org/format/2110.15343">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scatterbrain: Unifying Sparse and Low-rank Attention Approximation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Beidi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Winsor%2C+E">Eric Winsor</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Z">Zhao Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.15343v1-abstract-short" style="display: inline;">
        Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15343v1-abstract-full').style.display = 'inline'; document.getElementById('2110.15343v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.15343v1-abstract-full" style="display: none;">
        Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.15343v1-abstract-full').style.display = 'none'; document.getElementById('2110.15343v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.13985">arXiv:2110.13985</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.13985">pdf</a>, <a href="https://arxiv.org/format/2110.13985">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Johnson%2C+I">Isys Johnson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Goel%2C+K">Karan Goel</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Saab%2C+K">Khaled Saab</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.13985v1-abstract-short" style="display: inline;">
        Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.13985v1-abstract-full').style.display = 'inline'; document.getElementById('2110.13985v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.13985v1-abstract-full" style="display: none;">
        Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \mapsto y$ by simply simulating a linear continuous-time state-space representation $\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.13985v1-abstract-full').style.display = 'none'; document.getElementById('2110.13985v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.01775">arXiv:2108.01775</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.01775">pdf</a>, <a href="https://arxiv.org/format/2108.01775">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Costa%2C+V+G+T">Victor G. Turrisi da Costa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fini%2C+E">Enrico Fini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nabi%2C+M">Moin Nabi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sebe%2C+N">Nicu Sebe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ricci%2C+E">Elisa Ricci</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.01775v4-abstract-short" style="display: inline;">
        This paper presents solo-learn, a library of self-supervised methods for visual representation learning. Implemented in Python, using Pytorch and Pytorch lightning, the library fits both research and industry needs by featuring distributed training pipelines with mixed-precision, faster data loading via Nvidia DALI, online linear evaluation for better prototyping, and many additional training tric&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.01775v4-abstract-full').style.display = 'inline'; document.getElementById('2108.01775v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.01775v4-abstract-full" style="display: none;">
        This paper presents solo-learn, a library of self-supervised methods for visual representation learning. Implemented in Python, using Pytorch and Pytorch lightning, the library fits both research and industry needs by featuring distributed training pipelines with mixed-precision, faster data loading via Nvidia DALI, online linear evaluation for better prototyping, and many additional training tricks. Our goal is to provide an easy-to-use library comprising a large amount of Self-supervised Learning (SSL) methods, that can be easily extended and fine-tuned by the community. solo-learn opens up avenues for exploiting large-budget SSL solutions on inexpensive smaller infrastructures and seeks to democratize SSL by making it accessible to all. The source code is available at https://github.com/vturrisi/solo-learn.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.01775v4-abstract-full').style.display = 'none'; document.getElementById('2108.01775v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to JMLR</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.12092">arXiv:2105.12092</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.12092">pdf</a>, <a href="https://arxiv.org/format/2105.12092">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Trajectory Modeling via Random Utility Inverse Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pitombeira-Neto%2C+A+R">Anselmo R. Pitombeira-Neto</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Santos%2C+H+P">Helano P. Santos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+T+L+C">Ticiana L. Coelho da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Macedo%2C+J+A+F">José Antonio F. de Macedo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.12092v1-abstract-short" style="display: inline;">
        We consider the problem of modeling trajectories of drivers in a road network from the perspective of inverse reinforcement learning. As rational agents, drivers are trying to maximize some reward function unknown to an external observer as they make up their trajectories. We apply the concept of random utility from microeconomic theory to model the unknown reward function as a function of observa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12092v1-abstract-full').style.display = 'inline'; document.getElementById('2105.12092v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.12092v1-abstract-full" style="display: none;">
        We consider the problem of modeling trajectories of drivers in a road network from the perspective of inverse reinforcement learning. As rational agents, drivers are trying to maximize some reward function unknown to an external observer as they make up their trajectories. We apply the concept of random utility from microeconomic theory to model the unknown reward function as a function of observable features plus an error term which represents features known only to the driver. We develop a parameterized generative model for the trajectories based on a random utility Markov decision process formulation of drivers decisions. We show that maximum entropy inverse reinforcement learning is a particular case of our proposed formulation when we assume a Gumbel density function for the unobserved reward error terms. We illustrate Bayesian inference on model parameters through a case study with real trajectory data from a large city obtained from sensors placed on sparsely distributed points on the street network.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12092v1-abstract-full').style.display = 'none'; document.getElementById('2105.12092v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.05983">arXiv:2105.05983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.05983">pdf</a>, <a href="https://arxiv.org/format/2105.05983">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Open-Source Tool for Classification Models in Resource-Constrained Hardware
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+L+T">Lucas Tsutsui da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Souza%2C+V+M+A">Vinicius M. A. Souza</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Batista%2C+G+E+A+P+A">Gustavo E. A. P. A. Batista</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.05983v1-abstract-short" style="display: inline;">
        Applications that need to sense, measure, and gather real-time information from the environment frequently face three main restrictions: power consumption, cost, and lack of infrastructure. Most of the challenges imposed by these limitations can be better addressed by embedding Machine Learning (ML) classifiers in the hardware that senses the environment, creating smart sensors able to interpret t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.05983v1-abstract-full').style.display = 'inline'; document.getElementById('2105.05983v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.05983v1-abstract-full" style="display: none;">
        Applications that need to sense, measure, and gather real-time information from the environment frequently face three main restrictions: power consumption, cost, and lack of infrastructure. Most of the challenges imposed by these limitations can be better addressed by embedding Machine Learning (ML) classifiers in the hardware that senses the environment, creating smart sensors able to interpret the low-level data stream. However, for this approach to be cost-effective, we need highly efficient classifiers suitable to execute in unresourceful hardware, such as low-power microcontrollers. In this paper, we present an open-source tool named EmbML - Embedded Machine Learning that implements a pipeline to develop classifiers for resource-constrained hardware. We describe its implementation details and provide a comprehensive analysis of its classifiers considering accuracy, classification time, and memory usage. Moreover, we compare the performance of its classifiers with classifiers produced by related tools to demonstrate that our tool provides a diverse set of classification algorithms that are both compact and accurate. Finally, we validate EmbML classifiers in a practical application of a smart sensor and trap for disease vector mosquitoes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.05983v1-abstract-full').style.display = 'none'; document.getElementById('2105.05983v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T99
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.9
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.04072">arXiv:2105.04072</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.04072">pdf</a>, <a href="https://arxiv.org/format/2105.04072">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applications">stat.AP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Meteorological and human mobility data on predicting COVID-19 cases by a novel hybrid decomposition method with anomaly detection analysis: a case study in the capitals of Brazil
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+T+T">Tiago Tiburcio da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Francisquini%2C+R">Rodrigo Francisquini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nascimento%2C+M+C+V">Mariá C. V. Nascimento</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.04072v1-abstract-short" style="display: inline;">
        In 2020, Brazil was the leading country in COVID-19 cases in Latin America, and capital cities were the most severely affected by the outbreak. Climates vary in Brazil due to the territorial extension of the country, its relief, geography, and other factors. Since the most common COVID-19 symptoms are related to the respiratory system, many researchers have studied the correlation between the numb&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.04072v1-abstract-full').style.display = 'inline'; document.getElementById('2105.04072v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.04072v1-abstract-full" style="display: none;">
        In 2020, Brazil was the leading country in COVID-19 cases in Latin America, and capital cities were the most severely affected by the outbreak. Climates vary in Brazil due to the territorial extension of the country, its relief, geography, and other factors. Since the most common COVID-19 symptoms are related to the respiratory system, many researchers have studied the correlation between the number of COVID-19 cases with meteorological variables like temperature, humidity, rainfall, etc. Also, due to its high transmission rate, some researchers have analyzed the impact of human mobility on the dynamics of COVID-19 transmission. There is a dearth of literature that considers these two variables when predicting the spread of COVID-19 cases. In this paper, we analyzed the correlation between the number of COVID-19 cases and human mobility, and meteorological data in Brazilian capitals. We found that the correlation between such variables depends on the regions where the cities are located. We employed the variables with a significant correlation with COVID-19 cases to predict the number of COVID-19 infections in all Brazilian capitals and proposed a prediction method combining the Ensemble Empirical Mode Decomposition (EEMD) method with the Autoregressive Integrated Moving Average Exogenous inputs (ARIMAX) method, which we called EEMD-ARIMAX. After analyzing the results poor predictions were further investigated using a signal processing-based anomaly detection method. Computational tests showed that EEMD-ARIMAX achieved a forecast 26.73% better than ARIMAX. Moreover, an improvement of 30.69% in the average root mean squared error (RMSE) was noticed when applying the EEMD-ARIMAX method to the data normalized after the anomaly detection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.04072v1-abstract-full').style.display = 'none'; document.getElementById('2105.04072v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.09732">arXiv:2104.09732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.09732">pdf</a>, <a href="https://arxiv.org/format/2104.09732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Knowledge Distillation as Semiparametric Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kamath%2C+G+M">Govinda M Kamath</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Syrgkanis%2C+V">Vasilis Syrgkanis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mackey%2C+L">Lester Mackey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.09732v1-abstract-short" style="display: inline;">
        A popular approach to model compression is to train an inexpensive student model to mimic the class probabilities of a highly accurate but cumbersome teacher model. Surprisingly, this two-step knowledge distillation process often leads to higher accuracy than training the student directly on labeled data. To explain and enhance this phenomenon, we cast knowledge distillation as a semiparametric in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09732v1-abstract-full').style.display = 'inline'; document.getElementById('2104.09732v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.09732v1-abstract-full" style="display: none;">
        A popular approach to model compression is to train an inexpensive student model to mimic the class probabilities of a highly accurate but cumbersome teacher model. Surprisingly, this two-step knowledge distillation process often leads to higher accuracy than training the student directly on labeled data. To explain and enhance this phenomenon, we cast knowledge distillation as a semiparametric inference problem with the optimal student model as the target, the unknown Bayes class probabilities as nuisance, and the teacher probabilities as a plug-in nuisance estimate. By adapting modern semiparametric tools, we derive new guarantees for the prediction error of standard distillation and develop two enhancements -- cross-fitting and loss correction -- to mitigate the impact of teacher overfitting and underfitting on student performance. We validate our findings empirically on both tabular and image data and observe consistent improvements from our knowledge distillation enhancements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.09732v1-abstract-full').style.display = 'none'; document.getElementById('2104.09732v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.15798">arXiv:2103.15798</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.15798">pdf</a>, <a href="https://arxiv.org/format/2103.15798">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rethinking Neural Operations for Diverse Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Roberts%2C+N">Nicholas Roberts</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Khodak%2C+M">Mikhail Khodak</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+L">Liam Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Talwalkar%2C+A">Ameet Talwalkar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.15798v2-abstract-short" style="display: inline;">
        An important goal of AutoML is to automate-away the design of neural networks on new tasks in under-explored domains. Motivated by this goal, we study the problem of enabling users to discover the right neural operations given data from their specific domain. We introduce a search space of operations called XD-Operations that mimic the inductive bias of standard multi-channel convolutions while be&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15798v2-abstract-full').style.display = 'inline'; document.getElementById('2103.15798v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.15798v2-abstract-full" style="display: none;">
        An important goal of AutoML is to automate-away the design of neural networks on new tasks in under-explored domains. Motivated by this goal, we study the problem of enabling users to discover the right neural operations given data from their specific domain. We introduce a search space of operations called XD-Operations that mimic the inductive bias of standard multi-channel convolutions while being much more expressive: we prove that it includes many named operations across multiple application areas. Starting with any standard backbone such as ResNet, we show how to transform it into a search space over XD-operations and how to traverse the space using a simple weight-sharing scheme. On a diverse set of tasks -- solving PDEs, distance prediction for protein folding, and music modeling -- our approach consistently yields models with lower error than baseline networks and often even lower error than expert-designed domain-specific approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15798v2-abstract-full').style.display = 'none'; document.getElementById('2103.15798v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.14966">arXiv:2012.14966</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.14966">pdf</a>, <a href="https://arxiv.org/format/2012.14966">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sohoni%2C+N+S">Nimit S. Sohoni</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eichhorn%2C+M">Matthew Eichhorn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Blonder%2C+A">Amit Blonder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Leszczynski%2C+M">Megan Leszczynski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.14966v2-abstract-short" style="display: inline;">
        Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.14966v2-abstract-full').style.display = 'inline'; document.getElementById('2012.14966v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.14966v2-abstract-full" style="display: none;">
        Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. K-matrices can also simplify hand-engineered pipelines -- we replace filter bank feature computation in speech data preprocessing with a learnable kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. In addition, K-matrices can capture latent structure in models: for a challenging permuted image classification task, a K-matrix based representation of permutations is able to learn the right latent structure and improves accuracy of a downstream convolutional model by over 9%. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.14966v2-abstract-full').style.display = 'none'; document.getElementById('2012.14966v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Conference on Learning Representations (ICLR) 2020 spotlight</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.14192">arXiv:2009.14192</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.14192">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.5121/csit.2020.101115">10.5121/csit.2020.101115 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analysis of the displacement of terrestrial mobile robots in corridors using paraconsistent annotated evidential logic eτ
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Bernardini%2C+F+A">Flavio Amadeu Bernardini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+M+T">Marcia Terra da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abe%2C+J+M">Jair Minoro Abe</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Lima%2C+L+A">Luiz Antonio de Lima</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Miatluk%2C+K">Kanstantsin Miatluk</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.14192v1-abstract-short" style="display: inline;">
        This article proposes an algorithm for a servo motor that controls the movement of an autonomous terrestrial mobile robot using Paraconsistent Logic. The design process of mechatronic systems guided the robot construction phases. The project intends to monitor the robot through its sensors that send positioning signals to the microcontroller. The signals are adjusted by an embedded technology inte&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.14192v1-abstract-full').style.display = 'inline'; document.getElementById('2009.14192v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.14192v1-abstract-full" style="display: none;">
        This article proposes an algorithm for a servo motor that controls the movement of an autonomous terrestrial mobile robot using Paraconsistent Logic. The design process of mechatronic systems guided the robot construction phases. The project intends to monitor the robot through its sensors that send positioning signals to the microcontroller. The signals are adjusted by an embedded technology interface maintained in the concepts of Paraconsistent Annotated Logic acting directly on the servo steering motor. The electric signals sent to the servo motor were analyzed, and it indicates that the algorithm paraconsistent can contribute to the increase of precision of movements of servo motors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.14192v1-abstract-full').style.display = 'none'; document.getElementById('2009.14192v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.07669">arXiv:2008.07669</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.07669">pdf</a>, <a href="https://arxiv.org/format/2008.07669">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HiPPO: Recurrent Memory with Optimal Polynomial Projections
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ermon%2C+S">Stefano Ermon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Re%2C+C">Christopher Re</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.07669v2-abstract-short" style="display: inline;">
        A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal soluti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07669v2-abstract-full').style.display = 'inline'; document.getElementById('2008.07669v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.07669v2-abstract-full" style="display: none;">
        A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07669v2-abstract-full').style.display = 'none'; document.getElementById('2008.07669v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 August, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.09260">arXiv:2007.09260</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.09260">pdf</a>, <a href="https://arxiv.org/format/2007.09260">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual Explanation for Identification of the Brain Bases for Dyslexia on fMRI Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Da+Silva%2C+L+T">Laura Tomaz Da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Esper%2C+N+B">Nathalia Bianchini Esper</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ruiz%2C+D+D">Duncan D. Ruiz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Meneguzzi%2C+F">Felipe Meneguzzi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Buchweitz%2C+A">Augusto Buchweitz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.09260v1-abstract-short" style="display: inline;">
        Brain imaging of mental health, neurodevelopmental and learning disorders has coupled with machine learning to identify patients based only on their brain activation, and ultimately identify features that generalize from smaller samples of data to larger ones. However, the success of machine learning classification algorithms on neurofunctional data has been limited to more homogeneous data sets o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.09260v1-abstract-full').style.display = 'inline'; document.getElementById('2007.09260v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.09260v1-abstract-full" style="display: none;">
        Brain imaging of mental health, neurodevelopmental and learning disorders has coupled with machine learning to identify patients based only on their brain activation, and ultimately identify features that generalize from smaller samples of data to larger ones. However, the success of machine learning classification algorithms on neurofunctional data has been limited to more homogeneous data sets of dozens of participants. More recently, larger brain imaging data sets have allowed for the application of deep learning techniques to classify brain states and clinical groups solely from neurofunctional features. Deep learning techniques provide helpful tools for classification in healthcare applications, including classification of structural 3D brain images. Recent approaches improved classification performance of larger functional brain imaging data sets, but they fail to provide diagnostic insights about the underlying conditions or provide an explanation from the neural features that informed the classification. We address this challenge by leveraging a number of network visualization techniques to show that, using such techniques in convolutional neural network layers responsible for learning high-level features, we are able to provide meaningful images for expert-backed insights into the condition being classified. Our results show not only accurate classification of developmental dyslexia from the brain imaging alone, but also provide automatic visualizations of the features involved that match contemporary neuroscientific knowledge, indicating that the visual explanations do help in unveiling the neurological bases of the disorder being classified.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.09260v1-abstract-full').style.display = 'none'; document.getElementById('2007.09260v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages, 7 Figures, submitted to Journal of Visualization</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.11418">arXiv:2006.11418</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.11418">pdf</a>, <a href="https://arxiv.org/ps/2006.11418">ps</a>, <a href="https://arxiv.org/format/2006.11418">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.sigpro.2020.107685">10.1016/j.sigpro.2020.107685 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Multiparametric Class of Low-complexity Transforms for Image and Video Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Canterle%2C+D+R">D. R. Canterle</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silveira%2C+T+L+T">T. L. T. da Silveira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bayer%2C+F+M">F. M. Bayer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cintra%2C+R+J">R. J. Cintra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.11418v1-abstract-short" style="display: inline;">
        Discrete transforms play an important role in many signal processing applications, and low-complexity alternatives for classical transforms became popular in recent years. Particularly, the discrete cosine transform (DCT) has proven to be convenient for data compression, being employed in well-known image and video coding standards such as JPEG, H.264, and the recent high efficiency video coding (&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.11418v1-abstract-full').style.display = 'inline'; document.getElementById('2006.11418v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.11418v1-abstract-full" style="display: none;">
        Discrete transforms play an important role in many signal processing applications, and low-complexity alternatives for classical transforms became popular in recent years. Particularly, the discrete cosine transform (DCT) has proven to be convenient for data compression, being employed in well-known image and video coding standards such as JPEG, H.264, and the recent high efficiency video coding (HEVC). In this paper, we introduce a new class of low-complexity 8-point DCT approximations based on a series of works published by Bouguezel, Ahmed and Swamy. Also, a multiparametric fast algorithm that encompasses both known and novel transforms is derived. We select the best-performing DCT approximations after solving a multicriteria optimization problem, and submit them to a scaling method for obtaining larger size transforms. We assess these DCT approximations in both JPEG-like image compression and video coding experiments. We show that the optimal DCT approximations present compelling results in terms of coding efficiency and image quality metrics, and require only few addition or bit-shifting operations, being suitable for low-complexity and low-power systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.11418v1-abstract-full').style.display = 'none'; document.getElementById('2006.11418v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Fixed Figure 1 and typos in the reference list</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          94A08; MSC 33F05
        

        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Signal Processing, Volume 176, November 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.05958">arXiv:2004.05958</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.05958">pdf</a>, <a href="https://arxiv.org/format/2004.05958">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Anomaly Detection in Trajectory Data with Normalizing Flows
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dias%2C+M+L+D">Madson L. D. Dias</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mattos%2C+C+L+C">César Lincoln C. Mattos</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+T+L+C">Ticiana L. C. da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Macedo%2C+J+A+F">José Antônio F. de Macedo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Silva%2C+W+C+P">Wellington C. P. Silva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.05958v1-abstract-short" style="display: inline;">
        The task of detecting anomalous data patterns is as important in practical applications as challenging. In the context of spatial data, recognition of unexpected trajectories brings additional difficulties, such as high dimensionality and varying pattern lengths. We aim to tackle such a problem from a probability density estimation point of view, since it provides an unsupervised procedure to iden&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.05958v1-abstract-full').style.display = 'inline'; document.getElementById('2004.05958v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.05958v1-abstract-full" style="display: none;">
        The task of detecting anomalous data patterns is as important in practical applications as challenging. In the context of spatial data, recognition of unexpected trajectories brings additional difficulties, such as high dimensionality and varying pattern lengths. We aim to tackle such a problem from a probability density estimation point of view, since it provides an unsupervised procedure to identify out of distribution samples. More specifically, we pursue an approach based on normalizing flows, a recent framework that enables complex density estimation from data with neural networks. Our proposal computes exact model likelihood values, an important feature of normalizing flows, for each segment of the trajectory. Then, we aggregate the segments&#39; likelihoods into a single coherent trajectory anomaly score. Such a strategy enables handling possibly large sequences with different lengths. We evaluate our methodology, named aggregated anomaly detection with normalizing flows (GRADINGS), using real world trajectory data and compare it with more traditional anomaly detection techniques. The promising results obtained in the performed computational experiments indicate the feasibility of the GRADINGS, specially the variant that considers autoregressive normalizing flows.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.05958v1-abstract-full').style.display = 'none'; document.getElementById('2004.05958v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted as a conference paper at 2020 International Joint Conference on Neural Networks (IJCNN 2020), part of 2020 IEEE World Congress on Computational Intelligence (IEEE WCCI 2020)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.04468">arXiv:2004.04468</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.04468">pdf</a>, <a href="https://arxiv.org/format/2004.04468">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Multilingual Study of Multi-Sentence Compression using Word Vertex-Labeled Graphs and Integer Linear Programming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pontes%2C+E+L">Elvys Linhares Pontes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huet%2C+S">Stéphane Huet</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Torres-Moreno%2C+J">Juan-Manuel Torres-Moreno</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+T+G">Thiago G. da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Linhares%2C+A+C">Andréa Carneiro Linhares</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.04468v1-abstract-short" style="display: inline;">
        Multi-Sentence Compression (MSC) aims to generate a short sentence with the key information from a cluster of similar sentences. MSC enables summarization and question-answering systems to generate outputs combining fully formed sentences from one or several documents. This paper describes an Integer Linear Programming method for MSC using a vertex-labeled graph to select different keywords, with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.04468v1-abstract-full').style.display = 'inline'; document.getElementById('2004.04468v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.04468v1-abstract-full" style="display: none;">
        Multi-Sentence Compression (MSC) aims to generate a short sentence with the key information from a cluster of similar sentences. MSC enables summarization and question-answering systems to generate outputs combining fully formed sentences from one or several documents. This paper describes an Integer Linear Programming method for MSC using a vertex-labeled graph to select different keywords, with the goal of generating more informative sentences while maintaining their grammaticality. Our system is of good quality and outperforms the state of the art for evaluations led on news datasets in three languages: French, Portuguese and Spanish. We led both automatic and manual evaluations to determine the informativeness and the grammaticality of compressions for each dataset. In additional tests, which take advantage of the fact that the length of compressions can be modulated, we still improve ROUGE scores with shorter output sentences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.04468v1-abstract-full').style.display = 'none'; document.getElementById('2004.04468v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 April, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint version</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Computación y Sistemas Vo. 24, No. 2, 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.05544">arXiv:2002.05544</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.05544">pdf</a>, <a href="https://arxiv.org/format/2002.05544">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Superpixel Image Classification with Graph Attention Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Avelar%2C+P+H+C">Pedro H. C. Avelar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tavares%2C+A+R">Anderson R. Tavares</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silveira%2C+T+L+T">Thiago L. T. da Silveira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jung%2C+C+R">Cláudio R. Jung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lamb%2C+L+C">Luís C. Lamb</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.05544v2-abstract-short" style="display: inline;">
        This paper presents a methodology for image classification using Graph Neural Network (GNN) models. We transform the input images into region adjacency graphs (RAGs), in which regions are superpixels and edges connect neighboring superpixels. Our experiments suggest that Graph Attention Networks (GATs), which combine graph convolutions with self-attention mechanisms, outperforms other GNN models.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.05544v2-abstract-full').style.display = 'inline'; document.getElementById('2002.05544v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.05544v2-abstract-full" style="display: none;">
        This paper presents a methodology for image classification using Graph Neural Network (GNN) models. We transform the input images into region adjacency graphs (RAGs), in which regions are superpixels and edges connect neighboring superpixels. Our experiments suggest that Graph Attention Networks (GATs), which combine graph convolutions with self-attention mechanisms, outperforms other GNN models. Although raw image classifiers perform better than GATs due to information loss during the RAG generation, our methodology opens an interesting avenue of research on deep learning beyond rectangular-gridded images, such as 360-degree field of view panoramas. Traditional convolutional kernels of current state-of-the-art methods cannot handle panoramas, whereas the adapted superpixel algorithms and the resulting region adjacency graphs can naturally feed a GNN, without topology issues.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.05544v2-abstract-full').style.display = 'none'; document.getElementById('2002.05544v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 February, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.13301">arXiv:1912.13301</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.13301">pdf</a>, <a href="https://arxiv.org/format/1912.13301">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Positioning Patterns with Low Redundancy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chee%2C+Y+M">Yeow Meng Chee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+D+T">Duc Tu Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kiah%2C+H+M">Han Mao Kiah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ling%2C+S">San Ling</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+H">Hengjia Wei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.13301v1-abstract-short" style="display: inline;">
        A robust positioning pattern is a large array that allows a mobile device to locate its position by reading a possibly corrupted small window around it. In this paper, we provide constructions of binary positioning patterns, equipped with efficient locating algorithms, that are robust to a constant number of errors and have redundancy within a constant factor of optimality. Furthermore, we modify&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.13301v1-abstract-full').style.display = 'inline'; document.getElementById('1912.13301v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.13301v1-abstract-full" style="display: none;">
        A robust positioning pattern is a large array that allows a mobile device to locate its position by reading a possibly corrupted small window around it. In this paper, we provide constructions of binary positioning patterns, equipped with efficient locating algorithms, that are robust to a constant number of errors and have redundancy within a constant factor of optimality. Furthermore, we modify our constructions to correct rank errors and obtain binary positioning patterns robust to any errors of rank less than a constant number. Additionally, we construct $q$-ary robust positioning sequences robust to a large number of errors, some of which have length attaining the upper bound.
  Our construction of binary positioning sequences that are robust to a constant number of errors has the least known redundancy amongst those explicit constructions with efficient locating algorithms. On the other hand, for binary robust positioning arrays, our construction is the first explicit construction whose redundancy is within a constant factor of optimality. The locating algorithms accompanying both constructions run in time cubic in sequence length or array dimension.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.13301v1-abstract-full').style.display = 'none'; document.getElementById('1912.13301v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Extended Version of SODA 2019 Paper</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          05B30; 94C30
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.11856">arXiv:1911.11856</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.11856">pdf</a>, <a href="https://arxiv.org/format/1911.11856">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Approximating the Permanent by Sampling from Adaptive Partitions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kuck%2C+J">Jonathan Kuck</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rezatofighi%2C+H">Hamid Rezatofighi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sabharwal%2C+A">Ashish Sabharwal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ermon%2C+S">Stefano Ermon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.11856v1-abstract-short" style="display: inline;">
        Computing the permanent of a non-negative matrix is a core problem with practical applications ranging from target tracking to statistical thermodynamics. However, this problem is also #P-complete, which leaves little hope for finding an exact solution that can be computed efficiently. While the problem admits a fully polynomial randomized approximation scheme, this method has seen little use beca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11856v1-abstract-full').style.display = 'inline'; document.getElementById('1911.11856v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.11856v1-abstract-full" style="display: none;">
        Computing the permanent of a non-negative matrix is a core problem with practical applications ranging from target tracking to statistical thermodynamics. However, this problem is also #P-complete, which leaves little hope for finding an exact solution that can be computed efficiently. While the problem admits a fully polynomial randomized approximation scheme, this method has seen little use because it is both inefficient in practice and difficult to implement. We present AdaPart, a simple and efficient method for drawing exact samples from an unnormalized distribution. Using AdaPart, we show how to construct tight bounds on the permanent which hold with high probability, with guaranteed polynomial runtime for dense matrices. We find that AdaPart can provide empirical speedups exceeding 25x over prior sampling methods on matrices that are challenging for variational based approaches. Finally, in the context of multi-target tracking, exact sampling from the distribution defined by the matrix permanent allows us to use the optimal proposal distribution during particle filtering. Using AdaPart, we show that this leads to improved tracking performance using an order of magnitude fewer samples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11856v1-abstract-full').style.display = 'none'; document.getElementById('1911.11856v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.01264">arXiv:1909.01264</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.01264">pdf</a>, <a href="https://arxiv.org/format/1909.01264">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Downstream Performance of Compressed Word Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=May%2C+A">Avner May</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jian Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.01264v2-abstract-short" style="display: inline;">
        Compressing word embeddings is important for deploying NLP models in memory-constrained settings. However, understanding what makes compressed embeddings perform well on downstream tasks is challenging---existing measures of compression quality often fail to distinguish between embeddings that perform well and those that do not. We thus propose the eigenspace overlap score as a new measure. We rel&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.01264v2-abstract-full').style.display = 'inline'; document.getElementById('1909.01264v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.01264v2-abstract-full" style="display: none;">
        Compressing word embeddings is important for deploying NLP models in memory-constrained settings. However, understanding what makes compressed embeddings perform well on downstream tasks is challenging---existing measures of compression quality often fail to distinguish between embeddings that perform well and those that do not. We thus propose the eigenspace overlap score as a new measure. We relate the eigenspace overlap score to downstream performance by developing generalization bounds for the compressed embeddings in terms of this score, in the context of linear and logistic regression. We then show that we can lower bound the eigenspace overlap score for a simple uniform quantization compression method, helping to explain the strong empirical performance of this method. Finally, we show that by using the eigenspace overlap score as a selection criterion between embeddings drawn from a representative set we compressed, we can efficiently identify the better performing embedding with up to $2\times$ lower selection error rates than the next best measure of compression quality, and avoid the cost of training a model for each task of interest.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.01264v2-abstract-full').style.display = 'none'; document.getElementById('1909.01264v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 January, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 September, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2019 spotlight (Conference on Neural Information Processing Systems)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.07207">arXiv:1907.07207</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.07207">pdf</a>, <a href="https://arxiv.org/format/1907.07207">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Online Local Boosting: improving performance in online decision trees
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Costa%2C+V+G+T">Victor G. Turrisi da Costa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mastelini%2C+S+M">Saulo Martiello Mastelini</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Carvalho%2C+A+C+P+d+L+F">André C. Ponce de Leon Ferreira de Carvalho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Barbon%2C+S">Sylvio Barbon Jr</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.07207v1-abstract-short" style="display: inline;">
        As more data are produced each day, and faster, data stream mining is growing in importance, making clear the need for algorithms able to fast process these data. Data stream mining algorithms are meant to be solutions to extract knowledge online, specially tailored from continuous data problem. Many of the current algorithms for data stream mining have high processing and memory costs. Often, the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.07207v1-abstract-full').style.display = 'inline'; document.getElementById('1907.07207v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.07207v1-abstract-full" style="display: none;">
        As more data are produced each day, and faster, data stream mining is growing in importance, making clear the need for algorithms able to fast process these data. Data stream mining algorithms are meant to be solutions to extract knowledge online, specially tailored from continuous data problem. Many of the current algorithms for data stream mining have high processing and memory costs. Often, the higher the predictive performance, the higher these costs. To increase predictive performance without largely increasing memory and time costs, this paper introduces a novel algorithm, named Online Local Boosting (OLBoost), which can be combined into online decision tree algorithms to improve their predictive performance without modifying the structure of the induced decision trees. For such, OLBoost applies a boosting to small separate regions of the instances space. Experimental results presented in this paper show that by using OLBoost the online learning decision tree algorithms can significantly improve their predictive performance. Additionally, it can make smaller trees perform as good or better than larger trees.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.07207v1-abstract-full').style.display = 'none'; document.getElementById('1907.07207v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear on the 8th Brazilian Conference on Intelligent Systems (BRACIS)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.05895">arXiv:1903.05895</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.05895">pdf</a>, <a href="https://arxiv.org/format/1903.05895">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eichhorn%2C+M">Matthew Eichhorn</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.05895v2-abstract-short" style="display: inline;">
        Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.05895v2-abstract-full').style.display = 'inline'; document.getElementById('1903.05895v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.05895v2-abstract-full" style="display: none;">
        Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the $O(N \log N)$ Cooley-Tukey FFT algorithm to machine precision, for dimensions $N$ up to $1024$. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points -- the first time a structured approach has done so -- with 4X faster inference speed and 40X fewer parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.05895v2-abstract-full').style.display = 'none'; document.getElementById('1903.05895v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 December, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 March, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Conference on Machine Learning (ICML) 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.04319">arXiv:1903.04319</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.04319">pdf</a>, <a href="https://arxiv.org/format/1903.04319">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Discrete Mathematics">cs.DM</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Solving the minimum labeling global cut problem by mathematical programming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+T+G">Thiago Gouveia da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Filho%2C+G+F+d+S">Gilberto F. de Sousa Filho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ochi%2C+L+S">Luiz Satoru Ochi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Michelon%2C+P">Philippe Michelon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gueye%2C+S">Serigne Gueye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cabral%2C+L+A+F">Lucidio A. F. Cabral</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.04319v1-abstract-short" style="display: inline;">
        Let G = (V, E, L) be an edge-labeled graph such that V is the set of vertices, E is the set of edges, L is the set of labels (colors) and each edge e \in E has a label l(e) associated; The goal of the minimum labeling global cut problem (MLGCP) is to find a subset L \subseteq L of labels such that G = (V, E , LŁ) is not connected and |L| is minimized. This work proposes three new mathematical form&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.04319v1-abstract-full').style.display = 'inline'; document.getElementById('1903.04319v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.04319v1-abstract-full" style="display: none;">
        Let G = (V, E, L) be an edge-labeled graph such that V is the set of vertices, E is the set of edges, L is the set of labels (colors) and each edge e \in E has a label l(e) associated; The goal of the minimum labeling global cut problem (MLGCP) is to find a subset L \subseteq L of labels such that G = (V, E , LŁ) is not connected and |L| is minimized. This work proposes three new mathematical formulations for the MLGCP as well as branch-and-cut algorithms to solve them. The computational experiments showed that the proposed methods are able to solve small to average sized instances in a reasonable amount of time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.04319v1-abstract-full').style.display = 'none'; document.getElementById('1903.04319v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.00155">arXiv:1811.00155</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.00155">pdf</a>, <a href="https://arxiv.org/format/1811.00155">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low-Precision Random Fourier Features for Memory-Constrained Kernel Approximation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Jian Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=May%2C+A">Avner May</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.00155v2-abstract-short" style="display: inline;">
        We investigate how to train kernel approximation methods that generalize well under a memory budget. Building on recent theoretical work, we define a measure of kernel approximation error which we find to be more predictive of the empirical generalization performance of kernel approximation methods than conventional metrics. An important consequence of this definition is that a kernel approximatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.00155v2-abstract-full').style.display = 'inline'; document.getElementById('1811.00155v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.00155v2-abstract-full" style="display: none;">
        We investigate how to train kernel approximation methods that generalize well under a memory budget. Building on recent theoretical work, we define a measure of kernel approximation error which we find to be more predictive of the empirical generalization performance of kernel approximation methods than conventional metrics. An important consequence of this definition is that a kernel approximation matrix must be high rank to attain close approximation. Because storing a high-rank approximation is memory intensive, we propose using a low-precision quantization of random Fourier features (LP-RFFs) to build a high-rank approximation under a memory budget. Theoretically, we show quantization has a negligible effect on generalization performance in important settings. Empirically, we demonstrate across four benchmark datasets that LP-RFFs can match the performance of full-precision RFFs and the Nyström method, with 3x-10x and 50x-460x less memory, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.00155v2-abstract-full').style.display = 'none'; document.getElementById('1811.00155v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Conference on Artificial Intelligence and Statistics (AISTATS) 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.02309">arXiv:1810.02309</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.02309">pdf</a>, <a href="https://arxiv.org/format/1810.02309">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Compressed Transforms with Low Displacement Rank
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Thomas%2C+A+T">Anna T. Thomas</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rudra%2C+A">Atri Rudra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.02309v3-abstract-short" style="display: inline;">
        The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a class of LDR matrices with more general displacement operators, and explicitly learn over both&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.02309v3-abstract-full').style.display = 'inline'; document.getElementById('1810.02309v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.02309v3-abstract-full" style="display: none;">
        The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks also outperform general unstructured layers while using more than 20x fewer parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.02309v3-abstract-full').style.display = 'none'; document.getElementById('1810.02309v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 January, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2018. Code available at https://github.com/HazyResearch/structured-nets</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.02950">arXiv:1808.02950</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.02950">pdf</a>, <a href="https://arxiv.org/format/1808.02950">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation">stat.CO</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s11045-018-0601-5">10.1007/s11045-018-0601-5 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low-complexity 8-point DCT Approximation Based on Angle Similarity for Image and Video Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Oliveira%2C+R+S">R. S. Oliveira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cintra%2C+R+J">R. J. Cintra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bayer%2C+F+M">F. M. Bayer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silveira%2C+T+L+T">T. L. T. da Silveira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Madanayake%2C+A">A. Madanayake</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Leite%2C+A">A. Leite</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.02950v1-abstract-short" style="display: inline;">
        The principal component analysis (PCA) is widely used for data decorrelation and dimensionality reduction. However, the use of PCA may be impractical in real-time applications, or in situations were energy and computing constraints are severe. In this context, the discrete cosine transform (DCT) becomes a low-cost alternative to data decorrelation. This paper presents a method to derive computatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.02950v1-abstract-full').style.display = 'inline'; document.getElementById('1808.02950v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.02950v1-abstract-full" style="display: none;">
        The principal component analysis (PCA) is widely used for data decorrelation and dimensionality reduction. However, the use of PCA may be impractical in real-time applications, or in situations were energy and computing constraints are severe. In this context, the discrete cosine transform (DCT) becomes a low-cost alternative to data decorrelation. This paper presents a method to derive computationally efficient approximations to the DCT. The proposed method aims at the minimization of the angle between the rows of the exact DCT matrix and the rows of the approximated transformation matrix. The resulting transformations matrices are orthogonal and have extremely low arithmetic complexity. Considering popular performance measures, one of the proposed transformation matrices outperforms the best competitors in both matrix error and coding capabilities. Practical applications in image and video coding demonstrate the relevance of the proposed transformation. In fact, we show that the proposed approximate DCT can outperform the exact DCT for image encoding under certain compression ratios. The proposed transform and its direct competitors are also physically realized as digital prototype circuits using FPGA technology.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.02950v1-abstract-full').style.display = 'none'; document.getElementById('1808.02950v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 12 figures, 10 tables</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Multidimensional Systems and Signal Processing, 1-32, 2018
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.05716">arXiv:1807.05716</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.05716">pdf</a>, <a href="https://arxiv.org/format/1807.05716">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Smartphone-based user positioning in a multiple-user context with Wi-Fi and Bluetooth
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ta%2C+V">Viet-Cuong Ta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Trung-Kien Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vaufreydaz%2C+D">Dominique Vaufreydaz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Castelli%2C+E">Eric Castelli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.05716v1-abstract-short" style="display: inline;">
        In a multiuser context, the Bluetooth data from the smartphone could give an approximation of the distance between users. Meanwhile, the Wi-Fi data can be used to calculate the user&#39;s position directly. However, both the Wi-Fi-based position outputs and Bluetooth-based distances are affected by some degree of noise. In our work, we propose several approaches to combine the two types of outputs for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.05716v1-abstract-full').style.display = 'inline'; document.getElementById('1807.05716v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.05716v1-abstract-full" style="display: none;">
        In a multiuser context, the Bluetooth data from the smartphone could give an approximation of the distance between users. Meanwhile, the Wi-Fi data can be used to calculate the user&#39;s position directly. However, both the Wi-Fi-based position outputs and Bluetooth-based distances are affected by some degree of noise. In our work, we propose several approaches to combine the two types of outputs for improving the tracking accuracy in the context of collaborative positioning. The two proposed approaches attempt to build a model for measuring the errors of the Bluetooth output and Wi-Fi output. In a non-temporal approach, the model establishes the relationship in a specific interval of the Bluetooth output and Wi-Fi output. In a temporal approach, the error measurement model is expanded to include the time component between users&#39; movement. To evaluate the performance of the two approaches, we collected the data from several multiuser scenarios in indoor environment. The results show that the proposed approaches could reach a distance error around 3.0m for 75 percent of time, which outperforms the positioning results of the standard Wi-Fi fingerprinting model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.05716v1-abstract-full').style.display = 'none'; document.getElementById('1807.05716v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">International Conference on Indoor Positioning and Indoor Navigation (IPIN), Sep 2018, Nantes, France</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.06368">arXiv:1805.06368</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.06368">pdf</a>, <a href="https://arxiv.org/format/1805.06368">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Strict Very Fast Decision Tree: a memory conservative algorithm for data stream mining
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Costa%2C+V+G+T">Victor Guilherme Turrisi da Costa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Carvalho%2C+A+C+P+d+L+F">André Carlos Ponce de Leon Ferreira de Carvalho</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Junior%2C+S+B">Sylvio Barbon Junior</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.06368v2-abstract-short" style="display: inline;">
        Dealing with memory and time constraints are current challenges when learning from data streams with a massive amount of data. Many algorithms have been proposed to handle these difficulties, among them, the Very Fast Decision Tree (VFDT) algorithm. Although the VFDT has been widely used in data stream mining, in the last years, several authors have suggested modifications to increase its performa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.06368v2-abstract-full').style.display = 'inline'; document.getElementById('1805.06368v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.06368v2-abstract-full" style="display: none;">
        Dealing with memory and time constraints are current challenges when learning from data streams with a massive amount of data. Many algorithms have been proposed to handle these difficulties, among them, the Very Fast Decision Tree (VFDT) algorithm. Although the VFDT has been widely used in data stream mining, in the last years, several authors have suggested modifications to increase its performance, putting aside memory concerns by proposing memory-costly solutions. Besides, most data stream mining solutions have been centred around ensembles, which combine the memory costs of their weak learners, usually VFDTs. To reduce the memory cost, keeping the predictive performance, this study proposes the Strict VFDT (SVFDT), a novel algorithm based on the VFDT. The SVFDT algorithm minimises unnecessary tree growth, substantially reducing memory usage and keeping competitive predictive performance. Moreover, since it creates much more shallow trees than VFDT, SVFDT can achieve a shorter processing time. Experiments were carried out comparing the SVFDT with the VFDT in 11 benchmark data stream datasets. This comparison assessed the trade-off between accuracy, memory, and processing time. Statistical analysis showed that the proposed algorithm obtained similar predictive performance and significantly reduced processing time and memory use. Thus, SVFDT is a suitable option for data stream mining with memory and time limitations, recommended as a weak learner in ensemble-based solutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.06368v2-abstract-full').style.display = 'none'; document.getElementById('1805.06368v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 May, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 May, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 26 figures, Under R1 revision in Pattern Recognition Letters</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.06084">arXiv:1803.06084</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.06084">pdf</a>, <a href="https://arxiv.org/format/1803.06084">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Kernel Theory of Modern Data Augmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+A">Albert Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ratner%2C+A+J">Alexander J. Ratner</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Smith%2C+V">Virginia Smith</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=De+Sa%2C+C">Christopher De Sa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.06084v2-abstract-short" style="display: inline;">
        Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear natural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06084v2-abstract-full').style.display = 'inline'; document.getElementById('1803.06084v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.06084v2-abstract-full" style="display: none;">
        Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear naturally with respect to this model, even when we do not employ kernel classification. Next, we analyze more directly the effect of augmentation on kernel classifiers, showing that data augmentation can be approximated by first-order feature averaging and second-order variance regularization components. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses provide novel connections between prior work in invariant kernels, tangent propagation, and robust optimization. Finally, we provide several proof-of-concept applications showing that our theory can be useful for accelerating machine learning workflows, such as reducing the amount of computation needed to train using augmented data, and predicting the utility of a transformation prior to training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06084v2-abstract-full').style.display = 'none'; document.getElementById('1803.06084v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 March, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1711.06672">arXiv:1711.06672</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1711.06672">pdf</a>, <a href="https://arxiv.org/format/1711.06672">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decanting the Contribution of Instruction Types and Loop Structures in the Reuse of Traces
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Coppieters%2C+A+M">Andrey M. Coppieters</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=de+Oliveira%2C+S">Sheila de Oliveira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fran%C3%A7a%2C+F+M+G">Felipe M. G. França</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pilla%2C+M+L">Maurício L. Pilla</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Costa%2C+A+T">Amarildo T. da Costa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1711.06672v1-abstract-short" style="display: inline;">
        Reuse has been proposed as a microarchitecture-level mechanism to reduce the amount of executed instructions, collapsing dependencies and freeing resources for other instructions. Previous works have used reuse domains such as memory accesses, integer or not floating point, based on the reusability rate. However, these works have not studied the specific contribution of reusing different subsets o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1711.06672v1-abstract-full').style.display = 'inline'; document.getElementById('1711.06672v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1711.06672v1-abstract-full" style="display: none;">
        Reuse has been proposed as a microarchitecture-level mechanism to reduce the amount of executed instructions, collapsing dependencies and freeing resources for other instructions. Previous works have used reuse domains such as memory accesses, integer or not floating point, based on the reusability rate. However, these works have not studied the specific contribution of reusing different subsets of instructions for performance. In this work, we analysed the sensitivity of trace reuse to instruction subsets, comparing their efficiency to their complementary subsets. We also studied the amount of reuse that can be extracted from loops. Our experiments show that disabling trace reuse outside loops does not harm performance but reduces in 12% the number of accesses to the reuse table. Our experiments with reuse subsets show that most of the speedup can be retained even when not reusing all types of instructions previously found in the reuse domain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1711.06672v1-abstract-full').style.display = 'none'; document.getElementById('1711.06672v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 November, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1709.03270">arXiv:1709.03270</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1709.03270">pdf</a>, <a href="https://arxiv.org/ps/1709.03270">ps</a>, <a href="https://arxiv.org/format/1709.03270">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/S0888-613X(02)00066-X">10.1016/S0888-613X(02)00066-X <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Expert Opinion Extraction from a Biomedical Database
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Samet%2C+A">Ahmed Samet</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Guyet%2C+T">Thomas Guyet</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Negrevergne%2C+B">Benjamin Negrevergne</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tien-Tuan Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hoang%2C+T+N">Tuan Nha Hoang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tho%2C+M+H+B">Marie-Christine Ho Ba Tho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1709.03270v1-abstract-short" style="display: inline;">
        In this paper, we tackle the problem of extracting frequent opinions from uncertain databases. We introduce the foundation of an opinion mining approach with the definition of pattern and support measure. The support measure is derived from the commitment definition. A new algorithm called OpMiner that extracts the set of frequent opinions modelled as a mass functions is detailed. Finally, we appl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1709.03270v1-abstract-full').style.display = 'inline'; document.getElementById('1709.03270v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1709.03270v1-abstract-full" style="display: none;">
        In this paper, we tackle the problem of extracting frequent opinions from uncertain databases. We introduce the foundation of an opinion mining approach with the definition of pattern and support measure. The support measure is derived from the commitment definition. A new algorithm called OpMiner that extracts the set of frequent opinions modelled as a mass functions is detailed. Finally, we apply our approach on a real-world biomedical database that stores opinions of experts to evaluate the reliability level of biomedical data. Performance analysis showed a better quality patterns for our proposed model in comparison with literature-based methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1709.03270v1-abstract-full').style.display = 'none'; document.getElementById('1709.03270v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2017.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU), Jul 2017, Lugano, Switzerland. Springer, 31 (LNCS 10369), pp.1 - 12, 2017, Proceedings of 14th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1709.02605">arXiv:1709.02605</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1709.02605">pdf</a>, <a href="https://arxiv.org/ps/1709.02605">ps</a>, <a href="https://arxiv.org/format/1709.02605">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gaussian Quadrature for Kernel Features
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Tri Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=De+Sa%2C+C">Christopher De Sa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=R%C3%A9%2C+C">Christopher Ré</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1709.02605v3-abstract-short" style="display: inline;">
        Kernel methods have recently attracted resurgent interest, showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that $O(ε^{-2})$ samples are required to achieve an approximation error of at most $ε$. We investigate some a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1709.02605v3-abstract-full').style.display = 'inline'; document.getElementById('1709.02605v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1709.02605v3-abstract-full" style="display: none;">
        Kernel methods have recently attracted resurgent interest, showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that $O(ε^{-2})$ samples are required to achieve an approximation error of at most $ε$. We investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any $γ&gt; 0$, to achieve error $ε$ with $O(e^{e^γ} + ε^{-1/γ})$ samples as $ε$ goes to 0. Our method works particularly well with sparse ANOVA kernels, which are inspired by the convolutional layer of CNNs. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve accuracy comparable to the state-of-the-art kernel methods based on random Fourier features.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1709.02605v3-abstract-full').style.display = 'none'; document.getElementById('1709.02605v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 January, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 September, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Neural Information Processing Systems (NIPS) 2017</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1703.06501">arXiv:1703.06501</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1703.06501">pdf</a>, <a href="https://arxiv.org/format/1703.06501">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Métodos de Otimização Combinatória Aplicados ao Problema de Compressão MultiFrases
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Pontes%2C+E+L">Elvys Linhares Pontes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+T+G">Thiago Gouveia da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Linhares%2C+A+C">Andréa Carneiro Linhares</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Torres-Moreno%2C+J">Juan-Manuel Torres-Moreno</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huet%2C+S">Stéphane Huet</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1703.06501v1-abstract-short" style="display: inline;">
        The Internet has led to a dramatic increase in the amount of available information. In this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for Multi-Sentences Compression (MSC),&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1703.06501v1-abstract-full').style.display = 'inline'; document.getElementById('1703.06501v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1703.06501v1-abstract-full" style="display: none;">
        The Internet has led to a dramatic increase in the amount of available information. In this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for Multi-Sentences Compression (MSC), method which reduces the sentence length without removing core information, the insertion of optimization methods requires further study to improve the performance of MSC. This article describes a method for MSC using Combinatorial Optimization and Graph Theory to generate more informative sentences while maintaining their grammaticality. An experiment led on a corpus of 40 clusters of sentences shows that our system has achieved a very good quality and is better than the state-of-the-art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1703.06501v1-abstract-full').style.display = 'none'; document.getElementById('1703.06501v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 1 figure, 3 tables (paper in Portuguese), Preprint of XLVIII Simpósio Brasileiro de Pesquisa Operacional, 2016, Vitória, ES, (Brazil)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1606.05562">arXiv:1606.05562</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1606.05562">pdf</a>, <a href="https://arxiv.org/ps/1606.05562">ps</a>, <a href="https://arxiv.org/format/1606.05562">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s11045-014-0291-6">10.1007/s11045-014-0291-6 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Orthogonal 16-point Approximate DCT for Image and Video Compression
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silveira%2C+T+L+T">T. L. T. da Silveira</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Bayer%2C+F+M">F. M. Bayer</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cintra%2C+R+J">R. J. Cintra</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kulasekera%2C+S">S. Kulasekera</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Madanayake%2C+A">A. Madanayake</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kozakevicius%2C+A+J">A. J. Kozakevicius</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1606.05562v1-abstract-short" style="display: inline;">
        A low-complexity orthogonal multiplierless approximation for the 16-point discrete cosine transform (DCT) was introduced. The proposed method was designed to possess a very low computational cost. A fast algorithm based on matrix factorization was proposed requiring only 60~additions. The proposed architecture outperforms classical and state-of-the-art algorithms when assessed as a tool for image&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.05562v1-abstract-full').style.display = 'inline'; document.getElementById('1606.05562v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1606.05562v1-abstract-full" style="display: none;">
        A low-complexity orthogonal multiplierless approximation for the 16-point discrete cosine transform (DCT) was introduced. The proposed method was designed to possess a very low computational cost. A fast algorithm based on matrix factorization was proposed requiring only 60~additions. The proposed architecture outperforms classical and state-of-the-art algorithms when assessed as a tool for image and video compression. Digital VLSI hardware implementations were also proposed being physically realized in FPGA technology and implemented in 45 nm up to synthesis and place-route levels. Additionally, the proposed method was embedded into a high efficiency video coding (HEVC) reference software for actual proof-of-concept. Obtained results show negligible video degradation when compared to Chen DCT algorithm in HEVC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.05562v1-abstract-full').style.display = 'none'; document.getElementById('1606.05562v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 7 figures, 6 tables</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Multidimensional Systems and Signal Processing, vol. 27, no. 1, pp. 87-104, 2016
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1507.01929">arXiv:1507.01929</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1507.01929">pdf</a>, <a href="https://arxiv.org/format/1507.01929">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1103/PhysRevA.92.033855">10.1103/PhysRevA.92.033855 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Linear-Optic Heralded Photon Source
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Silva%2C+T+F">Thiago Ferreira da Silva</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Amaral%2C+G+C">Gustavo C. Amaral</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tempor%C3%A3o%2C+G+P">Guilherme P. Temporão</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=von+der+Weid%2C+J+P">Jean Pierre von der Weid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1507.01929v2-abstract-short" style="display: inline;">
        We present a Heralded Photon Source based only on linear optics and weak coherent states. By time-tuning a Hong-Ou-Mandel interferometer fed with frequency-displaced coherent states, the output photons can be synchronously heralded following sub-Poisson statistics, which is indicated by the second-order correlation function ($g^2\left(0\right)=0.556$). The absence of phase-matching restrictions ma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1507.01929v2-abstract-full').style.display = 'inline'; document.getElementById('1507.01929v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1507.01929v2-abstract-full" style="display: none;">
        We present a Heralded Photon Source based only on linear optics and weak coherent states. By time-tuning a Hong-Ou-Mandel interferometer fed with frequency-displaced coherent states, the output photons can be synchronously heralded following sub-Poisson statistics, which is indicated by the second-order correlation function ($g^2\left(0\right)=0.556$). The absence of phase-matching restrictions makes the source widely tunable, with 100-nm spectral tunability on the telecom bands. The technique presents yield comparable to state-of-the-art spontaneous parametric down-conversion-based sources, with high coherence and fiber-optic quantum communication compatibility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1507.01929v2-abstract-full').style.display = 'none'; document.getElementById('1507.01929v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 November, 2015; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 July, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 7 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Phys. Rev. A 92, 033855 (2015)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1105.2707">arXiv:1105.2707</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1105.2707">pdf</a>, <a href="https://arxiv.org/ps/1105.2707">ps</a>, <a href="https://arxiv.org/format/1105.2707">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generalized Symmetric Divergence Measures and Metric Spaces
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=da+Costa%2C+G+A+T+F">G. A. T. F. da Costa</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Taneja%2C+I+J">Inder Jeet Taneja</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1105.2707v1-abstract-short" style="display: inline;">
        Recently, Taneja studied two one parameter generalizations of J-divergence, Jensen-Shannon divergence and Arithmetic-Geometric divergence. These two generalizations in particular contain measures like: Hellinger discrimination, symmetric chi-square divergence, and triangular discrimination. These measures are well known in the literature of Statistics and Information theory. In this paper our aim&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1105.2707v1-abstract-full').style.display = 'inline'; document.getElementById('1105.2707v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1105.2707v1-abstract-full" style="display: none;">
        Recently, Taneja studied two one parameter generalizations of J-divergence, Jensen-Shannon divergence and Arithmetic-Geometric divergence. These two generalizations in particular contain measures like: Hellinger discrimination, symmetric chi-square divergence, and triangular discrimination. These measures are well known in the literature of Statistics and Information theory. In this paper our aim is to prove metric space properties for square root of these two symmetric generalized divergence measures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1105.2707v1-abstract-full').style.display = 'none'; document.getElementById('1105.2707v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2011; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2011.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/0706.4323">arXiv:0706.4323</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/0706.4323">pdf</a>, <a href="https://arxiv.org/format/0706.4323">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Theory of Finite or Infinite Trees Revisited
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Djelloul%2C+K">Khalil Djelloul</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dao%2C+T">Thi-bich-hanh Dao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fruehwirth%2C+T">Thom Fruehwirth</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="0706.4323v1-abstract-short" style="display: inline;">
        We present in this paper a first-order axiomatization of an extended theory $T$ of finite or infinite trees, built on a signature containing an infinite set of function symbols and a relation $\fini(t)$ which enables to distinguish between finite or infinite trees. We show that $T$ has at least one model and prove its completeness by giving not only a decision procedure, but a full first-order c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('0706.4323v1-abstract-full').style.display = 'inline'; document.getElementById('0706.4323v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="0706.4323v1-abstract-full" style="display: none;">
          We present in this paper a first-order axiomatization of an extended theory $T$ of finite or infinite trees, built on a signature containing an infinite set of function symbols and a relation $\fini(t)$ which enables to distinguish between finite or infinite trees. We show that $T$ has at least one model and prove its completeness by giving not only a decision procedure, but a full first-order constraint solver which gives clear and explicit solutions for any first-order constraint satisfaction problem in $T$. The solver is given in the form of 16 rewriting rules which transform any first-order constraint $φ$ into an equivalent disjunction $φ$ of simple formulas such that $φ$ is either the formula $\true$ or the formula $\false$ or a formula having at least one free variable, being equivalent neither to $\true$ nor to $\false$ and where the solutions of the free variables are expressed in a clear and explicit way. The correctness of our rules implies the completeness of $T$. We also describe an implementation of our algorithm in CHR (Constraint Handling Rules) and compare the performance with an implementation in C++ and that of a recent decision procedure for decomposable theories.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('0706.4323v1-abstract-full').style.display = 'none'; document.getElementById('0706.4323v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2007; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2007.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.4.1
        
      </p>
    

    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
        <button class="button is-small" id="feedback-button">Feedback?</button>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/about">About</a></li>
          <li><a href="https://arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://arxiv.org/help/contact"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/license">Copyright</a></li>
          <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  </body>
</html>