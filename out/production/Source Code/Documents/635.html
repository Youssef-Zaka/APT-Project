<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Research:Ethical and human-centered AI - Meta</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":true,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"3fe953c4-8ba7-4c1a-b473-4ae00e34378c","wgCSPNonce":false,"wgCanonicalNamespace":"Research","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":202,"wgPageName":"Research:Ethical_and_human-centered_AI","wgTitle":"Ethical and human-centered AI","wgCurRevisionId":19228623,"wgRevisionId":19228623,"wgArticleId":10758758,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: Unrecognized language","Pages using citations with old-style implicit et al. in editors","2018 projects","Open access projects","Completed research projects","Wikimedia Research project"],"wgPageContentLanguage":"en","wgPageContentModel":
"wikitext","wgRelevantPageName":"Research:Ethical_and_human-centered_AI","wgRelevantArticleId":10758758,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":40000,"wgNoticeProject":"meta","wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgULSCurrentAutonym":"English","wgEditSubmitButtonLabelPublish":true,"wgCentralAuthMobileDomain":false,"wgULSPosition":"personal","wgULSisCompactLinksEnabled":true};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","mediawiki.special":"ready","skins.vector.styles.legacy":"ready",
"ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.wikimediaBadges":"ready","ext.uls.pt":"ready","wikibase.client.init":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","site","mediawiki.page.ready","mediawiki.toc","skins.vector.legacy.js","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.CentralAuthInterlinkFixer","ext.gadget.wm-portal-preview","ext.gadget.AddTopic","ext.gadget.addMe","ext.gadget.formWizard","ext.gadget.susaresources","ext.gadget.aaq","ext.gadget.wrcEditor","ext.gadget.wrcCollapsible","ext.gadget.ReferenceTooltips","ext.gadget.wrcAddNewIndividual","ext.gadget.wrcAddNewGroup","ext.gadget.wadpGadgetsLoader","ext.centralauth.centralautologin","ext.uls.compactlinks","ext.uls.interface"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1i9g4",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.pt%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.special%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.39.0-wmf.10"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta name="format-detection" content="telephone=no"/>
<meta property="og:title" content="Research:Ethical and human-centered AI - Meta"/>
<meta property="og:type" content="website"/>
<link rel="preconnect" href="//upload.wikimedia.org"/>
<link rel="alternate" media="only screen and (max-width: 720px)" href="//meta.m.wikimedia.org/wiki/Research:Ethical_and_human-centered_AI"/>
<link rel="alternate" type="application/x-wiki" title="Edit" href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit"/>
<link rel="shortcut icon" href="/static/favicon/community.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Meta (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//meta.wikimedia.org/w/api.php?action=rsd"/>
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://meta.wikimedia.org/wiki/Research:Ethical_and_human-centered_AI"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-202 ns-subject mw-editable page-Research_Ethical_and_human-centered_AI rootpage-Research_Ethical_and_human-centered_AI skin-vector action-view skin-vector-legacy"><div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
	<div id="siteNotice"><!-- CentralNotice --></div>
	<div class="mw-indicators">
	</div>
	<h1 id="firstHeading" class="firstHeading mw-first-heading">Research:Ethical and human-centered AI</h1>
	<div id="bodyContent" class="vector-body">
		<div id="siteSub" class="noprint">From Meta, a Wikimedia project coordination wiki</div>
		<div id="contentSub"></div>
		<div id="contentSub2"></div>
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#searchInput">Jump to search</a>
		<div id="mw-content-text" class="mw-body-content mw-content-ltr" lang="en" dir="ltr"><div class="mw-parser-output"><div class="messagebox-pnutshell" style="position:relative;margin:2px auto;width:80%;min-width:18em;border:1px solid #AAA;background:#F9F9F9;padding:2px">
<div style="position:absolute;top:2px;left:2px"><a href="/wiki/File:Walnut.svg" class="image"><img alt="Walnut.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Walnut.svg/30px-Walnut.svg.png" decoding="async" width="30" height="30" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Walnut.svg/45px-Walnut.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Walnut.svg/60px-Walnut.svg.png 2x" data-file-width="192" data-file-height="192" /></a></div><div style="position:relative;margin:0;margin-left:34px;min-height:30px"><div style="margin:0;padding:5px"><b>This page in a nutshell:&#32;</b>This report represents an initial attempt to identify what a <i>minimum viable process</i> for the development of machine learning algorithms and other AI products within the Wikimedia Movement in an ethical and human-centered way.</div></div></div> 
<div style="float:right;clear:right;width:250px;margin-left:1em;margin-bottom:1em;background:#transparent;box-shadow: 0em 0em 0em rgba(0,0,0,.5);"><div style="padding: 0em 1em;"><div style="height: .5em;"></div><div style="background:rgba(0,0,0,.03); padding:.5ex .5em; margin-top:.5em;"><div style="border-bottom:1px solid #666"><b> Contact </b></div>
 <div>
    <div style="font-weight:bold;text-align:center;"><a href="/wiki/User:Jmorgan_(WMF)" title="User:Jmorgan (WMF)">Jonathan Morgan</a></div><div style="font-size:.8em;text-align:center;margin-bottom:.5em;"><i><a href="https://en.wikipedia.org/wiki/Wikimedia_Foundation" class="extiw" title="en:Wikimedia Foundation">Wikimedia Foundation</a></i></div>
</div>  </div><div style="background:rgba(0,0,0,.03); padding:.5ex .5em; margin-top:.5em;"><b>Duration:</b>&#160; 2018-August – 2019-May </div><div style="text-align: center; padding:.5ex .5em; margin-top:1em;"><a href="/wiki/Category:Open_access_projects" title="Open access project"><img alt="Open access project" src="//upload.wikimedia.org/wikipedia/commons/thumb/7/77/Open_Access_logo_PLoS_transparent.svg/10px-Open_Access_logo_PLoS_transparent.svg.png" decoding="async" width="10" height="16" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/77/Open_Access_logo_PLoS_transparent.svg/15px-Open_Access_logo_PLoS_transparent.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/77/Open_Access_logo_PLoS_transparent.svg/20px-Open_Access_logo_PLoS_transparent.svg.png 2x" data-file-width="640" data-file-height="1000" /></a>&#160; <a href="/wiki/Category:Open_access_projects" title="Category:Open access projects"><span style="color:black;">Open access</span></a><br /><small>via <a rel="nofollow" class="external text" href="https://doi.org/10.6084/m9.figshare.8044553">Figshare</a>&#160; Jonathan T. Morgan, 2019. ​Ethical &amp; Human Centered AI - Wikimedia Research 2030​.</small></div><div style="height: 1em;"><hr /><div class="center"><small><a href="/wiki/Research:Projects" title="Research:Projects">Research:Projects</a></small></div></div></div></div>
<div class="center"><div style="display: inline-block; width: 10em;"></div><div><div style="display: inline-block"><a href="/wiki/File:VisualEditor_-_Icon_-_Check.svg" class="image"><img alt="VisualEditor - Icon - Check.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/60/VisualEditor_-_Icon_-_Check.svg/24px-VisualEditor_-_Icon_-_Check.svg.png" decoding="async" width="24" height="24" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/60/VisualEditor_-_Icon_-_Check.svg/36px-VisualEditor_-_Icon_-_Check.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/60/VisualEditor_-_Icon_-_Check.svg/48px-VisualEditor_-_Icon_-_Check.svg.png 2x" data-file-width="12" data-file-height="12" /></a></div><b>This page documents a <a href="/wiki/Research:Completed_projects" class="mw-redirect" title="Research:Completed projects">completed research project</a>.</b></div>
</div>
<p><br />
</p>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="/wiki/File:Ethical_and_human-centererd_AI_-_Wikimedia_Research_2030.pdf" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/60/Ethical_and_human-centererd_AI_-_Wikimedia_Research_2030.pdf/page1-220px-Ethical_and_human-centererd_AI_-_Wikimedia_Research_2030.pdf.jpg" decoding="async" width="220" height="285" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/60/Ethical_and_human-centererd_AI_-_Wikimedia_Research_2030.pdf/page1-330px-Ethical_and_human-centererd_AI_-_Wikimedia_Research_2030.pdf.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/60/Ethical_and_human-centererd_AI_-_Wikimedia_Research_2030.pdf/page1-440px-Ethical_and_human-centererd_AI_-_Wikimedia_Research_2030.pdf.jpg 2x" data-file-width="1275" data-file-height="1650" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Ethical_and_human-centererd_AI_-_Wikimedia_Research_2030.pdf" class="internal" title="Enlarge"></a></div>Ethical &amp; human centered AI whitepaper <a rel="nofollow" class="external text" href="https://figshare.com/articles/Ethical_Human_Centered_AI/8044553">on Figshare</a></div></div></div>
<p><b>AI technologies have the potential to benefit the Wikimedia Movement, but they come with risks.</b> The Wikimedia Foundation has begun to build AI products around these technologies. The emerging domain of ethical AI proposes new approaches for addressing the discrimination, disruption, and damage that AI can cause. The established discipline of human centered design provides guidance on how to maintain a focus on human needs and wellbeing throughout product development.
</p><p><b>The purpose of this project is to help Wikimedia ensure ethical and human-centered outcomes in AI product development</b> given our current and anticipated goals, needs, capacities, and workflows. The project makes two contributions: 1) it motivates a set of <i>risk scenarios</i> intended to define the problem space and promote reflective decision-making, and 2) it presents a set of &#160;<i>process proposals</i> for improving AI product development. Taken together, these scenarios and proposals can help Wikimedia address anticipated challenges and identify emerging opportunities to leverage AI technologies to further our mission.
</p><p>This project page, and the associated white paper, represent an initial attempt to identify requirements for a <i>minimum viable process</i> developing machine learning models, tools, datasets, and other AI products within the Wikimedia Movement in an ethical and human-centered way. This project is oriented towards <a href="/wiki/Strategy/Wikimedia_movement/2017/Direction" title="Strategy/Wikimedia movement/2017/Direction">Wikimedia’s 2017 Strategic Direction</a>. It complements the strategic priorities described in the <a href="/wiki/Research:2030" title="Research:2030">Research 2030</a> white papers—<i>Knowledge gaps, Knowledge integrity,</i> and <i>Foundations</i>—by Wikimedia Research and <i><a href="https://www.mediawiki.org/wiki/Wikimedia_Audiences/Perspectives/Augmentation" class="extiw" title="mw:Wikimedia Audiences/Perspectives/Augmentation">Augmentation</a></i> by Wikimedia Audiences.
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Background"><span class="tocnumber">1</span> <span class="toctext">Background</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#AI_Products"><span class="tocnumber">1.1</span> <span class="toctext">AI Products</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Ethical_AI"><span class="tocnumber">1.2</span> <span class="toctext">Ethical AI</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Human-centered_AI"><span class="tocnumber">1.3</span> <span class="toctext">Human-centered AI</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#Risk_scenarios"><span class="tocnumber">2</span> <span class="toctext">Risk scenarios</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Scenario_A:_Reinforcing_existing_biases_in_article_content"><span class="tocnumber">2.1</span> <span class="toctext">Scenario A: Reinforcing existing biases in article content</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Scenario_B:_Discouraging_diversity_in_content_and_contributors"><span class="tocnumber">2.2</span> <span class="toctext">Scenario B: Discouraging diversity in content and contributors</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Scenario_C:_Lack_of_transparency_and_recourse"><span class="tocnumber">2.3</span> <span class="toctext">Scenario C: Lack of transparency and recourse</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Scenario_D:_External_re-use_and_harm"><span class="tocnumber">2.4</span> <span class="toctext">Scenario D: External re-use and harm</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Scenario_E:_Community_disruption_and_cultural_imperialism"><span class="tocnumber">2.5</span> <span class="toctext">Scenario E: Community disruption and cultural imperialism</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Scenario_F:_Culturally-mediated_assumptions_of_usefulness"><span class="tocnumber">2.6</span> <span class="toctext">Scenario F: Culturally-mediated assumptions of usefulness</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="#Process_proposals"><span class="tocnumber">3</span> <span class="toctext">Process proposals</span></a>
<ul>
<li class="toclevel-2 tocsection-13"><a href="#Checklists"><span class="tocnumber">3.1</span> <span class="toctext">Checklists</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Impact_assessments"><span class="tocnumber">3.2</span> <span class="toctext">Impact assessments</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Prototyping_and_user_testing"><span class="tocnumber">3.3</span> <span class="toctext">Prototyping and user testing</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Pilots_and_trials"><span class="tocnumber">3.4</span> <span class="toctext">Pilots and trials</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Interpretable_models"><span class="tocnumber">3.5</span> <span class="toctext">Interpretable models</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#End-user_documentation"><span class="tocnumber">3.6</span> <span class="toctext">End-user documentation</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#UI_explanations"><span class="tocnumber">3.7</span> <span class="toctext">UI explanations</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Auditing_mechanisms"><span class="tocnumber">3.8</span> <span class="toctext">Auditing mechanisms</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#Feedback_mechanisms"><span class="tocnumber">3.9</span> <span class="toctext">Feedback mechanisms</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-22"><a href="#See_also"><span class="tocnumber">4</span> <span class="toctext">See also</span></a>
<ul>
<li class="toclevel-2 tocsection-23"><a href="#Subpages_of_this_page"><span class="tocnumber">4.1</span> <span class="toctext">Subpages of this page</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-24"><a href="#References"><span class="tocnumber">5</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Background">Background</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=1" title="Edit section: Background">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="AI_Products">AI Products</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=2" title="Edit section: AI Products">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Machine learning systems consist of more than just algorithms themselves. They also include other technological components that allow the algorithm to be trained and used for a particular purpose. Each of these components is <i>designed</i>, each of these components is <i>released</i>; therefore each is an AI product in its own right.
</p><p>AI products developed by the Wikimedia Foundation include (at least):
</p>
<ol><li><b>Machine learning models:</b> algorithms that uses patterns in one set of data to make predictions about the characteristics of different data.</li>
<li><b>Curated datasets:</b> data collected or labeled to train machine learning models.</li>
<li><b>Machine learning platforms:</b> machine-learning-as-a-service applications that host models and provide programmatic access to those models.</li>
<li><b>AI-driven applications:</b> end-user facing apps, gadgets, and features powered by machine learning models.</li>
<li><b>Data labeling applications:</b> interfaces for humans to (re)classify or dispute model input and output data.</li></ol>
<h3><span class="mw-headline" id="Ethical_AI">Ethical AI</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=3" title="Edit section: Ethical AI">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The canonical definition of what constitutes ethical AI (or ethical behavior generally) is beyond the scope of this report. The general framework for ethical AI used in this report is based on the widely accepted principles of fairness, accountability, and transparency (FAT) viewed through the lens of the values of the Wikimedia Movement.
</p><p>Given that framework, a basic definition of FAT might look something like this:
</p>
<ul><li><b>Fairness:</b> the AI product does not actively or passively discriminate against groups of people in a harmful way.</li>
<li><b>Accountability:</b> everyone involved in the development and use of the AI product understands, accepts and is able to exercise their rights and responsibilities.</li>
<li><b>Transparency:</b> the intended users of an AI product can meaningfully understand the purpose of the product, how it works, and (where applicable) how specific decisions were made.<br /></li></ul>
<h3><span class="mw-headline" id="Human-centered_AI">Human-centered AI</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=4" title="Edit section: Human-centered AI">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Human-centered design is a philosophy and a set of methods for ensuring that any designed thing (artifact, process, system) meets the needs of the people who will use, interact with, or be affected by it. One definition of a human-centered system that applies well to an AI context is:
</p>
<ol><li>Designed to address <i>human needs</i></li>
<li>Based on an analysis of <i>human tasks</i></li>
<li>Built to account for <i>human skills</i></li>
<li>Evaluated in terms of <i>human benefit</i></li></ol>
<p>There are many other definitions of human-centered design, and closely related methodologies such as values-sensitive design and participatory design that prioritize, respectively, investigation and articulation of designer/stakeholder values and direct involvement of end-users in the design process. The definition presented above doesn't exclude either of these considerations; it's more a matter of focus.
</p>
<h2><span class="mw-headline" id="Risk_scenarios">Risk scenarios</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=5" title="Edit section: Risk scenarios">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Many AI ethics researchers have begun to develop scenarios as a way to communicate how even seemingly mundane or uncontroversial uses of machine learning can have negative consequences, and to spur discussion. Not all of the risk scenarios described by AI researchers are directly applicable in a Wikimedia context—for example, monetization of user data, or the dangers of autonomous cars. The following scenarios are inspired by those developed by Ethical OS<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup> and Princeton University<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup>, but adapted to AI products currently developed by the Wikimedia Foundation (or at least within the realm of possibility for Wikimedia Foundation products). 
</p><p>The scenarios below are fictional. They are designed around realistic AI products and product use cases within the Wikimedia movement, but they are not statements of fact or findings from empirical research. They are intended to illustrate some of the bad outcomes that might result from seemingly sensible design decisions made by different people at different points in the product development process.
</p><p>These scenarios are not intended to suggest that any particular person, product, or type of product is biased, harmful, malicious, or fundamentally flawed; rather, they are intended to illustrate some of types of ethical and human-centered design issues that this type of product could present in a Wikimedia Movement context. 
</p>
<h3><span class="mw-headline" id="Scenario_A:_Reinforcing_existing_biases_in_article_content">Scenario A: Reinforcing existing biases in article content</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=6" title="Edit section: Scenario A: Reinforcing existing biases in article content">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Wikimedia builds a section recommendation feature into the editing interface. This feature uses machine learning to suggest a list of potential section headings for very short articles—based on the sections that already exist in other articles that resemble them—creating hooks to encourage article expansion.
</p><p>The section recommender learns that Wikipedia biographies about men are likely to have section titles like “Career” and “Awards and honors”, while biographies of women are more likely to have sections with titles like “Personal life” and “Family”. The feature is widely used: it increases the overall quality of short Wikipedia articles, but also increases the systemic bias in the way women and men are portrayed on Wikipedia.
</p>
<h3><span class="mw-headline" id="Scenario_B:_Discouraging_diversity_in_content_and_contributors">Scenario B: Discouraging diversity in content and contributors</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=7" title="Edit section: Scenario B: Discouraging diversity in content and contributors">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Wikimedia adds a draft quality score into the new article review dashboard. The quality prediction model weighs spelling errors and grammatical disfluencies highly when scoring articles, but doesn’t consider the number of citations or the notability of the topic. As a result, articles written by people for whom English is a second language tend to have lower overall scores.
</p><p>Many content gaps in Wikipedia are around topics that native English speakers tend to be less knowledgeable about or interested in. However, reviewers find the scores work well enough and they can make acceptance and rejection decisions much faster based on the score alone. As a result, reviewer workload is decreased, but good quality new articles on important topics are rejected at an increased rate, and culturally diverse contributors frequently see their hard work deleted.
</p>
<h3><span class="mw-headline" id="Scenario_C:_Lack_of_transparency_and_recourse">Scenario C: Lack of transparency and recourse</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=8" title="Edit section: Scenario C: Lack of transparency and recourse">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Wikimedia builds filters powered by machine learning into the recent changes, article history, and watchlist feeds on Wikipedia. One of these filters highlights edits that have a high probability of being performed with malicious intent.
</p><p>A 5-year veteran editor with over 50,000 edits notices that many of their recent edits have been highlighted as likely bad faith, and that their edits are now being reverted at a much higher rate than before the filters were rolled out. After some sleuthing, they notice some patterns and believe they have figured out why their edits are being erroneously tagged as the result of a ‘corner case’—a rare combination of factors related to the kinds of edits they make and the articles they work on is confusing the model. However, they are unable to discover a way to report the issue, correct the faulty predictions, or confirm their suspicions about the cause. In the meantime, the heightened level of scrutiny and rejection they experience from their fellow editors leads to embarrassment, conflict, a sense of alienation, and they consider leaving Wikipedia.
</p>
<h3><span class="mw-headline" id="Scenario_D:_External_re-use_and_harm">Scenario D: External re-use and harm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=9" title="Edit section: Scenario D: External re-use and harm">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Wikimedia releases a dataset of Wikipedia talk page comments, labelled by crowdworkers for key words and phrases related to toxic speech. An external developer trains a machine learning model on this dataset, and uses to model to power an automated content moderation system for an online depression support forum for at-risk teens.
</p><p>Teens experiencing mental health crises tend to use emotionally charged language and words commonly associated (in other contexts) with aggression and hate speech. While the automated system proves effective at deleting trolling posts quickly, it also flags and deletes many legitimate support requests that are appropriate to the forum and permitted under its rules. The messages that demonstrate the greatest need for support are the most likely to be blocked by the tool.
</p>
<h3><span class="mw-headline" id="Scenario_E:_Community_disruption_and_cultural_imperialism">Scenario E: Community disruption and cultural imperialism</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=10" title="Edit section: Scenario E: Community disruption and cultural imperialism">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Wikimedia builds a tool that recommends articles to translate from one language to another. The tool uses machine translation to generate pretty-good translations of articles. The goal is to leverage the content and community of large Wikipedias to help smaller Wikipedias grow.
</p><p>The translations are good enough that editors from English Wikipedia who know a little bit of Amharic feel confident using this tool to publish lots of articles to Amharic Wikipedia after a quick review and some light clean-up. They appreciate the opportunity to find valuable work that matches their interests and their expertise. Amharic Wikipedia is much smaller than English, has fewer editors overall, and fewer bilingual editors, and the local community is currently focusing on expanding their Wikipedia organically and curating the content they already have.
</p><p>The Amharic community soon finds themselves overwhelmed by an influx of new imperfectly-translated articles. Although the Encyclopedia grows faster, local editors must now focus their energy on fixing errors and completing partial translations, rather than writing the articles that they are interested in writing, or that they believe are most important to their readers.
</p>
<h3><span class="mw-headline" id="Scenario_F:_Culturally-mediated_assumptions_of_usefulness">Scenario F: Culturally-mediated assumptions of usefulness</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=11" title="Edit section: Scenario F: Culturally-mediated assumptions of usefulness">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Wikimedia deploys a new ranking algorithm to power the top articles feed in the English Wikipedia Android App. The previous ranking algorithm was based on a simple pageview-based metric: it reflected what Wikipedia readers are interested in reading. The new ranking is based on a more sophisticated machine learning model that identifies trending articles based on patterns of editing activity associated with breaking news events: it reflects what Wikipedia editors are interested in editing.
</p><p>Most English Wikipedia editors are North American or Western European. Many members of the app development team are English Wikipedia editors. For them, the new ranking seems to perform better: it surfaces trending articles that are more relevant to their interests.
</p><p>However, a large proportion of mobile English readers come from India. &#160;These Indian readers value the pageview-based feed because it frequently surfaces articles that are culturally relevant to them. After the new algorithm is deployed, the feed contains fewer articles these readers find interesting. Over time, they unconsciously begin using the Wikipedia app less frequently in favor of information sources that reflect their interests and meet their needs better.
</p>
<h2><span class="mw-headline" id="Process_proposals">Process proposals</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=12" title="Edit section: Process proposals">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Checklists">Checklists</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=13" title="Edit section: Checklists">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<blockquote class="quote-frame pullquote toccolours" style="float:none; display:table; border: 1px solid #aaa;"><div style="padding: 10px 15px 10px 15px;">"“Checklists connect principle to practice. Everyone knows to scrub down before the operation. That's the principle. But if you have to check a box on a form after you've done it, you're not likely to forget. That's the practice. And checklists aren't one-shots. A checklist isn’t something you read once at some initiation ceremony; a checklist is something you work through with every procedure.”<sup id="cite_ref-patil_3-0" class="reference"><a href="#cite_note-patil-3">&#91;3&#93;</a></sup>
</div></blockquote>
<dl><dt>Overview</dt>
<dd>An ethical AI checklist consists of a list of important steps that must be taken, or questions that must be answered, at each stage of the product development. Checklists work best when the process of working through the checklist is performed consistently, transparently, and collaboratively among team members.</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Aids in identification of hidden assumptions, potential negative impacts
</td>
<td>Need to be flexible enough to work across products and team workflows, but standardized enough to ensure a baseline level of due diligence
</td></tr>
<tr>
<td>Can cover both concrete requirements ("do this") and softer requirements ("have a conversation about this before proceeding")
</td>
<td>Example AI checklists exist, but few have been vetted/tested in actual product development contexts
</td></tr>
<tr>
<td>Facilitates broader participation in decision-making among team members
</td>
<td>Binary outcome ("we talked about FOO") may encourage rubber-stamping
</td></tr>
<tr>
<td>Makes it easier for any member of the product team to "flag" missed steps or considerations without fear of reprisal
</td>
<td>
</td></tr>
<tr>
<td>Encourages articulation of audience, purpose, and context; success metrics and thresholds
</td>
<td>
</td></tr>
<tr>
<td>Increases process consistency between and across teams
</td>
<td>
</td></tr>
<tr>
<td>Tracks progress towards goals
</td>
<td>
</td></tr></tbody></table>
<dl><dt>Further reading</dt></dl>
<ol><li>Of oaths and checklists<sup id="cite_ref-patil_3-1" class="reference"><a href="#cite_note-patil-3">&#91;3&#93;</a></sup></li>
<li>Care about AI ethics? What you can do, starting today<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup></li>
<li>DEON: An Ethics Checklist for Data Scientists<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup></li>
<li>Ethical OS Toolkit<sup id="cite_ref-ethicalos_6-0" class="reference"><a href="#cite_note-ethicalos-6">&#91;6&#93;</a></sup></li></ol>
<h3><span class="mw-headline" id="Impact_assessments">Impact assessments</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=14" title="Edit section: Impact assessments">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<blockquote class="quote-frame pullquote toccolours" style="float:none; display:table; border: 1px solid #aaa;"><div style="padding: 10px 15px 10px 15px;">"Algorithms and the data that drive them are designed and created by people -- There is always a human ultimately responsible for decisions made or informed by an algorithm. "The algorithm did it" is not an acceptable excuse if algorithmic systems make mistakes or have undesired consequences."<sup id="cite_ref-fatml_7-0" class="reference"><a href="#cite_note-fatml-7">&#91;7&#93;</a></sup>
</div></blockquote>
<dl><dt>Overview</dt>
<dd>An ethical AI impact statement is a product plan that is published before substantial development begins. Impact statements include a detailed product rationale, supporting research, risk assessment, success criteria, and maintenance and monitoring plans.</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Aids in identification of hidden assumptions, potential negative impacts
</td>
<td>Time-consuming to create, and unclear whether the expense is justified
</td></tr>
<tr>
<td>Encourages in-depth justification for design decisions
</td>
<td>Few real-world examples of "algorithmic impact statements" available to learn from
</td></tr>
<tr>
<td>Encourages articulation of audience, purpose, and context; success metrics and thresholds
</td>
<td>Substantial overlap with checklists (depending on what's in the checklist)
</td></tr>
<tr>
<td>Documentation increases accountability for outcomes
</td>
<td>Not always clear who the target audience for the document is
</td></tr>
<tr>
<td>Encourages articulation of audience, purpose, and context; success metrics and thresholds
</td>
<td>
</td></tr></tbody></table>
<p><br />
</p>
<dl><dt>Further reading</dt></dl>
<ol><li>Social Impact Assessment: Guidance for assessing and managing the social impacts of projects<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup></li>
<li>Algorithmic Impact Assessments: a Practical Framework for Public Agency Accountability<sup id="cite_ref-ainow_9-0" class="reference"><a href="#cite_note-ainow-9">&#91;9&#93;</a></sup></li>
<li>Principles for Accountable Algorithms and a Social Impact Statement for Algorithms<sup id="cite_ref-fatml_7-1" class="reference"><a href="#cite_note-fatml-7">&#91;7&#93;</a></sup></li>
<li>Ethics &amp; Algorithms Toolkit<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup></li></ol>
<h3><span class="mw-headline" id="Prototyping_and_user_testing">Prototyping and user testing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=15" title="Edit section: Prototyping and user testing">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<blockquote class="quote-frame pullquote toccolours" style="float:none; display:table; border: 1px solid #aaa;"><div style="padding: 10px 15px 10px 15px;">"Understanding how people actually interact—and want to interact—with machine learning systems is critical to designing systems that people can use effectively. Exploring interaction techniques through user studies can reveal gaps in a designer’s assumptions about their end-users and may lead to helpful insights about the types of input and output that interfaces for interactive machine learning should support."<sup id="cite_ref-amershi_11-0" class="reference"><a href="#cite_note-amershi-11">&#91;11&#93;</a></sup>
</div></blockquote>
<dl><dt>Overview</dt>
<dd>Prototyping is a process of making iterative, incremental refinements to a design based on explicit feedback or observations of use before full deployment. Prototypes are usually lower fidelity than the final product: e.g. sketches, mock-ups, or simplified versions.</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Encourages definition of APC and success metrics ahead of time
</td>
<td>Works best when performed by people with some degree of familiarity with UX design or research methods, a resource not available to all teams
</td></tr>
<tr>
<td>Can be performed with low-fidelity interfaces, early stage models, or even before any software or ML engineering has begun. Can even be performed on documentation for datasets and APIs
</td>
<td>Can slow down development in some cases, can sometimes be challenging to implement in Agile/scrum or other XP paradigms
</td></tr>
<tr>
<td>Allows identification unanticipated issues (such as issues of bias or harm) before committing extensive resources towards a particular design solution
</td>
<td>Identification of issues (whether bias or use experience) requires access to representative test users and an approximation of a typical context of use
</td></tr>
<tr>
<td>Can help avoid costly failures that require teams to pivot or re-boot late in the design/dev process, or after deployment
</td>
<td>
</td></tr></tbody></table>
<p><br />
</p>
<dl><dt>Further reading</dt></dl>
<ol><li>Power to the People: The Role of Humans in Interactive Machine Learning<sup id="cite_ref-amershi_11-1" class="reference"><a href="#cite_note-amershi-11">&#91;11&#93;</a></sup></li>
<li>User perception of differences in recommender algorithms<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup></li>
<li>The usability and utility of recommender systems can be quantitatively measured through user studies<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup></li>
<li>Making recommendations better: an analytic model for human-recommender interaction<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup></li></ol>
<h3><span class="mw-headline" id="Pilots_and_trials">Pilots and trials</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=16" title="Edit section: Pilots and trials">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<blockquote class="quote-frame pullquote toccolours" style="float:none; display:table; border: 1px solid #aaa;"><div style="padding: 10px 15px 10px 15px;">"Many of our fundamentally held viewpoints continue to be ruled by outdated biases derived from the evaluation of a single user sitting in front of a single desktop computer. It is time for us to look at evaluations in more holistic ways. One way to do this is to engage with real users in 'Living Laboratories', in which researchers either adopt or create real useful systems that are used in real settings that are ecologically valid."<sup id="cite_ref-chi_15-0" class="reference"><a href="#cite_note-chi-15">&#91;15&#93;</a></sup>
</div></blockquote>
<dl><dt>Overview</dt>
<dd>A pilot is a fixed-term or limited scale deployment of a final product, where the decision to fully deploy is deferred until the outcome of the pilot is assessed. Unlike prototypes, pilots involve putting finished products in front of real users and tracking how the product performs in the wild over an extended period of time.</dd></dl>
<dl><dt>Audience</dt>
<dd>The product team, end-users of the product (individuals or communities)</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Allows team to understand the ecological validity ("does it work as intended?") of the AI product before committing to release it into their product ecosystem and maintain it long term
</td>
<td>Extends the product development timeline
</td></tr>
<tr>
<td>Allows team to measure the ecological impact ("what are the adjacent and downsteam effects?") of their AI product on the product ecosystem before committing
</td>
<td>Not a substitute for iterative prototyping and testing
</td></tr>
<tr>
<td>Allows long-term impact measurement of performance (longitudinal analysis) and comparative measurement (A/B testing) against success criteria with real users
</td>
<td>Unintended negative consequences impact people's lives
</td></tr>
<tr>
<td>Provides baselines for long-term performance monitoring (e.g. detecting <a href="https://en.wikipedia.org/wiki/Concept_drift" class="extiw" title="en:Concept drift">model drift</a>)
</td>
<td>
</td></tr>
<tr>
<td>Increases accountability and supports user trust in the product team and the organization ("if it doesn't work, we will turn it off")
</td>
<td>
</td></tr></tbody></table>
<p><br />
</p>
<dl><dt>Further reading</dt></dl>
<ol><li>A Position Paper on 'Living Laboratories': Rethinking Ecological Designs and Experimentation in Human-Computer Interaction<sup id="cite_ref-chi_15-1" class="reference"><a href="#cite_note-chi-15">&#91;15&#93;</a></sup></li>
<li>Behaviorism is Not Enough: Better Recommendations through Listening to Users<sup id="cite_ref-ekstrand_16-0" class="reference"><a href="#cite_note-ekstrand-16">&#91;16&#93;</a></sup></li>
<li><a href="/wiki/Research:Autoconfirmed_article_creation_trial" title="Research:Autoconfirmed article creation trial">Research:Autoconfirmed article creation trial</a></li></ol>
<h3><span class="mw-headline" id="Interpretable_models">Interpretable models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=17" title="Edit section: Interpretable models">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<dl><dt>Overview</dt>
<dd>Interpretable machine learning models are models that a) expose the logic behind a particular output or decision, and/or b) expose the general features, procedures, or probabilities implicated in their decision-making in a way that the intended audience can understand.</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Facilitates downstream explanations for individual algorithmic decisions
</td>
<td>Accuracy and performance may be lower overall compared to more opaque models (e.g. deep learning) for some ML tasks
</td></tr>
<tr>
<td>Facilitates iterative development and comparative evaluation towards fairness and utility benchmarks, not just accuracy and performance
</td>
<td>Making the model more interpretable may allow people to "game" the system in deceptive or damaging ways
</td></tr>
<tr>
<td>Facilitates external auditing, internal sanity checks, formal user testing, and end-user feedback
</td>
<td>
</td></tr></tbody></table>
<p><br />
</p>
<dl><dt>Further reading</dt></dl>
<ol><li>How the machine ‘thinks’: Understanding opacity in machine learning algorithms<sup id="cite_ref-Burrell_2016_17-0" class="reference"><a href="#cite_note-Burrell_2016-17">&#91;17&#93;</a></sup></li>
<li>The Promise and Peril of Human Evaluation for Model Interpretability<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup></li>
<li>Toward human-centered algorithm design<sup id="cite_ref-Baumer_2017_19-0" class="reference"><a href="#cite_note-Baumer_2017-19">&#91;19&#93;</a></sup></li></ol>
<h3><span class="mw-headline" id="End-user_documentation">End-user documentation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=18" title="Edit section: End-user documentation">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<blockquote class="quote-frame pullquote toccolours" style="float:none; display:table; border: 1px solid #aaa;"><div style="padding: 10px 15px 10px 15px;">"Because the linguistic data we use will always include pre-existing biases and because it is not possible to build an NLP system in such a way that it is immune to emergent bias, we must seek additional strategies for mitigating the scientific and ethical shortcomings that follow from imperfect datasets. We propose here that foregrounding the characteristics of our datasets can help, by allowing reasoning about what the likely effects may be and by making it clearer which populations are and are not represented."<sup id="cite_ref-bender_20-0" class="reference"><a href="#cite_note-bender-20">&#91;20&#93;</a></sup>
</div></blockquote>
<dl><dt>Overview</dt>
<dd>Detailed descriptions of the intended audience and use cases for an AI product, with a focus on potential issues and limitations, and other special considerations for use.</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Can ensure that AI product users have the information they need to make informed decisions about how to use the product and/or interpret its functionality
</td>
<td>Can be costly to create and maintain
</td></tr>
<tr>
<td>Easily transportable with the data/code, wherever it goes
</td>
<td>Writing good documentation is hard
</td></tr>
<tr>
<td>Easily adaptable to the needs of different users (e.g. third-party tool devs vs. data scientists) and different AI products (e.g. training datasets vs. AI platform APIs)
</td>
<td>Not always clear how much documentation is necessary and sufficient for a given audience; the documentation itself may require user testing
</td></tr>
<tr>
<td>Many existing frameworks and best practices from software dev are likely applicable to AI product context; some new ones have been proposed specifically for AI bias contexts
</td>
<td>People don't always read the docs
</td></tr></tbody></table>
<p><br />
</p>
<dl><dt>Further reading</dt></dl>
<ol><li>Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science<sup id="cite_ref-bender_20-1" class="reference"><a href="#cite_note-bender-20">&#91;20&#93;</a></sup></li>
<li>Increasing Trust in AI Services through Supplier's Declarations of Conformity<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup></li>
<li>The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup></li>
<li>Datasheets for Datasets<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup></li>
<li>The Types, Roles, and Practices of Documentation in Data Analytics Open Source Software Libraries<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup></li></ol>
<h3><span class="mw-headline" id="UI_explanations">UI explanations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=19" title="Edit section: UI explanations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<blockquote class="quote-frame pullquote toccolours" style="float:none; display:table; border: 1px solid #aaa;"><div style="padding: 10px 15px 10px 15px;">"How can we provide meaningful control over the recommendation process to users, so that they can understand the decisions they make about their recommendations and customize the system to their particular needs?"<sup id="cite_ref-ekstrand_16-1" class="reference"><a href="#cite_note-ekstrand-16">&#91;16&#93;</a></sup>
</div></blockquote>
<dl><dt>Overview</dt>
<dd>UI explanations consist of contextual metadata about how a model works that is made available to end users at the point of use. UI explanations can be written in words, or presented as statistical probabilities or graphical visualizations.</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Can encourage trust among end users
</td>
<td>Not always clear how much information to include—tension between overwhelming/distracting the user and depriving them of important insights
</td></tr>
<tr>
<td>Empowers end users to make informed decisions about how to use an AI product or how to interpret its decisions
</td>
<td>Potential tension between choosing the most correct and the most persuasive explanation
</td></tr>
<tr>
<td>Extensive research literature on the effectiveness of different textual, numeric, and visual approaches to explanation, at least in some domains (e.g. recommender systems)
</td>
<td>Depends on interpretable models (or computational methods for making opaque model output more interpretable)
</td></tr>
<tr>
<td>Testable; it's possible to empirically verify whether an explanation works or not—sometimes even before you build your model or your interface
</td>
<td>
</td></tr>
<tr>
<td>Encourages feedback, auditing, monitoring against drift, and potentially re-training of the model
</td>
<td>
</td></tr></tbody></table>
<p><br />
</p>
<dl><dt>Further reading</dt></dl>
<ol><li>Evaluating the effectiveness of explanations for recommender systems: Methodological issues and empirical studies on the impact of personalization<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup></li>
<li>Explaining data-driven document classifications<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup></li>
<li>User interface patterns in recommendation-empowered content intensive multimedia applications<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup></li></ol>
<h3><span class="mw-headline" id="Auditing_mechanisms">Auditing mechanisms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=20" title="Edit section: Auditing mechanisms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<blockquote class="quote-frame pullquote toccolours" style="float:none; display:table; border: 1px solid #aaa;"><div style="padding: 10px 15px 10px 15px;">"Algorithm transparency is a pressing societal problem. Algorithms provide functions like social sorting, market segmentation, personalization, recommendations, and the management of traffic flows from bits to cars. Making these infrastructures computational has made them much more powerful, but also much more opaque to public scrutiny and understanding. The history of sorting and discrimination across a variety of contexts would lead one to believe that public scrutiny of this transformation is critical. How can such public interest scrutiny of algorithms be achieved?"<sup id="cite_ref-sandvig_28-0" class="reference"><a href="#cite_note-sandvig-28">&#91;28&#93;</a></sup>
</div></blockquote>
<dl><dt>Overview</dt>
<dd>Auditing mechanisms are features that allow individuals or groups outside of the product team also inspect and spot-check individual inputs and outputs of a machine learning model, or critically evaluate the design process behind an AI product.</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Increases transparency and underscores organizational commitment to ethical and human-centered AI
</td>
<td>Most effective when paired with interpretable models and UI explanations
</td></tr>
<tr>
<td>Facilitates identification of potentially problematic edge- and corner-cases by external experts and power users
</td>
<td>Can expose organization to public embarrassment based on individual examples of failure, whether or not those examples are representative of a larger or problematic error patterns (e.g. unfair bias against a group)
</td></tr>
<tr>
<td>Facilitates early detection of model drift
</td>
<td>may require dedication of substantial platform or personnel resources to support ad hoc use
</td></tr>
<tr>
<td>
</td>
<td>support requirements vary depending on the capabilities of the auditor and the nature of the audit: do they need a fully-featured web application that supports arbitrary input and provides UI explanations, a well-documented API that exposes model and decision-level metadata, or just a sample dataset and a public GitHub repository?
</td></tr></tbody></table>
<p><br />
</p>
<dl><dt>Further reading</dt></dl>
<ol><li>Algorithmic Accountability Reporting: On the Investigation of Black Boxes<sup id="cite_ref-29" class="reference"><a href="#cite_note-29">&#91;29&#93;</a></sup></li>
<li>Auditing Algorithms : Research Methods for Detecting Discrimination on Internet Platforms<sup id="cite_ref-sandvig_28-1" class="reference"><a href="#cite_note-sandvig-28">&#91;28&#93;</a></sup></li></ol>
<h3><span class="mw-headline" id="Feedback_mechanisms">Feedback mechanisms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=21" title="Edit section: Feedback mechanisms">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<blockquote class="quote-frame pullquote toccolours" style="float:none; display:table; border: 1px solid #aaa;"><div style="padding: 10px 15px 10px 15px;">"Behavioral data without proper grounding in theory and in subjective evaluation might just result in local optimization or short term quick wins, rather than long term satisfaction. When can we know from the behavior of a user if the recommendations help to fulfill their needs and goals?"<sup id="cite_ref-ekstrand_16-2" class="reference"><a href="#cite_note-ekstrand-16">&#91;16&#93;</a></sup>
</div></blockquote>
<dl><dt>Overview</dt>
<dd>Feedback mechanisms are features that allow product users to correct, contest, refine, discuss, or dismiss the output of a machine learning model at the point of use.</dd></dl>
<dl><dt>Pros and cons</dt></dl>
<table class="wikitable">
<caption>
</caption>
<tbody><tr>
<th style="background: #CEE7DA;">Pros
</th>
<th style="background: #FFCCCC;">Cons
</th></tr>
<tr>
<td>Can be used to re-train the machine learning model
</td>
<td>Usefulness of feedback depends heavily on the design of the feedback mechanism
</td></tr>
<tr>
<td>Help the team flag emerging issues of bias, harm, or other unintended consequences
</td>
<td>Takes resources to monitor, triage, respond to, and make use of feedback (depending on the mechanism for feedback collection and the kind of feedback collected)
</td></tr>
<tr>
<td>Helps the team quickly identify technical and UX issues
</td>
<td>Privacy considerations around how feedback is captured and stored, and who has access
</td></tr>
<tr>
<td>Increases trust and user acceptance
</td>
<td>
</td></tr>
<tr>
<td>Can yield insights into user expectations, workflows, and context of use
</td>
<td>
</td></tr></tbody></table>
<p><br />
</p>
<dl><dt>Further reading</dt></dl>
<ol><li>Why Is That Relevant? Collecting Annotator Rationales for Relevance Judgments<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">&#91;30&#93;</a></sup></li>
<li><a href="https://www.mediawiki.org/wiki/JADE" class="extiw" title="mw:JADE">JADE: The Judgement and Dialogue Engine</a></li></ol>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=22" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="https://figshare.com/articles/Ethical_AI_at_Wikimedia_-_Berkman_Klein_Center_workshop_June_2019_-_Slides_and_notes_pdf/8434181">Ethical AI and product development at Wikimedia</a> - presentation slides (with notes) given at the Berkman Klein Center at Harvard University in June 2019</li>
<li><a href="/wiki/Strategy/Wikimedia_movement/2017/Direction" title="Strategy/Wikimedia movement/2017/Direction">The Wikimedia Movement Strategic Direction</a></li>
<li><a href="https://www.mediawiki.org/wiki/Wikimedia_Audiences/Perspectives" class="extiw" title="mw:Wikimedia Audiences/Perspectives">Wikimedia Audiences Perspectives</a> - set of white papers introducing key themes relevant to Wikimedia products and Movement strategy</li>
<li><a class="external text" href="https://research.wikimedia.org/">research.wikimedia.org</a> - the Wikimedia Foundation's Research team homepage</li></ul>
<h3><span class="mw-headline" id="Subpages_of_this_page">Subpages of this page</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=23" title="Edit section: Subpages of this page">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div lang="en" dir="ltr" class="mw-content-ltr" style="border:1px solid #CCC;">
<p>Pages with the prefix 'Ethical and human-centered AI' in the 'Research' and 'Research talk' namespaces:
</p>
<div style="margin-bottom:-1px; padding:0.1em; border-bottom:1px solid #CCC; font-size:0.9em;">
<p>Research:
</p>
<div lang=""><div class="mw-prefixindex-body"><ul class="mw-prefixindex-list"><li><a href="/wiki/Research:Ethical_and_human-centered_AI" title="Research:Ethical and human-centered AI">Ethical and human-centered AI</a></li>
<li><a href="/wiki/Research:Ethical_and_human-centered_AI/Bias_in_recommender_systems" title="Research:Ethical and human-centered AI/Bias in recommender systems">Ethical and human-centered AI/Bias in recommender systems</a></li>
<li><a href="/wiki/Research:Ethical_and_human-centered_AI/Process_frameworks" title="Research:Ethical and human-centered AI/Process frameworks">Ethical and human-centered AI/Process frameworks</a></li>
<li><a href="/wiki/Research:Ethical_and_human-centered_AI/Process_proposals" title="Research:Ethical and human-centered AI/Process proposals">Ethical and human-centered AI/Process proposals</a></li>
</ul></div></div>
</div>
<div style="padding:2px; font-size:0.9em;">
<p>Research talk:
</p>
<div lang=""></div>
</div>
</div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit&amp;section=24" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><a href="#cite_ref-1">↑</a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://ethicalos.org/">"Ethical OS Toolkit"</a>. <i>Ethical OS: A guide to anticipating future impacts of today's technologies</i> (in en-US)<span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.btitle=Ethical+OS+Toolkit&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fethicalos.org%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span><span class="citation-comment" style="display:none; color:#33aa33"> CS1 maint: Unrecognized language (<a href="/wiki/Category:CS1_maint:_Unrecognized_language" title="Category:CS1 maint: Unrecognized language">link</a>)</span></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><a href="#cite_ref-2">↑</a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://aiethics.princeton.edu/case-studies/">"Princeton Dialogues on AI and Ethics"</a>. <i>Princeton Dialogues on AI and Ethics</i> (in en-US). 2018-04-19<span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.btitle=Princeton+Dialogues+on+AI+and+Ethics&amp;rft.date=2018-04-19&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Faiethics.princeton.edu%2Fcase-studies%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span><span class="citation-comment" style="display:none; color:#33aa33"> CS1 maint: Unrecognized language (<a href="/wiki/Category:CS1_maint:_Unrecognized_language" title="Category:CS1 maint: Unrecognized language">link</a>)</span></span>
</li>
<li id="cite_note-patil-3"><span class="mw-cite-backlink">↑ <a href="#cite_ref-patil_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-patil_3-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web">Patil, DJ (2018-07-17). <a rel="nofollow" class="external text" href="https://www.oreilly.com/ideas/of-oaths-and-checklists">"Of oaths and checklists"</a>. <i>O'Reilly Media</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2018-12-17</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.aufirst=DJ&amp;rft.aulast=Patil&amp;rft.btitle=Of+oaths+and+checklists&amp;rft.date=2018-07-17&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fwww.oreilly.com%2Fideas%2Fof-oaths-and-checklists&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><a href="#cite_ref-4">↑</a></span> <span class="reference-text"><cite class="citation web">Adler, Steven (2018-09-25). <a rel="nofollow" class="external text" href="https://medium.com/artificial-intelligence-ai-for-social-impact/care-about-ai-ethics-what-you-can-do-starting-today-882a0e63d828">"Care about AI ethics? What you can do, starting today"</a>. <i>Medium</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.aufirst=Steven&amp;rft.aulast=Adler&amp;rft.btitle=Care+about+AI+ethics%3F+What+you+can+do%2C+starting+today&amp;rft.date=2018-09-25&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fmedium.com%2Fartificial-intelligence-ai-for-social-impact%2Fcare-about-ai-ethics-what-you-can-do-starting-today-882a0e63d828&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><a href="#cite_ref-5">↑</a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://drivendata.co/open-source/deon-ethics-checklist/">"Deon: An Ethics Checklist for Data Scientists - DrivenData Labs"</a>. <i>drivendata.co</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.btitle=Deon%3A+An+Ethics+Checklist+for+Data+Scientists+-+DrivenData+Labs&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fdrivendata.co%2Fopen-source%2Fdeon-ethics-checklist%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-ethicalos-6"><span class="mw-cite-backlink"><a href="#cite_ref-ethicalos_6-0">↑</a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://ethicalos.org/">"Ethical OS Toolkit"</a> (in en-US)<span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.btitle=Ethical+OS+Toolkit&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fethicalos.org%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span><span class="citation-comment" style="display:none; color:#33aa33"> CS1 maint: Unrecognized language (<a href="/wiki/Category:CS1_maint:_Unrecognized_language" title="Category:CS1 maint: Unrecognized language">link</a>)</span></span>
</li>
<li id="cite_note-fatml-7"><span class="mw-cite-backlink">↑ <a href="#cite_ref-fatml_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-fatml_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.fatml.org/resources/principles-for-accountable-algorithms">"Principles for Accountable Algorithms and a Social Impact Statement for Algorithms&#160;:: FAT ML"</a>. <i>www.fatml.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.btitle=Principles+for+Accountable+Algorithms+and+a+Social+Impact+Statement+for+Algorithms+%3A%3A+FAT+ML&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fwww.fatml.org%2Fresources%2Fprinciples-for-accountable-algorithms&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><a href="#cite_ref-8">↑</a></span> <span class="reference-text"><cite class="citation journal">Franks, Daniel; Aucamp, Ilse; Esteves, Ana Maria; Vanclay, Francis (2015-04-01). <a rel="nofollow" class="external text" href="https://www.rug.nl/research/portal/en/publications/social-impact-assessment(bf7d07e0-da85-47c2-a64d-e70b147d3938).html">"Social Impact Assessment: Guidance for assessing and managing the social impacts of projects"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Social+Impact+Assessment%3A+Guidance+for+assessing+and+managing+the+social+impacts+of+projects&amp;rft.au=Aucamp%2C+Ilse&amp;rft.au=Esteves%2C+Ana+Maria&amp;rft.au=Vanclay%2C+Francis&amp;rft.aufirst=Daniel&amp;rft.aulast=Franks&amp;rft.date=2015-04-01&amp;rft.genre=article&amp;rft_id=https%3A%2F%2Fwww.rug.nl%2Fresearch%2Fportal%2Fen%2Fpublications%2Fsocial-impact-assessment%28bf7d07e0-da85-47c2-a64d-e70b147d3938%29.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-ainow-9"><span class="mw-cite-backlink"><a href="#cite_ref-ainow_9-0">↑</a></span> <span class="reference-text">Reisman, D., Schultz, J., Crawford, K., &amp; Whittaker, M. (2018). <i>Algorithmic Impact Assessments: a Practical Framework for Public Agency Accountability</i>. Retrieved from https://ainowinstitute.org/aiareport2018.pdf</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><a href="#cite_ref-10">↑</a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://ethicstoolkit.ai/">"Ethics &amp; Algorithms Toolkit (beta)"</a>. <i>ethicstoolkit.ai</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.btitle=Ethics+%26+Algorithms+Toolkit+%28beta%29&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fethicstoolkit.ai%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-amershi-11"><span class="mw-cite-backlink">↑ <a href="#cite_ref-amershi_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-amershi_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Amershi, Saleema; Cakmak, Maya; Knox, William Bradley; Kulesza, Todd (2014-12-22). <a rel="nofollow" class="external text" href="https://aaai.org/ojs/index.php/aimagazine/article/view/2513">"Power to the People: The Role of Humans in Interactive Machine Learning"</a>. <i>AI Magazine</i> <b>35</b> (4): 105–120. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2371-9621">2371-9621</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1609%2Faimag.v35i4.2513">10.1609/aimag.v35i4.2513</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Power+to+the+People%3A+The+Role+of+Humans+in+Interactive+Machine+Learning&amp;rft.au=Cakmak%2C+Maya&amp;rft.au=Knox%2C+William+Bradley&amp;rft.au=Kulesza%2C+Todd&amp;rft.aufirst=Saleema&amp;rft.aulast=Amershi&amp;rft.date=2014-12-22&amp;rft.genre=article&amp;rft.issn=2371-9621&amp;rft.issue=4&amp;rft.jtitle=AI+Magazine&amp;rft.pages=105-120&amp;rft.volume=35&amp;rft_id=https%3A%2F%2Faaai.org%2Fojs%2Findex.php%2Faimagazine%2Farticle%2Fview%2F2513&amp;rft_id=info%3Adoi%2F10.1609%2Faimag.v35i4.2513&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><a href="#cite_ref-12">↑</a></span> <span class="reference-text"><cite class="citation journal">Ekstrand, Michael D.; Harper, F. Maxwell; Willemsen, Martijn C.; Konstan, Joseph A. (2014). <a rel="nofollow" class="external text" href="http://doi.acm.org/10.1145/2645710.2645737">"User Perception of Differences in Recommender Algorithms"</a>. <i>Proceedings of the 8th ACM Conference on Recommender Systems</i>. RecSys '14 (New York, NY, USA: ACM): 161–168. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450326681" title="Special:BookSources/9781450326681">9781450326681</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F2645710.2645737">10.1145/2645710.2645737</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=User+Perception+of+Differences+in+Recommender+Algorithms&amp;rft.au=Harper%2C+F.+Maxwell&amp;rft.au=Konstan%2C+Joseph+A.&amp;rft.au=Willemsen%2C+Martijn+C.&amp;rft.aufirst=Michael+D.&amp;rft.aulast=Ekstrand&amp;rft.date=2014&amp;rft.genre=article&amp;rft.isbn=9781450326681&amp;rft.jtitle=Proceedings+of+the+8th+ACM+Conference+on+Recommender+Systems&amp;rft.pages=161-168&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.pub=ACM&amp;rft.series=RecSys+%2714&amp;rft_id=http%3A%2F%2Fdoi.acm.org%2F10.1145%2F2645710.2645737&amp;rft_id=info%3Adoi%2F10.1145%2F2645710.2645737&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><a href="#cite_ref-13">↑</a></span> <span class="reference-text"><cite class="citation journal">Ricci, Francesco; Rokach, Lior; Shapira, Bracha;  et al., eds. (2011). <a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007/978-0-387-85820-3">"Recommender Systems Handbook"</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2F978-0-387-85820-3">10.1007/978-0-387-85820-3</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Recommender+Systems+Handbook&amp;rft.date=2011&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fdx.doi.org%2F10.1007%2F978-0-387-85820-3&amp;rft_id=info%3Adoi%2F10.1007%2F978-0-387-85820-3&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="display:none;font-size:100%" class="error citation-comment"><code style="color:inherit; border:inherit; padding:inherit;">&#124;display-editors=</code> suggested (<a href="/wiki/Help:CS1_errors#displayeditors" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><a href="#cite_ref-14">↑</a></span> <span class="reference-text"><cite class="citation journal">McNee, Sean M.; Riedl, John; Konstan, Joseph A. (2006-04-21). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1125451.1125660">"Making recommendations better: an analytic model for human-recommender interaction"</a>. ACM. pp.&#160;1103–1108. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1595932984" title="Special:BookSources/1595932984">1595932984</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F1125451.1125660">10.1145/1125451.1125660</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Making+recommendations+better%3A+an+analytic+model+for+human-recommender+interaction&amp;rft.au=Konstan%2C+Joseph+A.&amp;rft.au=Riedl%2C+John&amp;rft.aufirst=Sean+M.&amp;rft.aulast=McNee&amp;rft.date=2006-04-21&amp;rft.genre=article&amp;rft.isbn=1595932984&amp;rft.pages=1103-1108&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1125451.1125660&amp;rft_id=info%3Adoi%2F10.1145%2F1125451.1125660&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-chi-15"><span class="mw-cite-backlink">↑ <a href="#cite_ref-chi_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-chi_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Chi, Ed H. (2009).  Jacko, Julie A., ed. <a rel="nofollow" class="external text" href="https://link.springer.com/chapter/10.1007%2F978-3-642-02574-7_67">"A Position Paper on ’Living Laboratories’: Rethinking Ecological Designs and Experimentation in Human-Computer Interaction"</a>. <i>Human-Computer Interaction. New Trends</i>. Lecture Notes in Computer Science (Springer Berlin Heidelberg): 597–605. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9783642025747" title="Special:BookSources/9783642025747">9783642025747</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2F978-3-642-02574-7_67">10.1007/978-3-642-02574-7_67</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=A+Position+Paper+on+%E2%80%99Living+Laboratories%E2%80%99%3A+Rethinking+Ecological+Designs+and+Experimentation+in+Human-Computer+Interaction&amp;rft.aufirst=Ed+H.&amp;rft.aulast=Chi&amp;rft.date=2009&amp;rft.genre=article&amp;rft.isbn=9783642025747&amp;rft.jtitle=Human-Computer+Interaction.+New+Trends&amp;rft.pages=597-605&amp;rft.pub=Springer+Berlin+Heidelberg&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft_id=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%252F978-3-642-02574-7_67&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-02574-7_67&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-ekstrand-16"><span class="mw-cite-backlink">↑ <a href="#cite_ref-ekstrand_16-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ekstrand_16-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-ekstrand_16-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Ekstrand, Michael D.; Willemsen, Martijn C. (2016). <a rel="nofollow" class="external text" href="http://doi.acm.org/10.1145/2959100.2959179">"Behaviorism is Not Enough: Better Recommendations Through Listening to Users"</a>. <i>Proceedings of the 10th ACM Conference on Recommender Systems</i>. RecSys '16 (New York, NY, USA: ACM): 221–224. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" class="extiw" title="w:International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450340359" title="Special:BookSources/9781450340359">9781450340359</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1145%2F2959100.2959179">10.1145/2959100.2959179</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Behaviorism+is+Not+Enough%3A+Better+Recommendations+Through+Listening+to+Users&amp;rft.au=Willemsen%2C+Martijn+C.&amp;rft.aufirst=Michael+D.&amp;rft.aulast=Ekstrand&amp;rft.date=2016&amp;rft.genre=article&amp;rft.isbn=9781450340359&amp;rft.jtitle=Proceedings+of+the+10th+ACM+Conference+on+Recommender+Systems&amp;rft.pages=221-224&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.pub=ACM&amp;rft.series=RecSys+%2716&amp;rft_id=http%3A%2F%2Fdoi.acm.org%2F10.1145%2F2959100.2959179&amp;rft_id=info%3Adoi%2F10.1145%2F2959100.2959179&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-Burrell_2016-17"><span class="mw-cite-backlink"><a href="#cite_ref-Burrell_2016_17-0">↑</a></span> <span class="reference-text"><cite class="citation journal">Burrell, Jenna (2016-01-05). <a rel="nofollow" class="external text" href="https://doi.org/10.1177/2053951715622512">"How the machine ‘thinks’: Understanding opacity in machine learning algorithms"</a>. <i>Big Data &amp; Society</i> <b>3</b> (1): 205395171562251. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2053-9517">2053-9517</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1177%2F2053951715622512">10.1177/2053951715622512</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=How+the+machine+%E2%80%98thinks%E2%80%99%3A+Understanding+opacity+in+machine+learning+algorithms&amp;rft.aufirst=Jenna&amp;rft.aulast=Burrell&amp;rft.date=2016-01-05&amp;rft.genre=article&amp;rft.issn=2053-9517&amp;rft.issue=1&amp;rft.jtitle=Big+Data+%26+Society&amp;rft.pages=205395171562251&amp;rft.volume=3&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1177%2F2053951715622512&amp;rft_id=info%3Adoi%2F10.1177%2F2053951715622512&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><a href="#cite_ref-18">↑</a></span> <span class="reference-text"><cite class="citation journal">Herman, Bernease (2017-11-20). <a rel="nofollow" class="external text" href="http://arxiv.org/abs/1711.07414">"The Promise and Peril of Human Evaluation for Model Interpretability"</a>. <i>arXiv:1711.07414 [cs, stat]</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=The+Promise+and+Peril+of+Human+Evaluation+for+Model+Interpretability&amp;rft.aufirst=Bernease&amp;rft.aulast=Herman&amp;rft.date=2017-11-20&amp;rft.genre=article&amp;rft.jtitle=arXiv%3A1711.07414+%5Bcs%2C+stat%5D&amp;rft_id=http%3A%2F%2Farxiv.org%2Fabs%2F1711.07414&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-Baumer_2017-19"><span class="mw-cite-backlink"><a href="#cite_ref-Baumer_2017_19-0">↑</a></span> <span class="reference-text"><cite class="citation journal">Baumer, Eric PS (2017-07-25). <a rel="nofollow" class="external text" href="https://doi.org/10.1177/2053951717718854">"Toward human-centered algorithm design"</a>. <i>Big Data &amp; Society</i> <b>4</b> (2): 205395171771885. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/2053-9517">2053-9517</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1177%2F2053951717718854">10.1177/2053951717718854</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Toward+human-centered+algorithm+design&amp;rft.aufirst=Eric+PS&amp;rft.aulast=Baumer&amp;rft.date=2017-07-25&amp;rft.genre=article&amp;rft.issn=2053-9517&amp;rft.issue=2&amp;rft.jtitle=Big+Data+%26+Society&amp;rft.pages=205395171771885&amp;rft.volume=4&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1177%2F2053951717718854&amp;rft_id=info%3Adoi%2F10.1177%2F2053951717718854&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-bender-20"><span class="mw-cite-backlink">↑ <a href="#cite_ref-bender_20-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-bender_20-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Bender, Emily; Friedman, Batya (2018-09-24). <a rel="nofollow" class="external text" href="https://openreview.net/forum?id=By4oPeX9f">"Data Statements for NLP: Toward Mitigating System Bias and Enabling Better Science"</a>. <i>Transactions of the ACL</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Data+Statements+for+NLP%3A+Toward+Mitigating+System+Bias+and+Enabling+Better+Science&amp;rft.au=Friedman%2C+Batya&amp;rft.aufirst=Emily&amp;rft.aulast=Bender&amp;rft.date=2018-09-24&amp;rft.genre=article&amp;rft.jtitle=Transactions+of+the+ACL&amp;rft_id=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DBy4oPeX9f&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><a href="#cite_ref-21">↑</a></span> <span class="reference-text"><cite class="citation journal">Hind, Michael; Mehta, Sameep; Mojsilovic, Aleksandra; Nair, Ravi; Ramamurthy, Karthikeyan Natesan; Olteanu, Alexandra; Varshney, Kush R. (2018-08-22). <a rel="nofollow" class="external text" href="http://arxiv.org/abs/1808.07261">"Increasing Trust in AI Services through Supplier's Declarations of Conformity"</a>. <i>arXiv:1808.07261 [cs]</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Increasing+Trust+in+AI+Services+through+Supplier%27s+Declarations+of+Conformity&amp;rft.au=Mehta%2C+Sameep&amp;rft.au=Mojsilovic%2C+Aleksandra&amp;rft.au=Nair%2C+Ravi&amp;rft.au=Olteanu%2C+Alexandra&amp;rft.au=Ramamurthy%2C+Karthikeyan+Natesan&amp;rft.au=Varshney%2C+Kush+R.&amp;rft.aufirst=Michael&amp;rft.aulast=Hind&amp;rft.date=2018-08-22&amp;rft.genre=article&amp;rft.jtitle=arXiv%3A1808.07261+%5Bcs%5D&amp;rft_id=http%3A%2F%2Farxiv.org%2Fabs%2F1808.07261&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><a href="#cite_ref-22">↑</a></span> <span class="reference-text"><cite class="citation journal">Holland, Sarah; Hosny, Ahmed; Newman, Sarah; Joseph, Joshua; Chmielinski, Kasia (2018-05-09). <a rel="nofollow" class="external text" href="http://arxiv.org/abs/1805.03677">"The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards"</a>. <i>arXiv:1805.03677 [cs]</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=The+Dataset+Nutrition+Label%3A+A+Framework+To+Drive+Higher+Data+Quality+Standards&amp;rft.au=Chmielinski%2C+Kasia&amp;rft.au=Hosny%2C+Ahmed&amp;rft.au=Joseph%2C+Joshua&amp;rft.au=Newman%2C+Sarah&amp;rft.aufirst=Sarah&amp;rft.aulast=Holland&amp;rft.date=2018-05-09&amp;rft.genre=article&amp;rft.jtitle=arXiv%3A1805.03677+%5Bcs%5D&amp;rft_id=http%3A%2F%2Farxiv.org%2Fabs%2F1805.03677&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><a href="#cite_ref-23">↑</a></span> <span class="reference-text"><cite class="citation journal">Gebru, Timnit; Morgenstern, Jamie; Vecchione, Briana; Vaughan, Jennifer Wortman; Wallach, Hanna; Daumeé III, Hal; Crawford, Kate (2018-03-23). <a rel="nofollow" class="external text" href="http://arxiv.org/abs/1803.09010">"Datasheets for Datasets"</a>. <i>arXiv:1803.09010 [cs]</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Datasheets+for+Datasets&amp;rft.au=Crawford%2C+Kate&amp;rft.au=Daume%C3%A9+III%2C+Hal&amp;rft.au=Morgenstern%2C+Jamie&amp;rft.au=Vaughan%2C+Jennifer+Wortman&amp;rft.au=Vecchione%2C+Briana&amp;rft.au=Wallach%2C+Hanna&amp;rft.aufirst=Timnit&amp;rft.aulast=Gebru&amp;rft.date=2018-03-23&amp;rft.genre=article&amp;rft.jtitle=arXiv%3A1803.09010+%5Bcs%5D&amp;rft_id=http%3A%2F%2Farxiv.org%2Fabs%2F1803.09010&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><a href="#cite_ref-24">↑</a></span> <span class="reference-text"><cite class="citation journal">Geiger, R. Stuart; Varoquaux, Nelle; Mazel-Cabasse, Charlotte; Holdgraf, Chris (2018-12-01). <a rel="nofollow" class="external text" href="https://doi.org/10.1007/s10606-018-9333-1">"The Types, Roles, and Practices of Documentation in Data Analytics Open Source Software Libraries"</a>. <i>Computer Supported Cooperative Work (CSCW)</i> <b>27</b> (3): 767–802. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1573-7551">1573-7551</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2Fs10606-018-9333-1">10.1007/s10606-018-9333-1</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=The+Types%2C+Roles%2C+and+Practices+of+Documentation+in+Data+Analytics+Open+Source+Software+Libraries&amp;rft.au=Holdgraf%2C+Chris&amp;rft.au=Mazel-Cabasse%2C+Charlotte&amp;rft.au=Varoquaux%2C+Nelle&amp;rft.aufirst=R.+Stuart&amp;rft.aulast=Geiger&amp;rft.date=2018-12-01&amp;rft.genre=article&amp;rft.issn=1573-7551&amp;rft.issue=3&amp;rft.jtitle=Computer+Supported+Cooperative+Work+%28CSCW%29&amp;rft.pages=767-802&amp;rft.volume=27&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1007%2Fs10606-018-9333-1&amp;rft_id=info%3Adoi%2F10.1007%2Fs10606-018-9333-1&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><a href="#cite_ref-25">↑</a></span> <span class="reference-text"><cite class="citation journal">Tintarev, Nava; Masthoff, Judith (2012-10-01). <a rel="nofollow" class="external text" href="https://doi.org/10.1007/s11257-011-9117-5">"Evaluating the effectiveness of explanations for recommender systems"</a>. <i>User Modeling and User-Adapted Interaction</i> <b>22</b> (4): 399–439. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1573-1391">1573-1391</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2Fs11257-011-9117-5">10.1007/s11257-011-9117-5</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Evaluating+the+effectiveness+of+explanations+for+recommender+systems&amp;rft.au=Masthoff%2C+Judith&amp;rft.aufirst=Nava&amp;rft.aulast=Tintarev&amp;rft.date=2012-10-01&amp;rft.genre=article&amp;rft.issn=1573-1391&amp;rft.issue=4&amp;rft.jtitle=User+Modeling+and+User-Adapted+Interaction&amp;rft.pages=399-439&amp;rft.volume=22&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1007%2Fs11257-011-9117-5&amp;rft_id=info%3Adoi%2F10.1007%2Fs11257-011-9117-5&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><a href="#cite_ref-26">↑</a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://misq.org/explaining-data-driven-document-classifications.html">"MIS Quarterly"</a>. <i>misq.org</i>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.25300%2Fmisq%2F2014%2F38.1.04">10.25300/misq/2014/38.1.04</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2019-01-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.btitle=MIS+Quarterly&amp;rft.genre=book&amp;rft_id=https%3A%2F%2Fmisq.org%2Fexplaining-data-driven-document-classifications.html&amp;rft_id=info%3Adoi%2F10.25300%2Fmisq%2F2014%2F38.1.04&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><a href="#cite_ref-27">↑</a></span> <span class="reference-text"><cite class="citation journal">Cremonesi, Paolo; Elahi, Mehdi; Garzotto, Franca (2017-02-01). <a rel="nofollow" class="external text" href="https://doi.org/10.1007/s11042-016-3946-5">"User interface patterns in recommendation-empowered content intensive multimedia applications"</a>. <i>Multimedia Tools and Applications</i> <b>76</b> (4): 5275–5309. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" class="extiw" title="w:International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1573-7721">1573-7721</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2Fs11042-016-3946-5">10.1007/s11042-016-3946-5</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=User+interface+patterns+in+recommendation-empowered+content+intensive+multimedia+applications&amp;rft.au=Elahi%2C+Mehdi&amp;rft.au=Garzotto%2C+Franca&amp;rft.aufirst=Paolo&amp;rft.aulast=Cremonesi&amp;rft.date=2017-02-01&amp;rft.genre=article&amp;rft.issn=1573-7721&amp;rft.issue=4&amp;rft.jtitle=Multimedia+Tools+and+Applications&amp;rft.pages=5275-5309&amp;rft.volume=76&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1007%2Fs11042-016-3946-5&amp;rft_id=info%3Adoi%2F10.1007%2Fs11042-016-3946-5&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-sandvig-28"><span class="mw-cite-backlink">↑ <a href="#cite_ref-sandvig_28-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-sandvig_28-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Sandvig, C., Hamilton, K., Karahalios, K., &amp; Langbort, C. (2014). <i>Auditing Algorithms : Research Methods for Detecting Discrimination on Internet Platforms</i>. <i>Data and Discrimination: Converting Critical Concerns into Productive Inquiry, a preconference at the 64th Annual Meeting of the International Communication Association</i>. Seattle, Washington, USA. Retrieved from <a rel="nofollow" class="external free" href="http://www-personal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf">http://www-personal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf</a></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><a href="#cite_ref-29">↑</a></span> <span class="reference-text"><cite class="citation journal">Diakopoulos, Nicholas (2014). <a rel="nofollow" class="external text" href="https://doi.org/10.7916/D8ZK5TW2">"Algorithmic Accountability Reporting: On the Investigation of Black Boxes"</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" class="extiw" title="w:Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.7916%2FD8ZK5TW2">10.7916/D8ZK5TW2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Algorithmic+Accountability+Reporting%3A+On+the+Investigation+of+Black+Boxes&amp;rft.aufirst=Nicholas&amp;rft.aulast=Diakopoulos&amp;rft.date=2014&amp;rft.genre=article&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.7916%2FD8ZK5TW2&amp;rft_id=info%3Adoi%2F10.7916%2FD8ZK5TW2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><a href="#cite_ref-30">↑</a></span> <span class="reference-text"><cite class="citation journal">Elsayed, Tamer; Kutlu, Mucahid; Lease, Matthew; McDonnell, Tyler (2016-09-21). <a rel="nofollow" class="external text" href="https://www.aaai.org/ocs/index.php/HCOMP/HCOMP16/paper/view/14043">"Why Is That Relevant? Collecting Annotator Rationales for Relevance Judgments"</a>. <i>Fourth AAAI Conference on Human Computation and Crowdsourcing</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fmeta.wikimedia.org%3AResearch%3AEthical+and+human-centered+AI&amp;rft.atitle=Why+Is+That+Relevant%3F+Collecting+Annotator+Rationales+for+Relevance+Judgments&amp;rft.au=Kutlu%2C+Mucahid&amp;rft.au=Lease%2C+Matthew&amp;rft.au=McDonnell%2C+Tyler&amp;rft.aufirst=Tamer&amp;rft.aulast=Elsayed&amp;rft.date=2016-09-21&amp;rft.genre=article&amp;rft.jtitle=Fourth+AAAI+Conference+on+Human+Computation+and+Crowdsourcing&amp;rft_id=https%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FHCOMP%2FHCOMP16%2Fpaper%2Fview%2F14043&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span>
</li>
</ol></div>
<!-- 
NewPP limit report
Parsed by mw1384
Cached time: 20220505153102
Cache expiry: 1105
Reduced expiry: true
Complications: []
CPU time usage: 0.314 seconds
Real time usage: 0.366 seconds
Preprocessor visited node count: 2153/1000000
Post‐expand include size: 65592/2097152 bytes
Template argument size: 6862/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 0/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 39660/5000000 bytes
Lua time usage: 0.099/10.000 seconds
Lua memory usage: 3375662/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  258.292      1 -total
 23.84%   61.577      9 Template:Cite_web
 23.22%   59.972     19 Template:Cite_journal
 16.02%   41.369      1 Template:Research_project
 12.65%   32.683      1 Template:Research_project/Infobox
 12.08%   31.192      1 Template:Nutshell
 10.84%   27.996      1 Template:Basic_project/Infobox
  8.16%   21.083      1 Template:List_subpages
  7.92%   20.459      1 Template:LangSwitch
  4.38%   11.324      8 Template:Blockquote
-->

<!-- Saved in parser cache with key metawiki:pcache:idhash:10758758-0!canonical and timestamp 20220505153101 and revision id 19228623. Serialized with JSON.
 -->
</div><noscript><img src="//meta.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://meta.wikimedia.org/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;oldid=19228623">https://meta.wikimedia.org/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;oldid=19228623</a>"</div></div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Special:Categories" title="Special:Categories">Categories</a>: <ul><li><a href="/wiki/Category:2018_projects" title="Category:2018 projects">2018 projects</a></li><li><a href="/wiki/Category:Open_access_projects" title="Category:Open access projects">Open access projects</a></li><li><a href="/wiki/Category:Completed_research_projects" title="Category:Completed research projects">Completed research projects</a></li><li><a href="/wiki/Category:Wikimedia_Research_project" title="Category:Wikimedia Research project">Wikimedia Research project</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_Unrecognized_language" title="Category:CS1 maint: Unrecognized language">CS1 maint: Unrecognized language</a></li><li><a href="/wiki/Category:Pages_using_citations_with_old-style_implicit_et_al._in_editors" title="Category:Pages using citations with old-style implicit et al. in editors">Pages using citations with old-style implicit et al. in editors</a></li></ul></div></div>
	</div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<nav id="p-personal" class="mw-portlet mw-portlet-personal vector-user-menu-legacy vector-menu" aria-labelledby="p-personal-label" role="navigation" 
	 >
	<label id="p-personal-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Personal tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="pt-uls" class="mw-list-item active"><a class="uls-trigger" href="#"><span>English</span></a></li><li id="pt-anonuserpage" class="mw-list-item"><span title="The user page for the IP address you are editing as">Not logged in</span></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li><li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-createaccount" class="mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Research%3AEthical+and+human-centered+AI" title="You are encouraged to create an account and log in; however, it is not mandatory"><span>Create account</span></a></li><li id="pt-login" class="mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Research%3AEthical+and+human-centered+AI" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o"><span>Log in</span></a></li></ul>
		
	</div>
</nav>

		<div id="left-navigation">
			
<nav id="p-namespaces" class="mw-portlet mw-portlet-namespaces vector-menu vector-menu-tabs" aria-labelledby="p-namespaces-label" role="navigation" 
	 >
	<label id="p-namespaces-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Namespaces</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-nstab-research" class="selected mw-list-item"><a href="/wiki/Research:Ethical_and_human-centered_AI"><span>Research</span></a></li><li id="ca-talk" class="new mw-list-item"><a href="/w/index.php?title=Research_talk:Ethical_and_human-centered_AI&amp;action=edit&amp;redlink=1" rel="discussion" title="Discussion about the content page (page does not exist) [t]" accesskey="t"><span>Discussion</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-variants" class="mw-portlet mw-portlet-variants emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-variants-label" role="navigation" 
	 >
	<input type="checkbox"
		id="p-variants-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-variants"
		class="vector-menu-checkbox" aria-labelledby="p-variants-label" />
	<label id="p-variants-label" aria-label="Change language variant" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">English</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

		</div>
		<div id="right-navigation">
			
<nav id="p-views" class="mw-portlet mw-portlet-views vector-menu vector-menu-tabs" aria-labelledby="p-views-label" role="navigation" 
	 >
	<label id="p-views-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Views</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="ca-view" class="selected mw-list-item"><a href="/wiki/Research:Ethical_and_human-centered_AI"><span>Read</span></a></li><li id="ca-edit" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li></ul>
		
	</div>
</nav>

			
<nav id="p-cactions" class="mw-portlet mw-portlet-cactions emptyPortlet vector-menu-dropdown-noicon vector-menu vector-menu-dropdown" aria-labelledby="p-cactions-label" role="navigation"  title="More options"
	 >
	<input type="checkbox"
		id="p-cactions-checkbox"
		role="button"
		aria-haspopup="true"
		data-event-name="ui.dropdown-p-cactions"
		class="vector-menu-checkbox" aria-labelledby="p-cactions-label" />
	<label id="p-cactions-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">More</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		
	</div>
</nav>

			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<div>
			<h3 >
				<label for="searchInput">Search</label>
			</h3>
		<form action="/w/index.php" id="searchform"
			class="vector-search-box-form">
			<div id="simpleSearch"
				class="vector-search-box-inner"
				 data-search-loc="header-navigation">
				<input class="vector-search-box-input"
					 type="search" name="search" placeholder="Search Meta" aria-label="Search Meta" autocapitalize="sentences" title="Search Meta [f]" accesskey="f" id="searchInput"
				/>
				<input type="hidden" name="title" value="Special:Search"/>
				<input id="mw-searchButton"
					 class="searchButton mw-fallbackSearchButton" type="submit" name="fulltext" title="Search the pages for this text" value="Search" />
				<input id="searchButton"
					 class="searchButton" type="submit" name="go" title="Go to a page with this exact name if it exists" value="Go" />
			</div>
		</form>
	</div>
</div>

		</div>
	</div>
	

<div id="mw-panel">
	<div id="p-logo" role="banner">
		<a class="mw-wiki-logo" href="/wiki/Special:MyLanguage/Main_Page"
			title="Visit the main page"></a>
	</div>
	
<nav id="p-navigation" class="mw-portlet mw-portlet-navigation vector-menu vector-menu-portal portal" aria-labelledby="p-navigation-label" role="navigation" 
	 >
	<label id="p-navigation-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Navigation</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Main_Page" icon="home" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Wikimedia_News" title="Find background information on current events"><span>Wikimedia News</span></a></li><li id="n-translations" class="mw-list-item"><a href="/wiki/Special:LanguageStats" title="Help with translations"><span>Translations</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" icon="recentChanges" title="A list of recent changes in the wiki [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" icon="die" title="Load a random page [x]" accesskey="x"><span>Random page</span></a></li><li id="n-help" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Help:Help" icon="help" title="The place to find out"><span>Help</span></a></li><li id="n-Babel-text" class="mw-list-item"><a href="/wiki/Meta:Babel"><span>Babel</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-community" class="mw-portlet mw-portlet-community vector-menu vector-menu-portal portal" aria-labelledby="p-community-label" role="navigation" 
	 >
	<label id="p-community-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Community</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-WikimediaResourceCenter-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Wikimedia_Resource_Center"><span>Wikimedia Resource Center</span></a></li><li id="n-WikimediaForum-text" class="mw-list-item"><a href="/wiki/Wikimedia_Forum"><span>Wikimedia Forum</span></a></li><li id="n-mailinglists-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Mailing_lists/Overview"><span>Mailing lists</span></a></li><li id="n-Requests-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Meta:Requests_and_proposals"><span>Requests</span></a></li><li id="n-Babylon-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Meta:Babylon"><span>Babylon</span></a></li><li id="n-Reports-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Wikimedia_Affiliates_Data_Portal"><span>Reports</span></a></li><li id="n-Research-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Research:Index"><span>Research</span></a></li><li id="n-planet" class="mw-list-item"><a href="//en.planet.wikimedia.org/" title="Weblog aggregator of Wikimedia-related posts"><span>Planet Wikimedia</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-beyond_the_web" class="mw-portlet mw-portlet-beyond_the_web vector-menu vector-menu-portal portal" aria-labelledby="p-beyond_the_web-label" role="navigation" 
	 >
	<label id="p-beyond_the_web-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Beyond the Web</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="n-Meetup-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Meetup"><span>Meet Wikimedians</span></a></li><li id="n-Events-text" class="mw-list-item"><a href="/wiki/Events"><span>Events</span></a></li><li id="n-Affiliates-text" class="mw-list-item"><a href="/wiki/Special:MyLanguage/Wikimedia_movement_affiliates"><span>Movement affiliates</span></a></li><li id="n-sitesupport" class="mw-list-item"><a href="//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_meta.wikimedia.org&amp;uselang=en" title="Support us"><span>Donate</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-tb" class="mw-portlet mw-portlet-tb vector-menu vector-menu-portal portal" aria-labelledby="p-tb-label" role="navigation" 
	 >
	<label id="p-tb-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Tools</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/Research:Ethical_and_human-centered_AI" title="A list of all wiki pages that link here [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/Research:Ethical_and_human-centered_AI" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;oldid=19228623" title="Permanent link to this revision of the page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;action=info" title="More information about this page"><span>Page information</span></a></li></ul>
		
	</div>
</nav>

<nav id="p-coll-print_export" class="mw-portlet mw-portlet-coll-print_export vector-menu vector-menu-portal portal" aria-labelledby="p-coll-print_export-label" role="navigation" 
	 >
	<label id="p-coll-print_export-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">Print/export</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"><li id="coll-create_a_book" class="mw-list-item"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Research%3AEthical+and+human-centered+AI"><span>Create a book</span></a></li><li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=Research%3AEthical_and_human-centered_AI&amp;action=show-download-screen"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li></ul>
		
	</div>
</nav>

	
<nav id="p-lang" class="mw-portlet mw-portlet-lang vector-menu vector-menu-portal portal" aria-labelledby="p-lang-label" role="navigation" 
	 >
	<label id="p-lang-label" aria-label="" class="vector-menu-heading "  >
		<span class="vector-menu-heading-label">In other languages</span>
	</label>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list"></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-add wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:NewItem?site=metawiki&amp;page=Research%3AEthical+and+human-centered+AI" title="Add interlanguage links" class="wbc-editpage">Add links</a></span></div>
	</div>
</nav>

</div>

</div>

<footer id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 23 July 2019, at 22:55.</li>
	<li id="footer-info-copyright">Text is available under the <a href="https://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike License</a>;
additional terms may apply.
See <a href="https://foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> for details.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Meta:About" title="Meta:About">About Meta</a></li>
	<li id="footer-places-disclaimer"><a href="https://foundation.wikimedia.org/wiki/Wikimedia:General_disclaimer" class="extiw" title="wmf:Wikimedia:General disclaimer">Disclaimers</a></li>
	<li id="footer-places-mobileview"><a href="//meta.m.wikimedia.org/w/index.php?title=Research:Ethical_and_human-centered_AI&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/meta.wikimedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a href="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.png" srcset="/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation" loading="lazy" /></a></li>
	<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="/static/images/footer/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x" width="88" height="31" loading="lazy"/></a></li>
</ul>

</footer>

<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.314","walltime":"0.366","ppvisitednodes":{"value":2153,"limit":1000000},"postexpandincludesize":{"value":65592,"limit":2097152},"templateargumentsize":{"value":6862,"limit":2097152},"expansiondepth":{"value":12,"limit":100},"expensivefunctioncount":{"value":0,"limit":500},"unstrip-depth":{"value":0,"limit":20},"unstrip-size":{"value":39660,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  258.292      1 -total"," 23.84%   61.577      9 Template:Cite_web"," 23.22%   59.972     19 Template:Cite_journal"," 16.02%   41.369      1 Template:Research_project"," 12.65%   32.683      1 Template:Research_project/Infobox"," 12.08%   31.192      1 Template:Nutshell"," 10.84%   27.996      1 Template:Basic_project/Infobox","  8.16%   21.083      1 Template:List_subpages","  7.92%   20.459      1 Template:LangSwitch","  4.38%   11.324      8 Template:Blockquote"]},"scribunto":{"limitreport-timeusage":{"value":"0.099","limit":"10.000"},"limitreport-memusage":{"value":3375662,"limit":52428800}},"cachereport":{"origin":"mw1384","timestamp":"20220505153102","ttl":1105,"transientcontent":true}}});mw.config.set({"wgBackendResponseTime":538,"wgHostname":"mw1384"});});</script>
</body>
</html>